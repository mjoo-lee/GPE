set -e
set -x

eval "$(conda shell.bash hook)"
++ conda shell.bash hook
+ eval 'export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
export CONDA_EXE='/home/s2/mjoolee/anaconda/bin/conda'
++ export CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
++ CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
export _CE_M=''
++ export _CE_M=
++ _CE_M=
export _CE_CONDA=''
++ export _CE_CONDA=
++ _CE_CONDA=
export CONDA_PYTHON_EXE='/home/s2/mjoolee/anaconda/bin/python'
++ export CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
++ CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We're not allowing PS1 to be unbound. It must at least be set.
    # However, we're not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi
++ '[' -z x ']'

conda activate base
++ conda activate base
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate base
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate base
+++ /home/s2/mjoolee/anaconda/bin/conda shell.posix activate base
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_PREFIX='\''/home/s2/mjoolee/anaconda'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/home/s2/mjoolee/anaconda/envs/dassl'\''
export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_PREFIX='\''/home/s2/mjoolee/anaconda'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/home/s2/mjoolee/anaconda/envs/dassl'\''
export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\'''
PS1='(base) '
+++ PS1='(base) '
export PATH='/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'
+++ export PATH=/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
+++ PATH=/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
export CONDA_PREFIX='/home/s2/mjoolee/anaconda'
+++ export CONDA_PREFIX=/home/s2/mjoolee/anaconda
+++ CONDA_PREFIX=/home/s2/mjoolee/anaconda
export CONDA_SHLVL='3'
+++ export CONDA_SHLVL=3
+++ CONDA_SHLVL=3
export CONDA_DEFAULT_ENV='base'
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
export CONDA_PROMPT_MODIFIER='(base) '
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
export CONDA_PREFIX_2='/home/s2/mjoolee/anaconda/envs/dassl'
+++ export CONDA_PREFIX_2=/home/s2/mjoolee/anaconda/envs/dassl
+++ CONDA_PREFIX_2=/home/s2/mjoolee/anaconda/envs/dassl
export CONDA_EXE='/home/s2/mjoolee/anaconda/bin/conda'
+++ export CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
+++ CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
export _CE_M=''
+++ export _CE_M=
+++ _CE_M=
export _CE_CONDA=''
+++ export _CE_CONDA=
+++ _CE_CONDA=
export CONDA_PYTHON_EXE='/home/s2/mjoolee/anaconda/bin/python'
+++ export CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
+++ CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
conda activate dassl
+ conda activate dassl
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate dassl
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate dassl
++ /home/s2/mjoolee/anaconda/bin/conda shell.posix activate dassl
+ ask_conda='PS1='\''(dassl) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_PREFIX='\''/home/s2/mjoolee/anaconda/envs/dassl'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''dassl'\''
export CONDA_PROMPT_MODIFIER='\''(dassl) '\''
export CONDA_PREFIX_3='\''/home/s2/mjoolee/anaconda'\''
export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\'''
+ eval 'PS1='\''(dassl) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_PREFIX='\''/home/s2/mjoolee/anaconda/envs/dassl'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''dassl'\''
export CONDA_PROMPT_MODIFIER='\''(dassl) '\''
export CONDA_PREFIX_3='\''/home/s2/mjoolee/anaconda'\''
export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\'''
PS1='(dassl) '
++ PS1='(dassl) '
export PATH='/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'
++ export PATH=/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
++ PATH=/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
export CONDA_PREFIX='/home/s2/mjoolee/anaconda/envs/dassl'
++ export CONDA_PREFIX=/home/s2/mjoolee/anaconda/envs/dassl
++ CONDA_PREFIX=/home/s2/mjoolee/anaconda/envs/dassl
export CONDA_SHLVL='4'
++ export CONDA_SHLVL=4
++ CONDA_SHLVL=4
export CONDA_DEFAULT_ENV='dassl'
++ export CONDA_DEFAULT_ENV=dassl
++ CONDA_DEFAULT_ENV=dassl
export CONDA_PROMPT_MODIFIER='(dassl) '
++ export 'CONDA_PROMPT_MODIFIER=(dassl) '
++ CONDA_PROMPT_MODIFIER='(dassl) '
export CONDA_PREFIX_3='/home/s2/mjoolee/anaconda'
++ export CONDA_PREFIX_3=/home/s2/mjoolee/anaconda
++ CONDA_PREFIX_3=/home/s2/mjoolee/anaconda
export CONDA_EXE='/home/s2/mjoolee/anaconda/bin/conda'
++ export CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
++ CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
export _CE_M=''
++ export _CE_M=
++ _CE_M=
export _CE_CONDA=''
++ export _CE_CONDA=
++ _CE_CONDA=
export CONDA_PYTHON_EXE='/home/s2/mjoolee/anaconda/bin/python'
++ export CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
++ CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r

GPU=0
+ GPU=0
SHOT=16
+ SHOT=16
EPOCH=30
+ EPOCH=30
TRAINER=RPO_prime
+ TRAINER=RPO_prime

for seed in 1 2 3
 do
     #training
     sh scripts/rpo_prime/xd_train.sh eurosat ${seed} ${GPU} main_final1212 ${SHOT} ${TRAINER}
 done         
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_train.sh eurosat 1 0 main_final1212 16 RPO_prime
epoch [16/30] batch [2/40] time 0.281 (0.491) data 0.000 (0.183) loss 0.6460 (0.3983) lr 5.0000e-03 eta 0:04:53
epoch [16/30] batch [4/40] time 0.272 (0.383) data 0.000 (0.091) loss 0.4143 (0.5006) lr 5.0000e-03 eta 0:03:48
epoch [16/30] batch [6/40] time 0.275 (0.347) data 0.000 (0.061) loss 1.0479 (0.5576) lr 5.0000e-03 eta 0:03:26
epoch [16/30] batch [8/40] time 0.273 (0.329) data 0.000 (0.046) loss 0.2505 (0.5203) lr 5.0000e-03 eta 0:03:14
epoch [16/30] batch [10/40] time 0.287 (0.320) data 0.000 (0.037) loss 1.2432 (0.5738) lr 5.0000e-03 eta 0:03:08
epoch [16/30] batch [12/40] time 0.281 (0.313) data 0.000 (0.031) loss 0.6611 (0.6634) lr 5.0000e-03 eta 0:03:04
epoch [16/30] batch [14/40] time 0.278 (0.308) data 0.000 (0.026) loss 1.1641 (0.6674) lr 5.0000e-03 eta 0:03:00
epoch [16/30] batch [16/40] time 0.281 (0.305) data 0.000 (0.023) loss 0.8638 (0.7134) lr 5.0000e-03 eta 0:02:57
epoch [16/30] batch [18/40] time 0.276 (0.301) data 0.000 (0.021) loss 1.0918 (0.7294) lr 5.0000e-03 eta 0:02:55
epoch [16/30] batch [20/40] time 0.274 (0.299) data 0.000 (0.018) loss 0.9102 (0.7166) lr 5.0000e-03 eta 0:02:53
epoch [16/30] batch [22/40] time 0.273 (0.296) data 0.000 (0.017) loss 1.2383 (0.7612) lr 5.0000e-03 eta 0:02:51
epoch [16/30] batch [24/40] time 0.272 (0.294) data 0.000 (0.015) loss 0.5405 (0.8087) lr 5.0000e-03 eta 0:02:49
epoch [16/30] batch [26/40] time 0.272 (0.293) data 0.000 (0.014) loss 0.5493 (0.8603) lr 5.0000e-03 eta 0:02:48
epoch [16/30] batch [28/40] time 0.274 (0.291) data 0.000 (0.013) loss 2.4941 (0.9054) lr 5.0000e-03 eta 0:02:46
epoch [16/30] batch [30/40] time 0.271 (0.290) data 0.000 (0.012) loss 3.0488 (0.9819) lr 5.0000e-03 eta 0:02:45
epoch [16/30] batch [32/40] time 0.275 (0.289) data 0.000 (0.012) loss 1.2451 (0.9863) lr 5.0000e-03 eta 0:02:44
epoch [16/30] batch [34/40] time 0.276 (0.288) data 0.000 (0.011) loss 3.2324 (1.0306) lr 5.0000e-03 eta 0:02:43
epoch [16/30] batch [36/40] time 0.272 (0.287) data 0.000 (0.010) loss 2.1055 (1.0461) lr 5.0000e-03 eta 0:02:42
epoch [16/30] batch [38/40] time 0.272 (0.286) data 0.000 (0.010) loss 2.2598 (1.0626) lr 5.0000e-03 eta 0:02:40
epoch [16/30] batch [40/40] time 0.270 (0.286) data 0.000 (0.009) loss 1.4238 (1.0643) lr 4.4774e-03 eta 0:02:39
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_1.pkl
160 5400 8100
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      5,400
# test     8,100
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1 (will resume training)
Loading checkpoint from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1/prompt_learner/model.pth.tar-10"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 10
Initialize tensorboard (log_dir=output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [17/30] batch [2/40] time 0.285 (0.491) data 0.000 (0.176) loss 0.6504 (0.8062) lr 4.4774e-03 eta 0:04:34
epoch [17/30] batch [4/40] time 0.275 (0.385) data 0.000 (0.088) loss 0.7656 (0.8835) lr 4.4774e-03 eta 0:03:34
epoch [17/30] batch [6/40] time 0.278 (0.349) data 0.000 (0.059) loss 1.2051 (1.0628) lr 4.4774e-03 eta 0:03:13
epoch [17/30] batch [8/40] time 0.279 (0.331) data 0.000 (0.044) loss 0.6074 (0.9084) lr 4.4774e-03 eta 0:03:02
epoch [17/30] batch [10/40] time 0.285 (0.321) data 0.000 (0.035) loss 0.7529 (0.9179) lr 4.4774e-03 eta 0:02:56
epoch [17/30] batch [12/40] time 0.281 (0.315) data 0.000 (0.029) loss 0.3630 (1.0009) lr 4.4774e-03 eta 0:02:52
epoch [17/30] batch [14/40] time 0.279 (0.309) data 0.000 (0.025) loss 0.1152 (0.8762) lr 4.4774e-03 eta 0:02:48
epoch [17/30] batch [16/40] time 0.289 (0.307) data 0.000 (0.022) loss 0.2817 (0.8292) lr 4.4774e-03 eta 0:02:47
epoch [17/30] batch [18/40] time 0.288 (0.305) data 0.000 (0.020) loss 1.0049 (0.8514) lr 4.4774e-03 eta 0:02:45
epoch [17/30] batch [20/40] time 0.288 (0.303) data 0.000 (0.018) loss 1.6309 (0.9224) lr 4.4774e-03 eta 0:02:43
epoch [17/30] batch [22/40] time 0.285 (0.301) data 0.000 (0.016) loss 0.8262 (0.9331) lr 4.4774e-03 eta 0:02:42
epoch [17/30] batch [24/40] time 0.281 (0.300) data 0.000 (0.015) loss 1.1221 (0.9722) lr 4.4774e-03 eta 0:02:40
epoch [17/30] batch [26/40] time 0.284 (0.299) data 0.000 (0.014) loss 0.4424 (0.9267) lr 4.4774e-03 eta 0:02:39
epoch [17/30] batch [28/40] time 0.286 (0.298) data 0.000 (0.013) loss 1.0000 (0.9215) lr 4.4774e-03 eta 0:02:38
epoch [17/30] batch [30/40] time 0.287 (0.297) data 0.000 (0.012) loss 0.1528 (0.8791) lr 4.4774e-03 eta 0:02:37
epoch [17/30] batch [32/40] time 0.285 (0.296) data 0.000 (0.011) loss 0.5229 (0.8775) lr 4.4774e-03 eta 0:02:36
epoch [17/30] batch [34/40] time 0.274 (0.295) data 0.000 (0.011) loss 0.6562 (0.8587) lr 4.4774e-03 eta 0:02:35
epoch [17/30] batch [36/40] time 0.374 (0.297) data 0.000 (0.010) loss 0.8403 (0.8444) lr 4.4774e-03 eta 0:02:35
epoch [17/30] batch [38/40] time 0.283 (0.296) data 0.000 (0.009) loss 0.1072 (0.8423) lr 4.4774e-03 eta 0:02:34
epoch [17/30] batch [40/40] time 0.279 (0.295) data 0.000 (0.009) loss 0.5732 (0.8441) lr 3.9604e-03 eta 0:02:33
epoch [11/30] batch [2/40] time 0.315 (1.439) data 0.000 (0.401) loss 2.9102 (2.0811) lr 7.5000e-03 eta 0:19:08
epoch [11/30] batch [4/40] time 0.308 (0.875) data 0.000 (0.201) loss 0.8022 (1.4827) lr 7.5000e-03 eta 0:11:36
epoch [11/30] batch [6/40] time 0.299 (0.683) data 0.000 (0.134) loss 0.5044 (1.4374) lr 7.5000e-03 eta 0:09:02
epoch [11/30] batch [8/40] time 0.298 (0.589) data 0.000 (0.101) loss 1.3369 (1.5142) lr 7.5000e-03 eta 0:07:46
epoch [11/30] batch [10/40] time 0.297 (0.542) data 0.000 (0.080) loss 1.4736 (1.6107) lr 7.5000e-03 eta 0:07:07
epoch [11/30] batch [12/40] time 0.295 (0.501) data 0.000 (0.067) loss 1.9531 (1.6019) lr 7.5000e-03 eta 0:06:34
epoch [11/30] batch [14/40] time 0.292 (0.471) data 0.000 (0.058) loss 1.3477 (1.5495) lr 7.5000e-03 eta 0:06:10
epoch [11/30] batch [16/40] time 0.302 (0.450) data 0.000 (0.050) loss 1.6113 (1.5137) lr 7.5000e-03 eta 0:05:52
epoch [11/30] batch [18/40] time 0.291 (0.432) data 0.000 (0.045) loss 0.9370 (1.5203) lr 7.5000e-03 eta 0:05:37
epoch [11/30] batch [20/40] time 0.294 (0.418) data 0.000 (0.040) loss 1.6826 (1.4727) lr 7.5000e-03 eta 0:05:26
epoch [11/30] batch [22/40] time 0.294 (0.407) data 0.000 (0.037) loss 1.2148 (1.4461) lr 7.5000e-03 eta 0:05:16
epoch [11/30] batch [24/40] time 0.287 (0.397) data 0.000 (0.034) loss 0.9268 (1.4128) lr 7.5000e-03 eta 0:05:07
epoch [11/30] batch [26/40] time 0.289 (0.388) data 0.000 (0.031) loss 1.9980 (1.4242) lr 7.5000e-03 eta 0:05:00
epoch [11/30] batch [28/40] time 0.295 (0.382) data 0.000 (0.029) loss 2.0918 (1.4724) lr 7.5000e-03 eta 0:04:54
epoch [11/30] batch [30/40] time 0.297 (0.376) data 0.000 (0.027) loss 0.7617 (1.4220) lr 7.5000e-03 eta 0:04:49
epoch [11/30] batch [32/40] time 0.297 (0.371) data 0.000 (0.025) loss 1.8818 (1.4277) lr 7.5000e-03 eta 0:04:44
epoch [11/30] batch [34/40] time 0.293 (0.366) data 0.000 (0.024) loss 2.5273 (1.4400) lr 7.5000e-03 eta 0:04:40
epoch [11/30] batch [36/40] time 0.293 (0.362) data 0.000 (0.023) loss 0.9155 (1.4130) lr 7.5000e-03 eta 0:04:36
epoch [11/30] batch [38/40] time 0.294 (0.359) data 0.000 (0.021) loss 1.0020 (1.3802) lr 7.5000e-03 eta 0:04:33
epoch [11/30] batch [40/40] time 0.300 (0.356) data 0.000 (0.020) loss 1.8711 (1.3918) lr 7.0337e-03 eta 0:04:30
epoch [18/30] batch [2/40] time 0.296 (0.509) data 0.000 (0.192) loss 0.8838 (0.5690) lr 3.9604e-03 eta 0:04:23
epoch [18/30] batch [4/40] time 0.293 (0.401) data 0.000 (0.096) loss 4.5508 (1.5743) lr 3.9604e-03 eta 0:03:26
epoch [18/30] batch [6/40] time 0.288 (0.364) data 0.000 (0.064) loss 0.7476 (1.2152) lr 3.9604e-03 eta 0:03:07
epoch [18/30] batch [8/40] time 0.292 (0.346) data 0.000 (0.048) loss 0.4651 (1.0838) lr 3.9604e-03 eta 0:02:57
epoch [18/30] batch [10/40] time 0.288 (0.335) data 0.000 (0.039) loss 1.1895 (1.0498) lr 3.9604e-03 eta 0:02:50
epoch [18/30] batch [12/40] time 0.310 (0.329) data 0.000 (0.032) loss 1.9199 (1.0728) lr 3.9604e-03 eta 0:02:47
epoch [18/30] batch [14/40] time 0.291 (0.324) data 0.000 (0.028) loss 0.7588 (1.0247) lr 3.9604e-03 eta 0:02:44
epoch [18/30] batch [16/40] time 0.298 (0.321) data 0.000 (0.024) loss 2.5898 (1.1102) lr 3.9604e-03 eta 0:02:41
epoch [18/30] batch [18/40] time 0.304 (0.319) data 0.000 (0.022) loss 1.7559 (1.1643) lr 3.9604e-03 eta 0:02:40
epoch [18/30] batch [20/40] time 0.300 (0.317) data 0.000 (0.019) loss 0.5820 (1.1817) lr 3.9604e-03 eta 0:02:38
epoch [18/30] batch [22/40] time 0.288 (0.315) data 0.000 (0.018) loss 1.2715 (1.2809) lr 3.9604e-03 eta 0:02:36
epoch [18/30] batch [24/40] time 0.285 (0.312) data 0.000 (0.016) loss 1.2236 (1.2394) lr 3.9604e-03 eta 0:02:34
epoch [18/30] batch [26/40] time 0.299 (0.311) data 0.000 (0.015) loss 0.8276 (1.1916) lr 3.9604e-03 eta 0:02:33
epoch [18/30] batch [28/40] time 0.290 (0.309) data 0.000 (0.014) loss 0.2722 (1.1395) lr 3.9604e-03 eta 0:02:32
epoch [18/30] batch [30/40] time 0.285 (0.308) data 0.000 (0.013) loss 2.5918 (1.1697) lr 3.9604e-03 eta 0:02:30
epoch [18/30] batch [32/40] time 0.291 (0.307) data 0.000 (0.012) loss 0.5688 (1.1210) lr 3.9604e-03 eta 0:02:29
epoch [18/30] batch [34/40] time 0.292 (0.306) data 0.000 (0.012) loss 0.4998 (1.1029) lr 3.9604e-03 eta 0:02:28
epoch [18/30] batch [36/40] time 0.293 (0.305) data 0.000 (0.011) loss 0.6162 (1.0873) lr 3.9604e-03 eta 0:02:27
epoch [18/30] batch [38/40] time 0.289 (0.304) data 0.000 (0.010) loss 3.6426 (1.1311) lr 3.9604e-03 eta 0:02:26
epoch [18/30] batch [40/40] time 0.292 (0.304) data 0.000 (0.010) loss 0.9614 (1.1293) lr 3.4549e-03 eta 0:02:25
epoch [12/30] batch [2/40] time 0.290 (0.517) data 0.000 (0.197) loss 1.2080 (0.8577) lr 7.0337e-03 eta 0:06:32
epoch [12/30] batch [4/40] time 0.292 (0.403) data 0.000 (0.098) loss 0.8550 (1.1782) lr 7.0337e-03 eta 0:05:04
epoch [12/30] batch [6/40] time 0.294 (0.365) data 0.000 (0.066) loss 0.8003 (1.1153) lr 7.0337e-03 eta 0:04:35
epoch [12/30] batch [8/40] time 0.312 (0.349) data 0.000 (0.049) loss 1.6816 (1.4080) lr 7.0337e-03 eta 0:04:22
epoch [12/30] batch [10/40] time 0.303 (0.339) data 0.000 (0.040) loss 0.5283 (1.3083) lr 7.0337e-03 eta 0:04:14
epoch [12/30] batch [12/40] time 0.299 (0.333) data 0.000 (0.033) loss 2.0664 (1.4665) lr 7.0337e-03 eta 0:04:08
epoch [12/30] batch [14/40] time 0.295 (0.327) data 0.000 (0.028) loss 0.5688 (1.3572) lr 7.0337e-03 eta 0:04:04
epoch [12/30] batch [16/40] time 0.298 (0.323) data 0.000 (0.025) loss 0.5615 (1.3179) lr 7.0337e-03 eta 0:04:00
epoch [12/30] batch [18/40] time 0.295 (0.320) data 0.000 (0.022) loss 1.4824 (1.3604) lr 7.0337e-03 eta 0:03:57
epoch [12/30] batch [20/40] time 0.296 (0.318) data 0.000 (0.020) loss 0.9507 (1.3556) lr 7.0337e-03 eta 0:03:54
epoch [12/30] batch [22/40] time 0.292 (0.315) data 0.000 (0.018) loss 1.0293 (1.3075) lr 7.0337e-03 eta 0:03:52
epoch [12/30] batch [24/40] time 0.295 (0.313) data 0.000 (0.017) loss 1.2949 (1.2846) lr 7.0337e-03 eta 0:03:50
epoch [12/30] batch [26/40] time 0.293 (0.312) data 0.000 (0.015) loss 0.8022 (1.2413) lr 7.0337e-03 eta 0:03:49
epoch [12/30] batch [28/40] time 0.298 (0.311) data 0.000 (0.014) loss 2.5938 (1.2922) lr 7.0337e-03 eta 0:03:47
epoch [12/30] batch [30/40] time 0.296 (0.310) data 0.000 (0.013) loss 1.4004 (1.2638) lr 7.0337e-03 eta 0:03:46
epoch [12/30] batch [32/40] time 0.291 (0.308) data 0.000 (0.013) loss 0.4304 (1.2299) lr 7.0337e-03 eta 0:03:44
epoch [12/30] batch [34/40] time 0.295 (0.308) data 0.000 (0.012) loss 0.8506 (1.2072) lr 7.0337e-03 eta 0:03:43
epoch [12/30] batch [36/40] time 0.299 (0.307) data 0.000 (0.011) loss 3.4375 (1.2795) lr 7.0337e-03 eta 0:03:42
epoch [12/30] batch [38/40] time 0.294 (0.306) data 0.000 (0.011) loss 0.5166 (1.2375) lr 7.0337e-03 eta 0:03:41
epoch [12/30] batch [40/40] time 0.296 (0.306) data 0.000 (0.010) loss 1.3975 (1.2254) lr 6.5451e-03 eta 0:03:40
epoch [19/30] batch [2/40] time 0.303 (0.527) data 0.000 (0.197) loss 0.6895 (1.2520) lr 3.4549e-03 eta 0:04:11
epoch [19/30] batch [4/40] time 0.334 (0.423) data 0.000 (0.099) loss 0.7622 (0.9862) lr 3.4549e-03 eta 0:03:21
epoch [19/30] batch [6/40] time 0.296 (0.381) data 0.000 (0.066) loss 0.4062 (1.1376) lr 3.4549e-03 eta 0:03:00
epoch [19/30] batch [8/40] time 0.292 (0.359) data 0.000 (0.050) loss 0.5981 (1.0034) lr 3.4549e-03 eta 0:02:49
epoch [19/30] batch [10/40] time 0.303 (0.347) data 0.000 (0.040) loss 0.3027 (0.9668) lr 3.4549e-03 eta 0:02:43
epoch [19/30] batch [12/40] time 0.301 (0.339) data 0.000 (0.033) loss 0.6719 (0.8913) lr 3.4549e-03 eta 0:02:38
epoch [19/30] batch [14/40] time 0.308 (0.334) data 0.000 (0.028) loss 0.7676 (0.8481) lr 3.4549e-03 eta 0:02:35
epoch [19/30] batch [16/40] time 0.300 (0.330) data 0.000 (0.025) loss 1.6865 (0.9541) lr 3.4549e-03 eta 0:02:33
epoch [19/30] batch [18/40] time 0.403 (0.332) data 0.000 (0.022) loss 0.3936 (0.9353) lr 3.4549e-03 eta 0:02:33
epoch [19/30] batch [20/40] time 0.297 (0.329) data 0.000 (0.020) loss 0.4783 (0.8825) lr 3.4549e-03 eta 0:02:31
epoch [19/30] batch [22/40] time 0.300 (0.326) data 0.000 (0.018) loss 0.1882 (0.8273) lr 3.4549e-03 eta 0:02:29
epoch [19/30] batch [24/40] time 0.299 (0.324) data 0.000 (0.017) loss 0.2959 (0.8749) lr 3.4549e-03 eta 0:02:27
epoch [19/30] batch [26/40] time 0.295 (0.321) data 0.000 (0.015) loss 0.3945 (0.8731) lr 3.4549e-03 eta 0:02:25
epoch [19/30] batch [28/40] time 0.297 (0.320) data 0.000 (0.014) loss 0.4746 (0.8402) lr 3.4549e-03 eta 0:02:24
epoch [19/30] batch [30/40] time 0.303 (0.319) data 0.000 (0.013) loss 0.5063 (0.8254) lr 3.4549e-03 eta 0:02:23
epoch [19/30] batch [32/40] time 0.302 (0.317) data 0.000 (0.013) loss 1.5078 (0.8532) lr 3.4549e-03 eta 0:02:22
epoch [19/30] batch [34/40] time 0.300 (0.316) data 0.000 (0.012) loss 1.0947 (0.8404) lr 3.4549e-03 eta 0:02:21
epoch [19/30] batch [36/40] time 0.289 (0.315) data 0.000 (0.011) loss 0.4890 (0.8339) lr 3.4549e-03 eta 0:02:19
epoch [19/30] batch [38/40] time 0.293 (0.314) data 0.000 (0.011) loss 1.0215 (0.8636) lr 3.4549e-03 eta 0:02:18
epoch [19/30] batch [40/40] time 0.299 (0.313) data 0.000 (0.010) loss 2.3672 (0.9003) lr 2.9663e-03 eta 0:02:17
epoch [13/30] batch [2/40] time 0.291 (0.512) data 0.000 (0.204) loss 0.2507 (0.6088) lr 6.5451e-03 eta 0:06:07
epoch [13/30] batch [4/40] time 0.294 (0.402) data 0.000 (0.102) loss 1.9756 (1.1144) lr 6.5451e-03 eta 0:04:47
epoch [13/30] batch [6/40] time 0.291 (0.366) data 0.000 (0.068) loss 1.2539 (1.4829) lr 6.5451e-03 eta 0:04:21
epoch [13/30] batch [8/40] time 0.292 (0.348) data 0.000 (0.051) loss 0.8281 (1.2610) lr 6.5451e-03 eta 0:04:07
epoch [13/30] batch [10/40] time 0.285 (0.335) data 0.000 (0.041) loss 1.6924 (1.4014) lr 6.5451e-03 eta 0:03:58
epoch [13/30] batch [12/40] time 0.288 (0.327) data 0.000 (0.034) loss 2.1699 (1.4804) lr 6.5451e-03 eta 0:03:51
epoch [13/30] batch [14/40] time 0.294 (0.322) data 0.000 (0.029) loss 0.5596 (1.3414) lr 6.5451e-03 eta 0:03:47
epoch [13/30] batch [16/40] time 0.287 (0.318) data 0.000 (0.026) loss 0.8149 (1.2595) lr 6.5451e-03 eta 0:03:43
epoch [13/30] batch [18/40] time 0.285 (0.314) data 0.000 (0.023) loss 1.3926 (1.2403) lr 6.5451e-03 eta 0:03:40
epoch [13/30] batch [20/40] time 0.281 (0.311) data 0.000 (0.021) loss 0.5078 (1.1691) lr 6.5451e-03 eta 0:03:37
epoch [13/30] batch [22/40] time 0.287 (0.309) data 0.000 (0.019) loss 0.9507 (1.1491) lr 6.5451e-03 eta 0:03:35
epoch [13/30] batch [24/40] time 0.286 (0.307) data 0.000 (0.017) loss 0.4749 (1.1223) lr 6.5451e-03 eta 0:03:33
epoch [13/30] batch [26/40] time 0.285 (0.305) data 0.000 (0.016) loss 0.7290 (1.1639) lr 6.5451e-03 eta 0:03:31
epoch [13/30] batch [28/40] time 0.288 (0.304) data 0.000 (0.015) loss 1.0674 (1.1529) lr 6.5451e-03 eta 0:03:30
epoch [13/30] batch [30/40] time 0.286 (0.303) data 0.000 (0.014) loss 1.2490 (1.1607) lr 6.5451e-03 eta 0:03:28
epoch [13/30] batch [32/40] time 0.284 (0.301) data 0.000 (0.013) loss 2.4004 (1.2096) lr 6.5451e-03 eta 0:03:27
epoch [13/30] batch [34/40] time 0.390 (0.304) data 0.000 (0.012) loss 2.3203 (1.2720) lr 6.5451e-03 eta 0:03:28
epoch [13/30] batch [36/40] time 0.284 (0.302) data 0.000 (0.012) loss 0.4697 (1.2994) lr 6.5451e-03 eta 0:03:26
epoch [13/30] batch [38/40] time 0.283 (0.302) data 0.000 (0.011) loss 1.1240 (1.3018) lr 6.5451e-03 eta 0:03:25
epoch [13/30] batch [40/40] time 0.281 (0.301) data 0.000 (0.010) loss 1.1152 (1.2817) lr 6.0396e-03 eta 0:03:24
epoch [20/30] batch [2/40] time 0.298 (0.533) data 0.001 (0.211) loss 0.6089 (0.7454) lr 2.9663e-03 eta 0:03:53
epoch [20/30] batch [4/40] time 0.294 (0.413) data 0.000 (0.106) loss 0.4019 (0.8555) lr 2.9663e-03 eta 0:03:00
epoch [20/30] batch [6/40] time 0.298 (0.375) data 0.001 (0.071) loss 1.1797 (0.7950) lr 2.9663e-03 eta 0:02:42
epoch [20/30] batch [8/40] time 0.298 (0.355) data 0.000 (0.053) loss 0.3726 (0.6663) lr 2.9663e-03 eta 0:02:33
epoch [20/30] batch [10/40] time 0.299 (0.344) data 0.000 (0.042) loss 1.3613 (0.7125) lr 2.9663e-03 eta 0:02:27
epoch [20/30] batch [12/40] time 0.302 (0.337) data 0.000 (0.035) loss 2.0273 (0.8461) lr 2.9663e-03 eta 0:02:24
epoch [20/30] batch [14/40] time 0.300 (0.331) data 0.000 (0.030) loss 0.9785 (0.9243) lr 2.9663e-03 eta 0:02:21
epoch [20/30] batch [16/40] time 0.293 (0.327) data 0.000 (0.027) loss 0.7163 (0.8665) lr 2.9663e-03 eta 0:02:18
epoch [20/30] batch [18/40] time 0.296 (0.323) data 0.000 (0.024) loss 0.3289 (0.8006) lr 2.9663e-03 eta 0:02:16
epoch [20/30] batch [20/40] time 0.301 (0.321) data 0.000 (0.021) loss 0.2423 (0.7741) lr 2.9663e-03 eta 0:02:14
epoch [20/30] batch [22/40] time 0.301 (0.319) data 0.000 (0.019) loss 0.3330 (0.7488) lr 2.9663e-03 eta 0:02:13
epoch [20/30] batch [24/40] time 0.295 (0.317) data 0.000 (0.018) loss 0.1960 (0.7132) lr 2.9663e-03 eta 0:02:11
epoch [20/30] batch [26/40] time 0.292 (0.315) data 0.000 (0.017) loss 0.9751 (0.7877) lr 2.9663e-03 eta 0:02:10
epoch [20/30] batch [28/40] time 0.296 (0.313) data 0.000 (0.015) loss 0.8442 (0.7848) lr 2.9663e-03 eta 0:02:09
epoch [20/30] batch [30/40] time 0.301 (0.312) data 0.000 (0.014) loss 0.7529 (0.8172) lr 2.9663e-03 eta 0:02:08
epoch [20/30] batch [32/40] time 0.297 (0.311) data 0.000 (0.013) loss 1.4434 (0.8214) lr 2.9663e-03 eta 0:02:07
epoch [20/30] batch [34/40] time 0.293 (0.310) data 0.000 (0.013) loss 0.8311 (0.8424) lr 2.9663e-03 eta 0:02:06
epoch [20/30] batch [36/40] time 0.296 (0.310) data 0.000 (0.012) loss 0.2358 (0.8178) lr 2.9663e-03 eta 0:02:05
epoch [20/30] batch [38/40] time 0.289 (0.308) data 0.000 (0.011) loss 0.1536 (0.7962) lr 2.9663e-03 eta 0:02:04
epoch [20/30] batch [40/40] time 0.297 (0.308) data 0.000 (0.011) loss 0.3091 (0.7726) lr 2.5000e-03 eta 0:02:03
Checkpoint saved to output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1/prompt_learner/model.pth.tar-20
epoch [14/30] batch [2/40] time 0.300 (0.533) data 0.000 (0.199) loss 2.8242 (1.6298) lr 6.0396e-03 eta 0:06:01
epoch [14/30] batch [4/40] time 0.300 (0.416) data 0.000 (0.100) loss 0.6079 (1.1707) lr 6.0396e-03 eta 0:04:41
epoch [14/30] batch [6/40] time 0.304 (0.378) data 0.000 (0.067) loss 0.2343 (1.0625) lr 6.0396e-03 eta 0:04:14
epoch [14/30] batch [8/40] time 0.303 (0.360) data 0.000 (0.050) loss 0.8652 (1.0984) lr 6.0396e-03 eta 0:04:02
epoch [14/30] batch [10/40] time 0.308 (0.349) data 0.000 (0.040) loss 1.8438 (1.1354) lr 6.0396e-03 eta 0:03:53
epoch [14/30] batch [12/40] time 0.302 (0.341) data 0.000 (0.033) loss 1.7480 (1.2057) lr 6.0396e-03 eta 0:03:47
epoch [14/30] batch [14/40] time 0.302 (0.335) data 0.000 (0.029) loss 0.5073 (1.2378) lr 6.0396e-03 eta 0:03:43
epoch [14/30] batch [16/40] time 0.301 (0.331) data 0.000 (0.025) loss 0.8857 (1.1723) lr 6.0396e-03 eta 0:03:39
epoch [14/30] batch [18/40] time 0.293 (0.327) data 0.000 (0.022) loss 1.1143 (1.2823) lr 6.0396e-03 eta 0:03:36
epoch [14/30] batch [20/40] time 0.292 (0.323) data 0.000 (0.020) loss 1.8232 (1.2785) lr 6.0396e-03 eta 0:03:33
epoch [14/30] batch [22/40] time 0.296 (0.321) data 0.000 (0.018) loss 0.5410 (1.2634) lr 6.0396e-03 eta 0:03:30
epoch [14/30] batch [24/40] time 0.294 (0.318) data 0.000 (0.017) loss 1.4746 (1.3044) lr 6.0396e-03 eta 0:03:28
epoch [14/30] batch [26/40] time 0.294 (0.317) data 0.000 (0.016) loss 0.6582 (1.2620) lr 6.0396e-03 eta 0:03:27
epoch [14/30] batch [28/40] time 0.294 (0.315) data 0.000 (0.014) loss 1.2158 (1.2459) lr 6.0396e-03 eta 0:03:25
epoch [14/30] batch [30/40] time 0.294 (0.314) data 0.000 (0.014) loss 0.1094 (1.1849) lr 6.0396e-03 eta 0:03:23
epoch [14/30] batch [32/40] time 0.295 (0.313) data 0.000 (0.013) loss 1.8291 (1.1923) lr 6.0396e-03 eta 0:03:22
epoch [14/30] batch [34/40] time 0.294 (0.311) data 0.000 (0.012) loss 0.4077 (1.1581) lr 6.0396e-03 eta 0:03:21
epoch [14/30] batch [36/40] time 0.294 (0.310) data 0.000 (0.011) loss 0.5698 (1.2034) lr 6.0396e-03 eta 0:03:19
epoch [14/30] batch [38/40] time 0.294 (0.310) data 0.000 (0.011) loss 0.8120 (1.1905) lr 6.0396e-03 eta 0:03:18
epoch [14/30] batch [40/40] time 0.296 (0.309) data 0.000 (0.010) loss 1.2559 (1.1926) lr 5.5226e-03 eta 0:03:17
epoch [21/30] batch [2/40] time 0.295 (0.517) data 0.000 (0.194) loss 2.2129 (1.7798) lr 2.5000e-03 eta 0:03:25
epoch [21/30] batch [4/40] time 0.300 (0.408) data 0.000 (0.097) loss 0.4170 (1.0219) lr 2.5000e-03 eta 0:02:41
epoch [21/30] batch [6/40] time 0.302 (0.373) data 0.000 (0.065) loss 0.8701 (0.9176) lr 2.5000e-03 eta 0:02:26
epoch [21/30] batch [8/40] time 0.299 (0.355) data 0.000 (0.049) loss 0.5918 (0.8628) lr 2.5000e-03 eta 0:02:19
epoch [21/30] batch [10/40] time 0.403 (0.355) data 0.000 (0.039) loss 0.3491 (0.7523) lr 2.5000e-03 eta 0:02:18
epoch [21/30] batch [12/40] time 0.296 (0.346) data 0.000 (0.033) loss 0.1892 (0.7890) lr 2.5000e-03 eta 0:02:14
epoch [21/30] batch [14/40] time 0.292 (0.339) data 0.000 (0.028) loss 0.5938 (0.7747) lr 2.5000e-03 eta 0:02:10
epoch [21/30] batch [16/40] time 0.307 (0.334) data 0.000 (0.025) loss 0.4526 (0.7136) lr 2.5000e-03 eta 0:02:08
epoch [21/30] batch [18/40] time 0.294 (0.329) data 0.000 (0.022) loss 0.8350 (0.7073) lr 2.5000e-03 eta 0:02:05
epoch [21/30] batch [20/40] time 0.302 (0.326) data 0.000 (0.020) loss 1.7041 (0.7480) lr 2.5000e-03 eta 0:02:04
epoch [21/30] batch [22/40] time 0.299 (0.324) data 0.000 (0.018) loss 0.6279 (0.7148) lr 2.5000e-03 eta 0:02:02
epoch [21/30] batch [24/40] time 0.296 (0.322) data 0.000 (0.016) loss 0.2693 (0.6788) lr 2.5000e-03 eta 0:02:00
epoch [21/30] batch [26/40] time 0.297 (0.320) data 0.000 (0.015) loss 0.3147 (0.6626) lr 2.5000e-03 eta 0:01:59
epoch [21/30] batch [28/40] time 0.299 (0.318) data 0.000 (0.014) loss 0.4343 (0.7021) lr 2.5000e-03 eta 0:01:58
epoch [21/30] batch [30/40] time 0.304 (0.317) data 0.000 (0.013) loss 0.3425 (0.6775) lr 2.5000e-03 eta 0:01:57
epoch [21/30] batch [32/40] time 0.301 (0.316) data 0.000 (0.012) loss 0.1975 (0.6729) lr 2.5000e-03 eta 0:01:56
epoch [21/30] batch [34/40] time 0.301 (0.315) data 0.000 (0.012) loss 0.7905 (0.6693) lr 2.5000e-03 eta 0:01:55
epoch [21/30] batch [36/40] time 0.287 (0.314) data 0.000 (0.011) loss 1.4922 (0.6983) lr 2.5000e-03 eta 0:01:54
epoch [21/30] batch [38/40] time 0.293 (0.313) data 0.000 (0.010) loss 0.6460 (0.6855) lr 2.5000e-03 eta 0:01:53
epoch [21/30] batch [40/40] time 0.295 (0.312) data 0.000 (0.010) loss 0.1479 (0.6618) lr 2.0611e-03 eta 0:01:52
epoch [15/30] batch [2/40] time 0.295 (0.526) data 0.000 (0.207) loss 1.1182 (1.2725) lr 5.5226e-03 eta 0:05:35
epoch [15/30] batch [4/40] time 0.288 (0.408) data 0.000 (0.104) loss 0.3872 (0.9362) lr 5.5226e-03 eta 0:04:19
epoch [15/30] batch [6/40] time 0.286 (0.368) data 0.000 (0.069) loss 0.3872 (0.7725) lr 5.5226e-03 eta 0:03:53
epoch [15/30] batch [8/40] time 0.293 (0.349) data 0.000 (0.052) loss 1.9609 (1.0740) lr 5.5226e-03 eta 0:03:40
epoch [15/30] batch [10/40] time 0.304 (0.338) data 0.000 (0.042) loss 0.7119 (1.0375) lr 5.5226e-03 eta 0:03:33
epoch [15/30] batch [12/40] time 0.306 (0.333) data 0.000 (0.035) loss 0.2908 (0.9979) lr 5.5226e-03 eta 0:03:28
epoch [15/30] batch [14/40] time 0.302 (0.329) data 0.000 (0.030) loss 1.3584 (1.1170) lr 5.5226e-03 eta 0:03:25
epoch [15/30] batch [16/40] time 0.302 (0.326) data 0.000 (0.026) loss 0.9673 (1.0631) lr 5.5226e-03 eta 0:03:23
epoch [15/30] batch [18/40] time 0.293 (0.322) data 0.000 (0.023) loss 0.7788 (1.0598) lr 5.5226e-03 eta 0:03:20
epoch [15/30] batch [20/40] time 0.295 (0.324) data 0.000 (0.021) loss 0.5264 (1.0308) lr 5.5226e-03 eta 0:03:21
epoch [15/30] batch [22/40] time 0.302 (0.322) data 0.000 (0.019) loss 0.7729 (1.1017) lr 5.5226e-03 eta 0:03:18
epoch [15/30] batch [24/40] time 0.294 (0.319) data 0.000 (0.017) loss 0.4065 (1.0511) lr 5.5226e-03 eta 0:03:16
epoch [15/30] batch [26/40] time 0.298 (0.318) data 0.000 (0.016) loss 1.4648 (1.0340) lr 5.5226e-03 eta 0:03:15
epoch [15/30] batch [28/40] time 0.299 (0.316) data 0.000 (0.015) loss 0.5981 (0.9863) lr 5.5226e-03 eta 0:03:13
epoch [15/30] batch [30/40] time 0.293 (0.315) data 0.000 (0.014) loss 1.3965 (1.0337) lr 5.5226e-03 eta 0:03:11
epoch [15/30] batch [32/40] time 0.292 (0.313) data 0.000 (0.013) loss 0.2627 (1.0032) lr 5.5226e-03 eta 0:03:10
epoch [15/30] batch [34/40] time 0.295 (0.312) data 0.000 (0.012) loss 2.0742 (1.0326) lr 5.5226e-03 eta 0:03:09
epoch [15/30] batch [36/40] time 0.298 (0.311) data 0.000 (0.012) loss 0.5791 (1.0055) lr 5.5226e-03 eta 0:03:07
epoch [15/30] batch [38/40] time 0.293 (0.310) data 0.000 (0.011) loss 0.9219 (0.9932) lr 5.5226e-03 eta 0:03:06
epoch [15/30] batch [40/40] time 0.293 (0.309) data 0.000 (0.011) loss 1.6865 (1.0354) lr 5.0000e-03 eta 0:03:05
epoch [22/30] batch [2/40] time 0.302 (0.522) data 0.000 (0.196) loss 0.6567 (0.7417) lr 2.0611e-03 eta 0:03:06
epoch [22/30] batch [4/40] time 0.299 (0.411) data 0.000 (0.098) loss 3.0547 (1.4429) lr 2.0611e-03 eta 0:02:26
epoch [22/30] batch [6/40] time 0.298 (0.374) data 0.000 (0.066) loss 1.2686 (1.1884) lr 2.0611e-03 eta 0:02:12
epoch [22/30] batch [8/40] time 0.297 (0.355) data 0.000 (0.049) loss 1.1270 (1.0714) lr 2.0611e-03 eta 0:02:04
epoch [22/30] batch [10/40] time 0.299 (0.344) data 0.000 (0.039) loss 0.3484 (0.9566) lr 2.0611e-03 eta 0:02:00
epoch [22/30] batch [12/40] time 0.300 (0.337) data 0.000 (0.033) loss 1.1025 (0.9140) lr 2.0611e-03 eta 0:01:57
epoch [22/30] batch [14/40] time 0.302 (0.332) data 0.000 (0.028) loss 0.6636 (0.8347) lr 2.0611e-03 eta 0:01:54
epoch [22/30] batch [16/40] time 0.303 (0.328) data 0.000 (0.025) loss 0.3193 (0.7860) lr 2.0611e-03 eta 0:01:52
epoch [22/30] batch [18/40] time 0.294 (0.324) data 0.000 (0.022) loss 0.8330 (0.7520) lr 2.0611e-03 eta 0:01:50
epoch [22/30] batch [20/40] time 0.298 (0.322) data 0.000 (0.020) loss 0.3992 (0.7398) lr 2.0611e-03 eta 0:01:49
epoch [22/30] batch [22/40] time 0.295 (0.319) data 0.000 (0.018) loss 0.8491 (0.7458) lr 2.0611e-03 eta 0:01:47
epoch [22/30] batch [24/40] time 0.289 (0.317) data 0.000 (0.017) loss 0.4956 (0.7289) lr 2.0611e-03 eta 0:01:46
epoch [22/30] batch [26/40] time 0.292 (0.315) data 0.000 (0.015) loss 0.5430 (0.7107) lr 2.0611e-03 eta 0:01:45
epoch [22/30] batch [28/40] time 0.295 (0.313) data 0.000 (0.014) loss 0.5674 (0.6880) lr 2.0611e-03 eta 0:01:43
epoch [22/30] batch [30/40] time 0.297 (0.312) data 0.000 (0.013) loss 0.2491 (0.6696) lr 2.0611e-03 eta 0:01:42
epoch [22/30] batch [32/40] time 0.295 (0.311) data 0.000 (0.012) loss 1.1064 (0.7064) lr 2.0611e-03 eta 0:01:41
epoch [22/30] batch [34/40] time 0.287 (0.310) data 0.000 (0.012) loss 0.1992 (0.6908) lr 2.0611e-03 eta 0:01:40
epoch [22/30] batch [36/40] time 0.286 (0.308) data 0.000 (0.011) loss 0.9561 (0.6887) lr 2.0611e-03 eta 0:01:39
epoch [22/30] batch [38/40] time 0.294 (0.308) data 0.000 (0.011) loss 1.4600 (0.7032) lr 2.0611e-03 eta 0:01:39
epoch [22/30] batch [40/40] time 0.294 (0.307) data 0.000 (0.010) loss 1.4062 (0.7751) lr 1.6543e-03 eta 0:01:38
slurmstepd: error: *** JOB 403979 ON a05 CANCELLED AT 2024-03-02T15:13:30 ***
epoch [16/30] batch [2/40] time 0.287 (0.521) data 0.000 (0.208) loss 0.9238 (0.5610) lr 5.0000e-03 eta 0:05:11
epoch [16/30] batch [4/40] time 0.286 (0.405) data 0.000 (0.104) loss 0.3789 (0.4079) lr 5.0000e-03 eta 0:04:01
epoch [16/30] batch [6/40] time 0.286 (0.366) data 0.000 (0.069) loss 0.7378 (0.5769) lr 5.0000e-03 eta 0:03:37
epoch [16/30] batch [8/40] time 0.283 (0.345) data 0.000 (0.052) loss 1.9258 (0.8266) lr 5.0000e-03 eta 0:03:24
epoch [16/30] batch [10/40] time 0.285 (0.333) data 0.000 (0.042) loss 3.7207 (1.1900) lr 5.0000e-03 eta 0:03:16
epoch [16/30] batch [12/40] time 0.297 (0.326) data 0.000 (0.035) loss 0.7505 (1.1682) lr 5.0000e-03 eta 0:03:11
epoch [16/30] batch [14/40] time 0.297 (0.322) data 0.000 (0.030) loss 1.0938 (1.1188) lr 5.0000e-03 eta 0:03:08
epoch [16/30] batch [16/40] time 0.287 (0.318) data 0.000 (0.026) loss 0.6260 (1.0646) lr 5.0000e-03 eta 0:03:05
epoch [16/30] batch [18/40] time 0.286 (0.314) data 0.000 (0.023) loss 0.6738 (1.0463) lr 5.0000e-03 eta 0:03:02
epoch [16/30] batch [20/40] time 0.280 (0.310) data 0.000 (0.021) loss 1.1143 (1.0184) lr 5.0000e-03 eta 0:03:00
epoch [16/30] batch [22/40] time 0.284 (0.308) data 0.000 (0.019) loss 1.3564 (1.0303) lr 5.0000e-03 eta 0:02:57
epoch [16/30] batch [24/40] time 0.287 (0.306) data 0.000 (0.018) loss 1.2686 (1.1035) lr 5.0000e-03 eta 0:02:56
epoch [16/30] batch [26/40] time 0.284 (0.304) data 0.000 (0.016) loss 0.2092 (1.0525) lr 5.0000e-03 eta 0:02:54
epoch [16/30] batch [28/40] time 0.274 (0.302) data 0.000 (0.015) loss 3.8438 (1.1531) lr 5.0000e-03 eta 0:02:52
epoch [16/30] batch [30/40] time 0.282 (0.301) data 0.000 (0.014) loss 0.8867 (1.1252) lr 5.0000e-03 eta 0:02:51
epoch [16/30] batch [32/40] time 0.280 (0.300) data 0.000 (0.013) loss 0.2312 (1.0929) lr 5.0000e-03 eta 0:02:50
epoch [16/30] batch [34/40] time 0.286 (0.299) data 0.000 (0.012) loss 0.4663 (1.0635) lr 5.0000e-03 eta 0:02:49
epoch [16/30] batch [36/40] time 0.273 (0.297) data 0.000 (0.012) loss 0.4167 (1.0351) lr 5.0000e-03 eta 0:02:47
epoch [16/30] batch [38/40] time 0.274 (0.296) data 0.000 (0.011) loss 0.6069 (1.0152) lr 5.0000e-03 eta 0:02:46
epoch [16/30] batch [40/40] time 0.280 (0.295) data 0.000 (0.011) loss 0.8140 (0.9902) lr 4.4774e-03 eta 0:02:45
epoch [17/30] batch [2/40] time 0.281 (0.526) data 0.000 (0.205) loss 0.5488 (0.6606) lr 4.4774e-03 eta 0:04:53
epoch [17/30] batch [4/40] time 0.287 (0.407) data 0.000 (0.103) loss 0.8662 (0.7574) lr 4.4774e-03 eta 0:03:46
epoch [17/30] batch [6/40] time 0.283 (0.367) data 0.000 (0.069) loss 0.7852 (0.6624) lr 4.4774e-03 eta 0:03:23
epoch [17/30] batch [8/40] time 0.298 (0.348) data 0.000 (0.051) loss 1.9600 (0.7819) lr 4.4774e-03 eta 0:03:12
epoch [17/30] batch [10/40] time 0.285 (0.335) data 0.000 (0.041) loss 2.6133 (0.9988) lr 4.4774e-03 eta 0:03:04
epoch [17/30] batch [12/40] time 0.284 (0.335) data 0.000 (0.034) loss 0.5654 (0.9253) lr 4.4774e-03 eta 0:03:03
epoch [17/30] batch [14/40] time 0.285 (0.329) data 0.000 (0.030) loss 1.7803 (0.9996) lr 4.4774e-03 eta 0:02:59
epoch [17/30] batch [16/40] time 0.280 (0.323) data 0.000 (0.026) loss 1.9453 (1.0524) lr 4.4774e-03 eta 0:02:55
epoch [17/30] batch [18/40] time 0.276 (0.318) data 0.000 (0.023) loss 0.2163 (1.0266) lr 4.4774e-03 eta 0:02:52
epoch [17/30] batch [20/40] time 0.278 (0.314) data 0.000 (0.021) loss 0.3037 (1.0300) lr 4.4774e-03 eta 0:02:49
epoch [17/30] batch [22/40] time 0.278 (0.310) data 0.000 (0.019) loss 0.7910 (0.9932) lr 4.4774e-03 eta 0:02:46
epoch [17/30] batch [24/40] time 0.279 (0.308) data 0.000 (0.017) loss 0.7559 (0.9508) lr 4.4774e-03 eta 0:02:44
epoch [17/30] batch [26/40] time 0.273 (0.305) data 0.000 (0.016) loss 0.8428 (0.9526) lr 4.4774e-03 eta 0:02:42
epoch [17/30] batch [28/40] time 0.275 (0.303) data 0.000 (0.015) loss 1.7432 (0.9525) lr 4.4774e-03 eta 0:02:41
epoch [17/30] batch [30/40] time 0.279 (0.301) data 0.000 (0.014) loss 0.6802 (0.9342) lr 4.4774e-03 eta 0:02:39
epoch [17/30] batch [32/40] time 0.307 (0.301) data 0.000 (0.013) loss 0.2654 (0.8958) lr 4.4774e-03 eta 0:02:38
epoch [17/30] batch [34/40] time 0.276 (0.300) data 0.000 (0.012) loss 0.6602 (0.8957) lr 4.4774e-03 eta 0:02:37
epoch [17/30] batch [36/40] time 0.276 (0.298) data 0.000 (0.012) loss 0.3958 (0.8758) lr 4.4774e-03 eta 0:02:36
epoch [17/30] batch [38/40] time 0.274 (0.297) data 0.000 (0.011) loss 1.4287 (0.8984) lr 4.4774e-03 eta 0:02:35
epoch [17/30] batch [40/40] time 0.277 (0.296) data 0.000 (0.010) loss 0.5073 (0.8830) lr 3.9604e-03 eta 0:02:34
epoch [18/30] batch [2/40] time 0.283 (0.522) data 0.000 (0.204) loss 0.1765 (0.2562) lr 3.9604e-03 eta 0:04:30
epoch [18/30] batch [4/40] time 0.279 (0.401) data 0.000 (0.102) loss 0.7983 (0.4807) lr 3.9604e-03 eta 0:03:26
epoch [18/30] batch [6/40] time 0.281 (0.360) data 0.000 (0.068) loss 0.5186 (0.4198) lr 3.9604e-03 eta 0:03:04
epoch [18/30] batch [8/40] time 0.282 (0.341) data 0.000 (0.051) loss 1.2158 (0.5315) lr 3.9604e-03 eta 0:02:54
epoch [18/30] batch [10/40] time 0.284 (0.329) data 0.000 (0.041) loss 1.2266 (0.6517) lr 3.9604e-03 eta 0:02:47
epoch [18/30] batch [12/40] time 0.293 (0.322) data 0.000 (0.034) loss 0.8188 (0.7671) lr 3.9604e-03 eta 0:02:43
epoch [18/30] batch [14/40] time 0.281 (0.316) data 0.000 (0.029) loss 0.3057 (0.7198) lr 3.9604e-03 eta 0:02:39
epoch [18/30] batch [16/40] time 0.286 (0.313) data 0.000 (0.026) loss 0.9663 (0.7899) lr 3.9604e-03 eta 0:02:37
epoch [18/30] batch [18/40] time 0.276 (0.309) data 0.000 (0.023) loss 0.5864 (0.8022) lr 3.9604e-03 eta 0:02:35
epoch [18/30] batch [20/40] time 0.272 (0.305) data 0.000 (0.021) loss 0.3005 (0.7500) lr 3.9604e-03 eta 0:02:32
epoch [18/30] batch [22/40] time 0.274 (0.303) data 0.000 (0.019) loss 1.0029 (0.7401) lr 3.9604e-03 eta 0:02:30
epoch [18/30] batch [24/40] time 0.275 (0.301) data 0.000 (0.017) loss 1.2002 (0.7996) lr 3.9604e-03 eta 0:02:29
epoch [18/30] batch [26/40] time 0.278 (0.299) data 0.000 (0.016) loss 0.5820 (0.7849) lr 3.9604e-03 eta 0:02:27
epoch [18/30] batch [28/40] time 0.275 (0.297) data 0.000 (0.015) loss 0.6421 (0.7719) lr 3.9604e-03 eta 0:02:26
epoch [18/30] batch [30/40] time 0.274 (0.296) data 0.000 (0.014) loss 0.2832 (0.7680) lr 3.9604e-03 eta 0:02:24
epoch [18/30] batch [32/40] time 0.277 (0.295) data 0.000 (0.013) loss 0.5166 (0.7478) lr 3.9604e-03 eta 0:02:23
epoch [18/30] batch [34/40] time 0.277 (0.293) data 0.000 (0.012) loss 0.2278 (0.7162) lr 3.9604e-03 eta 0:02:22
epoch [18/30] batch [36/40] time 0.274 (0.292) data 0.000 (0.012) loss 0.4045 (0.7171) lr 3.9604e-03 eta 0:02:21
epoch [18/30] batch [38/40] time 0.278 (0.292) data 0.000 (0.011) loss 2.1094 (0.7392) lr 3.9604e-03 eta 0:02:20
epoch [18/30] batch [40/40] time 0.273 (0.291) data 0.000 (0.010) loss 0.4260 (0.7195) lr 3.4549e-03 eta 0:02:19
epoch [19/30] batch [2/40] time 0.287 (0.518) data 0.000 (0.202) loss 0.6807 (0.4604) lr 3.4549e-03 eta 0:04:07
epoch [19/30] batch [4/40] time 0.287 (0.402) data 0.000 (0.101) loss 0.4573 (0.6526) lr 3.4549e-03 eta 0:03:11
epoch [19/30] batch [6/40] time 0.287 (0.364) data 0.000 (0.068) loss 0.7700 (0.6308) lr 3.4549e-03 eta 0:02:52
epoch [19/30] batch [8/40] time 0.284 (0.344) data 0.000 (0.051) loss 0.6274 (0.6014) lr 3.4549e-03 eta 0:02:42
epoch [19/30] batch [10/40] time 0.292 (0.333) data 0.000 (0.041) loss 0.3760 (0.7914) lr 3.4549e-03 eta 0:02:36
epoch [19/30] batch [12/40] time 0.282 (0.325) data 0.000 (0.034) loss 0.9658 (0.8204) lr 3.4549e-03 eta 0:02:31
epoch [19/30] batch [14/40] time 0.280 (0.318) data 0.000 (0.029) loss 1.1504 (0.8570) lr 3.4549e-03 eta 0:02:28
epoch [19/30] batch [16/40] time 0.285 (0.314) data 0.000 (0.026) loss 0.8301 (0.8179) lr 3.4549e-03 eta 0:02:25
epoch [19/30] batch [18/40] time 0.281 (0.310) data 0.000 (0.023) loss 0.7300 (0.7959) lr 3.4549e-03 eta 0:02:23
epoch [19/30] batch [20/40] time 0.275 (0.307) data 0.000 (0.021) loss 1.1943 (0.7891) lr 3.4549e-03 eta 0:02:21
epoch [19/30] batch [22/40] time 0.276 (0.304) data 0.000 (0.019) loss 0.3762 (0.7614) lr 3.4549e-03 eta 0:02:19
epoch [19/30] batch [24/40] time 0.272 (0.302) data 0.000 (0.017) loss 1.2764 (0.7550) lr 3.4549e-03 eta 0:02:17
epoch [19/30] batch [26/40] time 0.275 (0.300) data 0.000 (0.016) loss 0.4810 (0.7347) lr 3.4549e-03 eta 0:02:16
epoch [19/30] batch [28/40] time 0.277 (0.298) data 0.000 (0.015) loss 0.9067 (0.7278) lr 3.4549e-03 eta 0:02:14
epoch [19/30] batch [30/40] time 0.275 (0.296) data 0.000 (0.014) loss 0.6074 (0.7708) lr 3.4549e-03 eta 0:02:13
epoch [19/30] batch [32/40] time 0.276 (0.295) data 0.000 (0.013) loss 0.4558 (0.7512) lr 3.4549e-03 eta 0:02:12
epoch [19/30] batch [34/40] time 0.277 (0.294) data 0.000 (0.012) loss 0.4292 (0.7770) lr 3.4549e-03 eta 0:02:11
epoch [19/30] batch [36/40] time 0.282 (0.293) data 0.000 (0.011) loss 0.6084 (0.7583) lr 3.4549e-03 eta 0:02:10
epoch [19/30] batch [38/40] time 0.276 (0.293) data 0.000 (0.011) loss 0.4561 (0.7353) lr 3.4549e-03 eta 0:02:09
epoch [19/30] batch [40/40] time 0.281 (0.292) data 0.000 (0.010) loss 1.1816 (0.7382) lr 2.9663e-03 eta 0:02:08
epoch [20/30] batch [2/40] time 0.383 (0.558) data 0.000 (0.202) loss 0.2460 (0.9277) lr 2.9663e-03 eta 0:04:04
epoch [20/30] batch [4/40] time 0.283 (0.422) data 0.000 (0.101) loss 0.2524 (0.6877) lr 2.9663e-03 eta 0:03:03
epoch [20/30] batch [6/40] time 0.285 (0.375) data 0.000 (0.067) loss 0.2402 (0.5975) lr 2.9663e-03 eta 0:02:42
epoch [20/30] batch [8/40] time 0.279 (0.352) data 0.000 (0.051) loss 0.7407 (0.5852) lr 2.9663e-03 eta 0:02:32
epoch [20/30] batch [10/40] time 0.282 (0.338) data 0.000 (0.041) loss 0.3831 (0.5247) lr 2.9663e-03 eta 0:02:25
epoch [20/30] batch [12/40] time 0.283 (0.330) data 0.000 (0.034) loss 0.8589 (0.5596) lr 2.9663e-03 eta 0:02:21
epoch [20/30] batch [14/40] time 0.283 (0.324) data 0.000 (0.029) loss 1.8291 (0.6463) lr 2.9663e-03 eta 0:02:17
epoch [20/30] batch [16/40] time 0.280 (0.318) data 0.000 (0.025) loss 0.1924 (0.6812) lr 2.9663e-03 eta 0:02:14
epoch [20/30] batch [18/40] time 0.273 (0.313) data 0.000 (0.023) loss 1.4062 (0.7307) lr 2.9663e-03 eta 0:02:12
epoch [20/30] batch [20/40] time 0.277 (0.310) data 0.000 (0.020) loss 0.3335 (0.8084) lr 2.9663e-03 eta 0:02:10
epoch [20/30] batch [22/40] time 0.280 (0.307) data 0.000 (0.019) loss 2.3418 (0.8683) lr 2.9663e-03 eta 0:02:08
epoch [20/30] batch [24/40] time 0.276 (0.304) data 0.000 (0.017) loss 0.2001 (0.8519) lr 2.9663e-03 eta 0:02:06
epoch [20/30] batch [26/40] time 0.274 (0.302) data 0.000 (0.016) loss 0.8965 (0.8270) lr 2.9663e-03 eta 0:02:05
epoch [20/30] batch [28/40] time 0.276 (0.300) data 0.000 (0.015) loss 0.3567 (0.8004) lr 2.9663e-03 eta 0:02:03
epoch [20/30] batch [30/40] time 0.284 (0.299) data 0.000 (0.014) loss 1.0283 (0.8046) lr 2.9663e-03 eta 0:02:02
epoch [20/30] batch [32/40] time 0.276 (0.298) data 0.000 (0.013) loss 0.4172 (0.8425) lr 2.9663e-03 eta 0:02:01
epoch [20/30] batch [34/40] time 0.276 (0.296) data 0.000 (0.012) loss 0.6704 (0.8305) lr 2.9663e-03 eta 0:02:00
epoch [20/30] batch [36/40] time 0.276 (0.295) data 0.000 (0.011) loss 0.1252 (0.7963) lr 2.9663e-03 eta 0:01:59
epoch [20/30] batch [38/40] time 0.278 (0.294) data 0.000 (0.011) loss 0.9927 (0.7861) lr 2.9663e-03 eta 0:01:58
epoch [20/30] batch [40/40] time 0.278 (0.294) data 0.000 (0.010) loss 0.2125 (0.7649) lr 2.5000e-03 eta 0:01:57
Checkpoint saved to output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1/prompt_learner/model.pth.tar-20
epoch [21/30] batch [2/40] time 0.290 (0.512) data 0.000 (0.202) loss 1.2803 (0.7460) lr 2.5000e-03 eta 0:03:23
epoch [21/30] batch [4/40] time 0.279 (0.396) data 0.000 (0.101) loss 0.9971 (0.6626) lr 2.5000e-03 eta 0:02:36
epoch [21/30] batch [6/40] time 0.284 (0.359) data 0.000 (0.068) loss 1.3203 (0.8189) lr 2.5000e-03 eta 0:02:21
epoch [21/30] batch [8/40] time 0.289 (0.341) data 0.000 (0.051) loss 0.3010 (0.7846) lr 2.5000e-03 eta 0:02:13
epoch [21/30] batch [10/40] time 0.281 (0.329) data 0.000 (0.041) loss 1.8125 (0.8742) lr 2.5000e-03 eta 0:02:08
epoch [21/30] batch [12/40] time 0.283 (0.321) data 0.000 (0.034) loss 0.5454 (0.8369) lr 2.5000e-03 eta 0:02:04
epoch [21/30] batch [14/40] time 0.293 (0.317) data 0.000 (0.029) loss 0.9268 (0.7993) lr 2.5000e-03 eta 0:02:02
epoch [21/30] batch [16/40] time 0.283 (0.313) data 0.000 (0.025) loss 0.1382 (0.7941) lr 2.5000e-03 eta 0:02:00
epoch [21/30] batch [18/40] time 0.281 (0.309) data 0.000 (0.023) loss 0.8882 (0.7835) lr 2.5000e-03 eta 0:01:58
epoch [21/30] batch [20/40] time 0.274 (0.305) data 0.000 (0.020) loss 0.4646 (0.7786) lr 2.5000e-03 eta 0:01:56
epoch [21/30] batch [22/40] time 0.281 (0.303) data 0.000 (0.019) loss 0.1707 (0.7303) lr 2.5000e-03 eta 0:01:54
epoch [21/30] batch [24/40] time 0.278 (0.301) data 0.000 (0.017) loss 0.2798 (0.6893) lr 2.5000e-03 eta 0:01:53
epoch [21/30] batch [26/40] time 0.279 (0.299) data 0.000 (0.016) loss 0.5566 (0.6784) lr 2.5000e-03 eta 0:01:52
epoch [21/30] batch [28/40] time 0.276 (0.298) data 0.000 (0.015) loss 0.7646 (0.6766) lr 2.5000e-03 eta 0:01:50
epoch [21/30] batch [30/40] time 0.275 (0.297) data 0.000 (0.014) loss 0.2290 (0.6462) lr 2.5000e-03 eta 0:01:49
epoch [21/30] batch [32/40] time 0.280 (0.296) data 0.000 (0.013) loss 0.5698 (0.6450) lr 2.5000e-03 eta 0:01:48
epoch [21/30] batch [34/40] time 0.276 (0.294) data 0.000 (0.012) loss 0.4543 (0.6392) lr 2.5000e-03 eta 0:01:47
epoch [21/30] batch [36/40] time 0.277 (0.293) data 0.000 (0.011) loss 2.1016 (0.6919) lr 2.5000e-03 eta 0:01:46
epoch [21/30] batch [38/40] time 0.275 (0.292) data 0.000 (0.011) loss 0.5220 (0.6959) lr 2.5000e-03 eta 0:01:45
epoch [21/30] batch [40/40] time 0.283 (0.292) data 0.000 (0.010) loss 1.9619 (0.7172) lr 2.0611e-03 eta 0:01:45
epoch [22/30] batch [2/40] time 0.279 (0.522) data 0.000 (0.212) loss 0.1943 (0.5239) lr 2.0611e-03 eta 0:03:06
epoch [22/30] batch [4/40] time 0.284 (0.403) data 0.000 (0.106) loss 0.3120 (0.6976) lr 2.0611e-03 eta 0:02:23
epoch [22/30] batch [6/40] time 0.293 (0.365) data 0.000 (0.071) loss 1.4033 (0.9032) lr 2.0611e-03 eta 0:02:09
epoch [22/30] batch [8/40] time 0.396 (0.360) data 0.000 (0.053) loss 0.3047 (0.7982) lr 2.0611e-03 eta 0:02:06
epoch [22/30] batch [10/40] time 0.289 (0.346) data 0.000 (0.043) loss 0.9604 (0.7739) lr 2.0611e-03 eta 0:02:01
epoch [22/30] batch [12/40] time 0.290 (0.337) data 0.000 (0.036) loss 1.7646 (0.8455) lr 2.0611e-03 eta 0:01:57
epoch [22/30] batch [14/40] time 0.291 (0.331) data 0.000 (0.030) loss 1.8379 (0.9477) lr 2.0611e-03 eta 0:01:54
epoch [22/30] batch [16/40] time 0.291 (0.326) data 0.000 (0.027) loss 0.5972 (0.8998) lr 2.0611e-03 eta 0:01:52
epoch [22/30] batch [18/40] time 0.275 (0.321) data 0.000 (0.024) loss 1.3838 (0.8932) lr 2.0611e-03 eta 0:01:49
epoch [22/30] batch [20/40] time 0.275 (0.316) data 0.000 (0.021) loss 0.0816 (0.8161) lr 2.0611e-03 eta 0:01:47
epoch [22/30] batch [22/40] time 0.278 (0.313) data 0.000 (0.019) loss 0.1764 (0.8281) lr 2.0611e-03 eta 0:01:45
epoch [22/30] batch [24/40] time 0.278 (0.310) data 0.000 (0.018) loss 0.1296 (0.7760) lr 2.0611e-03 eta 0:01:44
epoch [22/30] batch [26/40] time 0.279 (0.308) data 0.000 (0.017) loss 0.7197 (0.7781) lr 2.0611e-03 eta 0:01:42
epoch [22/30] batch [28/40] time 0.275 (0.305) data 0.000 (0.015) loss 1.6309 (0.7904) lr 2.0611e-03 eta 0:01:41
epoch [22/30] batch [30/40] time 0.274 (0.303) data 0.000 (0.014) loss 0.2294 (0.7557) lr 2.0611e-03 eta 0:01:40
epoch [22/30] batch [32/40] time 0.277 (0.302) data 0.000 (0.013) loss 0.1035 (0.7539) lr 2.0611e-03 eta 0:01:38
epoch [22/30] batch [34/40] time 0.276 (0.300) data 0.000 (0.013) loss 1.1973 (0.7565) lr 2.0611e-03 eta 0:01:37
epoch [22/30] batch [36/40] time 0.276 (0.299) data 0.000 (0.012) loss 0.9043 (0.7924) lr 2.0611e-03 eta 0:01:36
epoch [22/30] batch [38/40] time 0.274 (0.298) data 0.000 (0.011) loss 0.7021 (0.7838) lr 2.0611e-03 eta 0:01:35
epoch [22/30] batch [40/40] time 0.279 (0.297) data 0.000 (0.011) loss 0.2323 (0.7601) lr 1.6543e-03 eta 0:01:34
epoch [23/30] batch [2/40] time 0.282 (0.518) data 0.000 (0.204) loss 0.3030 (0.4645) lr 1.6543e-03 eta 0:02:44
epoch [23/30] batch [4/40] time 0.290 (0.403) data 0.000 (0.102) loss 0.3083 (0.7231) lr 1.6543e-03 eta 0:02:07
epoch [23/30] batch [6/40] time 0.283 (0.363) data 0.000 (0.068) loss 0.2419 (0.6411) lr 1.6543e-03 eta 0:01:53
epoch [23/30] batch [8/40] time 0.293 (0.344) data 0.000 (0.051) loss 1.5947 (0.8296) lr 1.6543e-03 eta 0:01:47
epoch [23/30] batch [10/40] time 0.282 (0.332) data 0.000 (0.041) loss 0.2937 (0.7421) lr 1.6543e-03 eta 0:01:42
epoch [23/30] batch [12/40] time 0.288 (0.324) data 0.000 (0.034) loss 2.0137 (0.8427) lr 1.6543e-03 eta 0:01:39
epoch [23/30] batch [14/40] time 0.279 (0.318) data 0.000 (0.029) loss 0.1307 (0.8274) lr 1.6543e-03 eta 0:01:37
epoch [23/30] batch [16/40] time 0.287 (0.314) data 0.000 (0.026) loss 0.5669 (0.7945) lr 1.6543e-03 eta 0:01:35
epoch [23/30] batch [18/40] time 0.279 (0.310) data 0.000 (0.023) loss 0.8228 (0.7685) lr 1.6543e-03 eta 0:01:33
epoch [23/30] batch [20/40] time 0.271 (0.306) data 0.000 (0.021) loss 0.7090 (0.7663) lr 1.6543e-03 eta 0:01:31
epoch [23/30] batch [22/40] time 0.273 (0.303) data 0.000 (0.019) loss 0.4307 (0.7344) lr 1.6543e-03 eta 0:01:30
epoch [23/30] batch [24/40] time 0.275 (0.301) data 0.000 (0.017) loss 0.6470 (0.7122) lr 1.6543e-03 eta 0:01:29
epoch [23/30] batch [26/40] time 0.274 (0.299) data 0.000 (0.016) loss 0.1913 (0.6707) lr 1.6543e-03 eta 0:01:27
epoch [23/30] batch [28/40] time 0.278 (0.298) data 0.000 (0.015) loss 0.1216 (0.7407) lr 1.6543e-03 eta 0:01:26
epoch [23/30] batch [30/40] time 0.275 (0.296) data 0.000 (0.014) loss 0.8564 (0.7323) lr 1.6543e-03 eta 0:01:25
epoch [23/30] batch [32/40] time 0.278 (0.295) data 0.000 (0.013) loss 2.1484 (0.7651) lr 1.6543e-03 eta 0:01:25
epoch [23/30] batch [34/40] time 0.276 (0.294) data 0.000 (0.012) loss 0.5874 (0.7656) lr 1.6543e-03 eta 0:01:24
epoch [23/30] batch [36/40] time 0.279 (0.293) data 0.000 (0.012) loss 0.3928 (0.7394) lr 1.6543e-03 eta 0:01:23
epoch [23/30] batch [38/40] time 0.277 (0.295) data 0.000 (0.011) loss 0.3669 (0.7249) lr 1.6543e-03 eta 0:01:23
epoch [23/30] batch [40/40] time 0.279 (0.294) data 0.000 (0.010) loss 0.2690 (0.7073) lr 1.2843e-03 eta 0:01:22
epoch [24/30] batch [2/40] time 0.290 (0.532) data 0.000 (0.216) loss 1.3594 (0.7103) lr 1.2843e-03 eta 0:02:28
epoch [24/30] batch [4/40] time 0.282 (0.409) data 0.000 (0.108) loss 0.3298 (0.5026) lr 1.2843e-03 eta 0:01:52
epoch [24/30] batch [6/40] time 0.281 (0.366) data 0.000 (0.072) loss 0.2037 (0.4363) lr 1.2843e-03 eta 0:01:40
epoch [24/30] batch [8/40] time 0.288 (0.346) data 0.000 (0.054) loss 0.8633 (0.4630) lr 1.2843e-03 eta 0:01:34
epoch [24/30] batch [10/40] time 0.278 (0.334) data 0.000 (0.043) loss 0.4482 (0.4303) lr 1.2843e-03 eta 0:01:30
epoch [24/30] batch [12/40] time 0.285 (0.325) data 0.000 (0.036) loss 0.1410 (0.4249) lr 1.2843e-03 eta 0:01:27
epoch [24/30] batch [14/40] time 0.287 (0.319) data 0.000 (0.031) loss 1.1924 (0.4793) lr 1.2843e-03 eta 0:01:24
epoch [24/30] batch [16/40] time 0.287 (0.315) data 0.000 (0.027) loss 1.3281 (0.5282) lr 1.2843e-03 eta 0:01:23
epoch [24/30] batch [18/40] time 0.276 (0.311) data 0.000 (0.024) loss 0.9395 (0.5526) lr 1.2843e-03 eta 0:01:21
epoch [24/30] batch [20/40] time 0.280 (0.308) data 0.000 (0.022) loss 0.6021 (0.5602) lr 1.2843e-03 eta 0:01:20
epoch [24/30] batch [22/40] time 0.277 (0.305) data 0.000 (0.020) loss 0.2025 (0.5601) lr 1.2843e-03 eta 0:01:18
epoch [24/30] batch [24/40] time 0.279 (0.303) data 0.000 (0.018) loss 0.3550 (0.5511) lr 1.2843e-03 eta 0:01:17
epoch [24/30] batch [26/40] time 0.274 (0.301) data 0.000 (0.017) loss 0.3093 (0.5453) lr 1.2843e-03 eta 0:01:16
epoch [24/30] batch [28/40] time 0.275 (0.299) data 0.000 (0.016) loss 0.4087 (0.5453) lr 1.2843e-03 eta 0:01:15
epoch [24/30] batch [30/40] time 0.277 (0.297) data 0.000 (0.015) loss 0.6333 (0.5373) lr 1.2843e-03 eta 0:01:14
epoch [24/30] batch [32/40] time 0.281 (0.297) data 0.000 (0.014) loss 0.4348 (0.5326) lr 1.2843e-03 eta 0:01:13
epoch [24/30] batch [34/40] time 0.276 (0.295) data 0.000 (0.013) loss 0.3752 (0.5189) lr 1.2843e-03 eta 0:01:12
epoch [24/30] batch [36/40] time 0.277 (0.294) data 0.000 (0.012) loss 0.2629 (0.5021) lr 1.2843e-03 eta 0:01:11
epoch [24/30] batch [38/40] time 0.279 (0.293) data 0.000 (0.012) loss 0.4919 (0.5026) lr 1.2843e-03 eta 0:01:11
epoch [24/30] batch [40/40] time 0.283 (0.293) data 0.000 (0.011) loss 1.2041 (0.5176) lr 9.5492e-04 eta 0:01:10
epoch [25/30] batch [2/40] time 0.279 (0.522) data 0.000 (0.208) loss 0.5337 (0.4753) lr 9.5492e-04 eta 0:02:04
epoch [25/30] batch [4/40] time 0.276 (0.402) data 0.000 (0.104) loss 1.4668 (0.7138) lr 9.5492e-04 eta 0:01:34
epoch [25/30] batch [6/40] time 0.293 (0.365) data 0.000 (0.069) loss 0.4333 (0.7948) lr 9.5492e-04 eta 0:01:25
epoch [25/30] batch [8/40] time 0.297 (0.347) data 0.000 (0.052) loss 0.3813 (0.6869) lr 9.5492e-04 eta 0:01:20
epoch [25/30] batch [10/40] time 0.297 (0.338) data 0.000 (0.042) loss 0.2441 (0.5964) lr 9.5492e-04 eta 0:01:17
epoch [25/30] batch [12/40] time 0.294 (0.331) data 0.000 (0.035) loss 1.3721 (0.6943) lr 9.5492e-04 eta 0:01:15
epoch [25/30] batch [14/40] time 0.302 (0.326) data 0.000 (0.030) loss 1.0928 (0.7808) lr 9.5492e-04 eta 0:01:13
epoch [25/30] batch [16/40] time 0.292 (0.322) data 0.000 (0.026) loss 0.2561 (0.7215) lr 9.5492e-04 eta 0:01:12
epoch [25/30] batch [18/40] time 0.288 (0.318) data 0.000 (0.023) loss 0.4534 (0.7314) lr 9.5492e-04 eta 0:01:10
epoch [25/30] batch [20/40] time 0.287 (0.315) data 0.000 (0.021) loss 1.6377 (0.7862) lr 9.5492e-04 eta 0:01:09
epoch [25/30] batch [22/40] time 0.289 (0.313) data 0.000 (0.019) loss 0.5537 (0.7624) lr 9.5492e-04 eta 0:01:08
epoch [25/30] batch [24/40] time 0.287 (0.310) data 0.000 (0.018) loss 1.0293 (0.7704) lr 9.5492e-04 eta 0:01:07
epoch [25/30] batch [26/40] time 0.289 (0.309) data 0.000 (0.016) loss 1.0469 (0.7766) lr 9.5492e-04 eta 0:01:06
epoch [25/30] batch [28/40] time 0.293 (0.307) data 0.000 (0.015) loss 0.4099 (0.7821) lr 9.5492e-04 eta 0:01:05
epoch [25/30] batch [30/40] time 0.284 (0.306) data 0.000 (0.014) loss 0.1130 (0.7405) lr 9.5492e-04 eta 0:01:04
epoch [25/30] batch [32/40] time 0.289 (0.305) data 0.000 (0.013) loss 0.4370 (0.7134) lr 9.5492e-04 eta 0:01:03
epoch [25/30] batch [34/40] time 0.290 (0.304) data 0.000 (0.012) loss 0.5581 (0.7124) lr 9.5492e-04 eta 0:01:02
epoch [25/30] batch [36/40] time 0.288 (0.303) data 0.000 (0.012) loss 0.4683 (0.6969) lr 9.5492e-04 eta 0:01:01
epoch [25/30] batch [38/40] time 0.289 (0.302) data 0.000 (0.011) loss 0.3586 (0.6744) lr 9.5492e-04 eta 0:01:01
epoch [25/30] batch [40/40] time 0.287 (0.302) data 0.000 (0.011) loss 0.7007 (0.6726) lr 6.6987e-04 eta 0:01:00
epoch [26/30] batch [2/40] time 0.291 (0.527) data 0.000 (0.207) loss 0.7432 (0.4154) lr 6.6987e-04 eta 0:01:44
epoch [26/30] batch [4/40] time 0.392 (0.435) data 0.000 (0.104) loss 0.2419 (0.3851) lr 6.6987e-04 eta 0:01:25
epoch [26/30] batch [6/40] time 0.293 (0.390) data 0.000 (0.069) loss 0.7148 (0.4099) lr 6.6987e-04 eta 0:01:15
epoch [26/30] batch [8/40] time 0.292 (0.366) data 0.000 (0.052) loss 0.2905 (0.3766) lr 6.6987e-04 eta 0:01:10
epoch [26/30] batch [10/40] time 0.290 (0.351) data 0.000 (0.042) loss 1.0703 (0.4214) lr 6.6987e-04 eta 0:01:06
epoch [26/30] batch [12/40] time 0.295 (0.342) data 0.000 (0.035) loss 0.3394 (0.4662) lr 6.6987e-04 eta 0:01:04
epoch [26/30] batch [14/40] time 0.283 (0.333) data 0.000 (0.030) loss 0.8086 (0.4665) lr 6.6987e-04 eta 0:01:01
epoch [26/30] batch [16/40] time 0.286 (0.327) data 0.000 (0.026) loss 0.4290 (0.4933) lr 6.6987e-04 eta 0:01:00
epoch [26/30] batch [18/40] time 0.271 (0.321) data 0.000 (0.023) loss 1.4424 (0.5421) lr 6.6987e-04 eta 0:00:58
epoch [26/30] batch [20/40] time 0.278 (0.317) data 0.000 (0.021) loss 0.8965 (0.5405) lr 6.6987e-04 eta 0:00:57
epoch [26/30] batch [22/40] time 0.278 (0.313) data 0.000 (0.019) loss 1.0605 (0.5773) lr 6.6987e-04 eta 0:00:55
epoch [26/30] batch [24/40] time 0.284 (0.311) data 0.000 (0.017) loss 0.3420 (0.6213) lr 6.6987e-04 eta 0:00:54
epoch [26/30] batch [26/40] time 0.280 (0.309) data 0.000 (0.016) loss 0.3899 (0.6340) lr 6.6987e-04 eta 0:00:53
epoch [26/30] batch [28/40] time 0.273 (0.306) data 0.000 (0.015) loss 1.0127 (0.6418) lr 6.6987e-04 eta 0:00:52
epoch [26/30] batch [30/40] time 0.277 (0.304) data 0.000 (0.014) loss 2.3418 (0.7017) lr 6.6987e-04 eta 0:00:51
epoch [26/30] batch [32/40] time 0.280 (0.302) data 0.000 (0.013) loss 0.6821 (0.6962) lr 6.6987e-04 eta 0:00:50
epoch [26/30] batch [34/40] time 0.281 (0.301) data 0.000 (0.012) loss 3.2637 (0.7578) lr 6.6987e-04 eta 0:00:49
epoch [26/30] batch [36/40] time 0.279 (0.300) data 0.000 (0.012) loss 1.7539 (0.7737) lr 6.6987e-04 eta 0:00:49
epoch [26/30] batch [38/40] time 0.273 (0.299) data 0.000 (0.011) loss 1.7930 (0.7865) lr 6.6987e-04 eta 0:00:48
epoch [26/30] batch [40/40] time 0.276 (0.298) data 0.000 (0.011) loss 0.8667 (0.7780) lr 4.3227e-04 eta 0:00:47
epoch [27/30] batch [2/40] time 0.283 (0.520) data 0.000 (0.209) loss 0.4893 (0.5605) lr 4.3227e-04 eta 0:01:22
epoch [27/30] batch [4/40] time 0.285 (0.401) data 0.000 (0.105) loss 0.8120 (0.5323) lr 4.3227e-04 eta 0:01:02
epoch [27/30] batch [6/40] time 0.285 (0.362) data 0.000 (0.070) loss 0.7339 (0.6548) lr 4.3227e-04 eta 0:00:55
epoch [27/30] batch [8/40] time 0.294 (0.343) data 0.000 (0.052) loss 0.4641 (0.5712) lr 4.3227e-04 eta 0:00:52
epoch [27/30] batch [10/40] time 0.290 (0.332) data 0.000 (0.042) loss 0.6323 (0.5894) lr 4.3227e-04 eta 0:00:49
epoch [27/30] batch [12/40] time 0.283 (0.324) data 0.000 (0.035) loss 0.2959 (0.6559) lr 4.3227e-04 eta 0:00:48
epoch [27/30] batch [14/40] time 0.289 (0.319) data 0.000 (0.030) loss 0.0853 (0.5749) lr 4.3227e-04 eta 0:00:46
epoch [27/30] batch [16/40] time 0.291 (0.315) data 0.000 (0.026) loss 0.2690 (0.5484) lr 4.3227e-04 eta 0:00:45
epoch [27/30] batch [18/40] time 0.277 (0.311) data 0.000 (0.023) loss 1.2168 (0.6073) lr 4.3227e-04 eta 0:00:44
epoch [27/30] batch [20/40] time 0.290 (0.308) data 0.000 (0.021) loss 1.0361 (0.6443) lr 4.3227e-04 eta 0:00:43
epoch [27/30] batch [22/40] time 0.275 (0.305) data 0.000 (0.019) loss 0.6641 (0.6546) lr 4.3227e-04 eta 0:00:42
epoch [27/30] batch [24/40] time 0.274 (0.303) data 0.000 (0.018) loss 0.8354 (0.6823) lr 4.3227e-04 eta 0:00:41
epoch [27/30] batch [26/40] time 0.281 (0.301) data 0.000 (0.016) loss 0.4099 (0.6526) lr 4.3227e-04 eta 0:00:40
epoch [27/30] batch [28/40] time 0.276 (0.299) data 0.000 (0.015) loss 0.4170 (0.6395) lr 4.3227e-04 eta 0:00:39
epoch [27/30] batch [30/40] time 0.277 (0.298) data 0.000 (0.014) loss 0.2130 (0.6088) lr 4.3227e-04 eta 0:00:38
epoch [27/30] batch [32/40] time 0.273 (0.296) data 0.000 (0.013) loss 0.2399 (0.5934) lr 4.3227e-04 eta 0:00:37
epoch [27/30] batch [34/40] time 0.274 (0.295) data 0.000 (0.013) loss 0.3010 (0.5749) lr 4.3227e-04 eta 0:00:37
epoch [27/30] batch [36/40] time 0.278 (0.294) data 0.000 (0.012) loss 0.6089 (0.5644) lr 4.3227e-04 eta 0:00:36
epoch [27/30] batch [38/40] time 0.282 (0.294) data 0.000 (0.011) loss 0.0763 (0.5671) lr 4.3227e-04 eta 0:00:35
epoch [27/30] batch [40/40] time 0.279 (0.293) data 0.000 (0.011) loss 0.6128 (0.5699) lr 2.4472e-04 eta 0:00:35
epoch [28/30] batch [2/40] time 0.294 (0.516) data 0.000 (0.199) loss 0.7026 (0.4337) lr 2.4472e-04 eta 0:01:00
epoch [28/30] batch [4/40] time 0.286 (0.403) data 0.000 (0.100) loss 4.0000 (1.3148) lr 2.4472e-04 eta 0:00:46
epoch [28/30] batch [6/40] time 0.288 (0.364) data 0.000 (0.066) loss 0.5879 (0.9934) lr 2.4472e-04 eta 0:00:41
epoch [28/30] batch [8/40] time 0.291 (0.345) data 0.000 (0.050) loss 0.6621 (0.8995) lr 2.4472e-04 eta 0:00:38
epoch [28/30] batch [10/40] time 0.286 (0.333) data 0.000 (0.040) loss 0.4312 (0.7972) lr 2.4472e-04 eta 0:00:36
epoch [28/30] batch [12/40] time 0.283 (0.325) data 0.000 (0.033) loss 1.6621 (0.8271) lr 2.4472e-04 eta 0:00:35
epoch [28/30] batch [14/40] time 0.285 (0.320) data 0.000 (0.029) loss 0.6094 (0.7986) lr 2.4472e-04 eta 0:00:33
epoch [28/30] batch [16/40] time 0.278 (0.315) data 0.000 (0.025) loss 2.6250 (0.9146) lr 2.4472e-04 eta 0:00:32
epoch [28/30] batch [18/40] time 0.277 (0.310) data 0.000 (0.022) loss 0.9893 (0.9068) lr 2.4472e-04 eta 0:00:31
epoch [28/30] batch [20/40] time 0.280 (0.307) data 0.000 (0.020) loss 0.4788 (0.9444) lr 2.4472e-04 eta 0:00:30
epoch [28/30] batch [22/40] time 0.276 (0.305) data 0.000 (0.018) loss 1.3926 (1.0633) lr 2.4472e-04 eta 0:00:29
epoch [28/30] batch [24/40] time 0.277 (0.302) data 0.000 (0.017) loss 0.8804 (1.0197) lr 2.4472e-04 eta 0:00:29
epoch [28/30] batch [26/40] time 0.385 (0.304) data 0.000 (0.016) loss 0.5762 (0.9780) lr 2.4472e-04 eta 0:00:28
epoch [28/30] batch [28/40] time 0.276 (0.302) data 0.000 (0.014) loss 0.2219 (0.9315) lr 2.4472e-04 eta 0:00:27
epoch [28/30] batch [30/40] time 0.279 (0.301) data 0.000 (0.013) loss 1.5508 (0.9469) lr 2.4472e-04 eta 0:00:27
epoch [28/30] batch [32/40] time 0.277 (0.299) data 0.000 (0.013) loss 0.2825 (0.9022) lr 2.4472e-04 eta 0:00:26
epoch [28/30] batch [34/40] time 0.274 (0.298) data 0.000 (0.012) loss 0.3691 (0.8840) lr 2.4472e-04 eta 0:00:25
epoch [28/30] batch [36/40] time 0.278 (0.296) data 0.000 (0.011) loss 0.4502 (0.8689) lr 2.4472e-04 eta 0:00:24
epoch [28/30] batch [38/40] time 0.276 (0.295) data 0.000 (0.011) loss 3.5859 (0.9204) lr 2.4472e-04 eta 0:00:24
epoch [28/30] batch [40/40] time 0.278 (0.295) data 0.000 (0.010) loss 0.7378 (0.9063) lr 1.0926e-04 eta 0:00:23
epoch [29/30] batch [2/40] time 0.290 (0.519) data 0.000 (0.205) loss 0.4553 (0.9777) lr 1.0926e-04 eta 0:00:40
epoch [29/30] batch [4/40] time 0.285 (0.401) data 0.000 (0.102) loss 0.3550 (0.7386) lr 1.0926e-04 eta 0:00:30
epoch [29/30] batch [6/40] time 0.302 (0.365) data 0.000 (0.068) loss 0.4121 (0.8451) lr 1.0926e-04 eta 0:00:27
epoch [29/30] batch [8/40] time 0.286 (0.346) data 0.000 (0.051) loss 0.6094 (0.7562) lr 1.0926e-04 eta 0:00:24
epoch [29/30] batch [10/40] time 0.284 (0.334) data 0.000 (0.041) loss 0.2756 (0.7084) lr 1.0926e-04 eta 0:00:23
epoch [29/30] batch [12/40] time 0.292 (0.326) data 0.000 (0.034) loss 0.7007 (0.6687) lr 1.0926e-04 eta 0:00:22
epoch [29/30] batch [14/40] time 0.282 (0.320) data 0.000 (0.029) loss 0.5049 (0.6402) lr 1.0926e-04 eta 0:00:21
epoch [29/30] batch [16/40] time 0.282 (0.316) data 0.000 (0.026) loss 1.3350 (0.7595) lr 1.0926e-04 eta 0:00:20
epoch [29/30] batch [18/40] time 0.282 (0.312) data 0.000 (0.023) loss 0.2639 (0.7451) lr 1.0926e-04 eta 0:00:19
epoch [29/30] batch [20/40] time 0.277 (0.309) data 0.000 (0.021) loss 0.2891 (0.6935) lr 1.0926e-04 eta 0:00:18
epoch [29/30] batch [22/40] time 0.276 (0.306) data 0.000 (0.019) loss 0.1285 (0.6488) lr 1.0926e-04 eta 0:00:17
epoch [29/30] batch [24/40] time 0.275 (0.303) data 0.000 (0.017) loss 0.1917 (0.6985) lr 1.0926e-04 eta 0:00:16
epoch [29/30] batch [26/40] time 0.277 (0.302) data 0.000 (0.016) loss 0.2600 (0.6875) lr 1.0926e-04 eta 0:00:16
epoch [29/30] batch [28/40] time 0.278 (0.300) data 0.000 (0.015) loss 0.3242 (0.6607) lr 1.0926e-04 eta 0:00:15
epoch [29/30] batch [30/40] time 0.280 (0.299) data 0.000 (0.014) loss 0.3755 (0.6580) lr 1.0926e-04 eta 0:00:14
epoch [29/30] batch [32/40] time 0.276 (0.297) data 0.000 (0.013) loss 0.8896 (0.6651) lr 1.0926e-04 eta 0:00:14
epoch [29/30] batch [34/40] time 0.273 (0.296) data 0.000 (0.012) loss 0.9009 (0.6586) lr 1.0926e-04 eta 0:00:13
epoch [29/30] batch [36/40] time 0.277 (0.295) data 0.000 (0.012) loss 0.3315 (0.6607) lr 1.0926e-04 eta 0:00:12
epoch [29/30] batch [38/40] time 0.276 (0.294) data 0.000 (0.011) loss 0.7715 (0.6970) lr 1.0926e-04 eta 0:00:12
epoch [29/30] batch [40/40] time 0.275 (0.293) data 0.000 (0.010) loss 0.9673 (0.7070) lr 2.7391e-05 eta 0:00:11
epoch [30/30] batch [2/40] time 0.296 (0.533) data 0.000 (0.221) loss 0.6255 (0.7358) lr 2.7391e-05 eta 0:00:20
epoch [30/30] batch [4/40] time 0.285 (0.409) data 0.000 (0.110) loss 0.1793 (0.7106) lr 2.7391e-05 eta 0:00:14
epoch [30/30] batch [6/40] time 0.287 (0.368) data 0.000 (0.074) loss 1.0000 (0.6704) lr 2.7391e-05 eta 0:00:12
epoch [30/30] batch [8/40] time 0.293 (0.349) data 0.000 (0.055) loss 0.2781 (0.5579) lr 2.7391e-05 eta 0:00:11
epoch [30/30] batch [10/40] time 0.284 (0.336) data 0.000 (0.044) loss 0.9072 (0.5636) lr 2.7391e-05 eta 0:00:10
epoch [30/30] batch [12/40] time 0.291 (0.328) data 0.000 (0.037) loss 0.9609 (0.6133) lr 2.7391e-05 eta 0:00:09
epoch [30/30] batch [14/40] time 0.277 (0.321) data 0.000 (0.032) loss 0.8501 (0.6745) lr 2.7391e-05 eta 0:00:08
epoch [30/30] batch [16/40] time 0.290 (0.317) data 0.000 (0.028) loss 0.5952 (0.6361) lr 2.7391e-05 eta 0:00:07
epoch [30/30] batch [18/40] time 0.279 (0.318) data 0.000 (0.025) loss 0.2544 (0.5864) lr 2.7391e-05 eta 0:00:06
epoch [30/30] batch [20/40] time 0.277 (0.314) data 0.000 (0.022) loss 0.1547 (0.5651) lr 2.7391e-05 eta 0:00:06
epoch [30/30] batch [22/40] time 0.274 (0.310) data 0.000 (0.020) loss 0.2410 (0.5514) lr 2.7391e-05 eta 0:00:05
epoch [30/30] batch [24/40] time 0.273 (0.307) data 0.000 (0.019) loss 0.1606 (0.5278) lr 2.7391e-05 eta 0:00:04
epoch [30/30] batch [26/40] time 0.276 (0.305) data 0.000 (0.017) loss 0.6753 (0.6120) lr 2.7391e-05 eta 0:00:04
epoch [30/30] batch [28/40] time 0.279 (0.303) data 0.000 (0.016) loss 0.4688 (0.6016) lr 2.7391e-05 eta 0:00:03
epoch [30/30] batch [30/40] time 0.281 (0.302) data 0.000 (0.015) loss 0.4668 (0.6262) lr 2.7391e-05 eta 0:00:03
epoch [30/30] batch [32/40] time 0.276 (0.300) data 0.000 (0.014) loss 1.0693 (0.6274) lr 2.7391e-05 eta 0:00:02
epoch [30/30] batch [34/40] time 0.275 (0.299) data 0.000 (0.013) loss 0.6709 (0.6426) lr 2.7391e-05 eta 0:00:01
epoch [30/30] batch [36/40] time 0.280 (0.298) data 0.000 (0.012) loss 0.2051 (0.6290) lr 2.7391e-05 eta 0:00:01
epoch [30/30] batch [38/40] time 0.275 (0.297) data 0.000 (0.012) loss 0.1731 (0.6150) lr 2.7391e-05 eta 0:00:00
epoch [30/30] batch [40/40] time 0.276 (0.296) data 0.000 (0.011) loss 0.1980 (0.5980) lr 0.0000e+00 eta 0:00:00
Checkpoint saved to output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1/prompt_learner/model.pth.tar-30
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/42 [00:00<?, ?it/s]  2%|         | 1/42 [00:02<01:37,  2.38s/it]  5%|         | 2/42 [00:03<00:55,  1.38s/it]  7%|         | 3/42 [00:03<00:42,  1.08s/it] 10%|         | 4/42 [00:04<00:32,  1.17it/s] 12%|        | 5/42 [00:04<00:24,  1.53it/s] 14%|        | 6/42 [00:04<00:19,  1.89it/s] 17%|        | 7/42 [00:05<00:16,  2.14it/s] 19%|        | 8/42 [00:05<00:14,  2.27it/s] 21%|       | 9/42 [00:05<00:13,  2.44it/s] 24%|       | 10/42 [00:06<00:12,  2.64it/s] 26%|       | 11/42 [00:06<00:11,  2.69it/s] 29%|       | 12/42 [00:06<00:10,  2.76it/s] 31%|       | 13/42 [00:07<00:10,  2.80it/s] 33%|      | 14/42 [00:07<00:09,  2.89it/s] 36%|      | 15/42 [00:07<00:09,  2.91it/s] 38%|      | 16/42 [00:08<00:08,  3.01it/s] 40%|      | 17/42 [00:08<00:08,  3.05it/s] 43%|     | 18/42 [00:08<00:08,  2.86it/s] 45%|     | 19/42 [00:09<00:08,  2.83it/s] 48%|     | 20/42 [00:09<00:07,  3.02it/s] 50%|     | 21/42 [00:09<00:06,  3.17it/s] 52%|    | 22/42 [00:10<00:06,  3.28it/s] 55%|    | 23/42 [00:10<00:05,  3.37it/s] 57%|    | 24/42 [00:10<00:05,  3.43it/s] 60%|    | 25/42 [00:11<00:04,  3.47it/s] 62%|   | 26/42 [00:11<00:04,  3.50it/s] 64%|   | 27/42 [00:11<00:04,  3.52it/s] 67%|   | 28/42 [00:11<00:03,  3.54it/s] 69%|   | 29/42 [00:12<00:03,  3.55it/s] 71%|  | 30/42 [00:12<00:03,  3.56it/s] 74%|  | 31/42 [00:12<00:03,  3.56it/s] 76%|  | 32/42 [00:12<00:02,  3.56it/s] 79%|  | 33/42 [00:13<00:02,  3.57it/s] 81%|  | 34/42 [00:13<00:02,  3.57it/s] 83%| | 35/42 [00:13<00:01,  3.57it/s] 86%| | 36/42 [00:14<00:01,  3.57it/s] 88%| | 37/42 [00:14<00:01,  3.57it/s] 90%| | 38/42 [00:14<00:01,  3.57it/s] 93%|| 39/42 [00:14<00:00,  3.58it/s] 95%|| 40/42 [00:15<00:00,  3.58it/s] 98%|| 41/42 [00:15<00:00,  3.58it/s]100%|| 42/42 [00:15<00:00,  4.39it/s]100%|| 42/42 [00:15<00:00,  2.67it/s]
=> result
* total: 8,100
* correct: 7,361
* accuracy: 90.9%
* error: 9.1%
* macro_f1: 90.6%
Elapsed: 0:04:17
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_train.sh eurosat 2 0 main_final1212 16 RPO_prime
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_2.pkl
160 5400 8100
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      5,400
# test     8,100
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/30] batch [2/40] time 0.285 (1.408) data 0.000 (0.408) loss 5.8008 (5.6562) lr 1.0000e-02 eta 0:28:07
epoch [1/30] batch [4/40] time 0.281 (0.846) data 0.000 (0.204) loss 5.2305 (5.5771) lr 1.0000e-02 eta 0:16:51
epoch [1/30] batch [6/40] time 0.284 (0.658) data 0.000 (0.136) loss 4.5430 (5.2025) lr 1.0000e-02 eta 0:13:06
epoch [1/30] batch [8/40] time 0.296 (0.567) data 0.000 (0.102) loss 4.4688 (5.0430) lr 1.0000e-02 eta 0:11:15
epoch [1/30] batch [10/40] time 0.299 (0.525) data 0.000 (0.082) loss 4.8438 (4.9734) lr 1.0000e-02 eta 0:10:24
epoch [1/30] batch [12/40] time 0.302 (0.487) data 0.000 (0.068) loss 4.8242 (4.9759) lr 1.0000e-02 eta 0:09:38
epoch [1/30] batch [14/40] time 0.298 (0.460) data 0.000 (0.058) loss 4.6836 (4.9408) lr 1.0000e-02 eta 0:09:05
epoch [1/30] batch [16/40] time 0.293 (0.440) data 0.000 (0.051) loss 4.1641 (4.8367) lr 1.0000e-02 eta 0:08:40
epoch [1/30] batch [18/40] time 0.292 (0.423) data 0.000 (0.046) loss 4.4023 (4.8012) lr 1.0000e-02 eta 0:08:19
epoch [1/30] batch [20/40] time 0.289 (0.410) data 0.000 (0.041) loss 4.5469 (4.7676) lr 1.0000e-02 eta 0:08:03
epoch [1/30] batch [22/40] time 0.287 (0.398) data 0.000 (0.037) loss 4.2344 (4.7656) lr 1.0000e-02 eta 0:07:49
epoch [1/30] batch [24/40] time 0.285 (0.389) data 0.000 (0.034) loss 4.5938 (4.7808) lr 1.0000e-02 eta 0:07:37
epoch [1/30] batch [26/40] time 0.287 (0.381) data 0.000 (0.032) loss 4.4961 (4.7634) lr 1.0000e-02 eta 0:07:27
epoch [1/30] batch [28/40] time 0.285 (0.374) data 0.000 (0.029) loss 4.2109 (4.7313) lr 1.0000e-02 eta 0:07:18
epoch [1/30] batch [30/40] time 0.285 (0.368) data 0.000 (0.027) loss 4.2656 (4.7072) lr 1.0000e-02 eta 0:07:11
epoch [1/30] batch [32/40] time 0.290 (0.363) data 0.000 (0.026) loss 4.8750 (4.7090) lr 1.0000e-02 eta 0:07:04
epoch [1/30] batch [34/40] time 0.289 (0.359) data 0.000 (0.024) loss 3.9668 (4.6768) lr 1.0000e-02 eta 0:06:58
epoch [1/30] batch [36/40] time 0.289 (0.355) data 0.000 (0.023) loss 4.2930 (4.6699) lr 1.0000e-02 eta 0:06:53
epoch [1/30] batch [38/40] time 0.287 (0.352) data 0.000 (0.022) loss 5.0078 (4.6841) lr 1.0000e-02 eta 0:06:48
epoch [1/30] batch [40/40] time 0.291 (0.349) data 0.000 (0.021) loss 4.2109 (4.6702) lr 9.9726e-03 eta 0:06:44
epoch [2/30] batch [2/40] time 0.286 (0.531) data 0.000 (0.218) loss 4.2383 (4.1914) lr 9.9726e-03 eta 0:10:15
epoch [2/30] batch [4/40] time 0.285 (0.410) data 0.000 (0.109) loss 4.5469 (4.3818) lr 9.9726e-03 eta 0:07:54
epoch [2/30] batch [6/40] time 0.281 (0.367) data 0.000 (0.073) loss 4.0234 (4.3288) lr 9.9726e-03 eta 0:07:03
epoch [2/30] batch [8/40] time 0.271 (0.344) data 0.000 (0.055) loss 4.6289 (4.3113) lr 9.9726e-03 eta 0:06:36
epoch [2/30] batch [10/40] time 0.284 (0.331) data 0.000 (0.044) loss 4.1914 (4.3201) lr 9.9726e-03 eta 0:06:21
epoch [2/30] batch [12/40] time 0.282 (0.324) data 0.000 (0.037) loss 3.6523 (4.2889) lr 9.9726e-03 eta 0:06:11
epoch [2/30] batch [14/40] time 0.280 (0.318) data 0.000 (0.031) loss 4.3672 (4.2914) lr 9.9726e-03 eta 0:06:04
epoch [2/30] batch [16/40] time 0.281 (0.314) data 0.000 (0.027) loss 3.5918 (4.2161) lr 9.9726e-03 eta 0:05:58
epoch [2/30] batch [18/40] time 0.280 (0.310) data 0.000 (0.024) loss 4.4414 (4.2063) lr 9.9726e-03 eta 0:05:53
epoch [2/30] batch [20/40] time 0.277 (0.307) data 0.000 (0.022) loss 4.5547 (4.2276) lr 9.9726e-03 eta 0:05:49
epoch [2/30] batch [22/40] time 0.273 (0.304) data 0.000 (0.020) loss 4.1484 (4.1903) lr 9.9726e-03 eta 0:05:45
epoch [2/30] batch [24/40] time 0.273 (0.301) data 0.000 (0.018) loss 3.9844 (4.1668) lr 9.9726e-03 eta 0:05:42
epoch [2/30] batch [26/40] time 0.277 (0.299) data 0.000 (0.017) loss 4.7617 (4.1982) lr 9.9726e-03 eta 0:05:39
epoch [2/30] batch [28/40] time 0.275 (0.298) data 0.000 (0.016) loss 5.0312 (4.2500) lr 9.9726e-03 eta 0:05:37
epoch [2/30] batch [30/40] time 0.275 (0.296) data 0.000 (0.015) loss 4.4688 (4.2538) lr 9.9726e-03 eta 0:05:34
epoch [2/30] batch [32/40] time 0.280 (0.295) data 0.000 (0.014) loss 4.0703 (4.2521) lr 9.9726e-03 eta 0:05:33
epoch [2/30] batch [34/40] time 0.276 (0.294) data 0.000 (0.013) loss 3.9570 (4.2306) lr 9.9726e-03 eta 0:05:31
epoch [2/30] batch [36/40] time 0.280 (0.293) data 0.000 (0.012) loss 4.5977 (4.2480) lr 9.9726e-03 eta 0:05:29
epoch [2/30] batch [38/40] time 0.273 (0.292) data 0.000 (0.012) loss 3.9824 (4.2257) lr 9.9726e-03 eta 0:05:27
epoch [2/30] batch [40/40] time 0.276 (0.291) data 0.000 (0.011) loss 4.0039 (4.2264) lr 9.8907e-03 eta 0:05:26
epoch [3/30] batch [2/40] time 0.287 (0.519) data 0.000 (0.210) loss 3.7305 (3.9414) lr 9.8907e-03 eta 0:09:40
epoch [3/30] batch [4/40] time 0.284 (0.401) data 0.000 (0.105) loss 3.8164 (3.9922) lr 9.8907e-03 eta 0:07:27
epoch [3/30] batch [6/40] time 0.281 (0.360) data 0.000 (0.070) loss 4.2695 (3.9401) lr 9.8907e-03 eta 0:06:41
epoch [3/30] batch [8/40] time 0.285 (0.341) data 0.000 (0.053) loss 4.8203 (4.1948) lr 9.8907e-03 eta 0:06:19
epoch [3/30] batch [10/40] time 0.282 (0.339) data 0.000 (0.042) loss 4.0508 (4.1922) lr 9.8907e-03 eta 0:06:16
epoch [3/30] batch [12/40] time 0.283 (0.330) data 0.000 (0.035) loss 3.7031 (4.0822) lr 9.8907e-03 eta 0:06:05
epoch [3/30] batch [14/40] time 0.285 (0.323) data 0.000 (0.030) loss 4.1133 (4.0140) lr 9.8907e-03 eta 0:05:57
epoch [3/30] batch [16/40] time 0.279 (0.318) data 0.000 (0.027) loss 2.9512 (3.9424) lr 9.8907e-03 eta 0:05:51
epoch [3/30] batch [18/40] time 0.275 (0.313) data 0.000 (0.024) loss 4.1211 (3.9421) lr 9.8907e-03 eta 0:05:45
epoch [3/30] batch [20/40] time 0.275 (0.309) data 0.000 (0.021) loss 4.6641 (3.9383) lr 9.8907e-03 eta 0:05:40
epoch [3/30] batch [22/40] time 0.276 (0.306) data 0.000 (0.019) loss 3.9648 (3.9005) lr 9.8907e-03 eta 0:05:36
epoch [3/30] batch [24/40] time 0.276 (0.304) data 0.000 (0.018) loss 4.8008 (3.9539) lr 9.8907e-03 eta 0:05:32
epoch [3/30] batch [26/40] time 0.272 (0.301) data 0.000 (0.016) loss 4.2422 (3.9623) lr 9.8907e-03 eta 0:05:29
epoch [3/30] batch [28/40] time 0.274 (0.299) data 0.000 (0.015) loss 3.8477 (3.9816) lr 9.8907e-03 eta 0:05:26
epoch [3/30] batch [30/40] time 0.281 (0.298) data 0.000 (0.014) loss 3.0957 (3.9617) lr 9.8907e-03 eta 0:05:24
epoch [3/30] batch [32/40] time 0.275 (0.297) data 0.000 (0.013) loss 3.6562 (3.9485) lr 9.8907e-03 eta 0:05:22
epoch [3/30] batch [34/40] time 0.277 (0.295) data 0.000 (0.013) loss 4.1094 (3.9720) lr 9.8907e-03 eta 0:05:20
epoch [3/30] batch [36/40] time 0.278 (0.294) data 0.000 (0.012) loss 4.1250 (3.9933) lr 9.8907e-03 eta 0:05:19
epoch [3/30] batch [38/40] time 0.282 (0.294) data 0.000 (0.011) loss 4.2422 (3.9918) lr 9.8907e-03 eta 0:05:17
epoch [3/30] batch [40/40] time 0.272 (0.293) data 0.000 (0.011) loss 3.3652 (3.9675) lr 9.7553e-03 eta 0:05:16
epoch [4/30] batch [2/40] time 0.281 (0.516) data 0.001 (0.203) loss 4.2617 (4.0420) lr 9.7553e-03 eta 0:09:16
epoch [4/30] batch [4/40] time 0.285 (0.400) data 0.000 (0.102) loss 3.1211 (3.6826) lr 9.7553e-03 eta 0:07:10
epoch [4/30] batch [6/40] time 0.285 (0.361) data 0.000 (0.068) loss 2.9902 (3.6305) lr 9.7553e-03 eta 0:06:28
epoch [4/30] batch [8/40] time 0.281 (0.341) data 0.000 (0.051) loss 3.8066 (3.6431) lr 9.7553e-03 eta 0:06:05
epoch [4/30] batch [10/40] time 0.280 (0.329) data 0.000 (0.041) loss 3.0645 (3.6400) lr 9.7553e-03 eta 0:05:51
epoch [4/30] batch [12/40] time 0.279 (0.321) data 0.000 (0.034) loss 3.7305 (3.5952) lr 9.7553e-03 eta 0:05:42
epoch [4/30] batch [14/40] time 0.282 (0.315) data 0.000 (0.029) loss 5.0742 (3.7094) lr 9.7553e-03 eta 0:05:36
epoch [4/30] batch [16/40] time 0.282 (0.312) data 0.000 (0.026) loss 3.0469 (3.6201) lr 9.7553e-03 eta 0:05:31
epoch [4/30] batch [18/40] time 0.272 (0.307) data 0.000 (0.023) loss 3.7070 (3.6840) lr 9.7553e-03 eta 0:05:26
epoch [4/30] batch [20/40] time 0.275 (0.304) data 0.000 (0.021) loss 3.6328 (3.6688) lr 9.7553e-03 eta 0:05:22
epoch [4/30] batch [22/40] time 0.278 (0.302) data 0.000 (0.019) loss 2.7070 (3.5981) lr 9.7553e-03 eta 0:05:19
epoch [4/30] batch [24/40] time 0.275 (0.299) data 0.000 (0.017) loss 2.9844 (3.5629) lr 9.7553e-03 eta 0:05:16
epoch [4/30] batch [26/40] time 0.275 (0.298) data 0.000 (0.016) loss 3.0664 (3.5314) lr 9.7553e-03 eta 0:05:13
epoch [4/30] batch [28/40] time 0.281 (0.296) data 0.000 (0.015) loss 3.8418 (3.5598) lr 9.7553e-03 eta 0:05:11
epoch [4/30] batch [30/40] time 0.281 (0.295) data 0.000 (0.014) loss 2.5117 (3.5215) lr 9.7553e-03 eta 0:05:09
epoch [4/30] batch [32/40] time 0.280 (0.294) data 0.000 (0.013) loss 2.2754 (3.4986) lr 9.7553e-03 eta 0:05:08
epoch [4/30] batch [34/40] time 0.277 (0.293) data 0.000 (0.012) loss 3.6699 (3.5190) lr 9.7553e-03 eta 0:05:06
epoch [4/30] batch [36/40] time 0.277 (0.292) data 0.000 (0.012) loss 2.3809 (3.4901) lr 9.7553e-03 eta 0:05:04
epoch [4/30] batch [38/40] time 0.378 (0.294) data 0.000 (0.011) loss 3.4199 (3.5044) lr 9.7553e-03 eta 0:05:06
epoch [4/30] batch [40/40] time 0.279 (0.293) data 0.000 (0.010) loss 2.4688 (3.5065) lr 9.5677e-03 eta 0:05:04
epoch [5/30] batch [2/40] time 0.290 (0.523) data 0.000 (0.210) loss 2.7578 (2.7920) lr 9.5677e-03 eta 0:09:02
epoch [5/30] batch [4/40] time 0.285 (0.403) data 0.000 (0.105) loss 2.5215 (2.6401) lr 9.5677e-03 eta 0:06:57
epoch [5/30] batch [6/40] time 0.282 (0.362) data 0.000 (0.070) loss 4.4180 (3.0384) lr 9.5677e-03 eta 0:06:14
epoch [5/30] batch [8/40] time 0.282 (0.342) data 0.000 (0.053) loss 4.5820 (3.1650) lr 9.5677e-03 eta 0:05:53
epoch [5/30] batch [10/40] time 0.295 (0.332) data 0.000 (0.042) loss 3.9531 (3.2729) lr 9.5677e-03 eta 0:05:41
epoch [5/30] batch [12/40] time 0.290 (0.324) data 0.000 (0.035) loss 3.0469 (3.2990) lr 9.5677e-03 eta 0:05:33
epoch [5/30] batch [14/40] time 0.290 (0.319) data 0.000 (0.030) loss 3.4688 (3.3096) lr 9.5677e-03 eta 0:05:27
epoch [5/30] batch [16/40] time 0.284 (0.315) data 0.000 (0.026) loss 2.5449 (3.2584) lr 9.5677e-03 eta 0:05:22
epoch [5/30] batch [18/40] time 0.273 (0.310) data 0.000 (0.024) loss 3.5645 (3.2727) lr 9.5677e-03 eta 0:05:17
epoch [5/30] batch [20/40] time 0.276 (0.307) data 0.000 (0.021) loss 3.3984 (3.2396) lr 9.5677e-03 eta 0:05:13
epoch [5/30] batch [22/40] time 0.276 (0.304) data 0.000 (0.019) loss 4.6133 (3.2642) lr 9.5677e-03 eta 0:05:09
epoch [5/30] batch [24/40] time 0.307 (0.304) data 0.000 (0.018) loss 3.0918 (3.2620) lr 9.5677e-03 eta 0:05:08
epoch [5/30] batch [26/40] time 0.275 (0.302) data 0.000 (0.016) loss 3.3242 (3.2374) lr 9.5677e-03 eta 0:05:06
epoch [5/30] batch [28/40] time 0.273 (0.300) data 0.000 (0.015) loss 3.1992 (3.2331) lr 9.5677e-03 eta 0:05:03
epoch [5/30] batch [30/40] time 0.275 (0.298) data 0.000 (0.014) loss 3.6465 (3.2416) lr 9.5677e-03 eta 0:05:01
epoch [5/30] batch [32/40] time 0.270 (0.296) data 0.000 (0.013) loss 4.2578 (3.2662) lr 9.5677e-03 eta 0:04:58
epoch [5/30] batch [34/40] time 0.271 (0.295) data 0.000 (0.013) loss 2.9961 (3.2313) lr 9.5677e-03 eta 0:04:56
epoch [5/30] batch [36/40] time 0.281 (0.294) data 0.000 (0.012) loss 2.5332 (3.2216) lr 9.5677e-03 eta 0:04:55
epoch [5/30] batch [38/40] time 0.282 (0.293) data 0.000 (0.011) loss 2.9102 (3.2146) lr 9.5677e-03 eta 0:04:53
epoch [5/30] batch [40/40] time 0.281 (0.293) data 0.000 (0.011) loss 2.2168 (3.1636) lr 9.3301e-03 eta 0:04:52
epoch [6/30] batch [2/40] time 0.281 (0.521) data 0.000 (0.207) loss 3.5684 (3.3428) lr 9.3301e-03 eta 0:08:40
epoch [6/30] batch [4/40] time 0.285 (0.403) data 0.000 (0.104) loss 1.9971 (2.8679) lr 9.3301e-03 eta 0:06:40
epoch [6/30] batch [6/40] time 0.279 (0.362) data 0.000 (0.069) loss 3.2188 (2.9471) lr 9.3301e-03 eta 0:05:59
epoch [6/30] batch [8/40] time 0.281 (0.341) data 0.000 (0.052) loss 2.6465 (2.9191) lr 9.3301e-03 eta 0:05:38
epoch [6/30] batch [10/40] time 0.282 (0.330) data 0.000 (0.042) loss 1.3965 (2.7360) lr 9.3301e-03 eta 0:05:26
epoch [6/30] batch [12/40] time 0.281 (0.322) data 0.000 (0.035) loss 3.5684 (2.8630) lr 9.3301e-03 eta 0:05:17
epoch [6/30] batch [14/40] time 0.292 (0.316) data 0.000 (0.030) loss 3.5039 (2.8844) lr 9.3301e-03 eta 0:05:12
epoch [6/30] batch [16/40] time 0.295 (0.314) data 0.000 (0.026) loss 2.0098 (2.8504) lr 9.3301e-03 eta 0:05:08
epoch [6/30] batch [18/40] time 0.279 (0.310) data 0.000 (0.023) loss 4.0820 (2.9788) lr 9.3301e-03 eta 0:05:04
epoch [6/30] batch [20/40] time 0.278 (0.306) data 0.000 (0.021) loss 2.5234 (2.9978) lr 9.3301e-03 eta 0:05:00
epoch [6/30] batch [22/40] time 0.274 (0.304) data 0.000 (0.019) loss 2.7363 (2.9852) lr 9.3301e-03 eta 0:04:56
epoch [6/30] batch [24/40] time 0.277 (0.301) data 0.000 (0.017) loss 2.5723 (3.0073) lr 9.3301e-03 eta 0:04:53
epoch [6/30] batch [26/40] time 0.274 (0.299) data 0.000 (0.016) loss 4.2695 (3.0522) lr 9.3301e-03 eta 0:04:51
epoch [6/30] batch [28/40] time 0.271 (0.297) data 0.000 (0.015) loss 3.2031 (3.0193) lr 9.3301e-03 eta 0:04:48
epoch [6/30] batch [30/40] time 0.276 (0.296) data 0.000 (0.014) loss 3.7812 (2.9868) lr 9.3301e-03 eta 0:04:47
epoch [6/30] batch [32/40] time 0.278 (0.295) data 0.000 (0.013) loss 2.4531 (2.9837) lr 9.3301e-03 eta 0:04:45
epoch [6/30] batch [34/40] time 0.277 (0.294) data 0.000 (0.012) loss 2.0977 (2.9377) lr 9.3301e-03 eta 0:04:43
epoch [6/30] batch [36/40] time 0.278 (0.293) data 0.000 (0.012) loss 3.2617 (2.9282) lr 9.3301e-03 eta 0:04:42
epoch [6/30] batch [38/40] time 0.277 (0.292) data 0.000 (0.011) loss 2.9258 (2.9654) lr 9.3301e-03 eta 0:04:40
epoch [6/30] batch [40/40] time 0.277 (0.291) data 0.000 (0.011) loss 3.8633 (2.9603) lr 9.0451e-03 eta 0:04:39
epoch [7/30] batch [2/40] time 0.278 (0.526) data 0.000 (0.210) loss 2.4121 (2.5117) lr 9.0451e-03 eta 0:08:24
epoch [7/30] batch [4/40] time 0.289 (0.405) data 0.000 (0.105) loss 3.1504 (2.7524) lr 9.0451e-03 eta 0:06:27
epoch [7/30] batch [6/40] time 0.280 (0.380) data 0.000 (0.070) loss 2.0566 (2.6344) lr 9.0451e-03 eta 0:06:02
epoch [7/30] batch [8/40] time 0.280 (0.356) data 0.000 (0.053) loss 3.5352 (2.8035) lr 9.0451e-03 eta 0:05:38
epoch [7/30] batch [10/40] time 0.290 (0.342) data 0.000 (0.042) loss 3.1660 (2.8965) lr 9.0451e-03 eta 0:05:24
epoch [7/30] batch [12/40] time 0.283 (0.332) data 0.000 (0.035) loss 2.9551 (3.0343) lr 9.0451e-03 eta 0:05:14
epoch [7/30] batch [14/40] time 0.280 (0.325) data 0.000 (0.030) loss 4.2344 (3.0730) lr 9.0451e-03 eta 0:05:07
epoch [7/30] batch [16/40] time 0.281 (0.320) data 0.000 (0.027) loss 1.7236 (3.0005) lr 9.0451e-03 eta 0:05:01
epoch [7/30] batch [18/40] time 0.266 (0.314) data 0.000 (0.024) loss 3.2539 (2.9977) lr 9.0451e-03 eta 0:04:55
epoch [7/30] batch [20/40] time 0.271 (0.309) data 0.000 (0.021) loss 2.3789 (2.9708) lr 9.0451e-03 eta 0:04:50
epoch [7/30] batch [22/40] time 0.267 (0.306) data 0.000 (0.019) loss 1.9492 (2.8585) lr 9.0451e-03 eta 0:04:46
epoch [7/30] batch [24/40] time 0.267 (0.302) data 0.000 (0.018) loss 2.0898 (2.8225) lr 9.0451e-03 eta 0:04:43
epoch [7/30] batch [26/40] time 0.270 (0.300) data 0.000 (0.016) loss 2.8398 (2.7978) lr 9.0451e-03 eta 0:04:40
epoch [7/30] batch [28/40] time 0.270 (0.298) data 0.000 (0.015) loss 3.7441 (2.8262) lr 9.0451e-03 eta 0:04:37
epoch [7/30] batch [30/40] time 0.268 (0.296) data 0.000 (0.014) loss 2.0137 (2.7781) lr 9.0451e-03 eta 0:04:35
epoch [7/30] batch [32/40] time 0.267 (0.294) data 0.000 (0.013) loss 4.4062 (2.8010) lr 9.0451e-03 eta 0:04:32
epoch [7/30] batch [34/40] time 0.271 (0.293) data 0.000 (0.013) loss 1.5400 (2.7608) lr 9.0451e-03 eta 0:04:31
epoch [7/30] batch [36/40] time 0.269 (0.291) data 0.000 (0.012) loss 5.4492 (2.8323) lr 9.0451e-03 eta 0:04:29
epoch [7/30] batch [38/40] time 0.268 (0.290) data 0.000 (0.011) loss 1.5830 (2.7788) lr 9.0451e-03 eta 0:04:27
epoch [7/30] batch [40/40] time 0.267 (0.289) data 0.000 (0.011) loss 4.5742 (2.8479) lr 8.7157e-03 eta 0:04:25
epoch [8/30] batch [2/40] time 0.284 (0.520) data 0.000 (0.203) loss 3.0352 (2.8115) lr 8.7157e-03 eta 0:07:57
epoch [8/30] batch [4/40] time 0.280 (0.400) data 0.000 (0.101) loss 3.3867 (3.0283) lr 8.7157e-03 eta 0:06:06
epoch [8/30] batch [6/40] time 0.287 (0.361) data 0.000 (0.068) loss 2.4570 (2.8424) lr 8.7157e-03 eta 0:05:29
epoch [8/30] batch [8/40] time 0.287 (0.341) data 0.000 (0.051) loss 1.8477 (2.5988) lr 8.7157e-03 eta 0:05:11
epoch [8/30] batch [10/40] time 0.290 (0.330) data 0.000 (0.041) loss 1.7383 (2.6411) lr 8.7157e-03 eta 0:05:00
epoch [8/30] batch [12/40] time 0.287 (0.323) data 0.000 (0.034) loss 2.2812 (2.7400) lr 8.7157e-03 eta 0:04:53
epoch [8/30] batch [14/40] time 0.279 (0.317) data 0.000 (0.029) loss 2.8906 (2.7579) lr 8.7157e-03 eta 0:04:47
epoch [8/30] batch [16/40] time 0.283 (0.313) data 0.000 (0.026) loss 1.9639 (2.7565) lr 8.7157e-03 eta 0:04:42
epoch [8/30] batch [18/40] time 0.277 (0.309) data 0.000 (0.023) loss 3.1348 (2.7486) lr 8.7157e-03 eta 0:04:38
epoch [8/30] batch [20/40] time 0.277 (0.306) data 0.000 (0.020) loss 1.6240 (2.6939) lr 8.7157e-03 eta 0:04:34
epoch [8/30] batch [22/40] time 0.280 (0.303) data 0.000 (0.019) loss 1.6650 (2.6071) lr 8.7157e-03 eta 0:04:32
epoch [8/30] batch [24/40] time 0.274 (0.301) data 0.000 (0.017) loss 3.8047 (2.6885) lr 8.7157e-03 eta 0:04:29
epoch [8/30] batch [26/40] time 0.276 (0.299) data 0.000 (0.016) loss 1.5771 (2.5966) lr 8.7157e-03 eta 0:04:27
epoch [8/30] batch [28/40] time 0.275 (0.297) data 0.000 (0.015) loss 3.5645 (2.6423) lr 8.7157e-03 eta 0:04:25
epoch [8/30] batch [30/40] time 0.275 (0.296) data 0.000 (0.014) loss 2.6152 (2.6520) lr 8.7157e-03 eta 0:04:23
epoch [8/30] batch [32/40] time 0.274 (0.295) data 0.000 (0.013) loss 2.7305 (2.6568) lr 8.7157e-03 eta 0:04:21
epoch [8/30] batch [34/40] time 0.272 (0.293) data 0.000 (0.012) loss 3.1523 (2.6899) lr 8.7157e-03 eta 0:04:19
epoch [8/30] batch [36/40] time 0.280 (0.293) data 0.000 (0.011) loss 2.3027 (2.6504) lr 8.7157e-03 eta 0:04:18
epoch [8/30] batch [38/40] time 0.279 (0.292) data 0.000 (0.011) loss 2.4160 (2.6188) lr 8.7157e-03 eta 0:04:17
epoch [8/30] batch [40/40] time 0.278 (0.291) data 0.000 (0.010) loss 2.2559 (2.5977) lr 8.3457e-03 eta 0:04:16
epoch [9/30] batch [2/40] time 0.285 (0.511) data 0.000 (0.206) loss 3.2109 (2.5083) lr 8.3457e-03 eta 0:07:28
epoch [9/30] batch [4/40] time 0.284 (0.397) data 0.000 (0.103) loss 3.0898 (2.6775) lr 8.3457e-03 eta 0:05:48
epoch [9/30] batch [6/40] time 0.280 (0.359) data 0.000 (0.069) loss 2.4785 (2.6864) lr 8.3457e-03 eta 0:05:13
epoch [9/30] batch [8/40] time 0.287 (0.340) data 0.000 (0.052) loss 2.4785 (2.7142) lr 8.3457e-03 eta 0:04:56
epoch [9/30] batch [10/40] time 0.279 (0.328) data 0.000 (0.041) loss 1.7129 (2.6351) lr 8.3457e-03 eta 0:04:45
epoch [9/30] batch [12/40] time 0.280 (0.320) data 0.000 (0.035) loss 1.1641 (2.4740) lr 8.3457e-03 eta 0:04:38
epoch [9/30] batch [14/40] time 0.295 (0.316) data 0.000 (0.030) loss 1.5449 (2.4193) lr 8.3457e-03 eta 0:04:33
epoch [9/30] batch [16/40] time 0.288 (0.313) data 0.000 (0.026) loss 1.2080 (2.3126) lr 8.3457e-03 eta 0:04:30
epoch [9/30] batch [18/40] time 0.274 (0.309) data 0.000 (0.023) loss 2.1777 (2.2845) lr 8.3457e-03 eta 0:04:26
epoch [9/30] batch [20/40] time 0.280 (0.306) data 0.000 (0.021) loss 1.9707 (2.3039) lr 8.3457e-03 eta 0:04:23
epoch [9/30] batch [22/40] time 0.277 (0.303) data 0.000 (0.019) loss 2.7305 (2.3622) lr 8.3457e-03 eta 0:04:20
epoch [9/30] batch [24/40] time 0.274 (0.301) data 0.000 (0.017) loss 2.5801 (2.3727) lr 8.3457e-03 eta 0:04:17
epoch [9/30] batch [26/40] time 0.278 (0.299) data 0.000 (0.016) loss 4.5430 (2.4494) lr 8.3457e-03 eta 0:04:15
epoch [9/30] batch [28/40] time 0.281 (0.301) data 0.000 (0.015) loss 1.3828 (2.3743) lr 8.3457e-03 eta 0:04:16
epoch [9/30] batch [30/40] time 0.275 (0.300) data 0.000 (0.014) loss 1.4805 (2.3043) lr 8.3457e-03 eta 0:04:14
epoch [9/30] batch [32/40] time 0.278 (0.298) data 0.000 (0.013) loss 2.2852 (2.2873) lr 8.3457e-03 eta 0:04:12
epoch [9/30] batch [34/40] time 0.279 (0.297) data 0.000 (0.012) loss 3.1816 (2.3470) lr 8.3457e-03 eta 0:04:11
epoch [9/30] batch [36/40] time 0.278 (0.296) data 0.000 (0.012) loss 1.0039 (2.3143) lr 8.3457e-03 eta 0:04:09
epoch [9/30] batch [38/40] time 0.274 (0.295) data 0.000 (0.011) loss 3.8359 (2.3396) lr 8.3457e-03 eta 0:04:08
epoch [9/30] batch [40/40] time 0.276 (0.294) data 0.000 (0.011) loss 1.7188 (2.3281) lr 7.9389e-03 eta 0:04:06
epoch [10/30] batch [2/40] time 0.277 (0.511) data 0.000 (0.204) loss 2.7793 (2.1392) lr 7.9389e-03 eta 0:07:08
epoch [10/30] batch [4/40] time 0.287 (0.397) data 0.000 (0.102) loss 1.4434 (1.9578) lr 7.9389e-03 eta 0:05:31
epoch [10/30] batch [6/40] time 0.283 (0.359) data 0.000 (0.068) loss 1.7393 (2.2266) lr 7.9389e-03 eta 0:04:59
epoch [10/30] batch [8/40] time 0.282 (0.340) data 0.000 (0.051) loss 1.4551 (2.1011) lr 7.9389e-03 eta 0:04:42
epoch [10/30] batch [10/40] time 0.279 (0.328) data 0.000 (0.041) loss 0.9131 (2.0854) lr 7.9389e-03 eta 0:04:32
epoch [10/30] batch [12/40] time 0.283 (0.321) data 0.000 (0.034) loss 1.7041 (2.0085) lr 7.9389e-03 eta 0:04:25
epoch [10/30] batch [14/40] time 0.283 (0.315) data 0.000 (0.029) loss 2.0098 (1.9697) lr 7.9389e-03 eta 0:04:20
epoch [10/30] batch [16/40] time 0.281 (0.311) data 0.000 (0.026) loss 1.8467 (1.9886) lr 7.9389e-03 eta 0:04:16
epoch [10/30] batch [18/40] time 0.273 (0.307) data 0.000 (0.023) loss 1.4209 (2.0871) lr 7.9389e-03 eta 0:04:12
epoch [10/30] batch [20/40] time 0.277 (0.304) data 0.000 (0.021) loss 2.0312 (2.1204) lr 7.9389e-03 eta 0:04:09
epoch [10/30] batch [22/40] time 0.275 (0.302) data 0.000 (0.019) loss 2.3125 (2.1078) lr 7.9389e-03 eta 0:04:06
epoch [10/30] batch [24/40] time 0.275 (0.299) data 0.000 (0.017) loss 1.0791 (2.1058) lr 7.9389e-03 eta 0:04:04
epoch [10/30] batch [26/40] time 0.278 (0.298) data 0.000 (0.016) loss 1.1885 (2.0546) lr 7.9389e-03 eta 0:04:02
epoch [10/30] batch [28/40] time 0.277 (0.296) data 0.000 (0.015) loss 1.6201 (2.0197) lr 7.9389e-03 eta 0:04:00
epoch [10/30] batch [30/40] time 0.276 (0.295) data 0.000 (0.014) loss 3.6113 (2.1026) lr 7.9389e-03 eta 0:03:58
epoch [10/30] batch [32/40] time 0.273 (0.294) data 0.000 (0.013) loss 1.5547 (2.0741) lr 7.9389e-03 eta 0:03:57
epoch [10/30] batch [34/40] time 0.297 (0.294) data 0.000 (0.012) loss 3.5469 (2.1035) lr 7.9389e-03 eta 0:03:56
epoch [10/30] batch [36/40] time 0.284 (0.293) data 0.000 (0.012) loss 3.6016 (2.1864) lr 7.9389e-03 eta 0:03:55
epoch [10/30] batch [38/40] time 0.275 (0.292) data 0.000 (0.011) loss 1.3721 (2.1288) lr 7.9389e-03 eta 0:03:54
epoch [10/30] batch [40/40] time 0.276 (0.291) data 0.000 (0.010) loss 2.6113 (2.1484) lr 7.5000e-03 eta 0:03:53
Checkpoint saved to output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2/prompt_learner/model.pth.tar-10
epoch [11/30] batch [2/40] time 0.282 (0.516) data 0.000 (0.201) loss 2.7949 (2.1108) lr 7.5000e-03 eta 0:06:52
epoch [11/30] batch [4/40] time 0.284 (0.401) data 0.000 (0.100) loss 2.0000 (1.9824) lr 7.5000e-03 eta 0:05:19
epoch [11/30] batch [6/40] time 0.288 (0.362) data 0.000 (0.067) loss 2.6719 (2.0752) lr 7.5000e-03 eta 0:04:47
epoch [11/30] batch [8/40] time 0.285 (0.342) data 0.000 (0.050) loss 2.4141 (2.1365) lr 7.5000e-03 eta 0:04:31
epoch [11/30] batch [10/40] time 0.278 (0.329) data 0.000 (0.040) loss 1.5576 (2.0622) lr 7.5000e-03 eta 0:04:20
epoch [11/30] batch [12/40] time 0.285 (0.322) data 0.000 (0.034) loss 2.2949 (2.1029) lr 7.5000e-03 eta 0:04:13
epoch [11/30] batch [14/40] time 0.283 (0.316) data 0.000 (0.029) loss 2.1992 (2.0856) lr 7.5000e-03 eta 0:04:08
epoch [11/30] batch [16/40] time 0.289 (0.319) data 0.000 (0.025) loss 1.5537 (2.0105) lr 7.5000e-03 eta 0:04:10
epoch [11/30] batch [18/40] time 0.281 (0.315) data 0.000 (0.023) loss 1.2773 (1.9984) lr 7.5000e-03 eta 0:04:06
epoch [11/30] batch [20/40] time 0.275 (0.311) data 0.000 (0.020) loss 2.3965 (2.0049) lr 7.5000e-03 eta 0:04:02
epoch [11/30] batch [22/40] time 0.277 (0.308) data 0.000 (0.018) loss 1.5605 (1.9347) lr 7.5000e-03 eta 0:03:59
epoch [11/30] batch [24/40] time 0.278 (0.305) data 0.000 (0.017) loss 0.9780 (1.8742) lr 7.5000e-03 eta 0:03:56
epoch [11/30] batch [26/40] time 0.281 (0.303) data 0.000 (0.016) loss 1.6875 (1.8916) lr 7.5000e-03 eta 0:03:54
epoch [11/30] batch [28/40] time 0.273 (0.301) data 0.000 (0.015) loss 0.9756 (1.8265) lr 7.5000e-03 eta 0:03:52
epoch [11/30] batch [30/40] time 0.276 (0.300) data 0.000 (0.014) loss 2.5449 (1.8707) lr 7.5000e-03 eta 0:03:50
epoch [11/30] batch [32/40] time 0.280 (0.298) data 0.000 (0.013) loss 2.0410 (1.8600) lr 7.5000e-03 eta 0:03:49
epoch [11/30] batch [34/40] time 0.277 (0.297) data 0.000 (0.012) loss 1.0498 (1.8510) lr 7.5000e-03 eta 0:03:47
epoch [11/30] batch [36/40] time 0.277 (0.296) data 0.000 (0.011) loss 1.2939 (1.9015) lr 7.5000e-03 eta 0:03:46
epoch [11/30] batch [38/40] time 0.276 (0.295) data 0.000 (0.011) loss 1.2578 (1.9022) lr 7.5000e-03 eta 0:03:44
epoch [11/30] batch [40/40] time 0.277 (0.294) data 0.000 (0.010) loss 1.2402 (1.8948) lr 7.0337e-03 eta 0:03:43
epoch [12/30] batch [2/40] time 0.294 (0.529) data 0.000 (0.201) loss 1.8535 (1.6343) lr 7.0337e-03 eta 0:06:40
epoch [12/30] batch [4/40] time 0.294 (0.412) data 0.000 (0.101) loss 1.8027 (1.7009) lr 7.0337e-03 eta 0:05:11
epoch [12/30] batch [6/40] time 0.290 (0.372) data 0.000 (0.067) loss 1.7588 (1.7010) lr 7.0337e-03 eta 0:04:40
epoch [12/30] batch [8/40] time 0.296 (0.353) data 0.000 (0.050) loss 1.8477 (1.6323) lr 7.0337e-03 eta 0:04:25
epoch [12/30] batch [10/40] time 0.294 (0.341) data 0.000 (0.040) loss 2.1855 (1.7297) lr 7.0337e-03 eta 0:04:15
epoch [12/30] batch [12/40] time 0.295 (0.332) data 0.000 (0.034) loss 0.5151 (1.5533) lr 7.0337e-03 eta 0:04:08
epoch [12/30] batch [14/40] time 0.295 (0.327) data 0.000 (0.029) loss 2.0918 (1.6542) lr 7.0337e-03 eta 0:04:03
epoch [12/30] batch [16/40] time 0.291 (0.323) data 0.000 (0.025) loss 2.4219 (1.7653) lr 7.0337e-03 eta 0:04:00
epoch [12/30] batch [18/40] time 0.285 (0.319) data 0.000 (0.023) loss 2.2129 (1.8205) lr 7.0337e-03 eta 0:03:56
epoch [12/30] batch [20/40] time 0.283 (0.316) data 0.000 (0.020) loss 2.5039 (1.8821) lr 7.0337e-03 eta 0:03:53
epoch [12/30] batch [22/40] time 0.284 (0.313) data 0.000 (0.019) loss 1.3672 (1.8620) lr 7.0337e-03 eta 0:03:50
epoch [12/30] batch [24/40] time 0.288 (0.311) data 0.000 (0.017) loss 4.5469 (2.0450) lr 7.0337e-03 eta 0:03:48
epoch [12/30] batch [26/40] time 0.290 (0.309) data 0.000 (0.016) loss 2.3262 (2.0909) lr 7.0337e-03 eta 0:03:46
epoch [12/30] batch [28/40] time 0.288 (0.307) data 0.000 (0.015) loss 1.3252 (2.0513) lr 7.0337e-03 eta 0:03:44
epoch [12/30] batch [30/40] time 0.289 (0.306) data 0.000 (0.014) loss 1.4336 (2.0025) lr 7.0337e-03 eta 0:03:43
epoch [12/30] batch [32/40] time 0.286 (0.305) data 0.000 (0.013) loss 0.7700 (1.9896) lr 7.0337e-03 eta 0:03:41
epoch [12/30] batch [34/40] time 0.289 (0.304) data 0.000 (0.012) loss 1.8545 (2.0133) lr 7.0337e-03 eta 0:03:40
epoch [12/30] batch [36/40] time 0.287 (0.303) data 0.000 (0.011) loss 2.3945 (1.9961) lr 7.0337e-03 eta 0:03:39
epoch [12/30] batch [38/40] time 0.288 (0.302) data 0.000 (0.011) loss 3.7656 (2.0325) lr 7.0337e-03 eta 0:03:38
epoch [12/30] batch [40/40] time 0.284 (0.301) data 0.000 (0.010) loss 2.8945 (2.0382) lr 6.5451e-03 eta 0:03:36
epoch [13/30] batch [2/40] time 0.298 (0.532) data 0.000 (0.204) loss 3.0000 (1.9475) lr 6.5451e-03 eta 0:06:21
epoch [13/30] batch [4/40] time 0.391 (0.438) data 0.000 (0.102) loss 1.0615 (1.6444) lr 6.5451e-03 eta 0:05:13
epoch [13/30] batch [6/40] time 0.302 (0.393) data 0.000 (0.068) loss 1.9746 (1.8821) lr 6.5451e-03 eta 0:04:40
epoch [13/30] batch [8/40] time 0.303 (0.370) data 0.000 (0.051) loss 2.3086 (1.8682) lr 6.5451e-03 eta 0:04:23
epoch [13/30] batch [10/40] time 0.297 (0.356) data 0.000 (0.041) loss 2.5664 (1.9341) lr 6.5451e-03 eta 0:04:12
epoch [13/30] batch [12/40] time 0.297 (0.346) data 0.000 (0.034) loss 1.9023 (1.9498) lr 6.5451e-03 eta 0:04:04
epoch [13/30] batch [14/40] time 0.295 (0.339) data 0.000 (0.029) loss 1.9912 (1.8835) lr 6.5451e-03 eta 0:03:59
epoch [13/30] batch [16/40] time 0.292 (0.333) data 0.000 (0.026) loss 2.4121 (1.8720) lr 6.5451e-03 eta 0:03:54
epoch [13/30] batch [18/40] time 0.290 (0.328) data 0.000 (0.023) loss 3.6523 (1.9310) lr 6.5451e-03 eta 0:03:50
epoch [13/30] batch [20/40] time 0.290 (0.325) data 0.000 (0.021) loss 3.1074 (1.9985) lr 6.5451e-03 eta 0:03:47
epoch [13/30] batch [22/40] time 0.292 (0.322) data 0.000 (0.019) loss 1.3418 (1.9268) lr 6.5451e-03 eta 0:03:44
epoch [13/30] batch [24/40] time 0.289 (0.319) data 0.000 (0.017) loss 1.1904 (1.9250) lr 6.5451e-03 eta 0:03:42
epoch [13/30] batch [26/40] time 0.292 (0.317) data 0.000 (0.016) loss 2.6758 (1.9901) lr 6.5451e-03 eta 0:03:39
epoch [13/30] batch [28/40] time 0.289 (0.315) data 0.000 (0.015) loss 4.0703 (2.0648) lr 6.5451e-03 eta 0:03:37
epoch [13/30] batch [30/40] time 0.290 (0.313) data 0.000 (0.014) loss 2.4004 (2.0387) lr 6.5451e-03 eta 0:03:36
epoch [13/30] batch [32/40] time 0.289 (0.312) data 0.000 (0.013) loss 1.5654 (2.0230) lr 6.5451e-03 eta 0:03:34
epoch [13/30] batch [34/40] time 0.290 (0.310) data 0.000 (0.012) loss 2.0605 (2.0049) lr 6.5451e-03 eta 0:03:32
epoch [13/30] batch [36/40] time 0.291 (0.309) data 0.000 (0.012) loss 1.7305 (1.9654) lr 6.5451e-03 eta 0:03:31
epoch [13/30] batch [38/40] time 0.291 (0.308) data 0.000 (0.011) loss 1.3965 (1.9692) lr 6.5451e-03 eta 0:03:30
epoch [13/30] batch [40/40] time 0.292 (0.308) data 0.000 (0.010) loss 2.0645 (1.9637) lr 6.0396e-03 eta 0:03:29
epoch [14/30] batch [2/40] time 0.297 (0.542) data 0.000 (0.217) loss 1.5908 (1.3931) lr 6.0396e-03 eta 0:06:07
epoch [14/30] batch [4/40] time 0.292 (0.417) data 0.000 (0.109) loss 2.3633 (1.5901) lr 6.0396e-03 eta 0:04:41
epoch [14/30] batch [6/40] time 0.296 (0.376) data 0.000 (0.073) loss 0.5322 (1.4129) lr 6.0396e-03 eta 0:04:13
epoch [14/30] batch [8/40] time 0.297 (0.356) data 0.000 (0.054) loss 1.1016 (1.3167) lr 6.0396e-03 eta 0:03:59
epoch [14/30] batch [10/40] time 0.293 (0.344) data 0.000 (0.044) loss 1.8369 (1.4583) lr 6.0396e-03 eta 0:03:50
epoch [14/30] batch [12/40] time 0.294 (0.336) data 0.000 (0.036) loss 0.9097 (1.4284) lr 6.0396e-03 eta 0:03:44
epoch [14/30] batch [14/40] time 0.291 (0.330) data 0.000 (0.031) loss 2.1621 (1.4517) lr 6.0396e-03 eta 0:03:39
epoch [14/30] batch [16/40] time 0.294 (0.325) data 0.001 (0.027) loss 0.8877 (1.4388) lr 6.0396e-03 eta 0:03:35
epoch [14/30] batch [18/40] time 0.287 (0.321) data 0.000 (0.024) loss 1.1934 (1.4170) lr 6.0396e-03 eta 0:03:32
epoch [14/30] batch [20/40] time 0.286 (0.318) data 0.000 (0.022) loss 3.1953 (1.5228) lr 6.0396e-03 eta 0:03:29
epoch [14/30] batch [22/40] time 0.287 (0.315) data 0.000 (0.020) loss 2.0078 (1.5172) lr 6.0396e-03 eta 0:03:27
epoch [14/30] batch [24/40] time 0.290 (0.313) data 0.000 (0.018) loss 3.6680 (1.5950) lr 6.0396e-03 eta 0:03:25
epoch [14/30] batch [26/40] time 0.286 (0.311) data 0.000 (0.017) loss 3.9746 (1.7480) lr 6.0396e-03 eta 0:03:23
epoch [14/30] batch [28/40] time 0.286 (0.309) data 0.000 (0.016) loss 1.5420 (1.7575) lr 6.0396e-03 eta 0:03:21
epoch [14/30] batch [30/40] time 0.286 (0.308) data 0.000 (0.015) loss 0.9014 (1.7125) lr 6.0396e-03 eta 0:03:20
epoch [14/30] batch [32/40] time 0.286 (0.306) data 0.000 (0.014) loss 1.8379 (1.7025) lr 6.0396e-03 eta 0:03:18
epoch [14/30] batch [34/40] time 0.291 (0.305) data 0.000 (0.013) loss 1.9814 (1.7307) lr 6.0396e-03 eta 0:03:17
epoch [14/30] batch [36/40] time 0.285 (0.305) data 0.000 (0.012) loss 0.8213 (1.6735) lr 6.0396e-03 eta 0:03:16
epoch [14/30] batch [38/40] time 0.286 (0.304) data 0.000 (0.012) loss 0.7412 (1.6715) lr 6.0396e-03 eta 0:03:14
epoch [14/30] batch [40/40] time 0.285 (0.303) data 0.000 (0.011) loss 1.9404 (1.6688) lr 5.5226e-03 eta 0:03:13
epoch [15/30] batch [2/40] time 0.289 (0.515) data 0.000 (0.205) loss 0.7085 (0.6099) lr 5.5226e-03 eta 0:05:28
epoch [15/30] batch [4/40] time 0.286 (0.401) data 0.000 (0.103) loss 1.1396 (0.7891) lr 5.5226e-03 eta 0:04:14
epoch [15/30] batch [6/40] time 0.283 (0.363) data 0.000 (0.069) loss 1.4717 (1.1092) lr 5.5226e-03 eta 0:03:50
epoch [15/30] batch [8/40] time 0.284 (0.343) data 0.001 (0.052) loss 1.9307 (1.1794) lr 5.5226e-03 eta 0:03:37
epoch [15/30] batch [10/40] time 0.280 (0.331) data 0.000 (0.041) loss 1.7715 (1.2017) lr 5.5226e-03 eta 0:03:28
epoch [15/30] batch [12/40] time 0.287 (0.323) data 0.000 (0.034) loss 2.1758 (1.2885) lr 5.5226e-03 eta 0:03:23
epoch [15/30] batch [14/40] time 0.282 (0.318) data 0.000 (0.030) loss 1.1270 (1.3425) lr 5.5226e-03 eta 0:03:18
epoch [15/30] batch [16/40] time 0.291 (0.314) data 0.000 (0.026) loss 0.6489 (1.3355) lr 5.5226e-03 eta 0:03:15
epoch [15/30] batch [18/40] time 0.276 (0.310) data 0.000 (0.023) loss 3.0371 (1.3873) lr 5.5226e-03 eta 0:03:12
epoch [15/30] batch [20/40] time 0.274 (0.306) data 0.000 (0.021) loss 1.2803 (1.4001) lr 5.5226e-03 eta 0:03:09
epoch [15/30] batch [22/40] time 0.282 (0.304) data 0.000 (0.019) loss 1.4434 (1.3756) lr 5.5226e-03 eta 0:03:07
epoch [15/30] batch [24/40] time 0.275 (0.302) data 0.000 (0.017) loss 2.6152 (1.4663) lr 5.5226e-03 eta 0:03:05
epoch [15/30] batch [26/40] time 0.372 (0.304) data 0.000 (0.016) loss 1.2412 (1.4513) lr 5.5226e-03 eta 0:03:06
epoch [15/30] batch [28/40] time 0.272 (0.301) data 0.000 (0.015) loss 1.5537 (1.4542) lr 5.5226e-03 eta 0:03:04
epoch [15/30] batch [30/40] time 0.272 (0.299) data 0.000 (0.014) loss 0.9014 (1.5760) lr 5.5226e-03 eta 0:03:02
epoch [15/30] batch [32/40] time 0.280 (0.298) data 0.000 (0.013) loss 0.9082 (1.5602) lr 5.5226e-03 eta 0:03:01
epoch [15/30] batch [34/40] time 0.277 (0.297) data 0.000 (0.012) loss 1.1904 (1.6057) lr 5.5226e-03 eta 0:02:59
epoch [15/30] batch [36/40] time 0.274 (0.296) data 0.000 (0.012) loss 3.5918 (1.6713) lr 5.5226e-03 eta 0:02:58
epoch [15/30] batch [38/40] time 0.273 (0.295) data 0.000 (0.011) loss 1.3008 (1.6368) lr 5.5226e-03 eta 0:02:57
epoch [15/30] batch [40/40] time 0.278 (0.294) data 0.000 (0.010) loss 2.9219 (1.6690) lr 5.0000e-03 eta 0:02:56
epoch [16/30] batch [2/40] time 0.282 (0.523) data 0.000 (0.208) loss 0.9561 (0.9636) lr 5.0000e-03 eta 0:05:12
epoch [16/30] batch [4/40] time 0.294 (0.406) data 0.000 (0.104) loss 1.7422 (1.4315) lr 5.0000e-03 eta 0:04:01
epoch [16/30] batch [6/40] time 0.292 (0.369) data 0.000 (0.069) loss 1.6514 (1.6085) lr 5.0000e-03 eta 0:03:38
epoch [16/30] batch [8/40] time 0.291 (0.349) data 0.000 (0.052) loss 1.1260 (1.4366) lr 5.0000e-03 eta 0:03:26
epoch [16/30] batch [10/40] time 0.297 (0.338) data 0.000 (0.042) loss 0.5459 (1.3577) lr 5.0000e-03 eta 0:03:19
epoch [16/30] batch [12/40] time 0.319 (0.334) data 0.000 (0.035) loss 1.3926 (1.3671) lr 5.0000e-03 eta 0:03:16
epoch [16/30] batch [14/40] time 0.304 (0.330) data 0.000 (0.030) loss 2.4043 (1.5664) lr 5.0000e-03 eta 0:03:13
epoch [16/30] batch [16/40] time 0.296 (0.325) data 0.000 (0.026) loss 0.5146 (1.5171) lr 5.0000e-03 eta 0:03:09
epoch [16/30] batch [18/40] time 0.295 (0.322) data 0.000 (0.023) loss 2.2148 (1.5827) lr 5.0000e-03 eta 0:03:07
epoch [16/30] batch [20/40] time 0.289 (0.318) data 0.000 (0.021) loss 0.5044 (1.5544) lr 5.0000e-03 eta 0:03:04
epoch [16/30] batch [22/40] time 0.291 (0.316) data 0.000 (0.019) loss 0.7900 (1.4988) lr 5.0000e-03 eta 0:03:02
epoch [16/30] batch [24/40] time 0.283 (0.313) data 0.000 (0.018) loss 0.7324 (1.4502) lr 5.0000e-03 eta 0:03:00
epoch [16/30] batch [26/40] time 0.288 (0.311) data 0.000 (0.016) loss 1.3398 (1.4754) lr 5.0000e-03 eta 0:02:58
epoch [16/30] batch [28/40] time 0.290 (0.310) data 0.000 (0.015) loss 0.8691 (1.4930) lr 5.0000e-03 eta 0:02:57
epoch [16/30] batch [30/40] time 0.286 (0.309) data 0.000 (0.014) loss 0.8022 (1.4593) lr 5.0000e-03 eta 0:02:55
epoch [16/30] batch [32/40] time 0.288 (0.307) data 0.000 (0.013) loss 1.7471 (1.4746) lr 5.0000e-03 eta 0:02:54
epoch [16/30] batch [34/40] time 0.288 (0.306) data 0.000 (0.012) loss 1.2295 (1.5279) lr 5.0000e-03 eta 0:02:53
epoch [16/30] batch [36/40] time 0.289 (0.305) data 0.000 (0.012) loss 1.0928 (1.5042) lr 5.0000e-03 eta 0:02:51
epoch [16/30] batch [38/40] time 0.287 (0.304) data 0.000 (0.011) loss 1.3818 (1.4879) lr 5.0000e-03 eta 0:02:50
epoch [16/30] batch [40/40] time 0.288 (0.303) data 0.000 (0.011) loss 1.4180 (1.5093) lr 4.4774e-03 eta 0:02:49
epoch [17/30] batch [2/40] time 0.284 (0.520) data 0.000 (0.204) loss 1.5391 (1.5049) lr 4.4774e-03 eta 0:04:50
epoch [17/30] batch [4/40] time 0.277 (0.402) data 0.000 (0.102) loss 1.3359 (1.3518) lr 4.4774e-03 eta 0:03:43
epoch [17/30] batch [6/40] time 0.283 (0.363) data 0.000 (0.068) loss 1.2676 (1.2985) lr 4.4774e-03 eta 0:03:20
epoch [17/30] batch [8/40] time 0.285 (0.344) data 0.000 (0.051) loss 2.2070 (1.4207) lr 4.4774e-03 eta 0:03:09
epoch [17/30] batch [10/40] time 0.278 (0.331) data 0.000 (0.041) loss 2.0391 (1.4375) lr 4.4774e-03 eta 0:03:02
epoch [17/30] batch [12/40] time 0.278 (0.323) data 0.000 (0.034) loss 0.6084 (1.4277) lr 4.4774e-03 eta 0:02:56
epoch [17/30] batch [14/40] time 0.280 (0.317) data 0.000 (0.029) loss 1.4473 (1.4914) lr 4.4774e-03 eta 0:02:53
epoch [17/30] batch [16/40] time 0.281 (0.313) data 0.000 (0.026) loss 0.9463 (1.4095) lr 4.4774e-03 eta 0:02:50
epoch [17/30] batch [18/40] time 0.277 (0.309) data 0.000 (0.023) loss 1.0381 (1.3965) lr 4.4774e-03 eta 0:02:47
epoch [17/30] batch [20/40] time 0.276 (0.306) data 0.000 (0.021) loss 1.3799 (1.3694) lr 4.4774e-03 eta 0:02:45
epoch [17/30] batch [22/40] time 0.275 (0.303) data 0.000 (0.019) loss 0.5620 (1.3748) lr 4.4774e-03 eta 0:02:43
epoch [17/30] batch [24/40] time 0.276 (0.301) data 0.000 (0.017) loss 0.8604 (1.3747) lr 4.4774e-03 eta 0:02:41
epoch [17/30] batch [26/40] time 0.280 (0.299) data 0.000 (0.016) loss 0.3435 (1.3418) lr 4.4774e-03 eta 0:02:39
epoch [17/30] batch [28/40] time 0.278 (0.298) data 0.000 (0.015) loss 1.3545 (1.3069) lr 4.4774e-03 eta 0:02:38
epoch [17/30] batch [30/40] time 0.279 (0.296) data 0.000 (0.014) loss 0.6445 (1.3035) lr 4.4774e-03 eta 0:02:37
epoch [17/30] batch [32/40] time 0.276 (0.295) data 0.000 (0.013) loss 1.3076 (1.2785) lr 4.4774e-03 eta 0:02:35
epoch [17/30] batch [34/40] time 0.274 (0.294) data 0.000 (0.012) loss 1.5771 (1.3450) lr 4.4774e-03 eta 0:02:34
epoch [17/30] batch [36/40] time 0.269 (0.293) data 0.000 (0.012) loss 1.9316 (1.3409) lr 4.4774e-03 eta 0:02:33
epoch [17/30] batch [38/40] time 0.273 (0.292) data 0.000 (0.011) loss 0.6558 (1.3365) lr 4.4774e-03 eta 0:02:32
epoch [17/30] batch [40/40] time 0.277 (0.294) data 0.000 (0.010) loss 3.6055 (1.3806) lr 3.9604e-03 eta 0:02:32
epoch [18/30] batch [2/40] time 0.281 (0.509) data 0.000 (0.201) loss 2.3223 (2.1865) lr 3.9604e-03 eta 0:04:23
epoch [18/30] batch [4/40] time 0.288 (0.397) data 0.000 (0.101) loss 0.7661 (1.4705) lr 3.9604e-03 eta 0:03:24
epoch [18/30] batch [6/40] time 0.283 (0.359) data 0.000 (0.067) loss 1.7705 (1.4523) lr 3.9604e-03 eta 0:03:04
epoch [18/30] batch [8/40] time 0.280 (0.340) data 0.000 (0.051) loss 0.7202 (1.4544) lr 3.9604e-03 eta 0:02:54
epoch [18/30] batch [10/40] time 0.280 (0.328) data 0.000 (0.040) loss 0.8242 (1.3445) lr 3.9604e-03 eta 0:02:47
epoch [18/30] batch [12/40] time 0.280 (0.321) data 0.000 (0.034) loss 1.3584 (1.3158) lr 3.9604e-03 eta 0:02:42
epoch [18/30] batch [14/40] time 0.281 (0.315) data 0.000 (0.029) loss 1.8008 (1.3682) lr 3.9604e-03 eta 0:02:39
epoch [18/30] batch [16/40] time 0.280 (0.311) data 0.000 (0.025) loss 0.9531 (1.4290) lr 3.9604e-03 eta 0:02:36
epoch [18/30] batch [18/40] time 0.275 (0.307) data 0.000 (0.023) loss 1.9775 (1.4121) lr 3.9604e-03 eta 0:02:34
epoch [18/30] batch [20/40] time 0.277 (0.304) data 0.000 (0.020) loss 0.8828 (1.3621) lr 3.9604e-03 eta 0:02:31
epoch [18/30] batch [22/40] time 0.280 (0.302) data 0.000 (0.019) loss 1.4297 (1.3842) lr 3.9604e-03 eta 0:02:30
epoch [18/30] batch [24/40] time 0.273 (0.299) data 0.000 (0.017) loss 0.9683 (1.3788) lr 3.9604e-03 eta 0:02:28
epoch [18/30] batch [26/40] time 0.275 (0.298) data 0.000 (0.016) loss 0.9995 (1.3428) lr 3.9604e-03 eta 0:02:26
epoch [18/30] batch [28/40] time 0.273 (0.296) data 0.000 (0.015) loss 1.5010 (1.3643) lr 3.9604e-03 eta 0:02:25
epoch [18/30] batch [30/40] time 0.274 (0.294) data 0.000 (0.014) loss 0.4749 (1.3439) lr 3.9604e-03 eta 0:02:24
epoch [18/30] batch [32/40] time 0.271 (0.293) data 0.000 (0.013) loss 0.4104 (1.2919) lr 3.9604e-03 eta 0:02:23
epoch [18/30] batch [34/40] time 0.273 (0.292) data 0.000 (0.012) loss 1.7383 (1.2806) lr 3.9604e-03 eta 0:02:21
epoch [18/30] batch [36/40] time 0.274 (0.291) data 0.000 (0.011) loss 0.4224 (1.2638) lr 3.9604e-03 eta 0:02:20
epoch [18/30] batch [38/40] time 0.274 (0.290) data 0.000 (0.011) loss 2.1504 (1.2980) lr 3.9604e-03 eta 0:02:19
epoch [18/30] batch [40/40] time 0.271 (0.289) data 0.000 (0.010) loss 0.6221 (1.2610) lr 3.4549e-03 eta 0:02:18
epoch [19/30] batch [2/40] time 0.282 (0.527) data 0.000 (0.210) loss 0.7251 (0.5430) lr 3.4549e-03 eta 0:04:12
epoch [19/30] batch [4/40] time 0.285 (0.405) data 0.000 (0.105) loss 3.9004 (1.9258) lr 3.4549e-03 eta 0:03:12
epoch [19/30] batch [6/40] time 0.299 (0.367) data 0.000 (0.070) loss 0.6401 (1.6116) lr 3.4549e-03 eta 0:02:53
epoch [19/30] batch [8/40] time 0.290 (0.349) data 0.000 (0.053) loss 1.2568 (1.5916) lr 3.4549e-03 eta 0:02:44
epoch [19/30] batch [10/40] time 0.283 (0.335) data 0.000 (0.042) loss 1.0879 (1.6844) lr 3.4549e-03 eta 0:02:37
epoch [19/30] batch [12/40] time 0.283 (0.326) data 0.000 (0.035) loss 1.0156 (1.5414) lr 3.4549e-03 eta 0:02:32
epoch [19/30] batch [14/40] time 0.285 (0.320) data 0.000 (0.030) loss 1.0352 (1.4294) lr 3.4549e-03 eta 0:02:29
epoch [19/30] batch [16/40] time 0.285 (0.316) data 0.000 (0.027) loss 0.4912 (1.3990) lr 3.4549e-03 eta 0:02:26
epoch [19/30] batch [18/40] time 0.276 (0.312) data 0.000 (0.024) loss 0.8008 (1.3340) lr 3.4549e-03 eta 0:02:24
epoch [19/30] batch [20/40] time 0.277 (0.308) data 0.000 (0.021) loss 2.8887 (1.4347) lr 3.4549e-03 eta 0:02:21
epoch [19/30] batch [22/40] time 0.279 (0.310) data 0.000 (0.019) loss 0.6929 (1.3660) lr 3.4549e-03 eta 0:02:21
epoch [19/30] batch [24/40] time 0.280 (0.307) data 0.000 (0.018) loss 0.9224 (1.3212) lr 3.4549e-03 eta 0:02:19
epoch [19/30] batch [26/40] time 0.280 (0.305) data 0.000 (0.016) loss 0.9888 (1.3217) lr 3.4549e-03 eta 0:02:18
epoch [19/30] batch [28/40] time 0.277 (0.303) data 0.000 (0.015) loss 0.6973 (1.2917) lr 3.4549e-03 eta 0:02:16
epoch [19/30] batch [30/40] time 0.276 (0.301) data 0.000 (0.014) loss 1.1758 (1.2701) lr 3.4549e-03 eta 0:02:15
epoch [19/30] batch [32/40] time 0.273 (0.300) data 0.000 (0.013) loss 0.2441 (1.2214) lr 3.4549e-03 eta 0:02:14
epoch [19/30] batch [34/40] time 0.283 (0.298) data 0.000 (0.013) loss 1.8125 (1.2367) lr 3.4549e-03 eta 0:02:13
epoch [19/30] batch [36/40] time 0.283 (0.297) data 0.000 (0.012) loss 0.9873 (1.2160) lr 3.4549e-03 eta 0:02:12
epoch [19/30] batch [38/40] time 0.278 (0.296) data 0.000 (0.011) loss 2.5820 (1.2416) lr 3.4549e-03 eta 0:02:10
epoch [19/30] batch [40/40] time 0.277 (0.295) data 0.000 (0.011) loss 0.6455 (1.2124) lr 2.9663e-03 eta 0:02:09
epoch [20/30] batch [2/40] time 0.285 (0.517) data 0.000 (0.202) loss 1.1152 (1.2373) lr 2.9663e-03 eta 0:03:46
epoch [20/30] batch [4/40] time 0.285 (0.401) data 0.000 (0.101) loss 2.0020 (1.3735) lr 2.9663e-03 eta 0:02:54
epoch [20/30] batch [6/40] time 0.286 (0.363) data 0.000 (0.067) loss 2.8105 (1.5479) lr 2.9663e-03 eta 0:02:37
epoch [20/30] batch [8/40] time 0.284 (0.344) data 0.000 (0.051) loss 1.1279 (1.4052) lr 2.9663e-03 eta 0:02:28
epoch [20/30] batch [10/40] time 0.293 (0.333) data 0.000 (0.041) loss 1.7021 (1.4873) lr 2.9663e-03 eta 0:02:23
epoch [20/30] batch [12/40] time 0.280 (0.324) data 0.000 (0.034) loss 1.7578 (1.4184) lr 2.9663e-03 eta 0:02:18
epoch [20/30] batch [14/40] time 0.285 (0.318) data 0.000 (0.029) loss 1.9307 (1.4391) lr 2.9663e-03 eta 0:02:15
epoch [20/30] batch [16/40] time 0.293 (0.315) data 0.000 (0.025) loss 0.5674 (1.3784) lr 2.9663e-03 eta 0:02:13
epoch [20/30] batch [18/40] time 0.275 (0.311) data 0.000 (0.023) loss 0.3140 (1.2706) lr 2.9663e-03 eta 0:02:11
epoch [20/30] batch [20/40] time 0.276 (0.307) data 0.000 (0.020) loss 4.6055 (1.4008) lr 2.9663e-03 eta 0:02:09
epoch [20/30] batch [22/40] time 0.276 (0.304) data 0.000 (0.019) loss 0.8110 (1.3497) lr 2.9663e-03 eta 0:02:07
epoch [20/30] batch [24/40] time 0.279 (0.302) data 0.000 (0.017) loss 1.0527 (1.3082) lr 2.9663e-03 eta 0:02:05
epoch [20/30] batch [26/40] time 0.279 (0.300) data 0.000 (0.016) loss 1.5537 (1.3313) lr 2.9663e-03 eta 0:02:04
epoch [20/30] batch [28/40] time 0.276 (0.298) data 0.000 (0.015) loss 1.6006 (1.3289) lr 2.9663e-03 eta 0:02:02
epoch [20/30] batch [30/40] time 0.278 (0.297) data 0.000 (0.014) loss 1.3682 (1.3026) lr 2.9663e-03 eta 0:02:01
epoch [20/30] batch [32/40] time 0.278 (0.296) data 0.000 (0.013) loss 0.3455 (1.2718) lr 2.9663e-03 eta 0:02:00
epoch [20/30] batch [34/40] time 0.278 (0.295) data 0.000 (0.012) loss 0.6943 (1.2653) lr 2.9663e-03 eta 0:01:59
epoch [20/30] batch [36/40] time 0.275 (0.294) data 0.000 (0.011) loss 0.6826 (1.2370) lr 2.9663e-03 eta 0:01:58
epoch [20/30] batch [38/40] time 0.275 (0.293) data 0.000 (0.011) loss 2.7344 (1.2732) lr 2.9663e-03 eta 0:01:57
epoch [20/30] batch [40/40] time 0.280 (0.292) data 0.000 (0.010) loss 1.3936 (1.2565) lr 2.5000e-03 eta 0:01:56
Checkpoint saved to output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2/prompt_learner/model.pth.tar-20
epoch [21/30] batch [2/40] time 0.285 (0.519) data 0.000 (0.203) loss 2.6602 (1.9092) lr 2.5000e-03 eta 0:03:26
epoch [21/30] batch [4/40] time 0.281 (0.401) data 0.000 (0.102) loss 1.1211 (1.6965) lr 2.5000e-03 eta 0:02:38
epoch [21/30] batch [6/40] time 0.285 (0.362) data 0.000 (0.068) loss 1.0469 (1.4347) lr 2.5000e-03 eta 0:02:22
epoch [21/30] batch [8/40] time 0.283 (0.342) data 0.000 (0.051) loss 0.5811 (1.1792) lr 2.5000e-03 eta 0:02:14
epoch [21/30] batch [10/40] time 0.288 (0.331) data 0.000 (0.041) loss 0.7651 (1.0767) lr 2.5000e-03 eta 0:02:08
epoch [21/30] batch [12/40] time 0.287 (0.323) data 0.000 (0.034) loss 2.9531 (1.1658) lr 2.5000e-03 eta 0:02:05
epoch [21/30] batch [14/40] time 0.288 (0.318) data 0.000 (0.029) loss 0.5420 (1.0855) lr 2.5000e-03 eta 0:02:02
epoch [21/30] batch [16/40] time 0.387 (0.321) data 0.000 (0.026) loss 1.4629 (1.0762) lr 2.5000e-03 eta 0:02:03
epoch [21/30] batch [18/40] time 0.274 (0.316) data 0.000 (0.023) loss 0.4949 (1.1469) lr 2.5000e-03 eta 0:02:00
epoch [21/30] batch [20/40] time 0.280 (0.312) data 0.000 (0.021) loss 1.0059 (1.1305) lr 2.5000e-03 eta 0:01:58
epoch [21/30] batch [22/40] time 0.279 (0.309) data 0.000 (0.019) loss 1.9629 (1.1605) lr 2.5000e-03 eta 0:01:56
epoch [21/30] batch [24/40] time 0.281 (0.307) data 0.000 (0.017) loss 0.4958 (1.1158) lr 2.5000e-03 eta 0:01:55
epoch [21/30] batch [26/40] time 0.276 (0.304) data 0.000 (0.016) loss 0.6528 (1.0997) lr 2.5000e-03 eta 0:01:53
epoch [21/30] batch [28/40] time 0.276 (0.302) data 0.000 (0.015) loss 1.9072 (1.1540) lr 2.5000e-03 eta 0:01:52
epoch [21/30] batch [30/40] time 0.279 (0.301) data 0.000 (0.014) loss 0.2258 (1.1272) lr 2.5000e-03 eta 0:01:51
epoch [21/30] batch [32/40] time 0.275 (0.299) data 0.000 (0.013) loss 0.4961 (1.1056) lr 2.5000e-03 eta 0:01:50
epoch [21/30] batch [34/40] time 0.277 (0.298) data 0.000 (0.012) loss 0.8457 (1.0966) lr 2.5000e-03 eta 0:01:49
epoch [21/30] batch [36/40] time 0.274 (0.297) data 0.000 (0.012) loss 3.0332 (1.1301) lr 2.5000e-03 eta 0:01:48
epoch [21/30] batch [38/40] time 0.277 (0.296) data 0.000 (0.011) loss 0.6743 (1.1124) lr 2.5000e-03 eta 0:01:47
epoch [21/30] batch [40/40] time 0.277 (0.295) data 0.000 (0.010) loss 1.0527 (1.1714) lr 2.0611e-03 eta 0:01:46
epoch [22/30] batch [2/40] time 0.277 (0.509) data 0.000 (0.209) loss 0.8193 (0.9199) lr 2.0611e-03 eta 0:03:02
epoch [22/30] batch [4/40] time 0.273 (0.392) data 0.000 (0.105) loss 0.7549 (1.1970) lr 2.0611e-03 eta 0:02:19
epoch [22/30] batch [6/40] time 0.286 (0.357) data 0.000 (0.070) loss 1.5215 (1.1703) lr 2.0611e-03 eta 0:02:06
epoch [22/30] batch [8/40] time 0.284 (0.340) data 0.000 (0.052) loss 1.5938 (1.2769) lr 2.0611e-03 eta 0:01:59
epoch [22/30] batch [10/40] time 0.285 (0.329) data 0.000 (0.042) loss 1.5459 (1.3139) lr 2.0611e-03 eta 0:01:54
epoch [22/30] batch [12/40] time 0.285 (0.321) data 0.000 (0.035) loss 2.1719 (1.3122) lr 2.0611e-03 eta 0:01:51
epoch [22/30] batch [14/40] time 0.285 (0.316) data 0.000 (0.030) loss 0.7246 (1.2164) lr 2.0611e-03 eta 0:01:49
epoch [22/30] batch [16/40] time 0.285 (0.312) data 0.000 (0.026) loss 1.3896 (1.3407) lr 2.0611e-03 eta 0:01:47
epoch [22/30] batch [18/40] time 0.273 (0.308) data 0.000 (0.023) loss 1.0625 (1.2612) lr 2.0611e-03 eta 0:01:45
epoch [22/30] batch [20/40] time 0.277 (0.305) data 0.000 (0.021) loss 0.3240 (1.1839) lr 2.0611e-03 eta 0:01:43
epoch [22/30] batch [22/40] time 0.282 (0.303) data 0.000 (0.019) loss 0.6240 (1.1220) lr 2.0611e-03 eta 0:01:42
epoch [22/30] batch [24/40] time 0.275 (0.300) data 0.000 (0.018) loss 0.9419 (1.1736) lr 2.0611e-03 eta 0:01:40
epoch [22/30] batch [26/40] time 0.277 (0.299) data 0.000 (0.016) loss 2.9824 (1.2321) lr 2.0611e-03 eta 0:01:39
epoch [22/30] batch [28/40] time 0.276 (0.297) data 0.000 (0.015) loss 0.2428 (1.1906) lr 2.0611e-03 eta 0:01:38
epoch [22/30] batch [30/40] time 0.277 (0.296) data 0.000 (0.014) loss 0.6094 (1.1704) lr 2.0611e-03 eta 0:01:37
epoch [22/30] batch [32/40] time 0.280 (0.295) data 0.000 (0.013) loss 0.7988 (1.1665) lr 2.0611e-03 eta 0:01:36
epoch [22/30] batch [34/40] time 0.276 (0.294) data 0.000 (0.012) loss 1.4160 (1.1767) lr 2.0611e-03 eta 0:01:35
epoch [22/30] batch [36/40] time 0.274 (0.292) data 0.000 (0.012) loss 0.4585 (1.1280) lr 2.0611e-03 eta 0:01:34
epoch [22/30] batch [38/40] time 0.275 (0.291) data 0.000 (0.011) loss 2.7793 (1.1824) lr 2.0611e-03 eta 0:01:33
epoch [22/30] batch [40/40] time 0.276 (0.291) data 0.000 (0.011) loss 0.4590 (1.1588) lr 1.6543e-03 eta 0:01:33
epoch [23/30] batch [2/40] time 0.285 (0.515) data 0.000 (0.200) loss 0.5713 (0.6978) lr 1.6543e-03 eta 0:02:43
epoch [23/30] batch [4/40] time 0.283 (0.400) data 0.000 (0.100) loss 0.5918 (0.6234) lr 1.6543e-03 eta 0:02:06
epoch [23/30] batch [6/40] time 0.284 (0.361) data 0.000 (0.067) loss 0.4094 (0.5730) lr 1.6543e-03 eta 0:01:53
epoch [23/30] batch [8/40] time 0.295 (0.344) data 0.000 (0.050) loss 0.4946 (0.8402) lr 1.6543e-03 eta 0:01:47
epoch [23/30] batch [10/40] time 0.279 (0.331) data 0.000 (0.040) loss 1.8369 (0.9357) lr 1.6543e-03 eta 0:01:42
epoch [23/30] batch [12/40] time 0.283 (0.323) data 0.000 (0.034) loss 0.7163 (0.9219) lr 1.6543e-03 eta 0:01:39
epoch [23/30] batch [14/40] time 0.283 (0.317) data 0.000 (0.029) loss 0.5615 (0.8935) lr 1.6543e-03 eta 0:01:37
epoch [23/30] batch [16/40] time 0.288 (0.314) data 0.000 (0.025) loss 0.3525 (0.8462) lr 1.6543e-03 eta 0:01:35
epoch [23/30] batch [18/40] time 0.280 (0.310) data 0.000 (0.022) loss 0.4106 (0.8444) lr 1.6543e-03 eta 0:01:33
epoch [23/30] batch [20/40] time 0.278 (0.307) data 0.000 (0.020) loss 2.0117 (0.9287) lr 1.6543e-03 eta 0:01:31
epoch [23/30] batch [22/40] time 0.273 (0.304) data 0.000 (0.018) loss 1.4521 (0.9446) lr 1.6543e-03 eta 0:01:30
epoch [23/30] batch [24/40] time 0.277 (0.301) data 0.000 (0.017) loss 0.7798 (0.9338) lr 1.6543e-03 eta 0:01:29
epoch [23/30] batch [26/40] time 0.280 (0.300) data 0.000 (0.016) loss 0.6816 (0.9246) lr 1.6543e-03 eta 0:01:28
epoch [23/30] batch [28/40] time 0.280 (0.298) data 0.000 (0.014) loss 2.9668 (1.0295) lr 1.6543e-03 eta 0:01:27
epoch [23/30] batch [30/40] time 0.278 (0.297) data 0.000 (0.014) loss 1.2129 (1.0688) lr 1.6543e-03 eta 0:01:26
epoch [23/30] batch [32/40] time 0.273 (0.295) data 0.000 (0.013) loss 0.8984 (1.0644) lr 1.6543e-03 eta 0:01:24
epoch [23/30] batch [34/40] time 0.285 (0.294) data 0.000 (0.012) loss 0.6782 (1.0394) lr 1.6543e-03 eta 0:01:24
epoch [23/30] batch [36/40] time 0.277 (0.293) data 0.000 (0.011) loss 0.9004 (1.0295) lr 1.6543e-03 eta 0:01:23
epoch [23/30] batch [38/40] time 0.276 (0.292) data 0.000 (0.011) loss 0.7051 (1.0190) lr 1.6543e-03 eta 0:01:22
epoch [23/30] batch [40/40] time 0.280 (0.292) data 0.000 (0.010) loss 1.2695 (1.0104) lr 1.2843e-03 eta 0:01:21
epoch [24/30] batch [2/40] time 0.288 (0.516) data 0.000 (0.200) loss 1.1357 (0.9937) lr 1.2843e-03 eta 0:02:23
epoch [24/30] batch [4/40] time 0.286 (0.402) data 0.000 (0.100) loss 0.4431 (0.9824) lr 1.2843e-03 eta 0:01:50
epoch [24/30] batch [6/40] time 0.287 (0.364) data 0.000 (0.067) loss 0.3130 (0.7697) lr 1.2843e-03 eta 0:01:39
epoch [24/30] batch [8/40] time 0.285 (0.356) data 0.000 (0.050) loss 2.3223 (0.8978) lr 1.2843e-03 eta 0:01:36
epoch [24/30] batch [10/40] time 0.291 (0.342) data 0.000 (0.040) loss 1.6367 (1.0243) lr 1.2843e-03 eta 0:01:32
epoch [24/30] batch [12/40] time 0.279 (0.333) data 0.000 (0.034) loss 0.2222 (0.9244) lr 1.2843e-03 eta 0:01:29
epoch [24/30] batch [14/40] time 0.284 (0.327) data 0.000 (0.029) loss 0.2241 (0.8433) lr 1.2843e-03 eta 0:01:26
epoch [24/30] batch [16/40] time 0.279 (0.321) data 0.000 (0.025) loss 2.0293 (1.0532) lr 1.2843e-03 eta 0:01:24
epoch [24/30] batch [18/40] time 0.276 (0.316) data 0.000 (0.022) loss 0.7173 (1.0118) lr 1.2843e-03 eta 0:01:22
epoch [24/30] batch [20/40] time 0.280 (0.312) data 0.000 (0.020) loss 0.3767 (0.9623) lr 1.2843e-03 eta 0:01:21
epoch [24/30] batch [22/40] time 0.274 (0.309) data 0.000 (0.018) loss 2.3516 (1.0107) lr 1.2843e-03 eta 0:01:19
epoch [24/30] batch [24/40] time 0.276 (0.306) data 0.000 (0.017) loss 0.6338 (0.9862) lr 1.2843e-03 eta 0:01:18
epoch [24/30] batch [26/40] time 0.277 (0.304) data 0.000 (0.016) loss 2.4004 (1.0295) lr 1.2843e-03 eta 0:01:17
epoch [24/30] batch [28/40] time 0.278 (0.302) data 0.000 (0.015) loss 2.4980 (1.1302) lr 1.2843e-03 eta 0:01:16
epoch [24/30] batch [30/40] time 0.271 (0.300) data 0.000 (0.014) loss 0.7627 (1.1008) lr 1.2843e-03 eta 0:01:14
epoch [24/30] batch [32/40] time 0.275 (0.298) data 0.000 (0.013) loss 0.4402 (1.0636) lr 1.2843e-03 eta 0:01:13
epoch [24/30] batch [34/40] time 0.276 (0.297) data 0.000 (0.012) loss 0.5635 (1.0979) lr 1.2843e-03 eta 0:01:13
epoch [24/30] batch [36/40] time 0.274 (0.296) data 0.000 (0.011) loss 1.6162 (1.1090) lr 1.2843e-03 eta 0:01:12
epoch [24/30] batch [38/40] time 0.275 (0.294) data 0.000 (0.011) loss 0.7285 (1.0880) lr 1.2843e-03 eta 0:01:11
epoch [24/30] batch [40/40] time 0.274 (0.294) data 0.000 (0.010) loss 4.1641 (1.1766) lr 9.5492e-04 eta 0:01:10
epoch [25/30] batch [2/40] time 0.285 (0.523) data 0.000 (0.208) loss 0.2092 (0.3378) lr 9.5492e-04 eta 0:02:04
epoch [25/30] batch [4/40] time 0.278 (0.401) data 0.000 (0.104) loss 2.2852 (0.8780) lr 9.5492e-04 eta 0:01:34
epoch [25/30] batch [6/40] time 0.284 (0.362) data 0.000 (0.070) loss 4.1250 (1.4616) lr 9.5492e-04 eta 0:01:24
epoch [25/30] batch [8/40] time 0.280 (0.342) data 0.000 (0.052) loss 0.6260 (1.4489) lr 9.5492e-04 eta 0:01:19
epoch [25/30] batch [10/40] time 0.282 (0.329) data 0.000 (0.042) loss 2.3789 (1.4422) lr 9.5492e-04 eta 0:01:15
epoch [25/30] batch [12/40] time 0.285 (0.322) data 0.000 (0.035) loss 0.8076 (1.3356) lr 9.5492e-04 eta 0:01:13
epoch [25/30] batch [14/40] time 0.287 (0.317) data 0.000 (0.030) loss 1.3018 (1.3014) lr 9.5492e-04 eta 0:01:11
epoch [25/30] batch [16/40] time 0.281 (0.313) data 0.000 (0.026) loss 1.2266 (1.2567) lr 9.5492e-04 eta 0:01:10
epoch [25/30] batch [18/40] time 0.276 (0.309) data 0.000 (0.023) loss 0.6069 (1.1633) lr 9.5492e-04 eta 0:01:08
epoch [25/30] batch [20/40] time 0.278 (0.306) data 0.000 (0.021) loss 1.8447 (1.1506) lr 9.5492e-04 eta 0:01:07
epoch [25/30] batch [22/40] time 0.276 (0.303) data 0.000 (0.019) loss 3.0215 (1.2993) lr 9.5492e-04 eta 0:01:06
epoch [25/30] batch [24/40] time 0.275 (0.301) data 0.000 (0.018) loss 1.4033 (1.2784) lr 9.5492e-04 eta 0:01:05
epoch [25/30] batch [26/40] time 0.276 (0.299) data 0.000 (0.016) loss 0.7949 (1.2397) lr 9.5492e-04 eta 0:01:04
epoch [25/30] batch [28/40] time 0.273 (0.297) data 0.000 (0.015) loss 0.2661 (1.1868) lr 9.5492e-04 eta 0:01:03
epoch [25/30] batch [30/40] time 0.280 (0.296) data 0.000 (0.014) loss 0.7988 (1.1435) lr 9.5492e-04 eta 0:01:02
epoch [25/30] batch [32/40] time 0.271 (0.295) data 0.000 (0.013) loss 1.1953 (1.1255) lr 9.5492e-04 eta 0:01:01
epoch [25/30] batch [34/40] time 0.274 (0.293) data 0.000 (0.012) loss 0.6377 (1.0944) lr 9.5492e-04 eta 0:01:00
epoch [25/30] batch [36/40] time 0.277 (0.292) data 0.000 (0.012) loss 1.5234 (1.0925) lr 9.5492e-04 eta 0:00:59
epoch [25/30] batch [38/40] time 0.275 (0.292) data 0.000 (0.011) loss 1.5557 (1.1268) lr 9.5492e-04 eta 0:00:58
epoch [25/30] batch [40/40] time 0.270 (0.291) data 0.000 (0.011) loss 1.2207 (1.1368) lr 6.6987e-04 eta 0:00:58
epoch [26/30] batch [2/40] time 0.286 (0.525) data 0.000 (0.206) loss 1.2705 (0.9653) lr 6.6987e-04 eta 0:01:43
epoch [26/30] batch [4/40] time 0.290 (0.405) data 0.000 (0.103) loss 0.9780 (0.9258) lr 6.6987e-04 eta 0:01:19
epoch [26/30] batch [6/40] time 0.291 (0.366) data 0.000 (0.069) loss 0.9888 (0.8768) lr 6.6987e-04 eta 0:01:10
epoch [26/30] batch [8/40] time 0.280 (0.345) data 0.000 (0.052) loss 0.5386 (0.7511) lr 6.6987e-04 eta 0:01:06
epoch [26/30] batch [10/40] time 0.382 (0.343) data 0.000 (0.041) loss 0.6411 (0.7893) lr 6.6987e-04 eta 0:01:05
epoch [26/30] batch [12/40] time 0.287 (0.334) data 0.000 (0.035) loss 1.2236 (0.7819) lr 6.6987e-04 eta 0:01:02
epoch [26/30] batch [14/40] time 0.286 (0.329) data 0.000 (0.030) loss 1.1025 (0.7765) lr 6.6987e-04 eta 0:01:01
epoch [26/30] batch [16/40] time 0.283 (0.323) data 0.000 (0.026) loss 0.6953 (0.7425) lr 6.6987e-04 eta 0:00:59
epoch [26/30] batch [18/40] time 0.274 (0.318) data 0.000 (0.023) loss 0.6216 (0.7747) lr 6.6987e-04 eta 0:00:57
epoch [26/30] batch [20/40] time 0.280 (0.314) data 0.000 (0.021) loss 0.6987 (0.7454) lr 6.6987e-04 eta 0:00:56
epoch [26/30] batch [22/40] time 0.277 (0.311) data 0.000 (0.019) loss 0.6128 (0.7237) lr 6.6987e-04 eta 0:00:55
epoch [26/30] batch [24/40] time 0.279 (0.308) data 0.000 (0.017) loss 3.6973 (0.9153) lr 6.6987e-04 eta 0:00:54
epoch [26/30] batch [26/40] time 0.275 (0.306) data 0.000 (0.016) loss 1.4639 (0.9448) lr 6.6987e-04 eta 0:00:53
epoch [26/30] batch [28/40] time 0.275 (0.303) data 0.000 (0.015) loss 0.2771 (0.9075) lr 6.6987e-04 eta 0:00:52
epoch [26/30] batch [30/40] time 0.277 (0.302) data 0.000 (0.014) loss 1.8398 (0.9351) lr 6.6987e-04 eta 0:00:51
epoch [26/30] batch [32/40] time 0.277 (0.300) data 0.000 (0.013) loss 0.7090 (0.9072) lr 6.6987e-04 eta 0:00:50
epoch [26/30] batch [34/40] time 0.274 (0.299) data 0.000 (0.012) loss 1.1182 (0.9024) lr 6.6987e-04 eta 0:00:49
epoch [26/30] batch [36/40] time 0.273 (0.297) data 0.000 (0.012) loss 0.5527 (0.8811) lr 6.6987e-04 eta 0:00:48
epoch [26/30] batch [38/40] time 0.278 (0.296) data 0.000 (0.011) loss 0.8813 (0.8873) lr 6.6987e-04 eta 0:00:47
epoch [26/30] batch [40/40] time 0.278 (0.295) data 0.000 (0.010) loss 1.3867 (0.9307) lr 4.3227e-04 eta 0:00:47
epoch [27/30] batch [2/40] time 0.283 (0.514) data 0.000 (0.212) loss 0.6133 (0.3738) lr 4.3227e-04 eta 0:01:21
epoch [27/30] batch [4/40] time 0.288 (0.400) data 0.000 (0.106) loss 0.5317 (0.5013) lr 4.3227e-04 eta 0:01:02
epoch [27/30] batch [6/40] time 0.292 (0.365) data 0.000 (0.071) loss 0.3892 (0.7289) lr 4.3227e-04 eta 0:00:56
epoch [27/30] batch [8/40] time 0.294 (0.347) data 0.000 (0.053) loss 0.4998 (0.6648) lr 4.3227e-04 eta 0:00:52
epoch [27/30] batch [10/40] time 0.308 (0.338) data 0.000 (0.043) loss 0.6123 (0.6752) lr 4.3227e-04 eta 0:00:50
epoch [27/30] batch [12/40] time 0.293 (0.330) data 0.000 (0.036) loss 0.6299 (0.7087) lr 4.3227e-04 eta 0:00:48
epoch [27/30] batch [14/40] time 0.290 (0.325) data 0.000 (0.031) loss 0.5361 (0.7300) lr 4.3227e-04 eta 0:00:47
epoch [27/30] batch [16/40] time 0.289 (0.320) data 0.000 (0.027) loss 1.6826 (0.7978) lr 4.3227e-04 eta 0:00:46
epoch [27/30] batch [18/40] time 0.287 (0.317) data 0.000 (0.024) loss 1.4717 (0.8250) lr 4.3227e-04 eta 0:00:44
epoch [27/30] batch [20/40] time 0.289 (0.314) data 0.000 (0.021) loss 0.2090 (0.8230) lr 4.3227e-04 eta 0:00:43
epoch [27/30] batch [22/40] time 0.284 (0.312) data 0.000 (0.020) loss 1.1260 (0.8502) lr 4.3227e-04 eta 0:00:42
epoch [27/30] batch [24/40] time 0.287 (0.309) data 0.000 (0.018) loss 0.7412 (0.8223) lr 4.3227e-04 eta 0:00:42
epoch [27/30] batch [26/40] time 0.287 (0.308) data 0.000 (0.017) loss 1.8271 (0.8432) lr 4.3227e-04 eta 0:00:41
epoch [27/30] batch [28/40] time 0.285 (0.306) data 0.000 (0.015) loss 2.0039 (0.8740) lr 4.3227e-04 eta 0:00:40
epoch [27/30] batch [30/40] time 0.292 (0.305) data 0.000 (0.014) loss 0.5859 (0.8565) lr 4.3227e-04 eta 0:00:39
epoch [27/30] batch [32/40] time 0.289 (0.304) data 0.000 (0.013) loss 0.6006 (0.8428) lr 4.3227e-04 eta 0:00:38
epoch [27/30] batch [34/40] time 0.289 (0.303) data 0.000 (0.013) loss 2.2441 (0.8748) lr 4.3227e-04 eta 0:00:38
epoch [27/30] batch [36/40] time 0.286 (0.302) data 0.000 (0.012) loss 0.2388 (0.9001) lr 4.3227e-04 eta 0:00:37
epoch [27/30] batch [38/40] time 0.289 (0.302) data 0.000 (0.011) loss 0.1938 (0.8648) lr 4.3227e-04 eta 0:00:36
epoch [27/30] batch [40/40] time 0.286 (0.303) data 0.000 (0.011) loss 1.6807 (0.9024) lr 2.4472e-04 eta 0:00:36
epoch [28/30] batch [2/40] time 0.278 (0.513) data 0.000 (0.201) loss 0.3428 (1.2153) lr 2.4472e-04 eta 0:01:00
epoch [28/30] batch [4/40] time 0.291 (0.400) data 0.000 (0.101) loss 0.2104 (0.7982) lr 2.4472e-04 eta 0:00:46
epoch [28/30] batch [6/40] time 0.284 (0.361) data 0.000 (0.067) loss 0.3325 (0.7728) lr 2.4472e-04 eta 0:00:41
epoch [28/30] batch [8/40] time 0.285 (0.343) data 0.000 (0.050) loss 0.8135 (0.9080) lr 2.4472e-04 eta 0:00:38
epoch [28/30] batch [10/40] time 0.289 (0.333) data 0.000 (0.040) loss 1.0957 (0.9152) lr 2.4472e-04 eta 0:00:36
epoch [28/30] batch [12/40] time 0.290 (0.326) data 0.000 (0.034) loss 1.6523 (0.9389) lr 2.4472e-04 eta 0:00:35
epoch [28/30] batch [14/40] time 0.289 (0.321) data 0.000 (0.029) loss 0.7446 (0.9728) lr 2.4472e-04 eta 0:00:34
epoch [28/30] batch [16/40] time 0.292 (0.317) data 0.000 (0.025) loss 1.6064 (1.0257) lr 2.4472e-04 eta 0:00:33
epoch [28/30] batch [18/40] time 0.285 (0.314) data 0.000 (0.023) loss 0.4004 (0.9951) lr 2.4472e-04 eta 0:00:31
epoch [28/30] batch [20/40] time 0.288 (0.311) data 0.000 (0.020) loss 0.6279 (0.9854) lr 2.4472e-04 eta 0:00:31
epoch [28/30] batch [22/40] time 0.282 (0.309) data 0.000 (0.018) loss 0.8828 (0.9633) lr 2.4472e-04 eta 0:00:30
epoch [28/30] batch [24/40] time 0.289 (0.307) data 0.000 (0.017) loss 0.6816 (0.9518) lr 2.4472e-04 eta 0:00:29
epoch [28/30] batch [26/40] time 0.283 (0.305) data 0.000 (0.016) loss 1.3389 (0.9538) lr 2.4472e-04 eta 0:00:28
epoch [28/30] batch [28/40] time 0.288 (0.304) data 0.000 (0.015) loss 1.2295 (0.9479) lr 2.4472e-04 eta 0:00:27
epoch [28/30] batch [30/40] time 0.290 (0.303) data 0.000 (0.014) loss 0.3586 (0.9097) lr 2.4472e-04 eta 0:00:27
epoch [28/30] batch [32/40] time 0.292 (0.302) data 0.000 (0.013) loss 1.3643 (0.9397) lr 2.4472e-04 eta 0:00:26
epoch [28/30] batch [34/40] time 0.289 (0.301) data 0.000 (0.012) loss 1.3672 (0.9370) lr 2.4472e-04 eta 0:00:25
epoch [28/30] batch [36/40] time 0.282 (0.300) data 0.000 (0.011) loss 1.5186 (0.9776) lr 2.4472e-04 eta 0:00:25
epoch [28/30] batch [38/40] time 0.288 (0.299) data 0.000 (0.011) loss 0.5356 (0.9587) lr 2.4472e-04 eta 0:00:24
epoch [28/30] batch [40/40] time 0.285 (0.299) data 0.000 (0.010) loss 1.7607 (0.9826) lr 1.0926e-04 eta 0:00:23
epoch [29/30] batch [2/40] time 0.293 (0.526) data 0.000 (0.206) loss 1.5332 (0.8694) lr 1.0926e-04 eta 0:00:41
epoch [29/30] batch [4/40] time 0.297 (0.412) data 0.000 (0.103) loss 1.1914 (0.8824) lr 1.0926e-04 eta 0:00:31
epoch [29/30] batch [6/40] time 0.295 (0.373) data 0.000 (0.069) loss 0.5903 (0.8139) lr 1.0926e-04 eta 0:00:27
epoch [29/30] batch [8/40] time 0.288 (0.353) data 0.000 (0.052) loss 0.7339 (0.9905) lr 1.0926e-04 eta 0:00:25
epoch [29/30] batch [10/40] time 0.284 (0.340) data 0.000 (0.041) loss 0.8794 (0.9443) lr 1.0926e-04 eta 0:00:23
epoch [29/30] batch [12/40] time 0.287 (0.331) data 0.000 (0.035) loss 2.1816 (1.0365) lr 1.0926e-04 eta 0:00:22
epoch [29/30] batch [14/40] time 0.282 (0.324) data 0.000 (0.030) loss 1.4717 (1.0051) lr 1.0926e-04 eta 0:00:21
epoch [29/30] batch [16/40] time 0.292 (0.320) data 0.000 (0.026) loss 0.4304 (0.9845) lr 1.0926e-04 eta 0:00:20
epoch [29/30] batch [18/40] time 0.274 (0.315) data 0.000 (0.023) loss 0.6260 (0.9893) lr 1.0926e-04 eta 0:00:19
epoch [29/30] batch [20/40] time 0.274 (0.311) data 0.000 (0.021) loss 0.9150 (1.0260) lr 1.0926e-04 eta 0:00:18
epoch [29/30] batch [22/40] time 0.271 (0.307) data 0.000 (0.019) loss 3.0332 (1.1183) lr 1.0926e-04 eta 0:00:17
epoch [29/30] batch [24/40] time 0.273 (0.304) data 0.000 (0.017) loss 0.4729 (1.0614) lr 1.0926e-04 eta 0:00:17
epoch [29/30] batch [26/40] time 0.269 (0.302) data 0.000 (0.016) loss 0.4512 (1.0093) lr 1.0926e-04 eta 0:00:16
epoch [29/30] batch [28/40] time 0.271 (0.300) data 0.000 (0.015) loss 1.9395 (1.0285) lr 1.0926e-04 eta 0:00:15
epoch [29/30] batch [30/40] time 0.275 (0.298) data 0.000 (0.014) loss 0.8667 (1.0218) lr 1.0926e-04 eta 0:00:14
epoch [29/30] batch [32/40] time 0.276 (0.296) data 0.000 (0.013) loss 0.7207 (1.0020) lr 1.0926e-04 eta 0:00:14
epoch [29/30] batch [34/40] time 0.281 (0.295) data 0.000 (0.012) loss 0.4250 (0.9663) lr 1.0926e-04 eta 0:00:13
epoch [29/30] batch [36/40] time 0.276 (0.294) data 0.000 (0.012) loss 0.7612 (0.9401) lr 1.0926e-04 eta 0:00:12
epoch [29/30] batch [38/40] time 0.274 (0.293) data 0.000 (0.011) loss 1.8574 (0.9524) lr 1.0926e-04 eta 0:00:12
epoch [29/30] batch [40/40] time 0.277 (0.292) data 0.000 (0.011) loss 1.0781 (0.9749) lr 2.7391e-05 eta 0:00:11
epoch [30/30] batch [2/40] time 0.292 (0.533) data 0.000 (0.214) loss 1.4736 (1.1707) lr 2.7391e-05 eta 0:00:20
epoch [30/30] batch [4/40] time 0.282 (0.407) data 0.000 (0.107) loss 0.8271 (0.8723) lr 2.7391e-05 eta 0:00:14
epoch [30/30] batch [6/40] time 0.289 (0.367) data 0.000 (0.072) loss 0.6528 (0.7547) lr 2.7391e-05 eta 0:00:12
epoch [30/30] batch [8/40] time 0.288 (0.347) data 0.000 (0.054) loss 1.1621 (0.8372) lr 2.7391e-05 eta 0:00:11
epoch [30/30] batch [10/40] time 0.281 (0.343) data 0.000 (0.043) loss 1.0723 (0.8863) lr 2.7391e-05 eta 0:00:10
epoch [30/30] batch [12/40] time 0.282 (0.334) data 0.000 (0.036) loss 0.9229 (0.8916) lr 2.7391e-05 eta 0:00:09
epoch [30/30] batch [14/40] time 0.290 (0.327) data 0.000 (0.031) loss 0.9585 (0.9076) lr 2.7391e-05 eta 0:00:08
epoch [30/30] batch [16/40] time 0.280 (0.321) data 0.000 (0.027) loss 0.4048 (0.8478) lr 2.7391e-05 eta 0:00:07
epoch [30/30] batch [18/40] time 0.278 (0.316) data 0.000 (0.024) loss 0.1729 (0.8136) lr 2.7391e-05 eta 0:00:06
epoch [30/30] batch [20/40] time 0.277 (0.312) data 0.000 (0.022) loss 0.3032 (0.8394) lr 2.7391e-05 eta 0:00:06
epoch [30/30] batch [22/40] time 0.280 (0.309) data 0.000 (0.020) loss 0.2908 (0.8223) lr 2.7391e-05 eta 0:00:05
epoch [30/30] batch [24/40] time 0.275 (0.306) data 0.000 (0.018) loss 0.5366 (0.7880) lr 2.7391e-05 eta 0:00:04
epoch [30/30] batch [26/40] time 0.273 (0.304) data 0.000 (0.017) loss 0.4934 (0.7721) lr 2.7391e-05 eta 0:00:04
epoch [30/30] batch [28/40] time 0.278 (0.302) data 0.000 (0.016) loss 0.5176 (0.7518) lr 2.7391e-05 eta 0:00:03
epoch [30/30] batch [30/40] time 0.276 (0.300) data 0.000 (0.015) loss 1.3662 (0.8275) lr 2.7391e-05 eta 0:00:03
epoch [30/30] batch [32/40] time 0.276 (0.299) data 0.000 (0.014) loss 1.1094 (0.8404) lr 2.7391e-05 eta 0:00:02
epoch [30/30] batch [34/40] time 0.274 (0.297) data 0.000 (0.013) loss 1.8018 (0.8568) lr 2.7391e-05 eta 0:00:01
epoch [30/30] batch [36/40] time 0.279 (0.296) data 0.000 (0.012) loss 2.1602 (0.8903) lr 2.7391e-05 eta 0:00:01
epoch [30/30] batch [38/40] time 0.279 (0.295) data 0.000 (0.011) loss 0.9053 (0.8974) lr 2.7391e-05 eta 0:00:00
epoch [30/30] batch [40/40] time 0.275 (0.294) data 0.000 (0.011) loss 0.6875 (0.8833) lr 0.0000e+00 eta 0:00:00
Checkpoint saved to output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2/prompt_learner/model.pth.tar-30
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/42 [00:00<?, ?it/s]  2%|         | 1/42 [00:03<02:10,  3.18s/it]  5%|         | 2/42 [00:03<01:05,  1.63s/it]  7%|         | 3/42 [00:04<00:41,  1.06s/it] 10%|         | 4/42 [00:04<00:29,  1.27it/s] 12%|        | 5/42 [00:04<00:22,  1.61it/s] 14%|        | 6/42 [00:05<00:19,  1.84it/s] 17%|        | 7/42 [00:05<00:16,  2.14it/s] 19%|        | 8/42 [00:05<00:14,  2.36it/s] 21%|       | 9/42 [00:06<00:13,  2.51it/s] 24%|       | 10/42 [00:06<00:11,  2.68it/s] 26%|       | 11/42 [00:06<00:11,  2.79it/s] 29%|       | 12/42 [00:07<00:10,  2.82it/s] 31%|       | 13/42 [00:07<00:10,  2.88it/s] 33%|      | 14/42 [00:07<00:09,  2.90it/s] 36%|      | 15/42 [00:08<00:09,  2.85it/s] 38%|      | 16/42 [00:08<00:08,  2.93it/s] 40%|      | 17/42 [00:08<00:08,  2.95it/s] 43%|     | 18/42 [00:09<00:07,  3.02it/s] 45%|     | 19/42 [00:09<00:07,  3.07it/s] 48%|     | 20/42 [00:09<00:06,  3.20it/s] 50%|     | 21/42 [00:10<00:06,  3.30it/s] 52%|    | 22/42 [00:10<00:05,  3.38it/s] 55%|    | 23/42 [00:10<00:05,  3.43it/s] 57%|    | 24/42 [00:10<00:05,  3.47it/s] 60%|    | 25/42 [00:11<00:04,  3.50it/s] 62%|   | 26/42 [00:11<00:04,  3.52it/s] 64%|   | 27/42 [00:11<00:04,  3.54it/s] 67%|   | 28/42 [00:12<00:03,  3.54it/s] 69%|   | 29/42 [00:12<00:03,  3.55it/s] 71%|  | 30/42 [00:12<00:03,  3.55it/s] 74%|  | 31/42 [00:12<00:03,  3.56it/s] 76%|  | 32/42 [00:13<00:02,  3.56it/s] 79%|  | 33/42 [00:13<00:02,  3.56it/s] 81%|  | 34/42 [00:13<00:02,  3.57it/s] 83%| | 35/42 [00:13<00:01,  3.57it/s] 86%| | 36/42 [00:14<00:01,  3.56it/s] 88%| | 37/42 [00:14<00:01,  3.57it/s] 90%| | 38/42 [00:14<00:01,  3.57it/s] 93%|| 39/42 [00:15<00:00,  3.57it/s] 95%|| 40/42 [00:15<00:00,  3.57it/s] 98%|| 41/42 [00:15<00:00,  3.57it/s]100%|| 42/42 [00:15<00:00,  4.37it/s]100%|| 42/42 [00:15<00:00,  2.65it/s]
=> result
* total: 8,100
* correct: 7,090
* accuracy: 87.5%
* error: 12.5%
* macro_f1: 87.1%
Elapsed: 0:06:14
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_train.sh eurosat 3 0 main_final1212 16 RPO_prime
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_3.pkl
160 5400 8100
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      5,400
# test     8,100
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/30] batch [2/40] time 0.306 (1.430) data 0.000 (0.407) loss 5.0586 (4.8574) lr 1.0000e-02 eta 0:28:33
epoch [1/30] batch [4/40] time 0.302 (0.866) data 0.000 (0.204) loss 4.7617 (4.6191) lr 1.0000e-02 eta 0:17:15
epoch [1/30] batch [6/40] time 0.295 (0.677) data 0.000 (0.136) loss 4.6562 (4.6816) lr 1.0000e-02 eta 0:13:28
epoch [1/30] batch [8/40] time 0.298 (0.582) data 0.000 (0.102) loss 4.9414 (4.6782) lr 1.0000e-02 eta 0:11:33
epoch [1/30] batch [10/40] time 0.290 (0.537) data 0.000 (0.082) loss 4.5547 (4.7289) lr 1.0000e-02 eta 0:10:38
epoch [1/30] batch [12/40] time 0.304 (0.496) data 0.000 (0.068) loss 4.9883 (4.7656) lr 1.0000e-02 eta 0:09:49
epoch [1/30] batch [14/40] time 0.284 (0.466) data 0.000 (0.058) loss 4.6172 (4.7533) lr 1.0000e-02 eta 0:09:12
epoch [1/30] batch [16/40] time 0.286 (0.443) data 0.000 (0.051) loss 4.7422 (4.7429) lr 1.0000e-02 eta 0:08:44
epoch [1/30] batch [18/40] time 0.273 (0.424) data 0.000 (0.045) loss 4.5742 (4.7533) lr 1.0000e-02 eta 0:08:21
epoch [1/30] batch [20/40] time 0.280 (0.410) data 0.000 (0.041) loss 4.3359 (4.7277) lr 1.0000e-02 eta 0:08:03
epoch [1/30] batch [22/40] time 0.282 (0.398) data 0.000 (0.037) loss 4.4727 (4.7061) lr 1.0000e-02 eta 0:07:48
epoch [1/30] batch [24/40] time 0.277 (0.388) data 0.000 (0.034) loss 3.8555 (4.6641) lr 1.0000e-02 eta 0:07:35
epoch [1/30] batch [26/40] time 0.275 (0.379) data 0.000 (0.032) loss 4.8086 (4.6782) lr 1.0000e-02 eta 0:07:25
epoch [1/30] batch [28/40] time 0.277 (0.372) data 0.000 (0.029) loss 4.6875 (4.6911) lr 1.0000e-02 eta 0:07:15
epoch [1/30] batch [30/40] time 0.274 (0.365) data 0.000 (0.027) loss 4.7656 (4.7086) lr 1.0000e-02 eta 0:07:07
epoch [1/30] batch [32/40] time 0.277 (0.360) data 0.000 (0.026) loss 4.8555 (4.7058) lr 1.0000e-02 eta 0:07:00
epoch [1/30] batch [34/40] time 0.276 (0.355) data 0.000 (0.024) loss 4.5820 (4.6912) lr 1.0000e-02 eta 0:06:53
epoch [1/30] batch [36/40] time 0.272 (0.350) data 0.000 (0.023) loss 4.2227 (4.6618) lr 1.0000e-02 eta 0:06:47
epoch [1/30] batch [38/40] time 0.280 (0.346) data 0.000 (0.022) loss 4.1250 (4.6345) lr 1.0000e-02 eta 0:06:42
epoch [1/30] batch [40/40] time 0.276 (0.343) data 0.000 (0.021) loss 4.3203 (4.6267) lr 9.9726e-03 eta 0:06:37
epoch [2/30] batch [2/40] time 0.280 (0.515) data 0.000 (0.198) loss 3.8691 (4.1357) lr 9.9726e-03 eta 0:09:56
epoch [2/30] batch [4/40] time 0.287 (0.399) data 0.000 (0.099) loss 4.3711 (4.1831) lr 9.9726e-03 eta 0:07:41
epoch [2/30] batch [6/40] time 0.286 (0.362) data 0.000 (0.066) loss 4.6094 (4.2679) lr 9.9726e-03 eta 0:06:57
epoch [2/30] batch [8/40] time 0.284 (0.343) data 0.000 (0.050) loss 4.1914 (4.1394) lr 9.9726e-03 eta 0:06:34
epoch [2/30] batch [10/40] time 0.288 (0.331) data 0.000 (0.040) loss 4.8203 (4.2201) lr 9.9726e-03 eta 0:06:20
epoch [2/30] batch [12/40] time 0.280 (0.323) data 0.000 (0.033) loss 4.4492 (4.2534) lr 9.9726e-03 eta 0:06:11
epoch [2/30] batch [14/40] time 0.288 (0.318) data 0.000 (0.029) loss 3.7637 (4.2458) lr 9.9726e-03 eta 0:06:04
epoch [2/30] batch [16/40] time 0.280 (0.314) data 0.001 (0.025) loss 3.5742 (4.2009) lr 9.9726e-03 eta 0:05:58
epoch [2/30] batch [18/40] time 0.277 (0.309) data 0.000 (0.022) loss 4.7930 (4.2697) lr 9.9726e-03 eta 0:05:53
epoch [2/30] batch [20/40] time 0.274 (0.306) data 0.000 (0.020) loss 5.3984 (4.3584) lr 9.9726e-03 eta 0:05:48
epoch [2/30] batch [22/40] time 0.274 (0.303) data 0.000 (0.018) loss 4.4062 (4.3468) lr 9.9726e-03 eta 0:05:45
epoch [2/30] batch [24/40] time 0.275 (0.301) data 0.000 (0.017) loss 5.0078 (4.3525) lr 9.9726e-03 eta 0:05:41
epoch [2/30] batch [26/40] time 0.280 (0.299) data 0.000 (0.015) loss 3.9863 (4.3244) lr 9.9726e-03 eta 0:05:39
epoch [2/30] batch [28/40] time 0.274 (0.297) data 0.000 (0.014) loss 4.3477 (4.2844) lr 9.9726e-03 eta 0:05:36
epoch [2/30] batch [30/40] time 0.276 (0.296) data 0.000 (0.013) loss 4.5469 (4.2819) lr 9.9726e-03 eta 0:05:34
epoch [2/30] batch [32/40] time 0.275 (0.295) data 0.000 (0.013) loss 4.3711 (4.2591) lr 9.9726e-03 eta 0:05:32
epoch [2/30] batch [34/40] time 0.286 (0.294) data 0.000 (0.012) loss 4.0664 (4.2343) lr 9.9726e-03 eta 0:05:30
epoch [2/30] batch [36/40] time 0.276 (0.293) data 0.000 (0.011) loss 4.4766 (4.2592) lr 9.9726e-03 eta 0:05:29
epoch [2/30] batch [38/40] time 0.279 (0.292) data 0.000 (0.011) loss 4.6719 (4.2768) lr 9.9726e-03 eta 0:05:28
epoch [2/30] batch [40/40] time 0.273 (0.292) data 0.000 (0.010) loss 4.9922 (4.2739) lr 9.8907e-03 eta 0:05:26
epoch [3/30] batch [2/40] time 0.283 (0.512) data 0.000 (0.208) loss 4.0977 (4.2676) lr 9.8907e-03 eta 0:09:32
epoch [3/30] batch [4/40] time 0.288 (0.401) data 0.000 (0.104) loss 3.6387 (4.1274) lr 9.8907e-03 eta 0:07:27
epoch [3/30] batch [6/40] time 0.291 (0.365) data 0.000 (0.070) loss 3.6172 (4.0570) lr 9.8907e-03 eta 0:06:47
epoch [3/30] batch [8/40] time 0.292 (0.347) data 0.000 (0.052) loss 4.8164 (4.1667) lr 9.8907e-03 eta 0:06:25
epoch [3/30] batch [10/40] time 0.293 (0.347) data 0.000 (0.042) loss 4.2930 (4.2479) lr 9.8907e-03 eta 0:06:25
epoch [3/30] batch [12/40] time 0.289 (0.338) data 0.000 (0.035) loss 4.3359 (4.2378) lr 9.8907e-03 eta 0:06:14
epoch [3/30] batch [14/40] time 0.295 (0.332) data 0.000 (0.030) loss 2.8691 (4.1398) lr 9.8907e-03 eta 0:06:06
epoch [3/30] batch [16/40] time 0.289 (0.326) data 0.000 (0.026) loss 4.3164 (4.1833) lr 9.8907e-03 eta 0:06:00
epoch [3/30] batch [18/40] time 0.288 (0.322) data 0.000 (0.023) loss 3.2148 (4.1119) lr 9.8907e-03 eta 0:05:54
epoch [3/30] batch [20/40] time 0.289 (0.319) data 0.000 (0.021) loss 4.4883 (4.1112) lr 9.8907e-03 eta 0:05:50
epoch [3/30] batch [22/40] time 0.287 (0.316) data 0.000 (0.019) loss 3.9453 (4.1295) lr 9.8907e-03 eta 0:05:46
epoch [3/30] batch [24/40] time 0.286 (0.313) data 0.000 (0.018) loss 2.6953 (4.0751) lr 9.8907e-03 eta 0:05:43
epoch [3/30] batch [26/40] time 0.282 (0.311) data 0.000 (0.016) loss 3.8223 (4.0853) lr 9.8907e-03 eta 0:05:40
epoch [3/30] batch [28/40] time 0.294 (0.309) data 0.000 (0.015) loss 3.9570 (4.0587) lr 9.8907e-03 eta 0:05:37
epoch [3/30] batch [30/40] time 0.288 (0.308) data 0.000 (0.014) loss 4.3789 (4.0457) lr 9.8907e-03 eta 0:05:35
epoch [3/30] batch [32/40] time 0.291 (0.307) data 0.000 (0.013) loss 2.7949 (4.0170) lr 9.8907e-03 eta 0:05:33
epoch [3/30] batch [34/40] time 0.288 (0.306) data 0.000 (0.012) loss 4.2617 (4.0070) lr 9.8907e-03 eta 0:05:32
epoch [3/30] batch [36/40] time 0.285 (0.305) data 0.000 (0.012) loss 3.5742 (4.0241) lr 9.8907e-03 eta 0:05:30
epoch [3/30] batch [38/40] time 0.283 (0.304) data 0.000 (0.011) loss 3.3574 (4.0387) lr 9.8907e-03 eta 0:05:28
epoch [3/30] batch [40/40] time 0.283 (0.303) data 0.000 (0.011) loss 4.2188 (4.0641) lr 9.7553e-03 eta 0:05:26
epoch [4/30] batch [2/40] time 0.291 (0.536) data 0.000 (0.215) loss 3.5508 (3.7383) lr 9.7553e-03 eta 0:09:37
epoch [4/30] batch [4/40] time 0.289 (0.412) data 0.000 (0.108) loss 3.7539 (3.9609) lr 9.7553e-03 eta 0:07:23
epoch [4/30] batch [6/40] time 0.295 (0.372) data 0.000 (0.072) loss 3.0527 (3.8363) lr 9.7553e-03 eta 0:06:39
epoch [4/30] batch [8/40] time 0.291 (0.352) data 0.000 (0.054) loss 3.4922 (3.7610) lr 9.7553e-03 eta 0:06:17
epoch [4/30] batch [10/40] time 0.290 (0.339) data 0.000 (0.043) loss 4.2695 (3.8826) lr 9.7553e-03 eta 0:06:02
epoch [4/30] batch [12/40] time 0.289 (0.331) data 0.000 (0.036) loss 2.6543 (3.6953) lr 9.7553e-03 eta 0:05:53
epoch [4/30] batch [14/40] time 0.293 (0.326) data 0.000 (0.031) loss 4.0547 (3.7208) lr 9.7553e-03 eta 0:05:47
epoch [4/30] batch [16/40] time 0.290 (0.321) data 0.000 (0.027) loss 3.3750 (3.6464) lr 9.7553e-03 eta 0:05:41
epoch [4/30] batch [18/40] time 0.285 (0.317) data 0.000 (0.024) loss 3.2695 (3.6260) lr 9.7553e-03 eta 0:05:37
epoch [4/30] batch [20/40] time 0.282 (0.314) data 0.000 (0.022) loss 3.7441 (3.6214) lr 9.7553e-03 eta 0:05:32
epoch [4/30] batch [22/40] time 0.285 (0.311) data 0.000 (0.020) loss 2.6094 (3.6372) lr 9.7553e-03 eta 0:05:29
epoch [4/30] batch [24/40] time 0.289 (0.309) data 0.000 (0.018) loss 4.8750 (3.7026) lr 9.7553e-03 eta 0:05:26
epoch [4/30] batch [26/40] time 0.286 (0.307) data 0.000 (0.017) loss 3.1211 (3.6545) lr 9.7553e-03 eta 0:05:23
epoch [4/30] batch [28/40] time 0.291 (0.306) data 0.000 (0.016) loss 3.8379 (3.6749) lr 9.7553e-03 eta 0:05:22
epoch [4/30] batch [30/40] time 0.286 (0.305) data 0.000 (0.015) loss 2.9180 (3.6691) lr 9.7553e-03 eta 0:05:20
epoch [4/30] batch [32/40] time 0.287 (0.304) data 0.000 (0.014) loss 4.3711 (3.7028) lr 9.7553e-03 eta 0:05:18
epoch [4/30] batch [34/40] time 0.285 (0.303) data 0.000 (0.013) loss 3.9023 (3.6861) lr 9.7553e-03 eta 0:05:16
epoch [4/30] batch [36/40] time 0.289 (0.302) data 0.000 (0.012) loss 3.6777 (3.6882) lr 9.7553e-03 eta 0:05:15
epoch [4/30] batch [38/40] time 0.393 (0.304) data 0.000 (0.012) loss 4.7070 (3.7111) lr 9.7553e-03 eta 0:05:16
epoch [4/30] batch [40/40] time 0.289 (0.303) data 0.000 (0.011) loss 4.0391 (3.7297) lr 9.5677e-03 eta 0:05:15
epoch [5/30] batch [2/40] time 0.280 (0.522) data 0.000 (0.213) loss 2.5879 (3.4014) lr 9.5677e-03 eta 0:09:01
epoch [5/30] batch [4/40] time 0.299 (0.407) data 0.000 (0.107) loss 3.1641 (3.7046) lr 9.5677e-03 eta 0:07:01
epoch [5/30] batch [6/40] time 0.294 (0.369) data 0.000 (0.071) loss 3.6953 (3.7490) lr 9.5677e-03 eta 0:06:21
epoch [5/30] batch [8/40] time 0.293 (0.350) data 0.000 (0.054) loss 3.9473 (3.7898) lr 9.5677e-03 eta 0:06:01
epoch [5/30] batch [10/40] time 0.297 (0.339) data 0.000 (0.043) loss 4.4141 (3.7162) lr 9.5677e-03 eta 0:05:49
epoch [5/30] batch [12/40] time 0.293 (0.332) data 0.000 (0.036) loss 2.6934 (3.5386) lr 9.5677e-03 eta 0:05:40
epoch [5/30] batch [14/40] time 0.291 (0.326) data 0.000 (0.031) loss 3.0215 (3.5014) lr 9.5677e-03 eta 0:05:34
epoch [5/30] batch [16/40] time 0.295 (0.322) data 0.000 (0.027) loss 3.1543 (3.4539) lr 9.5677e-03 eta 0:05:29
epoch [5/30] batch [18/40] time 0.286 (0.318) data 0.000 (0.024) loss 3.2676 (3.3888) lr 9.5677e-03 eta 0:05:25
epoch [5/30] batch [20/40] time 0.284 (0.315) data 0.000 (0.022) loss 4.2227 (3.4209) lr 9.5677e-03 eta 0:05:21
epoch [5/30] batch [22/40] time 0.290 (0.312) data 0.000 (0.020) loss 4.0312 (3.3980) lr 9.5677e-03 eta 0:05:18
epoch [5/30] batch [24/40] time 0.285 (0.310) data 0.000 (0.018) loss 3.9258 (3.3976) lr 9.5677e-03 eta 0:05:15
epoch [5/30] batch [26/40] time 0.289 (0.308) data 0.000 (0.017) loss 3.3691 (3.3958) lr 9.5677e-03 eta 0:05:12
epoch [5/30] batch [28/40] time 0.285 (0.307) data 0.000 (0.015) loss 3.8379 (3.3892) lr 9.5677e-03 eta 0:05:10
epoch [5/30] batch [30/40] time 0.283 (0.305) data 0.000 (0.014) loss 3.2734 (3.3580) lr 9.5677e-03 eta 0:05:08
epoch [5/30] batch [32/40] time 0.289 (0.304) data 0.000 (0.014) loss 3.1016 (3.3585) lr 9.5677e-03 eta 0:05:06
epoch [5/30] batch [34/40] time 0.285 (0.303) data 0.000 (0.013) loss 1.1416 (3.3132) lr 9.5677e-03 eta 0:05:04
epoch [5/30] batch [36/40] time 0.289 (0.302) data 0.000 (0.012) loss 2.0508 (3.2977) lr 9.5677e-03 eta 0:05:03
epoch [5/30] batch [38/40] time 0.289 (0.302) data 0.000 (0.011) loss 4.1328 (3.2986) lr 9.5677e-03 eta 0:05:02
epoch [5/30] batch [40/40] time 0.284 (0.301) data 0.000 (0.011) loss 2.6113 (3.2935) lr 9.3301e-03 eta 0:05:00
epoch [6/30] batch [2/40] time 0.281 (0.516) data 0.000 (0.212) loss 5.2891 (4.5186) lr 9.3301e-03 eta 0:08:35
epoch [6/30] batch [4/40] time 0.287 (0.400) data 0.000 (0.106) loss 3.0801 (3.4172) lr 9.3301e-03 eta 0:06:38
epoch [6/30] batch [6/40] time 0.281 (0.361) data 0.000 (0.071) loss 2.8926 (3.2710) lr 9.3301e-03 eta 0:05:58
epoch [6/30] batch [8/40] time 0.281 (0.341) data 0.000 (0.053) loss 3.0234 (3.1932) lr 9.3301e-03 eta 0:05:37
epoch [6/30] batch [10/40] time 0.276 (0.328) data 0.000 (0.043) loss 3.1875 (3.1243) lr 9.3301e-03 eta 0:05:24
epoch [6/30] batch [12/40] time 0.286 (0.320) data 0.000 (0.036) loss 2.3574 (2.9810) lr 9.3301e-03 eta 0:05:16
epoch [6/30] batch [14/40] time 0.292 (0.316) data 0.000 (0.031) loss 3.8086 (3.0193) lr 9.3301e-03 eta 0:05:11
epoch [6/30] batch [16/40] time 0.286 (0.312) data 0.000 (0.027) loss 2.6230 (3.0168) lr 9.3301e-03 eta 0:05:07
epoch [6/30] batch [18/40] time 0.277 (0.308) data 0.000 (0.024) loss 2.4961 (3.0244) lr 9.3301e-03 eta 0:05:02
epoch [6/30] batch [20/40] time 0.278 (0.305) data 0.000 (0.021) loss 1.7402 (2.9710) lr 9.3301e-03 eta 0:04:59
epoch [6/30] batch [22/40] time 0.280 (0.303) data 0.000 (0.020) loss 2.6836 (2.9683) lr 9.3301e-03 eta 0:04:56
epoch [6/30] batch [24/40] time 0.277 (0.301) data 0.000 (0.018) loss 2.8164 (2.9589) lr 9.3301e-03 eta 0:04:53
epoch [6/30] batch [26/40] time 0.277 (0.299) data 0.000 (0.017) loss 4.6523 (3.0427) lr 9.3301e-03 eta 0:04:51
epoch [6/30] batch [28/40] time 0.273 (0.297) data 0.000 (0.015) loss 3.2129 (3.0482) lr 9.3301e-03 eta 0:04:49
epoch [6/30] batch [30/40] time 0.276 (0.296) data 0.000 (0.014) loss 4.2031 (3.0456) lr 9.3301e-03 eta 0:04:46
epoch [6/30] batch [32/40] time 0.277 (0.295) data 0.000 (0.014) loss 3.1113 (3.0596) lr 9.3301e-03 eta 0:04:45
epoch [6/30] batch [34/40] time 0.286 (0.294) data 0.000 (0.013) loss 4.4258 (3.0585) lr 9.3301e-03 eta 0:04:43
epoch [6/30] batch [36/40] time 0.283 (0.293) data 0.000 (0.012) loss 2.8633 (3.0584) lr 9.3301e-03 eta 0:04:42
epoch [6/30] batch [38/40] time 0.284 (0.293) data 0.000 (0.011) loss 3.6523 (3.0720) lr 9.3301e-03 eta 0:04:41
epoch [6/30] batch [40/40] time 0.282 (0.292) data 0.000 (0.011) loss 1.9375 (3.0452) lr 9.0451e-03 eta 0:04:40
epoch [7/30] batch [2/40] time 0.287 (0.526) data 0.000 (0.211) loss 2.8047 (2.1489) lr 9.0451e-03 eta 0:08:23
epoch [7/30] batch [4/40] time 0.286 (0.405) data 0.000 (0.105) loss 2.7031 (2.6497) lr 9.0451e-03 eta 0:06:27
epoch [7/30] batch [6/40] time 0.280 (0.363) data 0.000 (0.070) loss 2.9629 (2.6313) lr 9.0451e-03 eta 0:05:46
epoch [7/30] batch [8/40] time 0.383 (0.355) data 0.000 (0.053) loss 3.2363 (2.6373) lr 9.0451e-03 eta 0:05:38
epoch [7/30] batch [10/40] time 0.277 (0.340) data 0.000 (0.042) loss 2.8457 (2.5344) lr 9.0451e-03 eta 0:05:22
epoch [7/30] batch [12/40] time 0.281 (0.330) data 0.000 (0.035) loss 1.3125 (2.4967) lr 9.0451e-03 eta 0:05:13
epoch [7/30] batch [14/40] time 0.285 (0.324) data 0.000 (0.030) loss 2.5605 (2.5504) lr 9.0451e-03 eta 0:05:06
epoch [7/30] batch [16/40] time 0.289 (0.319) data 0.000 (0.027) loss 5.2969 (2.8182) lr 9.0451e-03 eta 0:05:01
epoch [7/30] batch [18/40] time 0.273 (0.314) data 0.000 (0.024) loss 2.5879 (2.7848) lr 9.0451e-03 eta 0:04:55
epoch [7/30] batch [20/40] time 0.275 (0.310) data 0.000 (0.021) loss 4.0508 (2.8802) lr 9.0451e-03 eta 0:04:51
epoch [7/30] batch [22/40] time 0.275 (0.307) data 0.000 (0.019) loss 4.0156 (2.9344) lr 9.0451e-03 eta 0:04:47
epoch [7/30] batch [24/40] time 0.281 (0.305) data 0.000 (0.018) loss 3.5508 (2.9352) lr 9.0451e-03 eta 0:04:45
epoch [7/30] batch [26/40] time 0.279 (0.303) data 0.000 (0.016) loss 2.2070 (2.8884) lr 9.0451e-03 eta 0:04:42
epoch [7/30] batch [28/40] time 0.272 (0.301) data 0.000 (0.015) loss 3.8633 (2.9351) lr 9.0451e-03 eta 0:04:40
epoch [7/30] batch [30/40] time 0.274 (0.299) data 0.000 (0.014) loss 2.4453 (2.9318) lr 9.0451e-03 eta 0:04:38
epoch [7/30] batch [32/40] time 0.279 (0.298) data 0.000 (0.013) loss 2.0840 (2.8812) lr 9.0451e-03 eta 0:04:36
epoch [7/30] batch [34/40] time 0.277 (0.296) data 0.000 (0.013) loss 3.1699 (2.8642) lr 9.0451e-03 eta 0:04:34
epoch [7/30] batch [36/40] time 0.274 (0.295) data 0.000 (0.012) loss 2.6152 (2.8582) lr 9.0451e-03 eta 0:04:32
epoch [7/30] batch [38/40] time 0.276 (0.294) data 0.000 (0.011) loss 3.0879 (2.8333) lr 9.0451e-03 eta 0:04:31
epoch [7/30] batch [40/40] time 0.274 (0.293) data 0.000 (0.011) loss 2.3848 (2.8035) lr 8.7157e-03 eta 0:04:29
epoch [8/30] batch [2/40] time 0.278 (0.510) data 0.000 (0.202) loss 2.6172 (2.2852) lr 8.7157e-03 eta 0:07:48
epoch [8/30] batch [4/40] time 0.278 (0.395) data 0.000 (0.101) loss 2.7246 (2.6699) lr 8.7157e-03 eta 0:06:01
epoch [8/30] batch [6/40] time 0.288 (0.357) data 0.000 (0.067) loss 1.7178 (2.6802) lr 8.7157e-03 eta 0:05:26
epoch [8/30] batch [8/40] time 0.283 (0.339) data 0.000 (0.051) loss 2.1738 (2.7067) lr 8.7157e-03 eta 0:05:08
epoch [8/30] batch [10/40] time 0.281 (0.327) data 0.000 (0.041) loss 3.5645 (2.6762) lr 8.7157e-03 eta 0:04:57
epoch [8/30] batch [12/40] time 0.284 (0.320) data 0.000 (0.034) loss 1.3994 (2.6021) lr 8.7157e-03 eta 0:04:50
epoch [8/30] batch [14/40] time 0.285 (0.315) data 0.000 (0.029) loss 2.1562 (2.5895) lr 8.7157e-03 eta 0:04:44
epoch [8/30] batch [16/40] time 0.278 (0.310) data 0.000 (0.025) loss 2.5117 (2.5516) lr 8.7157e-03 eta 0:04:39
epoch [8/30] batch [18/40] time 0.277 (0.306) data 0.000 (0.023) loss 1.9170 (2.4944) lr 8.7157e-03 eta 0:04:36
epoch [8/30] batch [20/40] time 0.274 (0.303) data 0.000 (0.020) loss 2.1113 (2.4680) lr 8.7157e-03 eta 0:04:32
epoch [8/30] batch [22/40] time 0.275 (0.300) data 0.000 (0.019) loss 1.9072 (2.4445) lr 8.7157e-03 eta 0:04:29
epoch [8/30] batch [24/40] time 0.275 (0.298) data 0.000 (0.017) loss 1.6953 (2.4271) lr 8.7157e-03 eta 0:04:27
epoch [8/30] batch [26/40] time 0.288 (0.297) data 0.000 (0.016) loss 1.8418 (2.4233) lr 8.7157e-03 eta 0:04:25
epoch [8/30] batch [28/40] time 0.290 (0.296) data 0.000 (0.015) loss 1.7910 (2.4056) lr 8.7157e-03 eta 0:04:24
epoch [8/30] batch [30/40] time 0.271 (0.295) data 0.000 (0.014) loss 2.6836 (2.4071) lr 8.7157e-03 eta 0:04:22
epoch [8/30] batch [32/40] time 0.277 (0.293) data 0.000 (0.013) loss 3.2559 (2.4536) lr 8.7157e-03 eta 0:04:20
epoch [8/30] batch [34/40] time 0.282 (0.293) data 0.000 (0.012) loss 2.6172 (2.4169) lr 8.7157e-03 eta 0:04:19
epoch [8/30] batch [36/40] time 0.297 (0.293) data 0.000 (0.011) loss 3.6660 (2.4277) lr 8.7157e-03 eta 0:04:19
epoch [8/30] batch [38/40] time 0.272 (0.292) data 0.000 (0.011) loss 3.5625 (2.4488) lr 8.7157e-03 eta 0:04:17
epoch [8/30] batch [40/40] time 0.278 (0.291) data 0.000 (0.010) loss 1.8252 (2.5012) lr 8.3457e-03 eta 0:04:16
epoch [9/30] batch [2/40] time 0.285 (0.524) data 0.000 (0.211) loss 3.7109 (3.8945) lr 8.3457e-03 eta 0:07:40
epoch [9/30] batch [4/40] time 0.282 (0.402) data 0.000 (0.106) loss 1.0596 (2.5747) lr 8.3457e-03 eta 0:05:52
epoch [9/30] batch [6/40] time 0.288 (0.364) data 0.000 (0.071) loss 3.2461 (2.5840) lr 8.3457e-03 eta 0:05:18
epoch [9/30] batch [8/40] time 0.285 (0.345) data 0.000 (0.053) loss 2.0547 (2.5127) lr 8.3457e-03 eta 0:05:00
epoch [9/30] batch [10/40] time 0.281 (0.333) data 0.000 (0.043) loss 2.1250 (2.4213) lr 8.3457e-03 eta 0:04:49
epoch [9/30] batch [12/40] time 0.284 (0.325) data 0.000 (0.035) loss 1.7842 (2.4602) lr 8.3457e-03 eta 0:04:41
epoch [9/30] batch [14/40] time 0.281 (0.318) data 0.000 (0.030) loss 1.0146 (2.3714) lr 8.3457e-03 eta 0:04:35
epoch [9/30] batch [16/40] time 0.278 (0.314) data 0.000 (0.027) loss 2.2207 (2.3778) lr 8.3457e-03 eta 0:04:30
epoch [9/30] batch [18/40] time 0.278 (0.310) data 0.000 (0.024) loss 3.0605 (2.4249) lr 8.3457e-03 eta 0:04:26
epoch [9/30] batch [20/40] time 0.274 (0.306) data 0.000 (0.021) loss 2.6992 (2.3793) lr 8.3457e-03 eta 0:04:23
epoch [9/30] batch [22/40] time 0.277 (0.303) data 0.000 (0.019) loss 1.6104 (2.3639) lr 8.3457e-03 eta 0:04:20
epoch [9/30] batch [24/40] time 0.272 (0.301) data 0.000 (0.018) loss 3.0742 (2.3874) lr 8.3457e-03 eta 0:04:17
epoch [9/30] batch [26/40] time 0.279 (0.299) data 0.000 (0.016) loss 2.0625 (2.3308) lr 8.3457e-03 eta 0:04:15
epoch [9/30] batch [28/40] time 0.282 (0.301) data 0.000 (0.015) loss 2.2715 (2.3670) lr 8.3457e-03 eta 0:04:16
epoch [9/30] batch [30/40] time 0.279 (0.300) data 0.000 (0.014) loss 2.9707 (2.3932) lr 8.3457e-03 eta 0:04:14
epoch [9/30] batch [32/40] time 0.275 (0.298) data 0.000 (0.013) loss 2.9531 (2.4102) lr 8.3457e-03 eta 0:04:12
epoch [9/30] batch [34/40] time 0.271 (0.297) data 0.000 (0.013) loss 2.7734 (2.4102) lr 8.3457e-03 eta 0:04:11
epoch [9/30] batch [36/40] time 0.275 (0.296) data 0.000 (0.012) loss 2.5176 (2.3964) lr 8.3457e-03 eta 0:04:09
epoch [9/30] batch [38/40] time 0.278 (0.295) data 0.000 (0.011) loss 3.2266 (2.4158) lr 8.3457e-03 eta 0:04:07
epoch [9/30] batch [40/40] time 0.276 (0.294) data 0.000 (0.011) loss 3.1094 (2.4668) lr 7.9389e-03 eta 0:04:06
epoch [10/30] batch [2/40] time 0.286 (0.524) data 0.000 (0.216) loss 1.9482 (2.6206) lr 7.9389e-03 eta 0:07:19
epoch [10/30] batch [4/40] time 0.290 (0.405) data 0.000 (0.108) loss 2.1660 (2.5173) lr 7.9389e-03 eta 0:05:38
epoch [10/30] batch [6/40] time 0.277 (0.363) data 0.000 (0.072) loss 1.7930 (2.2223) lr 7.9389e-03 eta 0:05:02
epoch [10/30] batch [8/40] time 0.284 (0.342) data 0.000 (0.054) loss 2.7910 (2.4744) lr 7.9389e-03 eta 0:04:44
epoch [10/30] batch [10/40] time 0.277 (0.329) data 0.000 (0.043) loss 2.0039 (2.3359) lr 7.9389e-03 eta 0:04:33
epoch [10/30] batch [12/40] time 0.286 (0.322) data 0.000 (0.036) loss 1.7686 (2.2243) lr 7.9389e-03 eta 0:04:26
epoch [10/30] batch [14/40] time 0.283 (0.316) data 0.000 (0.031) loss 2.3379 (2.3860) lr 7.9389e-03 eta 0:04:21
epoch [10/30] batch [16/40] time 0.281 (0.312) data 0.000 (0.027) loss 3.1348 (2.4364) lr 7.9389e-03 eta 0:04:17
epoch [10/30] batch [18/40] time 0.273 (0.308) data 0.000 (0.024) loss 3.5078 (2.4317) lr 7.9389e-03 eta 0:04:12
epoch [10/30] batch [20/40] time 0.272 (0.304) data 0.000 (0.022) loss 1.7217 (2.3204) lr 7.9389e-03 eta 0:04:09
epoch [10/30] batch [22/40] time 0.274 (0.301) data 0.000 (0.020) loss 1.4883 (2.2743) lr 7.9389e-03 eta 0:04:06
epoch [10/30] batch [24/40] time 0.276 (0.299) data 0.000 (0.018) loss 2.1738 (2.3128) lr 7.9389e-03 eta 0:04:04
epoch [10/30] batch [26/40] time 0.271 (0.297) data 0.000 (0.017) loss 0.7666 (2.2297) lr 7.9389e-03 eta 0:04:01
epoch [10/30] batch [28/40] time 0.279 (0.296) data 0.000 (0.016) loss 1.8838 (2.1818) lr 7.9389e-03 eta 0:04:00
epoch [10/30] batch [30/40] time 0.278 (0.294) data 0.000 (0.015) loss 2.7031 (2.1579) lr 7.9389e-03 eta 0:03:58
epoch [10/30] batch [32/40] time 0.275 (0.293) data 0.000 (0.014) loss 1.2676 (2.1132) lr 7.9389e-03 eta 0:03:57
epoch [10/30] batch [34/40] time 0.277 (0.292) data 0.000 (0.013) loss 2.1562 (2.1158) lr 7.9389e-03 eta 0:03:55
epoch [10/30] batch [36/40] time 0.272 (0.291) data 0.000 (0.012) loss 2.3730 (2.1256) lr 7.9389e-03 eta 0:03:54
epoch [10/30] batch [38/40] time 0.276 (0.290) data 0.000 (0.012) loss 1.5615 (2.0889) lr 7.9389e-03 eta 0:03:52
epoch [10/30] batch [40/40] time 0.274 (0.290) data 0.000 (0.011) loss 2.0000 (2.0935) lr 7.5000e-03 eta 0:03:51
Checkpoint saved to output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3/prompt_learner/model.pth.tar-10
epoch [11/30] batch [2/40] time 0.283 (0.531) data 0.001 (0.200) loss 1.8926 (1.1837) lr 7.5000e-03 eta 0:07:03
epoch [11/30] batch [4/40] time 0.281 (0.407) data 0.000 (0.100) loss 1.9492 (1.4944) lr 7.5000e-03 eta 0:05:24
epoch [11/30] batch [6/40] time 0.286 (0.366) data 0.000 (0.067) loss 1.2773 (1.5692) lr 7.5000e-03 eta 0:04:50
epoch [11/30] batch [8/40] time 0.282 (0.346) data 0.000 (0.050) loss 1.2705 (1.6739) lr 7.5000e-03 eta 0:04:33
epoch [11/30] batch [10/40] time 0.295 (0.336) data 0.000 (0.040) loss 2.0645 (1.7821) lr 7.5000e-03 eta 0:04:25
epoch [11/30] batch [12/40] time 0.287 (0.328) data 0.000 (0.034) loss 1.3223 (1.9644) lr 7.5000e-03 eta 0:04:18
epoch [11/30] batch [14/40] time 0.290 (0.323) data 0.000 (0.029) loss 0.9189 (1.8666) lr 7.5000e-03 eta 0:04:13
epoch [11/30] batch [16/40] time 0.299 (0.325) data 0.000 (0.025) loss 1.9268 (1.8059) lr 7.5000e-03 eta 0:04:15
epoch [11/30] batch [18/40] time 0.282 (0.321) data 0.000 (0.022) loss 1.3936 (1.7153) lr 7.5000e-03 eta 0:04:10
epoch [11/30] batch [20/40] time 0.285 (0.317) data 0.000 (0.020) loss 1.7939 (1.7429) lr 7.5000e-03 eta 0:04:07
epoch [11/30] batch [22/40] time 0.286 (0.314) data 0.000 (0.018) loss 1.7119 (1.7506) lr 7.5000e-03 eta 0:04:04
epoch [11/30] batch [24/40] time 0.292 (0.312) data 0.000 (0.017) loss 2.1035 (1.7675) lr 7.5000e-03 eta 0:04:02
epoch [11/30] batch [26/40] time 0.287 (0.310) data 0.000 (0.016) loss 1.2471 (1.7443) lr 7.5000e-03 eta 0:03:59
epoch [11/30] batch [28/40] time 0.285 (0.308) data 0.000 (0.015) loss 1.4502 (1.7257) lr 7.5000e-03 eta 0:03:58
epoch [11/30] batch [30/40] time 0.288 (0.307) data 0.000 (0.014) loss 2.1055 (1.7580) lr 7.5000e-03 eta 0:03:56
epoch [11/30] batch [32/40] time 0.282 (0.305) data 0.000 (0.013) loss 2.9668 (1.8311) lr 7.5000e-03 eta 0:03:54
epoch [11/30] batch [34/40] time 0.288 (0.304) data 0.000 (0.012) loss 1.9297 (1.8467) lr 7.5000e-03 eta 0:03:53
epoch [11/30] batch [36/40] time 0.282 (0.303) data 0.000 (0.011) loss 1.0928 (1.8343) lr 7.5000e-03 eta 0:03:51
epoch [11/30] batch [38/40] time 0.284 (0.302) data 0.000 (0.011) loss 2.0039 (1.8463) lr 7.5000e-03 eta 0:03:50
epoch [11/30] batch [40/40] time 0.286 (0.301) data 0.000 (0.010) loss 1.9912 (1.8499) lr 7.0337e-03 eta 0:03:49
epoch [12/30] batch [2/40] time 0.288 (0.516) data 0.000 (0.206) loss 2.2363 (1.3904) lr 7.0337e-03 eta 0:06:31
epoch [12/30] batch [4/40] time 0.280 (0.401) data 0.000 (0.103) loss 1.9004 (1.5948) lr 7.0337e-03 eta 0:05:03
epoch [12/30] batch [6/40] time 0.286 (0.361) data 0.000 (0.069) loss 3.5273 (1.9897) lr 7.0337e-03 eta 0:04:32
epoch [12/30] batch [8/40] time 0.277 (0.342) data 0.000 (0.052) loss 1.2734 (1.8652) lr 7.0337e-03 eta 0:04:17
epoch [12/30] batch [10/40] time 0.279 (0.329) data 0.000 (0.041) loss 1.2939 (1.6744) lr 7.0337e-03 eta 0:04:07
epoch [12/30] batch [12/40] time 0.286 (0.322) data 0.000 (0.035) loss 0.6689 (1.5726) lr 7.0337e-03 eta 0:04:00
epoch [12/30] batch [14/40] time 0.278 (0.315) data 0.000 (0.030) loss 2.2168 (1.7596) lr 7.0337e-03 eta 0:03:55
epoch [12/30] batch [16/40] time 0.278 (0.311) data 0.000 (0.026) loss 2.4961 (1.7928) lr 7.0337e-03 eta 0:03:51
epoch [12/30] batch [18/40] time 0.272 (0.307) data 0.000 (0.023) loss 0.5981 (1.6994) lr 7.0337e-03 eta 0:03:47
epoch [12/30] batch [20/40] time 0.273 (0.304) data 0.000 (0.021) loss 0.5244 (1.6512) lr 7.0337e-03 eta 0:03:44
epoch [12/30] batch [22/40] time 0.271 (0.301) data 0.000 (0.019) loss 2.7598 (1.6604) lr 7.0337e-03 eta 0:03:41
epoch [12/30] batch [24/40] time 0.274 (0.298) data 0.000 (0.017) loss 1.5283 (1.6135) lr 7.0337e-03 eta 0:03:39
epoch [12/30] batch [26/40] time 0.271 (0.296) data 0.000 (0.016) loss 1.6162 (1.5750) lr 7.0337e-03 eta 0:03:37
epoch [12/30] batch [28/40] time 0.279 (0.295) data 0.000 (0.015) loss 2.4746 (1.5723) lr 7.0337e-03 eta 0:03:35
epoch [12/30] batch [30/40] time 0.268 (0.293) data 0.000 (0.014) loss 0.5977 (1.5319) lr 7.0337e-03 eta 0:03:33
epoch [12/30] batch [32/40] time 0.278 (0.292) data 0.000 (0.013) loss 0.7588 (1.4760) lr 7.0337e-03 eta 0:03:32
epoch [12/30] batch [34/40] time 0.275 (0.291) data 0.000 (0.012) loss 2.5156 (1.5487) lr 7.0337e-03 eta 0:03:31
epoch [12/30] batch [36/40] time 0.270 (0.290) data 0.000 (0.012) loss 4.0742 (1.6087) lr 7.0337e-03 eta 0:03:29
epoch [12/30] batch [38/40] time 0.274 (0.289) data 0.000 (0.011) loss 0.8096 (1.6214) lr 7.0337e-03 eta 0:03:28
epoch [12/30] batch [40/40] time 0.271 (0.288) data 0.000 (0.011) loss 1.3369 (1.6102) lr 6.5451e-03 eta 0:03:27
epoch [13/30] batch [2/40] time 0.281 (0.515) data 0.001 (0.210) loss 3.9941 (2.2126) lr 6.5451e-03 eta 0:06:09
epoch [13/30] batch [4/40] time 0.387 (0.424) data 0.000 (0.105) loss 1.6504 (1.8478) lr 6.5451e-03 eta 0:05:03
epoch [13/30] batch [6/40] time 0.279 (0.378) data 0.000 (0.070) loss 1.7842 (1.6811) lr 6.5451e-03 eta 0:04:30
epoch [13/30] batch [8/40] time 0.281 (0.354) data 0.000 (0.053) loss 2.7012 (1.6999) lr 6.5451e-03 eta 0:04:11
epoch [13/30] batch [10/40] time 0.279 (0.339) data 0.000 (0.042) loss 1.1465 (1.5976) lr 6.5451e-03 eta 0:04:00
epoch [13/30] batch [12/40] time 0.285 (0.329) data 0.000 (0.035) loss 0.8296 (1.4356) lr 6.5451e-03 eta 0:03:52
epoch [13/30] batch [14/40] time 0.285 (0.323) data 0.000 (0.030) loss 1.9570 (1.4938) lr 6.5451e-03 eta 0:03:47
epoch [13/30] batch [16/40] time 0.281 (0.318) data 0.000 (0.027) loss 1.1738 (1.6274) lr 6.5451e-03 eta 0:03:43
epoch [13/30] batch [18/40] time 0.274 (0.313) data 0.000 (0.024) loss 1.6113 (1.6161) lr 6.5451e-03 eta 0:03:39
epoch [13/30] batch [20/40] time 0.279 (0.309) data 0.000 (0.021) loss 1.5898 (1.6278) lr 6.5451e-03 eta 0:03:36
epoch [13/30] batch [22/40] time 0.273 (0.306) data 0.000 (0.019) loss 1.7578 (1.5773) lr 6.5451e-03 eta 0:03:33
epoch [13/30] batch [24/40] time 0.276 (0.304) data 0.000 (0.018) loss 0.8760 (1.5316) lr 6.5451e-03 eta 0:03:31
epoch [13/30] batch [26/40] time 0.272 (0.301) data 0.000 (0.016) loss 2.2305 (1.6163) lr 6.5451e-03 eta 0:03:29
epoch [13/30] batch [28/40] time 0.274 (0.299) data 0.000 (0.015) loss 0.8809 (1.6766) lr 6.5451e-03 eta 0:03:27
epoch [13/30] batch [30/40] time 0.279 (0.298) data 0.000 (0.014) loss 1.6562 (1.6590) lr 6.5451e-03 eta 0:03:25
epoch [13/30] batch [32/40] time 0.275 (0.296) data 0.000 (0.013) loss 1.7100 (1.6284) lr 6.5451e-03 eta 0:03:23
epoch [13/30] batch [34/40] time 0.274 (0.295) data 0.000 (0.013) loss 0.7280 (1.5898) lr 6.5451e-03 eta 0:03:22
epoch [13/30] batch [36/40] time 0.280 (0.294) data 0.000 (0.012) loss 1.7598 (1.5740) lr 6.5451e-03 eta 0:03:21
epoch [13/30] batch [38/40] time 0.275 (0.293) data 0.000 (0.011) loss 1.5889 (1.6121) lr 6.5451e-03 eta 0:03:20
epoch [13/30] batch [40/40] time 0.274 (0.293) data 0.000 (0.011) loss 3.6172 (1.6465) lr 6.0396e-03 eta 0:03:18
epoch [14/30] batch [2/40] time 0.289 (0.516) data 0.000 (0.203) loss 4.0898 (2.3130) lr 6.0396e-03 eta 0:05:49
epoch [14/30] batch [4/40] time 0.301 (0.403) data 0.000 (0.102) loss 1.5732 (1.6483) lr 6.0396e-03 eta 0:04:32
epoch [14/30] batch [6/40] time 0.284 (0.363) data 0.000 (0.068) loss 1.8594 (1.7506) lr 6.0396e-03 eta 0:04:04
epoch [14/30] batch [8/40] time 0.280 (0.342) data 0.000 (0.051) loss 1.5322 (1.6680) lr 6.0396e-03 eta 0:03:49
epoch [14/30] batch [10/40] time 0.279 (0.330) data 0.000 (0.041) loss 0.4695 (1.4881) lr 6.0396e-03 eta 0:03:41
epoch [14/30] batch [12/40] time 0.280 (0.322) data 0.000 (0.034) loss 1.5635 (1.4397) lr 6.0396e-03 eta 0:03:34
epoch [14/30] batch [14/40] time 0.280 (0.316) data 0.000 (0.029) loss 3.6113 (1.6305) lr 6.0396e-03 eta 0:03:30
epoch [14/30] batch [16/40] time 0.282 (0.311) data 0.000 (0.026) loss 0.8037 (1.5941) lr 6.0396e-03 eta 0:03:26
epoch [14/30] batch [18/40] time 0.275 (0.308) data 0.000 (0.023) loss 0.6646 (1.4761) lr 6.0396e-03 eta 0:03:23
epoch [14/30] batch [20/40] time 0.274 (0.304) data 0.000 (0.021) loss 0.7104 (1.4034) lr 6.0396e-03 eta 0:03:20
epoch [14/30] batch [22/40] time 0.271 (0.301) data 0.000 (0.019) loss 0.7354 (1.3650) lr 6.0396e-03 eta 0:03:18
epoch [14/30] batch [24/40] time 0.277 (0.299) data 0.000 (0.017) loss 2.1992 (1.3879) lr 6.0396e-03 eta 0:03:16
epoch [14/30] batch [26/40] time 0.278 (0.298) data 0.000 (0.016) loss 0.4883 (1.3332) lr 6.0396e-03 eta 0:03:14
epoch [14/30] batch [28/40] time 0.285 (0.296) data 0.000 (0.015) loss 1.2119 (1.3185) lr 6.0396e-03 eta 0:03:13
epoch [14/30] batch [30/40] time 0.273 (0.295) data 0.000 (0.014) loss 1.5254 (1.4387) lr 6.0396e-03 eta 0:03:11
epoch [14/30] batch [32/40] time 0.276 (0.294) data 0.000 (0.013) loss 1.7480 (1.4301) lr 6.0396e-03 eta 0:03:10
epoch [14/30] batch [34/40] time 0.280 (0.293) data 0.000 (0.012) loss 1.3750 (1.4482) lr 6.0396e-03 eta 0:03:09
epoch [14/30] batch [36/40] time 0.275 (0.292) data 0.000 (0.011) loss 1.2275 (1.4252) lr 6.0396e-03 eta 0:03:08
epoch [14/30] batch [38/40] time 0.277 (0.291) data 0.000 (0.011) loss 2.3145 (1.4668) lr 6.0396e-03 eta 0:03:06
epoch [14/30] batch [40/40] time 0.283 (0.290) data 0.000 (0.010) loss 1.4258 (1.4485) lr 5.5226e-03 eta 0:03:05
epoch [15/30] batch [2/40] time 0.281 (0.519) data 0.000 (0.210) loss 1.2969 (1.6157) lr 5.5226e-03 eta 0:05:30
epoch [15/30] batch [4/40] time 0.280 (0.400) data 0.000 (0.105) loss 0.6880 (1.3256) lr 5.5226e-03 eta 0:04:14
epoch [15/30] batch [6/40] time 0.278 (0.359) data 0.000 (0.070) loss 1.5244 (1.2429) lr 5.5226e-03 eta 0:03:47
epoch [15/30] batch [8/40] time 0.287 (0.341) data 0.000 (0.053) loss 2.7559 (1.4641) lr 5.5226e-03 eta 0:03:35
epoch [15/30] batch [10/40] time 0.291 (0.330) data 0.000 (0.042) loss 1.8447 (1.4521) lr 5.5226e-03 eta 0:03:27
epoch [15/30] batch [12/40] time 0.283 (0.322) data 0.000 (0.035) loss 3.8750 (1.6294) lr 5.5226e-03 eta 0:03:22
epoch [15/30] batch [14/40] time 0.281 (0.317) data 0.000 (0.030) loss 1.2500 (1.5467) lr 5.5226e-03 eta 0:03:18
epoch [15/30] batch [16/40] time 0.277 (0.312) data 0.000 (0.027) loss 0.5854 (1.5749) lr 5.5226e-03 eta 0:03:14
epoch [15/30] batch [18/40] time 0.278 (0.308) data 0.000 (0.024) loss 2.0723 (1.5604) lr 5.5226e-03 eta 0:03:11
epoch [15/30] batch [20/40] time 0.281 (0.306) data 0.000 (0.021) loss 1.4443 (1.5975) lr 5.5226e-03 eta 0:03:09
epoch [15/30] batch [22/40] time 0.277 (0.303) data 0.000 (0.019) loss 0.5107 (1.5591) lr 5.5226e-03 eta 0:03:07
epoch [15/30] batch [24/40] time 0.278 (0.301) data 0.000 (0.018) loss 0.9136 (1.5296) lr 5.5226e-03 eta 0:03:05
epoch [15/30] batch [26/40] time 0.381 (0.303) data 0.000 (0.016) loss 0.5332 (1.4901) lr 5.5226e-03 eta 0:03:06
epoch [15/30] batch [28/40] time 0.278 (0.301) data 0.000 (0.015) loss 0.4500 (1.4490) lr 5.5226e-03 eta 0:03:04
epoch [15/30] batch [30/40] time 0.274 (0.299) data 0.000 (0.014) loss 1.6953 (1.4359) lr 5.5226e-03 eta 0:03:02
epoch [15/30] batch [32/40] time 0.274 (0.298) data 0.000 (0.013) loss 3.1836 (1.4852) lr 5.5226e-03 eta 0:03:00
epoch [15/30] batch [34/40] time 0.277 (0.297) data 0.000 (0.013) loss 1.0049 (1.4781) lr 5.5226e-03 eta 0:02:59
epoch [15/30] batch [36/40] time 0.278 (0.295) data 0.000 (0.012) loss 0.3779 (1.4134) lr 5.5226e-03 eta 0:02:58
epoch [15/30] batch [38/40] time 0.276 (0.294) data 0.000 (0.011) loss 1.3066 (1.4160) lr 5.5226e-03 eta 0:02:57
epoch [15/30] batch [40/40] time 0.275 (0.293) data 0.000 (0.011) loss 0.6436 (1.3860) lr 5.0000e-03 eta 0:02:56
epoch [16/30] batch [2/40] time 0.283 (0.533) data 0.000 (0.223) loss 0.9111 (1.2822) lr 5.0000e-03 eta 0:05:18
epoch [16/30] batch [4/40] time 0.282 (0.408) data 0.000 (0.112) loss 2.6465 (2.3418) lr 5.0000e-03 eta 0:04:03
epoch [16/30] batch [6/40] time 0.280 (0.365) data 0.000 (0.075) loss 0.9517 (2.1567) lr 5.0000e-03 eta 0:03:36
epoch [16/30] batch [8/40] time 0.283 (0.345) data 0.000 (0.056) loss 3.1562 (2.1644) lr 5.0000e-03 eta 0:03:24
epoch [16/30] batch [10/40] time 0.285 (0.332) data 0.000 (0.045) loss 0.5278 (1.9330) lr 5.0000e-03 eta 0:03:15
epoch [16/30] batch [12/40] time 0.282 (0.323) data 0.000 (0.037) loss 0.5884 (1.7287) lr 5.0000e-03 eta 0:03:09
epoch [16/30] batch [14/40] time 0.279 (0.317) data 0.000 (0.032) loss 1.2549 (1.6739) lr 5.0000e-03 eta 0:03:05
epoch [16/30] batch [16/40] time 0.282 (0.313) data 0.000 (0.028) loss 1.3672 (1.6773) lr 5.0000e-03 eta 0:03:02
epoch [16/30] batch [18/40] time 0.276 (0.309) data 0.000 (0.025) loss 1.6475 (1.6368) lr 5.0000e-03 eta 0:02:59
epoch [16/30] batch [20/40] time 0.275 (0.305) data 0.000 (0.023) loss 0.2834 (1.5192) lr 5.0000e-03 eta 0:02:57
epoch [16/30] batch [22/40] time 0.273 (0.302) data 0.000 (0.021) loss 1.0615 (1.4793) lr 5.0000e-03 eta 0:02:54
epoch [16/30] batch [24/40] time 0.275 (0.300) data 0.000 (0.019) loss 0.6025 (1.4514) lr 5.0000e-03 eta 0:02:52
epoch [16/30] batch [26/40] time 0.278 (0.298) data 0.000 (0.017) loss 1.6250 (1.4227) lr 5.0000e-03 eta 0:02:51
epoch [16/30] batch [28/40] time 0.279 (0.297) data 0.000 (0.016) loss 0.9404 (1.4095) lr 5.0000e-03 eta 0:02:49
epoch [16/30] batch [30/40] time 0.277 (0.295) data 0.000 (0.015) loss 0.5591 (1.3954) lr 5.0000e-03 eta 0:02:48
epoch [16/30] batch [32/40] time 0.276 (0.294) data 0.000 (0.014) loss 2.3789 (1.3912) lr 5.0000e-03 eta 0:02:47
epoch [16/30] batch [34/40] time 0.269 (0.293) data 0.000 (0.013) loss 3.2480 (1.4345) lr 5.0000e-03 eta 0:02:45
epoch [16/30] batch [36/40] time 0.275 (0.292) data 0.000 (0.013) loss 0.8574 (1.4355) lr 5.0000e-03 eta 0:02:44
epoch [16/30] batch [38/40] time 0.280 (0.291) data 0.000 (0.012) loss 1.1104 (1.4015) lr 5.0000e-03 eta 0:02:43
epoch [16/30] batch [40/40] time 0.276 (0.290) data 0.000 (0.011) loss 0.6904 (1.4180) lr 4.4774e-03 eta 0:02:42
epoch [17/30] batch [2/40] time 0.289 (0.530) data 0.000 (0.221) loss 0.8472 (0.7097) lr 4.4774e-03 eta 0:04:55
epoch [17/30] batch [4/40] time 0.282 (0.406) data 0.000 (0.111) loss 1.2725 (1.1127) lr 4.4774e-03 eta 0:03:45
epoch [17/30] batch [6/40] time 0.285 (0.365) data 0.000 (0.074) loss 0.4092 (1.0408) lr 4.4774e-03 eta 0:03:22
epoch [17/30] batch [8/40] time 0.284 (0.345) data 0.000 (0.056) loss 1.9453 (1.3143) lr 4.4774e-03 eta 0:03:10
epoch [17/30] batch [10/40] time 0.297 (0.334) data 0.000 (0.044) loss 1.3350 (1.2665) lr 4.4774e-03 eta 0:03:03
epoch [17/30] batch [12/40] time 0.276 (0.325) data 0.000 (0.037) loss 0.4736 (1.1779) lr 4.4774e-03 eta 0:02:58
epoch [17/30] batch [14/40] time 0.280 (0.318) data 0.000 (0.032) loss 0.6050 (1.1780) lr 4.4774e-03 eta 0:02:53
epoch [17/30] batch [16/40] time 0.285 (0.314) data 0.000 (0.028) loss 0.6035 (1.1740) lr 4.4774e-03 eta 0:02:50
epoch [17/30] batch [18/40] time 0.274 (0.310) data 0.000 (0.025) loss 0.6245 (1.1907) lr 4.4774e-03 eta 0:02:48
epoch [17/30] batch [20/40] time 0.275 (0.306) data 0.000 (0.022) loss 0.7427 (1.1900) lr 4.4774e-03 eta 0:02:45
epoch [17/30] batch [22/40] time 0.278 (0.304) data 0.000 (0.020) loss 0.8638 (1.2176) lr 4.4774e-03 eta 0:02:43
epoch [17/30] batch [24/40] time 0.277 (0.301) data 0.000 (0.019) loss 1.4785 (1.1874) lr 4.4774e-03 eta 0:02:41
epoch [17/30] batch [26/40] time 0.276 (0.299) data 0.000 (0.017) loss 0.3459 (1.1276) lr 4.4774e-03 eta 0:02:39
epoch [17/30] batch [28/40] time 0.277 (0.298) data 0.000 (0.016) loss 0.3464 (1.1263) lr 4.4774e-03 eta 0:02:38
epoch [17/30] batch [30/40] time 0.273 (0.296) data 0.000 (0.015) loss 0.6709 (1.1150) lr 4.4774e-03 eta 0:02:36
epoch [17/30] batch [32/40] time 0.274 (0.295) data 0.000 (0.014) loss 2.3418 (1.2313) lr 4.4774e-03 eta 0:02:35
epoch [17/30] batch [34/40] time 0.278 (0.294) data 0.000 (0.013) loss 1.1562 (1.2097) lr 4.4774e-03 eta 0:02:34
epoch [17/30] batch [36/40] time 0.274 (0.293) data 0.000 (0.013) loss 1.6074 (1.2120) lr 4.4774e-03 eta 0:02:33
epoch [17/30] batch [38/40] time 0.274 (0.292) data 0.000 (0.012) loss 0.9658 (1.2004) lr 4.4774e-03 eta 0:02:32
epoch [17/30] batch [40/40] time 0.272 (0.293) data 0.000 (0.011) loss 0.6323 (1.2105) lr 3.9604e-03 eta 0:02:32
epoch [18/30] batch [2/40] time 0.280 (0.517) data 0.000 (0.204) loss 0.4397 (0.4399) lr 3.9604e-03 eta 0:04:27
epoch [18/30] batch [4/40] time 0.281 (0.400) data 0.000 (0.102) loss 3.0332 (1.1725) lr 3.9604e-03 eta 0:03:26
epoch [18/30] batch [6/40] time 0.287 (0.362) data 0.000 (0.068) loss 1.5127 (1.1793) lr 3.9604e-03 eta 0:03:05
epoch [18/30] batch [8/40] time 0.282 (0.342) data 0.000 (0.051) loss 1.0234 (1.0652) lr 3.9604e-03 eta 0:02:54
epoch [18/30] batch [10/40] time 0.277 (0.329) data 0.000 (0.041) loss 1.3994 (1.0220) lr 3.9604e-03 eta 0:02:47
epoch [18/30] batch [12/40] time 0.286 (0.322) data 0.000 (0.034) loss 0.9312 (0.9963) lr 3.9604e-03 eta 0:02:43
epoch [18/30] batch [14/40] time 0.282 (0.316) data 0.000 (0.029) loss 0.5942 (0.9381) lr 3.9604e-03 eta 0:02:39
epoch [18/30] batch [16/40] time 0.286 (0.312) data 0.000 (0.026) loss 0.7500 (0.9021) lr 3.9604e-03 eta 0:02:37
epoch [18/30] batch [18/40] time 0.273 (0.308) data 0.000 (0.023) loss 0.4290 (0.8370) lr 3.9604e-03 eta 0:02:34
epoch [18/30] batch [20/40] time 0.272 (0.305) data 0.000 (0.021) loss 0.3418 (0.9307) lr 3.9604e-03 eta 0:02:32
epoch [18/30] batch [22/40] time 0.276 (0.302) data 0.000 (0.019) loss 1.2588 (0.9531) lr 3.9604e-03 eta 0:02:30
epoch [18/30] batch [24/40] time 0.274 (0.300) data 0.000 (0.017) loss 1.4365 (0.9525) lr 3.9604e-03 eta 0:02:28
epoch [18/30] batch [26/40] time 0.270 (0.298) data 0.000 (0.016) loss 0.2014 (0.9213) lr 3.9604e-03 eta 0:02:26
epoch [18/30] batch [28/40] time 0.273 (0.296) data 0.000 (0.015) loss 0.4160 (0.9638) lr 3.9604e-03 eta 0:02:25
epoch [18/30] batch [30/40] time 0.279 (0.295) data 0.000 (0.014) loss 2.8867 (1.0546) lr 3.9604e-03 eta 0:02:24
epoch [18/30] batch [32/40] time 0.273 (0.293) data 0.000 (0.013) loss 2.6484 (1.0745) lr 3.9604e-03 eta 0:02:23
epoch [18/30] batch [34/40] time 0.276 (0.292) data 0.000 (0.012) loss 0.3494 (1.0580) lr 3.9604e-03 eta 0:02:22
epoch [18/30] batch [36/40] time 0.275 (0.291) data 0.000 (0.012) loss 0.8052 (1.0400) lr 3.9604e-03 eta 0:02:20
epoch [18/30] batch [38/40] time 0.277 (0.290) data 0.000 (0.011) loss 0.7329 (1.0340) lr 3.9604e-03 eta 0:02:20
epoch [18/30] batch [40/40] time 0.279 (0.290) data 0.000 (0.010) loss 3.0820 (1.1291) lr 3.4549e-03 eta 0:02:19
epoch [19/30] batch [2/40] time 0.281 (0.513) data 0.000 (0.210) loss 2.6562 (2.4443) lr 3.4549e-03 eta 0:04:05
epoch [19/30] batch [4/40] time 0.284 (0.398) data 0.000 (0.105) loss 0.7646 (1.5093) lr 3.4549e-03 eta 0:03:09
epoch [19/30] batch [6/40] time 0.280 (0.360) data 0.000 (0.070) loss 1.2734 (1.2751) lr 3.4549e-03 eta 0:02:50
epoch [19/30] batch [8/40] time 0.283 (0.340) data 0.000 (0.053) loss 1.1055 (1.1639) lr 3.4549e-03 eta 0:02:40
epoch [19/30] batch [10/40] time 0.281 (0.328) data 0.000 (0.042) loss 2.9961 (1.2901) lr 3.4549e-03 eta 0:02:34
epoch [19/30] batch [12/40] time 0.286 (0.320) data 0.000 (0.035) loss 1.6035 (1.2661) lr 3.4549e-03 eta 0:02:29
epoch [19/30] batch [14/40] time 0.281 (0.315) data 0.000 (0.030) loss 1.4326 (1.3371) lr 3.4549e-03 eta 0:02:26
epoch [19/30] batch [16/40] time 0.285 (0.311) data 0.000 (0.026) loss 0.8701 (1.2461) lr 3.4549e-03 eta 0:02:24
epoch [19/30] batch [18/40] time 0.272 (0.307) data 0.000 (0.024) loss 0.7871 (1.2244) lr 3.4549e-03 eta 0:02:21
epoch [19/30] batch [20/40] time 0.274 (0.303) data 0.000 (0.021) loss 0.8950 (1.1672) lr 3.4549e-03 eta 0:02:19
epoch [19/30] batch [22/40] time 0.275 (0.305) data 0.000 (0.019) loss 1.9160 (1.1691) lr 3.4549e-03 eta 0:02:19
epoch [19/30] batch [24/40] time 0.275 (0.303) data 0.000 (0.018) loss 1.2939 (1.1644) lr 3.4549e-03 eta 0:02:17
epoch [19/30] batch [26/40] time 0.275 (0.301) data 0.000 (0.016) loss 0.6436 (1.1349) lr 3.4549e-03 eta 0:02:16
epoch [19/30] batch [28/40] time 0.275 (0.299) data 0.000 (0.015) loss 0.3594 (1.1031) lr 3.4549e-03 eta 0:02:15
epoch [19/30] batch [30/40] time 0.273 (0.297) data 0.000 (0.014) loss 0.2588 (1.0792) lr 3.4549e-03 eta 0:02:13
epoch [19/30] batch [32/40] time 0.269 (0.296) data 0.000 (0.013) loss 0.2065 (1.0422) lr 3.4549e-03 eta 0:02:12
epoch [19/30] batch [34/40] time 0.272 (0.294) data 0.000 (0.013) loss 2.1016 (1.1145) lr 3.4549e-03 eta 0:02:11
epoch [19/30] batch [36/40] time 0.273 (0.293) data 0.000 (0.012) loss 1.0684 (1.1034) lr 3.4549e-03 eta 0:02:10
epoch [19/30] batch [38/40] time 0.275 (0.292) data 0.000 (0.011) loss 1.9844 (1.1400) lr 3.4549e-03 eta 0:02:09
epoch [19/30] batch [40/40] time 0.277 (0.291) data 0.000 (0.011) loss 1.4033 (1.1304) lr 2.9663e-03 eta 0:02:08
epoch [20/30] batch [2/40] time 0.295 (0.535) data 0.000 (0.208) loss 0.3386 (0.4681) lr 2.9663e-03 eta 0:03:54
epoch [20/30] batch [4/40] time 0.297 (0.415) data 0.000 (0.104) loss 0.6099 (0.4439) lr 2.9663e-03 eta 0:03:00
epoch [20/30] batch [6/40] time 0.297 (0.376) data 0.000 (0.069) loss 1.3145 (0.7653) lr 2.9663e-03 eta 0:02:43
epoch [20/30] batch [8/40] time 0.293 (0.356) data 0.000 (0.052) loss 0.7964 (0.7898) lr 2.9663e-03 eta 0:02:33
epoch [20/30] batch [10/40] time 0.290 (0.343) data 0.000 (0.042) loss 0.3440 (0.7832) lr 2.9663e-03 eta 0:02:27
epoch [20/30] batch [12/40] time 0.300 (0.335) data 0.000 (0.035) loss 1.2783 (0.7987) lr 2.9663e-03 eta 0:02:23
epoch [20/30] batch [14/40] time 0.293 (0.329) data 0.000 (0.030) loss 0.3665 (0.7416) lr 2.9663e-03 eta 0:02:20
epoch [20/30] batch [16/40] time 0.303 (0.325) data 0.000 (0.026) loss 2.5469 (0.8705) lr 2.9663e-03 eta 0:02:17
epoch [20/30] batch [18/40] time 0.287 (0.321) data 0.000 (0.023) loss 1.0020 (0.8475) lr 2.9663e-03 eta 0:02:15
epoch [20/30] batch [20/40] time 0.283 (0.317) data 0.000 (0.021) loss 0.7158 (0.8465) lr 2.9663e-03 eta 0:02:13
epoch [20/30] batch [22/40] time 0.285 (0.314) data 0.000 (0.019) loss 0.4160 (0.8109) lr 2.9663e-03 eta 0:02:11
epoch [20/30] batch [24/40] time 0.286 (0.312) data 0.000 (0.018) loss 2.0273 (0.8347) lr 2.9663e-03 eta 0:02:09
epoch [20/30] batch [26/40] time 0.288 (0.310) data 0.000 (0.016) loss 0.1221 (0.8202) lr 2.9663e-03 eta 0:02:08
epoch [20/30] batch [28/40] time 0.283 (0.308) data 0.000 (0.015) loss 0.6445 (0.8076) lr 2.9663e-03 eta 0:02:06
epoch [20/30] batch [30/40] time 0.284 (0.307) data 0.000 (0.014) loss 0.6084 (0.8612) lr 2.9663e-03 eta 0:02:05
epoch [20/30] batch [32/40] time 0.287 (0.305) data 0.000 (0.013) loss 2.6133 (0.9259) lr 2.9663e-03 eta 0:02:04
epoch [20/30] batch [34/40] time 0.283 (0.304) data 0.000 (0.012) loss 0.5698 (0.9409) lr 2.9663e-03 eta 0:02:03
epoch [20/30] batch [36/40] time 0.290 (0.303) data 0.000 (0.012) loss 0.7266 (0.9298) lr 2.9663e-03 eta 0:02:02
epoch [20/30] batch [38/40] time 0.288 (0.303) data 0.000 (0.011) loss 0.5513 (0.9553) lr 2.9663e-03 eta 0:02:01
epoch [20/30] batch [40/40] time 0.284 (0.302) data 0.000 (0.011) loss 0.8354 (0.9529) lr 2.5000e-03 eta 0:02:00
Checkpoint saved to output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3/prompt_learner/model.pth.tar-20
epoch [21/30] batch [2/40] time 0.295 (0.538) data 0.000 (0.210) loss 0.9502 (0.7393) lr 2.5000e-03 eta 0:03:33
epoch [21/30] batch [4/40] time 0.293 (0.414) data 0.000 (0.105) loss 0.2174 (0.5381) lr 2.5000e-03 eta 0:02:44
epoch [21/30] batch [6/40] time 0.295 (0.374) data 0.000 (0.070) loss 1.7988 (0.7598) lr 2.5000e-03 eta 0:02:27
epoch [21/30] batch [8/40] time 0.295 (0.354) data 0.000 (0.053) loss 0.2201 (0.7560) lr 2.5000e-03 eta 0:02:18
epoch [21/30] batch [10/40] time 0.289 (0.342) data 0.000 (0.042) loss 1.2539 (0.8825) lr 2.5000e-03 eta 0:02:13
epoch [21/30] batch [12/40] time 0.283 (0.332) data 0.000 (0.035) loss 0.3621 (0.7769) lr 2.5000e-03 eta 0:02:08
epoch [21/30] batch [14/40] time 0.283 (0.325) data 0.000 (0.030) loss 0.5703 (0.7653) lr 2.5000e-03 eta 0:02:05
epoch [21/30] batch [16/40] time 0.384 (0.326) data 0.000 (0.026) loss 0.6519 (0.7602) lr 2.5000e-03 eta 0:02:05
epoch [21/30] batch [18/40] time 0.273 (0.320) data 0.000 (0.024) loss 0.2080 (0.8008) lr 2.5000e-03 eta 0:02:02
epoch [21/30] batch [20/40] time 0.276 (0.316) data 0.000 (0.021) loss 1.5459 (0.8161) lr 2.5000e-03 eta 0:02:00
epoch [21/30] batch [22/40] time 0.276 (0.312) data 0.000 (0.019) loss 0.2871 (0.7957) lr 2.5000e-03 eta 0:01:58
epoch [21/30] batch [24/40] time 0.275 (0.309) data 0.000 (0.018) loss 0.2363 (0.7565) lr 2.5000e-03 eta 0:01:56
epoch [21/30] batch [26/40] time 0.274 (0.307) data 0.000 (0.016) loss 2.4219 (0.8017) lr 2.5000e-03 eta 0:01:54
epoch [21/30] batch [28/40] time 0.273 (0.304) data 0.000 (0.015) loss 0.7808 (0.7988) lr 2.5000e-03 eta 0:01:53
epoch [21/30] batch [30/40] time 0.275 (0.302) data 0.000 (0.014) loss 2.2754 (0.8594) lr 2.5000e-03 eta 0:01:51
epoch [21/30] batch [32/40] time 0.276 (0.301) data 0.000 (0.013) loss 0.6221 (0.9113) lr 2.5000e-03 eta 0:01:50
epoch [21/30] batch [34/40] time 0.271 (0.299) data 0.000 (0.013) loss 0.7393 (0.8959) lr 2.5000e-03 eta 0:01:49
epoch [21/30] batch [36/40] time 0.273 (0.298) data 0.000 (0.012) loss 2.2266 (0.9317) lr 2.5000e-03 eta 0:01:48
epoch [21/30] batch [38/40] time 0.275 (0.296) data 0.000 (0.011) loss 0.6782 (0.9147) lr 2.5000e-03 eta 0:01:47
epoch [21/30] batch [40/40] time 0.274 (0.295) data 0.000 (0.011) loss 0.7510 (0.8917) lr 2.0611e-03 eta 0:01:46
epoch [22/30] batch [2/40] time 0.293 (0.528) data 0.000 (0.214) loss 0.3918 (0.2947) lr 2.0611e-03 eta 0:03:09
epoch [22/30] batch [4/40] time 0.281 (0.405) data 0.000 (0.107) loss 0.5225 (0.2999) lr 2.0611e-03 eta 0:02:24
epoch [22/30] batch [6/40] time 0.285 (0.364) data 0.000 (0.072) loss 1.2100 (0.8661) lr 2.0611e-03 eta 0:02:09
epoch [22/30] batch [8/40] time 0.284 (0.345) data 0.000 (0.054) loss 0.5488 (0.7310) lr 2.0611e-03 eta 0:02:01
epoch [22/30] batch [10/40] time 0.282 (0.333) data 0.000 (0.043) loss 2.0293 (0.8511) lr 2.0611e-03 eta 0:01:56
epoch [22/30] batch [12/40] time 0.278 (0.324) data 0.000 (0.036) loss 2.4004 (0.9348) lr 2.0611e-03 eta 0:01:52
epoch [22/30] batch [14/40] time 0.284 (0.319) data 0.000 (0.031) loss 0.9624 (0.9054) lr 2.0611e-03 eta 0:01:50
epoch [22/30] batch [16/40] time 0.274 (0.315) data 0.000 (0.027) loss 1.1934 (0.8967) lr 2.0611e-03 eta 0:01:48
epoch [22/30] batch [18/40] time 0.277 (0.311) data 0.000 (0.024) loss 0.2484 (0.8437) lr 2.0611e-03 eta 0:01:46
epoch [22/30] batch [20/40] time 0.280 (0.308) data 0.000 (0.022) loss 0.6182 (0.8714) lr 2.0611e-03 eta 0:01:44
epoch [22/30] batch [22/40] time 0.273 (0.304) data 0.000 (0.020) loss 0.3105 (0.9094) lr 2.0611e-03 eta 0:01:42
epoch [22/30] batch [24/40] time 0.275 (0.302) data 0.000 (0.018) loss 2.1738 (0.9407) lr 2.0611e-03 eta 0:01:41
epoch [22/30] batch [26/40] time 0.274 (0.300) data 0.000 (0.017) loss 0.6694 (0.9110) lr 2.0611e-03 eta 0:01:40
epoch [22/30] batch [28/40] time 0.275 (0.298) data 0.000 (0.015) loss 2.0898 (0.9774) lr 2.0611e-03 eta 0:01:38
epoch [22/30] batch [30/40] time 0.275 (0.297) data 0.000 (0.014) loss 0.7236 (0.9686) lr 2.0611e-03 eta 0:01:37
epoch [22/30] batch [32/40] time 0.271 (0.295) data 0.000 (0.014) loss 0.6582 (0.9733) lr 2.0611e-03 eta 0:01:36
epoch [22/30] batch [34/40] time 0.275 (0.294) data 0.000 (0.013) loss 0.7876 (0.9513) lr 2.0611e-03 eta 0:01:35
epoch [22/30] batch [36/40] time 0.280 (0.293) data 0.000 (0.012) loss 1.7744 (0.9697) lr 2.0611e-03 eta 0:01:34
epoch [22/30] batch [38/40] time 0.276 (0.292) data 0.000 (0.011) loss 0.5518 (0.9453) lr 2.0611e-03 eta 0:01:34
epoch [22/30] batch [40/40] time 0.272 (0.291) data 0.000 (0.011) loss 0.5322 (0.9309) lr 1.6543e-03 eta 0:01:33
epoch [23/30] batch [2/40] time 0.290 (0.522) data 0.000 (0.211) loss 0.1069 (0.4292) lr 1.6543e-03 eta 0:02:45
epoch [23/30] batch [4/40] time 0.282 (0.403) data 0.000 (0.106) loss 1.0156 (0.6992) lr 1.6543e-03 eta 0:02:07
epoch [23/30] batch [6/40] time 0.285 (0.363) data 0.000 (0.070) loss 1.2637 (0.7351) lr 1.6543e-03 eta 0:01:54
epoch [23/30] batch [8/40] time 0.278 (0.342) data 0.000 (0.053) loss 0.2520 (0.6394) lr 1.6543e-03 eta 0:01:46
epoch [23/30] batch [10/40] time 0.280 (0.330) data 0.000 (0.042) loss 1.2354 (0.7311) lr 1.6543e-03 eta 0:01:42
epoch [23/30] batch [12/40] time 0.283 (0.322) data 0.000 (0.035) loss 0.2800 (0.6944) lr 1.6543e-03 eta 0:01:39
epoch [23/30] batch [14/40] time 0.288 (0.317) data 0.000 (0.030) loss 0.3208 (0.6289) lr 1.6543e-03 eta 0:01:37
epoch [23/30] batch [16/40] time 0.281 (0.313) data 0.000 (0.027) loss 1.1426 (0.7073) lr 1.6543e-03 eta 0:01:35
epoch [23/30] batch [18/40] time 0.272 (0.308) data 0.000 (0.024) loss 0.8740 (0.7000) lr 1.6543e-03 eta 0:01:33
epoch [23/30] batch [20/40] time 0.276 (0.305) data 0.000 (0.021) loss 0.5098 (0.6885) lr 1.6543e-03 eta 0:01:31
epoch [23/30] batch [22/40] time 0.276 (0.302) data 0.000 (0.019) loss 1.8311 (0.7342) lr 1.6543e-03 eta 0:01:30
epoch [23/30] batch [24/40] time 0.275 (0.300) data 0.000 (0.018) loss 0.8105 (0.9638) lr 1.6543e-03 eta 0:01:28
epoch [23/30] batch [26/40] time 0.275 (0.298) data 0.000 (0.016) loss 4.4297 (1.0692) lr 1.6543e-03 eta 0:01:27
epoch [23/30] batch [28/40] time 0.273 (0.296) data 0.000 (0.015) loss 0.4521 (1.0462) lr 1.6543e-03 eta 0:01:26
epoch [23/30] batch [30/40] time 0.277 (0.295) data 0.000 (0.014) loss 3.0781 (1.0966) lr 1.6543e-03 eta 0:01:25
epoch [23/30] batch [32/40] time 0.275 (0.294) data 0.000 (0.013) loss 0.4619 (1.0797) lr 1.6543e-03 eta 0:01:24
epoch [23/30] batch [34/40] time 0.275 (0.292) data 0.000 (0.013) loss 0.3132 (1.0687) lr 1.6543e-03 eta 0:01:23
epoch [23/30] batch [36/40] time 0.277 (0.292) data 0.000 (0.012) loss 0.2676 (1.0357) lr 1.6543e-03 eta 0:01:22
epoch [23/30] batch [38/40] time 0.272 (0.291) data 0.000 (0.011) loss 1.2275 (1.0213) lr 1.6543e-03 eta 0:01:21
epoch [23/30] batch [40/40] time 0.276 (0.290) data 0.000 (0.011) loss 0.8877 (1.0077) lr 1.2843e-03 eta 0:01:21
epoch [24/30] batch [2/40] time 0.282 (0.511) data 0.000 (0.209) loss 1.0771 (0.9375) lr 1.2843e-03 eta 0:02:22
epoch [24/30] batch [4/40] time 0.281 (0.396) data 0.000 (0.105) loss 0.4573 (0.6081) lr 1.2843e-03 eta 0:01:49
epoch [24/30] batch [6/40] time 0.283 (0.358) data 0.000 (0.070) loss 1.4062 (0.7054) lr 1.2843e-03 eta 0:01:38
epoch [24/30] batch [8/40] time 0.284 (0.351) data 0.000 (0.053) loss 0.8945 (0.7106) lr 1.2843e-03 eta 0:01:35
epoch [24/30] batch [10/40] time 0.288 (0.338) data 0.000 (0.042) loss 0.3066 (0.6567) lr 1.2843e-03 eta 0:01:31
epoch [24/30] batch [12/40] time 0.277 (0.329) data 0.000 (0.035) loss 0.4373 (0.6862) lr 1.2843e-03 eta 0:01:28
epoch [24/30] batch [14/40] time 0.280 (0.322) data 0.000 (0.030) loss 0.9644 (0.7859) lr 1.2843e-03 eta 0:01:25
epoch [24/30] batch [16/40] time 0.282 (0.317) data 0.000 (0.026) loss 0.7778 (0.8108) lr 1.2843e-03 eta 0:01:23
epoch [24/30] batch [18/40] time 0.280 (0.313) data 0.000 (0.023) loss 0.5151 (0.7618) lr 1.2843e-03 eta 0:01:21
epoch [24/30] batch [20/40] time 0.274 (0.309) data 0.000 (0.021) loss 3.1680 (0.8502) lr 1.2843e-03 eta 0:01:20
epoch [24/30] batch [22/40] time 0.278 (0.306) data 0.000 (0.019) loss 0.6470 (0.8186) lr 1.2843e-03 eta 0:01:18
epoch [24/30] batch [24/40] time 0.276 (0.304) data 0.000 (0.018) loss 0.7368 (0.8203) lr 1.2843e-03 eta 0:01:17
epoch [24/30] batch [26/40] time 0.274 (0.301) data 0.000 (0.016) loss 0.7417 (0.8639) lr 1.2843e-03 eta 0:01:16
epoch [24/30] batch [28/40] time 0.274 (0.299) data 0.000 (0.015) loss 1.0479 (0.8601) lr 1.2843e-03 eta 0:01:15
epoch [24/30] batch [30/40] time 0.278 (0.298) data 0.000 (0.014) loss 0.0995 (0.8293) lr 1.2843e-03 eta 0:01:14
epoch [24/30] batch [32/40] time 0.276 (0.297) data 0.000 (0.013) loss 0.3574 (0.7983) lr 1.2843e-03 eta 0:01:13
epoch [24/30] batch [34/40] time 0.272 (0.295) data 0.000 (0.013) loss 1.0156 (0.8136) lr 1.2843e-03 eta 0:01:12
epoch [24/30] batch [36/40] time 0.273 (0.294) data 0.000 (0.012) loss 0.5161 (0.8015) lr 1.2843e-03 eta 0:01:11
epoch [24/30] batch [38/40] time 0.278 (0.293) data 0.000 (0.011) loss 0.4910 (0.7872) lr 1.2843e-03 eta 0:01:10
epoch [24/30] batch [40/40] time 0.275 (0.292) data 0.000 (0.011) loss 1.6172 (0.7942) lr 9.5492e-04 eta 0:01:10
epoch [25/30] batch [2/40] time 0.280 (0.510) data 0.000 (0.208) loss 0.6201 (0.5603) lr 9.5492e-04 eta 0:02:01
epoch [25/30] batch [4/40] time 0.282 (0.394) data 0.000 (0.104) loss 2.3359 (0.8855) lr 9.5492e-04 eta 0:01:33
epoch [25/30] batch [6/40] time 0.276 (0.356) data 0.000 (0.069) loss 0.3350 (0.7031) lr 9.5492e-04 eta 0:01:23
epoch [25/30] batch [8/40] time 0.272 (0.335) data 0.000 (0.052) loss 4.4102 (1.1097) lr 9.5492e-04 eta 0:01:17
epoch [25/30] batch [10/40] time 0.279 (0.323) data 0.000 (0.042) loss 0.3982 (0.9452) lr 9.5492e-04 eta 0:01:14
epoch [25/30] batch [12/40] time 0.283 (0.316) data 0.000 (0.035) loss 0.5918 (0.8511) lr 9.5492e-04 eta 0:01:12
epoch [25/30] batch [14/40] time 0.287 (0.312) data 0.000 (0.030) loss 0.9780 (0.8740) lr 9.5492e-04 eta 0:01:10
epoch [25/30] batch [16/40] time 0.282 (0.308) data 0.000 (0.026) loss 1.7803 (0.9163) lr 9.5492e-04 eta 0:01:09
epoch [25/30] batch [18/40] time 0.273 (0.304) data 0.000 (0.023) loss 1.0645 (0.9785) lr 9.5492e-04 eta 0:01:07
epoch [25/30] batch [20/40] time 0.276 (0.301) data 0.000 (0.021) loss 0.7222 (0.9806) lr 9.5492e-04 eta 0:01:06
epoch [25/30] batch [22/40] time 0.281 (0.299) data 0.000 (0.019) loss 0.7993 (0.9473) lr 9.5492e-04 eta 0:01:05
epoch [25/30] batch [24/40] time 0.275 (0.297) data 0.000 (0.018) loss 0.1891 (0.9075) lr 9.5492e-04 eta 0:01:04
epoch [25/30] batch [26/40] time 0.272 (0.295) data 0.000 (0.016) loss 0.4097 (0.9138) lr 9.5492e-04 eta 0:01:03
epoch [25/30] batch [28/40] time 0.277 (0.294) data 0.000 (0.015) loss 2.6719 (1.0163) lr 9.5492e-04 eta 0:01:02
epoch [25/30] batch [30/40] time 0.278 (0.293) data 0.000 (0.014) loss 0.7485 (0.9869) lr 9.5492e-04 eta 0:01:01
epoch [25/30] batch [32/40] time 0.279 (0.292) data 0.000 (0.013) loss 1.2949 (0.9715) lr 9.5492e-04 eta 0:01:00
epoch [25/30] batch [34/40] time 0.277 (0.291) data 0.000 (0.012) loss 0.3184 (0.9399) lr 9.5492e-04 eta 0:00:59
epoch [25/30] batch [36/40] time 0.273 (0.290) data 0.000 (0.012) loss 1.1270 (0.9272) lr 9.5492e-04 eta 0:00:59
epoch [25/30] batch [38/40] time 0.277 (0.289) data 0.000 (0.011) loss 0.3811 (0.9224) lr 9.5492e-04 eta 0:00:58
epoch [25/30] batch [40/40] time 0.273 (0.289) data 0.000 (0.011) loss 0.3682 (0.8915) lr 6.6987e-04 eta 0:00:57
epoch [26/30] batch [2/40] time 0.279 (0.517) data 0.000 (0.209) loss 0.3843 (0.8015) lr 6.6987e-04 eta 0:01:42
epoch [26/30] batch [4/40] time 0.280 (0.398) data 0.000 (0.105) loss 0.2622 (0.6040) lr 6.6987e-04 eta 0:01:18
epoch [26/30] batch [6/40] time 0.285 (0.360) data 0.000 (0.070) loss 0.9141 (0.6102) lr 6.6987e-04 eta 0:01:09
epoch [26/30] batch [8/40] time 0.287 (0.354) data 0.000 (0.053) loss 0.6562 (0.6049) lr 6.6987e-04 eta 0:01:07
epoch [26/30] batch [10/40] time 0.286 (0.339) data 0.000 (0.042) loss 1.2891 (0.6311) lr 6.6987e-04 eta 0:01:04
epoch [26/30] batch [12/40] time 0.278 (0.329) data 0.000 (0.035) loss 0.1564 (0.5885) lr 6.6987e-04 eta 0:01:01
epoch [26/30] batch [14/40] time 0.275 (0.322) data 0.000 (0.030) loss 0.2223 (0.5407) lr 6.6987e-04 eta 0:00:59
epoch [26/30] batch [16/40] time 0.288 (0.317) data 0.000 (0.026) loss 0.4448 (0.5366) lr 6.6987e-04 eta 0:00:58
epoch [26/30] batch [18/40] time 0.272 (0.312) data 0.000 (0.023) loss 0.4758 (0.5829) lr 6.6987e-04 eta 0:00:56
epoch [26/30] batch [20/40] time 0.274 (0.308) data 0.000 (0.021) loss 0.3621 (0.6663) lr 6.6987e-04 eta 0:00:55
epoch [26/30] batch [22/40] time 0.268 (0.305) data 0.000 (0.019) loss 0.5151 (0.6576) lr 6.6987e-04 eta 0:00:54
epoch [26/30] batch [24/40] time 0.272 (0.302) data 0.000 (0.018) loss 1.4082 (0.7807) lr 6.6987e-04 eta 0:00:53
epoch [26/30] batch [26/40] time 0.276 (0.300) data 0.000 (0.016) loss 1.1494 (0.7875) lr 6.6987e-04 eta 0:00:52
epoch [26/30] batch [28/40] time 0.269 (0.298) data 0.000 (0.015) loss 1.4404 (0.8042) lr 6.6987e-04 eta 0:00:51
epoch [26/30] batch [30/40] time 0.272 (0.296) data 0.000 (0.014) loss 0.5659 (0.7926) lr 6.6987e-04 eta 0:00:50
epoch [26/30] batch [32/40] time 0.274 (0.295) data 0.000 (0.013) loss 1.2061 (0.8147) lr 6.6987e-04 eta 0:00:49
epoch [26/30] batch [34/40] time 0.278 (0.294) data 0.000 (0.013) loss 1.7910 (0.9094) lr 6.6987e-04 eta 0:00:48
epoch [26/30] batch [36/40] time 0.273 (0.293) data 0.000 (0.012) loss 1.6777 (0.9210) lr 6.6987e-04 eta 0:00:47
epoch [26/30] batch [38/40] time 0.267 (0.291) data 0.000 (0.011) loss 0.1725 (0.8848) lr 6.6987e-04 eta 0:00:47
epoch [26/30] batch [40/40] time 0.272 (0.290) data 0.000 (0.011) loss 0.3433 (0.8631) lr 4.3227e-04 eta 0:00:46
epoch [27/30] batch [2/40] time 0.279 (0.517) data 0.000 (0.209) loss 0.1637 (1.2401) lr 4.3227e-04 eta 0:01:21
epoch [27/30] batch [4/40] time 0.287 (0.400) data 0.000 (0.105) loss 0.4731 (1.1243) lr 4.3227e-04 eta 0:01:02
epoch [27/30] batch [6/40] time 0.296 (0.363) data 0.000 (0.070) loss 0.6753 (0.9050) lr 4.3227e-04 eta 0:00:55
epoch [27/30] batch [8/40] time 0.293 (0.345) data 0.000 (0.052) loss 0.1987 (0.8923) lr 4.3227e-04 eta 0:00:52
epoch [27/30] batch [10/40] time 0.295 (0.335) data 0.000 (0.042) loss 0.3599 (0.7928) lr 4.3227e-04 eta 0:00:50
epoch [27/30] batch [12/40] time 0.298 (0.329) data 0.000 (0.035) loss 0.3418 (0.7256) lr 4.3227e-04 eta 0:00:48
epoch [27/30] batch [14/40] time 0.290 (0.323) data 0.000 (0.030) loss 1.1250 (0.7128) lr 4.3227e-04 eta 0:00:47
epoch [27/30] batch [16/40] time 0.277 (0.318) data 0.000 (0.026) loss 0.7788 (0.7158) lr 4.3227e-04 eta 0:00:45
epoch [27/30] batch [18/40] time 0.276 (0.313) data 0.000 (0.023) loss 0.5972 (0.6972) lr 4.3227e-04 eta 0:00:44
epoch [27/30] batch [20/40] time 0.275 (0.309) data 0.000 (0.021) loss 0.8691 (0.7250) lr 4.3227e-04 eta 0:00:43
epoch [27/30] batch [22/40] time 0.276 (0.306) data 0.000 (0.019) loss 0.5752 (0.7472) lr 4.3227e-04 eta 0:00:42
epoch [27/30] batch [24/40] time 0.276 (0.304) data 0.000 (0.018) loss 0.5054 (0.7317) lr 4.3227e-04 eta 0:00:41
epoch [27/30] batch [26/40] time 0.274 (0.301) data 0.000 (0.016) loss 0.9839 (0.7393) lr 4.3227e-04 eta 0:00:40
epoch [27/30] batch [28/40] time 0.272 (0.300) data 0.000 (0.015) loss 0.3569 (0.7248) lr 4.3227e-04 eta 0:00:39
epoch [27/30] batch [30/40] time 0.275 (0.298) data 0.000 (0.014) loss 0.6138 (0.7397) lr 4.3227e-04 eta 0:00:38
epoch [27/30] batch [32/40] time 0.275 (0.296) data 0.000 (0.013) loss 1.4346 (0.7461) lr 4.3227e-04 eta 0:00:37
epoch [27/30] batch [34/40] time 0.275 (0.298) data 0.000 (0.012) loss 0.8613 (0.7423) lr 4.3227e-04 eta 0:00:37
epoch [27/30] batch [36/40] time 0.273 (0.297) data 0.000 (0.012) loss 1.0273 (0.7518) lr 4.3227e-04 eta 0:00:36
epoch [27/30] batch [38/40] time 0.278 (0.296) data 0.000 (0.011) loss 0.9023 (0.7750) lr 4.3227e-04 eta 0:00:36
epoch [27/30] batch [40/40] time 0.275 (0.294) data 0.000 (0.011) loss 0.5244 (0.7753) lr 2.4472e-04 eta 0:00:35
epoch [28/30] batch [2/40] time 0.286 (0.520) data 0.000 (0.211) loss 1.0127 (0.8279) lr 2.4472e-04 eta 0:01:01
epoch [28/30] batch [4/40] time 0.280 (0.400) data 0.000 (0.106) loss 0.1147 (0.5453) lr 2.4472e-04 eta 0:00:46
epoch [28/30] batch [6/40] time 0.286 (0.360) data 0.000 (0.071) loss 0.9946 (0.5594) lr 2.4472e-04 eta 0:00:41
epoch [28/30] batch [8/40] time 0.281 (0.341) data 0.000 (0.053) loss 0.3911 (0.4931) lr 2.4472e-04 eta 0:00:38
epoch [28/30] batch [10/40] time 0.283 (0.329) data 0.000 (0.042) loss 0.4839 (0.4732) lr 2.4472e-04 eta 0:00:36
epoch [28/30] batch [12/40] time 0.280 (0.321) data 0.000 (0.035) loss 0.8926 (0.5120) lr 2.4472e-04 eta 0:00:34
epoch [28/30] batch [14/40] time 0.278 (0.315) data 0.000 (0.030) loss 1.5859 (0.5997) lr 2.4472e-04 eta 0:00:33
epoch [28/30] batch [16/40] time 0.282 (0.311) data 0.000 (0.027) loss 0.4983 (0.6791) lr 2.4472e-04 eta 0:00:32
epoch [28/30] batch [18/40] time 0.273 (0.307) data 0.000 (0.024) loss 0.4333 (0.6417) lr 2.4472e-04 eta 0:00:31
epoch [28/30] batch [20/40] time 0.271 (0.304) data 0.000 (0.021) loss 0.7676 (0.6339) lr 2.4472e-04 eta 0:00:30
epoch [28/30] batch [22/40] time 0.277 (0.301) data 0.000 (0.019) loss 0.5137 (0.6221) lr 2.4472e-04 eta 0:00:29
epoch [28/30] batch [24/40] time 0.274 (0.299) data 0.000 (0.018) loss 0.4976 (0.6071) lr 2.4472e-04 eta 0:00:28
epoch [28/30] batch [26/40] time 0.274 (0.297) data 0.000 (0.016) loss 0.8096 (0.6247) lr 2.4472e-04 eta 0:00:27
epoch [28/30] batch [28/40] time 0.273 (0.295) data 0.000 (0.015) loss 0.7026 (0.6757) lr 2.4472e-04 eta 0:00:27
epoch [28/30] batch [30/40] time 0.277 (0.294) data 0.000 (0.014) loss 0.7598 (0.6682) lr 2.4472e-04 eta 0:00:26
epoch [28/30] batch [32/40] time 0.274 (0.293) data 0.000 (0.013) loss 0.2510 (0.6682) lr 2.4472e-04 eta 0:00:25
epoch [28/30] batch [34/40] time 0.271 (0.291) data 0.000 (0.013) loss 1.1299 (0.6979) lr 2.4472e-04 eta 0:00:25
epoch [28/30] batch [36/40] time 0.271 (0.290) data 0.000 (0.012) loss 0.5273 (0.7025) lr 2.4472e-04 eta 0:00:24
epoch [28/30] batch [38/40] time 0.276 (0.290) data 0.000 (0.011) loss 1.5967 (0.7142) lr 2.4472e-04 eta 0:00:23
epoch [28/30] batch [40/40] time 0.273 (0.289) data 0.000 (0.011) loss 1.1055 (0.7122) lr 1.0926e-04 eta 0:00:23
epoch [29/30] batch [2/40] time 0.279 (0.521) data 0.000 (0.211) loss 0.3467 (0.2549) lr 1.0926e-04 eta 0:00:40
epoch [29/30] batch [4/40] time 0.286 (0.402) data 0.000 (0.106) loss 0.1885 (0.6663) lr 1.0926e-04 eta 0:00:30
epoch [29/30] batch [6/40] time 0.287 (0.363) data 0.000 (0.070) loss 0.5381 (0.7005) lr 1.0926e-04 eta 0:00:26
epoch [29/30] batch [8/40] time 0.282 (0.343) data 0.000 (0.053) loss 0.3345 (0.7742) lr 1.0926e-04 eta 0:00:24
epoch [29/30] batch [10/40] time 0.287 (0.331) data 0.000 (0.042) loss 0.1948 (0.6711) lr 1.0926e-04 eta 0:00:23
epoch [29/30] batch [12/40] time 0.285 (0.324) data 0.000 (0.035) loss 2.8770 (0.8988) lr 1.0926e-04 eta 0:00:22
epoch [29/30] batch [14/40] time 0.281 (0.318) data 0.000 (0.030) loss 0.9824 (0.9171) lr 1.0926e-04 eta 0:00:21
epoch [29/30] batch [16/40] time 0.285 (0.314) data 0.000 (0.027) loss 0.2156 (0.8868) lr 1.0926e-04 eta 0:00:20
epoch [29/30] batch [18/40] time 0.275 (0.310) data 0.000 (0.024) loss 3.4004 (0.9898) lr 1.0926e-04 eta 0:00:19
epoch [29/30] batch [20/40] time 0.275 (0.306) data 0.000 (0.021) loss 1.5566 (0.9935) lr 1.0926e-04 eta 0:00:18
epoch [29/30] batch [22/40] time 0.275 (0.303) data 0.000 (0.019) loss 0.8550 (0.9613) lr 1.0926e-04 eta 0:00:17
epoch [29/30] batch [24/40] time 0.278 (0.301) data 0.000 (0.018) loss 0.1116 (0.9068) lr 1.0926e-04 eta 0:00:16
epoch [29/30] batch [26/40] time 0.276 (0.299) data 0.000 (0.016) loss 2.7598 (0.9589) lr 1.0926e-04 eta 0:00:16
epoch [29/30] batch [28/40] time 0.283 (0.298) data 0.000 (0.015) loss 0.5088 (0.9439) lr 1.0926e-04 eta 0:00:15
epoch [29/30] batch [30/40] time 0.269 (0.296) data 0.000 (0.014) loss 0.0835 (0.9080) lr 1.0926e-04 eta 0:00:14
epoch [29/30] batch [32/40] time 0.272 (0.294) data 0.000 (0.013) loss 1.8848 (0.9381) lr 1.0926e-04 eta 0:00:14
epoch [29/30] batch [34/40] time 0.273 (0.293) data 0.000 (0.013) loss 0.2524 (0.9028) lr 1.0926e-04 eta 0:00:13
epoch [29/30] batch [36/40] time 0.271 (0.292) data 0.000 (0.012) loss 0.3943 (0.8795) lr 1.0926e-04 eta 0:00:12
epoch [29/30] batch [38/40] time 0.277 (0.294) data 0.000 (0.011) loss 1.6914 (0.8818) lr 1.0926e-04 eta 0:00:12
epoch [29/30] batch [40/40] time 0.270 (0.293) data 0.000 (0.011) loss 1.3809 (0.8841) lr 2.7391e-05 eta 0:00:11
epoch [30/30] batch [2/40] time 0.287 (0.522) data 0.000 (0.210) loss 0.1957 (0.9831) lr 2.7391e-05 eta 0:00:19
epoch [30/30] batch [4/40] time 0.286 (0.403) data 0.000 (0.105) loss 0.8228 (0.8148) lr 2.7391e-05 eta 0:00:14
epoch [30/30] batch [6/40] time 0.279 (0.362) data 0.001 (0.070) loss 0.8730 (0.7672) lr 2.7391e-05 eta 0:00:12
epoch [30/30] batch [8/40] time 0.282 (0.342) data 0.000 (0.053) loss 0.7002 (0.7156) lr 2.7391e-05 eta 0:00:10
epoch [30/30] batch [10/40] time 0.278 (0.330) data 0.000 (0.042) loss 0.2751 (0.6475) lr 2.7391e-05 eta 0:00:09
epoch [30/30] batch [12/40] time 0.284 (0.322) data 0.000 (0.035) loss 0.7798 (0.6543) lr 2.7391e-05 eta 0:00:09
epoch [30/30] batch [14/40] time 0.288 (0.317) data 0.000 (0.030) loss 0.2871 (0.6724) lr 2.7391e-05 eta 0:00:08
epoch [30/30] batch [16/40] time 0.283 (0.313) data 0.000 (0.026) loss 0.6396 (0.7341) lr 2.7391e-05 eta 0:00:07
epoch [30/30] batch [18/40] time 0.273 (0.309) data 0.000 (0.024) loss 0.3889 (0.7018) lr 2.7391e-05 eta 0:00:06
epoch [30/30] batch [20/40] time 0.276 (0.306) data 0.000 (0.021) loss 0.7148 (0.6897) lr 2.7391e-05 eta 0:00:06
epoch [30/30] batch [22/40] time 0.276 (0.303) data 0.000 (0.019) loss 0.1954 (0.6641) lr 2.7391e-05 eta 0:00:05
epoch [30/30] batch [24/40] time 0.280 (0.301) data 0.000 (0.018) loss 0.6802 (0.6841) lr 2.7391e-05 eta 0:00:04
epoch [30/30] batch [26/40] time 0.273 (0.299) data 0.000 (0.016) loss 1.2822 (0.6941) lr 2.7391e-05 eta 0:00:04
epoch [30/30] batch [28/40] time 0.273 (0.297) data 0.000 (0.015) loss 0.8281 (0.6912) lr 2.7391e-05 eta 0:00:03
epoch [30/30] batch [30/40] time 0.277 (0.296) data 0.000 (0.014) loss 0.1917 (0.6705) lr 2.7391e-05 eta 0:00:02
epoch [30/30] batch [32/40] time 0.276 (0.295) data 0.000 (0.013) loss 0.5430 (0.6778) lr 2.7391e-05 eta 0:00:02
epoch [30/30] batch [34/40] time 0.275 (0.293) data 0.000 (0.013) loss 1.7383 (0.7374) lr 2.7391e-05 eta 0:00:01
epoch [30/30] batch [36/40] time 0.273 (0.292) data 0.000 (0.012) loss 1.5332 (0.7489) lr 2.7391e-05 eta 0:00:01
epoch [30/30] batch [38/40] time 0.276 (0.291) data 0.000 (0.011) loss 0.2334 (0.7831) lr 2.7391e-05 eta 0:00:00
epoch [30/30] batch [40/40] time 0.278 (0.291) data 0.000 (0.011) loss 2.1133 (0.8089) lr 0.0000e+00 eta 0:00:00
Checkpoint saved to output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3/prompt_learner/model.pth.tar-30
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/42 [00:00<?, ?it/s]  2%|         | 1/42 [00:02<01:48,  2.65s/it]  5%|         | 2/42 [00:03<00:58,  1.47s/it]  7%|         | 3/42 [00:03<00:42,  1.08s/it] 10%|         | 4/42 [00:04<00:29,  1.28it/s] 12%|        | 5/42 [00:04<00:22,  1.65it/s] 14%|        | 6/42 [00:04<00:18,  1.90it/s] 17%|        | 7/42 [00:05<00:16,  2.19it/s] 19%|        | 8/42 [00:05<00:14,  2.34it/s] 21%|       | 9/42 [00:05<00:13,  2.51it/s] 24%|       | 10/42 [00:06<00:12,  2.65it/s] 26%|       | 11/42 [00:06<00:11,  2.67it/s] 29%|       | 12/42 [00:06<00:10,  2.75it/s] 31%|       | 13/42 [00:07<00:10,  2.90it/s] 33%|      | 14/42 [00:07<00:09,  2.90it/s] 36%|      | 15/42 [00:07<00:09,  2.95it/s] 38%|      | 16/42 [00:08<00:08,  2.96it/s] 40%|      | 17/42 [00:08<00:08,  3.05it/s] 43%|     | 18/42 [00:08<00:07,  3.07it/s] 45%|     | 19/42 [00:09<00:07,  2.97it/s] 48%|     | 20/42 [00:09<00:07,  3.13it/s] 50%|     | 21/42 [00:09<00:06,  3.25it/s] 52%|    | 22/42 [00:10<00:05,  3.34it/s] 55%|    | 23/42 [00:10<00:05,  3.41it/s] 57%|    | 24/42 [00:10<00:05,  3.46it/s] 60%|    | 25/42 [00:10<00:04,  3.49it/s] 62%|   | 26/42 [00:11<00:04,  3.52it/s] 64%|   | 27/42 [00:11<00:04,  3.53it/s] 67%|   | 28/42 [00:11<00:03,  3.54it/s] 69%|   | 29/42 [00:12<00:03,  3.55it/s] 71%|  | 30/42 [00:12<00:03,  3.56it/s] 74%|  | 31/42 [00:12<00:03,  3.56it/s] 76%|  | 32/42 [00:12<00:02,  3.56it/s] 79%|  | 33/42 [00:13<00:02,  3.57it/s] 81%|  | 34/42 [00:13<00:02,  3.57it/s] 83%| | 35/42 [00:13<00:01,  3.57it/s] 86%| | 36/42 [00:14<00:01,  3.57it/s] 88%| | 37/42 [00:14<00:01,  3.57it/s] 90%| | 38/42 [00:14<00:01,  3.57it/s] 93%|| 39/42 [00:14<00:00,  3.57it/s] 95%|| 40/42 [00:15<00:00,  3.57it/s] 98%|| 41/42 [00:15<00:00,  3.57it/s]100%|| 42/42 [00:15<00:00,  4.38it/s]100%|| 42/42 [00:15<00:00,  2.68it/s]
=> result
* total: 8,100
* correct: 7,095
* accuracy: 87.6%
* error: 12.4%
* macro_f1: 87.3%
Elapsed: 0:06:12

for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet  
do
    for seed in 1 2 3
    do
        # evaluation
        sh scripts/rpo_prime/xd_test.sh eurosat ${dataset}  ${seed} ${GPU} main_final1212 ${SHOT} ${EPOCH} ${TRAINER}
    done
done
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat eurosat 1 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/eurosat/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/eurosat/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_1.pkl
160 5400 8100
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      5,400
# test     8,100
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/42 [00:00<?, ?it/s]  2%|         | 1/42 [00:05<03:28,  5.09s/it]  5%|         | 2/42 [00:05<01:30,  2.27s/it]  7%|         | 3/42 [00:05<00:55,  1.42s/it] 10%|         | 4/42 [00:06<00:38,  1.01s/it] 12%|        | 5/42 [00:06<00:28,  1.32it/s] 14%|        | 6/42 [00:06<00:21,  1.64it/s] 17%|        | 7/42 [00:07<00:18,  1.88it/s] 19%|        | 8/42 [00:07<00:16,  2.06it/s] 21%|       | 9/42 [00:07<00:14,  2.33it/s] 24%|       | 10/42 [00:08<00:12,  2.49it/s] 26%|       | 11/42 [00:08<00:11,  2.64it/s] 29%|       | 12/42 [00:08<00:10,  2.78it/s] 31%|       | 13/42 [00:09<00:10,  2.69it/s] 33%|      | 14/42 [00:09<00:10,  2.64it/s] 36%|      | 15/42 [00:10<00:10,  2.60it/s] 38%|      | 16/42 [00:10<00:10,  2.58it/s] 40%|      | 17/42 [00:10<00:09,  2.57it/s] 43%|     | 18/42 [00:11<00:09,  2.58it/s] 45%|     | 19/42 [00:11<00:08,  2.80it/s] 48%|     | 20/42 [00:11<00:07,  2.98it/s] 50%|     | 21/42 [00:12<00:06,  3.13it/s] 52%|    | 22/42 [00:12<00:06,  3.25it/s] 55%|    | 23/42 [00:12<00:05,  3.34it/s] 57%|    | 24/42 [00:12<00:05,  3.40it/s] 60%|    | 25/42 [00:13<00:04,  3.45it/s] 62%|   | 26/42 [00:13<00:04,  3.48it/s] 64%|   | 27/42 [00:13<00:04,  3.50it/s] 67%|   | 28/42 [00:14<00:03,  3.52it/s] 69%|   | 29/42 [00:14<00:03,  3.53it/s] 71%|  | 30/42 [00:14<00:03,  3.54it/s] 74%|  | 31/42 [00:14<00:03,  3.55it/s] 76%|  | 32/42 [00:15<00:02,  3.55it/s] 79%|  | 33/42 [00:15<00:02,  3.55it/s] 81%|  | 34/42 [00:15<00:02,  3.55it/s] 83%| | 35/42 [00:16<00:01,  3.56it/s] 86%| | 36/42 [00:16<00:01,  3.56it/s] 88%| | 37/42 [00:16<00:01,  3.56it/s] 90%| | 38/42 [00:16<00:01,  3.55it/s] 93%|| 39/42 [00:17<00:00,  3.55it/s] 95%|| 40/42 [00:17<00:00,  3.55it/s] 98%|| 41/42 [00:17<00:00,  3.55it/s]100%|| 42/42 [00:17<00:00,  4.36it/s]100%|| 42/42 [00:17<00:00,  2.34it/s]
=> result
* total: 8,100
* correct: 7,361
* accuracy: 90.9%
* error: 9.1%
* macro_f1: 90.6%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat eurosat 2 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/eurosat/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/eurosat/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_2.pkl
160 5400 8100
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      5,400
# test     8,100
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/42 [00:00<?, ?it/s]  2%|         | 1/42 [00:05<03:32,  5.19s/it]  5%|         | 2/42 [00:05<01:32,  2.31s/it]  7%|         | 3/42 [00:05<00:55,  1.43s/it] 10%|         | 4/42 [00:06<00:39,  1.03s/it] 12%|        | 5/42 [00:06<00:29,  1.25it/s] 14%|        | 6/42 [00:07<00:23,  1.52it/s] 17%|        | 7/42 [00:07<00:20,  1.75it/s] 19%|        | 8/42 [00:07<00:17,  1.94it/s] 21%|       | 9/42 [00:08<00:15,  2.09it/s] 24%|       | 10/42 [00:08<00:14,  2.20it/s] 26%|       | 11/42 [00:09<00:13,  2.29it/s] 29%|       | 12/42 [00:09<00:12,  2.33it/s] 31%|       | 13/42 [00:09<00:12,  2.38it/s] 33%|      | 14/42 [00:10<00:11,  2.42it/s] 36%|      | 15/42 [00:10<00:11,  2.43it/s] 38%|      | 16/42 [00:11<00:10,  2.44it/s] 40%|      | 17/42 [00:11<00:10,  2.47it/s] 43%|     | 18/42 [00:11<00:09,  2.60it/s] 45%|     | 19/42 [00:12<00:08,  2.83it/s] 48%|     | 20/42 [00:12<00:07,  3.01it/s] 50%|     | 21/42 [00:12<00:06,  3.16it/s] 52%|    | 22/42 [00:12<00:06,  3.27it/s] 55%|    | 23/42 [00:13<00:05,  3.35it/s] 57%|    | 24/42 [00:13<00:05,  3.41it/s] 60%|    | 25/42 [00:13<00:04,  3.45it/s] 62%|   | 26/42 [00:14<00:04,  3.48it/s] 64%|   | 27/42 [00:14<00:04,  3.51it/s] 67%|   | 28/42 [00:14<00:03,  3.52it/s] 69%|   | 29/42 [00:14<00:03,  3.53it/s] 71%|  | 30/42 [00:15<00:03,  3.54it/s] 74%|  | 31/42 [00:15<00:03,  3.54it/s] 76%|  | 32/42 [00:15<00:02,  3.55it/s] 79%|  | 33/42 [00:16<00:02,  3.55it/s] 81%|  | 34/42 [00:16<00:02,  3.55it/s] 83%| | 35/42 [00:16<00:01,  3.55it/s] 86%| | 36/42 [00:16<00:01,  3.55it/s] 88%| | 37/42 [00:17<00:01,  3.56it/s] 90%| | 38/42 [00:17<00:01,  3.56it/s] 93%|| 39/42 [00:17<00:00,  3.56it/s] 95%|| 40/42 [00:17<00:00,  3.54it/s] 98%|| 41/42 [00:18<00:00,  3.55it/s]100%|| 42/42 [00:18<00:00,  4.35it/s]100%|| 42/42 [00:18<00:00,  2.27it/s]
=> result
* total: 8,100
* correct: 7,090
* accuracy: 87.5%
* error: 12.5%
* macro_f1: 87.1%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat eurosat 3 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/eurosat/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/eurosat/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_3.pkl
160 5400 8100
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      5,400
# test     8,100
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/42 [00:00<?, ?it/s]  2%|         | 1/42 [00:05<03:25,  5.02s/it]  5%|         | 2/42 [00:05<01:29,  2.23s/it]  7%|         | 3/42 [00:05<00:53,  1.38s/it] 10%|         | 4/42 [00:06<00:37,  1.02it/s] 12%|        | 5/42 [00:06<00:28,  1.29it/s] 14%|        | 6/42 [00:06<00:23,  1.55it/s] 17%|        | 7/42 [00:07<00:19,  1.78it/s] 19%|        | 8/42 [00:07<00:16,  2.06it/s] 21%|       | 9/42 [00:07<00:15,  2.19it/s] 24%|       | 10/42 [00:08<00:14,  2.28it/s] 26%|       | 11/42 [00:08<00:13,  2.34it/s] 29%|       | 12/42 [00:09<00:12,  2.37it/s] 31%|       | 13/42 [00:09<00:12,  2.41it/s] 33%|      | 14/42 [00:09<00:11,  2.45it/s] 36%|      | 15/42 [00:10<00:10,  2.48it/s] 38%|      | 16/42 [00:10<00:09,  2.70it/s] 40%|      | 17/42 [00:11<00:09,  2.63it/s] 43%|     | 18/42 [00:11<00:09,  2.59it/s] 45%|     | 19/42 [00:11<00:08,  2.82it/s] 48%|     | 20/42 [00:12<00:07,  3.01it/s] 50%|     | 21/42 [00:12<00:06,  3.15it/s] 52%|    | 22/42 [00:12<00:06,  3.26it/s] 55%|    | 23/42 [00:12<00:05,  3.35it/s] 57%|    | 24/42 [00:13<00:05,  3.41it/s] 60%|    | 25/42 [00:13<00:04,  3.45it/s] 62%|   | 26/42 [00:13<00:04,  3.47it/s] 64%|   | 27/42 [00:13<00:04,  3.49it/s] 67%|   | 28/42 [00:14<00:03,  3.51it/s] 69%|   | 29/42 [00:14<00:03,  3.52it/s] 71%|  | 30/42 [00:14<00:03,  3.53it/s] 74%|  | 31/42 [00:15<00:03,  3.53it/s] 76%|  | 32/42 [00:15<00:02,  3.53it/s] 79%|  | 33/42 [00:15<00:02,  3.54it/s] 81%|  | 34/42 [00:15<00:02,  3.50it/s] 83%| | 35/42 [00:16<00:01,  3.51it/s] 86%| | 36/42 [00:16<00:01,  3.52it/s] 88%| | 37/42 [00:16<00:01,  3.53it/s] 90%| | 38/42 [00:17<00:01,  3.54it/s] 93%|| 39/42 [00:17<00:00,  3.54it/s] 95%|| 40/42 [00:17<00:00,  3.54it/s] 98%|| 41/42 [00:17<00:00,  3.55it/s]100%|| 42/42 [00:18<00:00,  4.35it/s]100%|| 42/42 [00:18<00:00,  2.31it/s]
=> result
* total: 8,100
* correct: 7,095
* accuracy: 87.6%
* error: 12.4%
* macro_f1: 87.3%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat dtd 1 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/dtd/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/dtd/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_1.pkl
752 1128 1692
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      1,128
# test     1,692
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|         | 1/9 [00:05<00:42,  5.27s/it] 22%|       | 2/9 [00:05<00:16,  2.34s/it] 33%|      | 3/9 [00:05<00:08,  1.40s/it] 44%|     | 4/9 [00:06<00:04,  1.04it/s] 56%|    | 5/9 [00:06<00:02,  1.39it/s] 67%|   | 6/9 [00:06<00:01,  1.74it/s] 78%|  | 7/9 [00:07<00:00,  2.08it/s] 89%| | 8/9 [00:07<00:00,  2.38it/s]100%|| 9/9 [00:07<00:00,  2.86it/s]100%|| 9/9 [00:07<00:00,  1.19it/s]
=> result
* total: 1,692
* correct: 726
* accuracy: 42.9%
* error: 57.1%
* macro_f1: 40.0%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat dtd 2 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/dtd/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/dtd/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_2.pkl
752 1128 1692
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      1,128
# test     1,692
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|         | 1/9 [00:05<00:40,  5.10s/it] 22%|       | 2/9 [00:05<00:15,  2.27s/it] 33%|      | 3/9 [00:05<00:08,  1.37s/it] 44%|     | 4/9 [00:05<00:04,  1.06it/s] 56%|    | 5/9 [00:06<00:02,  1.41it/s] 67%|   | 6/9 [00:06<00:01,  1.76it/s] 78%|  | 7/9 [00:06<00:00,  2.10it/s] 89%| | 8/9 [00:07<00:00,  2.40it/s]100%|| 9/9 [00:07<00:00,  2.88it/s]100%|| 9/9 [00:07<00:00,  1.21it/s]
=> result
* total: 1,692
* correct: 701
* accuracy: 41.4%
* error: 58.6%
* macro_f1: 37.6%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat dtd 3 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/dtd/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/dtd/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_3.pkl
752 1128 1692
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      1,128
# test     1,692
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|         | 1/9 [00:05<00:40,  5.11s/it] 22%|       | 2/9 [00:05<00:15,  2.27s/it] 33%|      | 3/9 [00:05<00:08,  1.37s/it] 44%|     | 4/9 [00:05<00:04,  1.06it/s] 56%|    | 5/9 [00:06<00:02,  1.41it/s] 67%|   | 6/9 [00:06<00:01,  1.77it/s] 78%|  | 7/9 [00:06<00:00,  2.11it/s] 89%| | 8/9 [00:07<00:00,  2.41it/s]100%|| 9/9 [00:07<00:00,  2.89it/s]100%|| 9/9 [00:07<00:00,  1.21it/s]
=> result
* total: 1,692
* correct: 653
* accuracy: 38.6%
* error: 61.4%
* macro_f1: 35.2%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat fgvc_aircraft 1 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/fgvc_aircraft/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/fgvc_aircraft/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_1.pkl
1600 3333 3333
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  100
# train_x  1,600
# val      3,333
# test     3,333
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/18 [00:00<?, ?it/s]  6%|         | 1/18 [00:11<03:11, 11.26s/it] 11%|         | 2/18 [00:11<01:18,  4.90s/it] 17%|        | 3/18 [00:12<00:42,  2.86s/it] 22%|       | 4/18 [00:12<00:26,  1.91s/it] 28%|       | 5/18 [00:13<00:17,  1.38s/it] 33%|      | 6/18 [00:13<00:12,  1.06s/it] 39%|      | 7/18 [00:13<00:09,  1.16it/s] 44%|     | 8/18 [00:14<00:07,  1.38it/s] 50%|     | 9/18 [00:14<00:05,  1.58it/s] 56%|    | 10/18 [00:15<00:04,  1.76it/s] 61%|    | 11/18 [00:15<00:03,  1.92it/s] 67%|   | 12/18 [00:15<00:02,  2.10it/s] 72%|  | 13/18 [00:16<00:02,  2.34it/s] 78%|  | 14/18 [00:16<00:01,  2.54it/s] 83%| | 15/18 [00:16<00:01,  2.70it/s] 89%| | 16/18 [00:17<00:00,  2.83it/s] 94%|| 17/18 [00:17<00:00,  2.92it/s]100%|| 18/18 [00:17<00:00,  1.01it/s]
=> result
* total: 3,333
* correct: 552
* accuracy: 16.6%
* error: 83.4%
* macro_f1: 12.7%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat fgvc_aircraft 2 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/fgvc_aircraft/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/fgvc_aircraft/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_2.pkl
1600 3333 3333
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  100
# train_x  1,600
# val      3,333
# test     3,333
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/18 [00:00<?, ?it/s]  6%|         | 1/18 [00:11<03:13, 11.37s/it] 11%|         | 2/18 [00:11<01:19,  4.94s/it] 17%|        | 3/18 [00:12<00:43,  2.88s/it] 22%|       | 4/18 [00:12<00:26,  1.92s/it] 28%|       | 5/18 [00:13<00:17,  1.38s/it] 33%|      | 6/18 [00:13<00:12,  1.06s/it] 39%|      | 7/18 [00:13<00:09,  1.17it/s] 44%|     | 8/18 [00:14<00:07,  1.39it/s] 50%|     | 9/18 [00:14<00:05,  1.58it/s] 56%|    | 10/18 [00:15<00:04,  1.76it/s] 61%|    | 11/18 [00:15<00:03,  1.92it/s] 67%|   | 12/18 [00:15<00:02,  2.18it/s] 72%|  | 13/18 [00:16<00:02,  2.41it/s] 78%|  | 14/18 [00:16<00:01,  2.60it/s] 83%| | 15/18 [00:16<00:01,  2.74it/s] 89%| | 16/18 [00:17<00:00,  2.86it/s] 94%|| 17/18 [00:17<00:00,  2.95it/s]100%|| 18/18 [00:17<00:00,  1.01it/s]
=> result
* total: 3,333
* correct: 555
* accuracy: 16.7%
* error: 83.3%
* macro_f1: 12.9%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat fgvc_aircraft 3 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/fgvc_aircraft/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/fgvc_aircraft/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_3.pkl
1600 3333 3333
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  100
# train_x  1,600
# val      3,333
# test     3,333
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/18 [00:00<?, ?it/s]  6%|         | 1/18 [00:11<03:07, 11.03s/it] 11%|         | 2/18 [00:11<01:16,  4.81s/it] 17%|        | 3/18 [00:11<00:42,  2.82s/it] 22%|       | 4/18 [00:12<00:26,  1.88s/it] 28%|       | 5/18 [00:12<00:17,  1.36s/it] 33%|      | 6/18 [00:13<00:12,  1.04s/it] 39%|      | 7/18 [00:13<00:09,  1.19it/s] 44%|     | 8/18 [00:14<00:07,  1.41it/s] 50%|     | 9/18 [00:14<00:05,  1.60it/s] 56%|    | 10/18 [00:14<00:04,  1.77it/s] 61%|    | 11/18 [00:15<00:03,  1.93it/s] 67%|   | 12/18 [00:15<00:02,  2.06it/s] 72%|  | 13/18 [00:16<00:02,  2.16it/s] 78%|  | 14/18 [00:16<00:01,  2.38it/s] 83%| | 15/18 [00:16<00:01,  2.58it/s] 89%| | 16/18 [00:17<00:00,  2.73it/s] 94%|| 17/18 [00:17<00:00,  2.85it/s]100%|| 18/18 [00:17<00:00,  1.02it/s]
=> result
* total: 3,333
* correct: 496
* accuracy: 14.9%
* error: 85.1%
* macro_f1: 11.6%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat oxford_flowers 1 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/oxford_flowers/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/oxford_flowers/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_1.pkl
1632 1633 2463
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      1,633
# test     2,463
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/13 [00:00<?, ?it/s]  8%|         | 1/13 [00:06<01:16,  6.35s/it] 15%|        | 2/13 [00:06<00:30,  2.80s/it] 23%|       | 3/13 [00:06<00:16,  1.67s/it] 31%|       | 4/13 [00:07<00:10,  1.13s/it] 38%|      | 5/13 [00:07<00:06,  1.20it/s] 46%|     | 6/13 [00:07<00:04,  1.52it/s] 54%|    | 7/13 [00:08<00:03,  1.84it/s] 62%|   | 8/13 [00:08<00:02,  2.13it/s] 69%|   | 9/13 [00:08<00:01,  2.38it/s] 77%|  | 10/13 [00:09<00:01,  2.58it/s] 85%| | 11/13 [00:09<00:00,  2.74it/s] 92%|| 12/13 [00:09<00:00,  2.87it/s]100%|| 13/13 [00:09<00:00,  3.30it/s]100%|| 13/13 [00:10<00:00,  1.29it/s]
=> result
* total: 2,463
* correct: 1,178
* accuracy: 47.8%
* error: 52.2%
* macro_f1: 39.8%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat oxford_flowers 2 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/oxford_flowers/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/oxford_flowers/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_2.pkl
1632 1633 2463
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      1,633
# test     2,463
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/13 [00:00<?, ?it/s]  8%|         | 1/13 [00:06<01:16,  6.35s/it] 15%|        | 2/13 [00:06<00:30,  2.80s/it] 23%|       | 3/13 [00:06<00:16,  1.66s/it] 31%|       | 4/13 [00:07<00:10,  1.13s/it] 38%|      | 5/13 [00:07<00:06,  1.20it/s] 46%|     | 6/13 [00:07<00:04,  1.52it/s] 54%|    | 7/13 [00:08<00:03,  1.84it/s] 62%|   | 8/13 [00:08<00:02,  2.13it/s] 69%|   | 9/13 [00:08<00:01,  2.38it/s] 77%|  | 10/13 [00:09<00:01,  2.59it/s] 85%| | 11/13 [00:09<00:00,  2.76it/s] 92%|| 12/13 [00:09<00:00,  2.88it/s]100%|| 13/13 [00:09<00:00,  3.32it/s]100%|| 13/13 [00:10<00:00,  1.29it/s]
=> result
* total: 2,463
* correct: 1,062
* accuracy: 43.1%
* error: 56.9%
* macro_f1: 36.1%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat oxford_flowers 3 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/oxford_flowers/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/oxford_flowers/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_3.pkl
1632 1633 2463
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      1,633
# test     2,463
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/13 [00:00<?, ?it/s]  8%|         | 1/13 [00:06<01:18,  6.55s/it] 15%|        | 2/13 [00:06<00:31,  2.88s/it] 23%|       | 3/13 [00:07<00:17,  1.71s/it] 31%|       | 4/13 [00:07<00:10,  1.16s/it] 38%|      | 5/13 [00:07<00:06,  1.17it/s] 46%|     | 6/13 [00:08<00:04,  1.50it/s] 54%|    | 7/13 [00:08<00:03,  1.82it/s] 62%|   | 8/13 [00:08<00:02,  2.11it/s] 69%|   | 9/13 [00:09<00:01,  2.36it/s] 77%|  | 10/13 [00:09<00:01,  2.57it/s] 85%| | 11/13 [00:09<00:00,  2.72it/s] 92%|| 12/13 [00:09<00:00,  2.86it/s]100%|| 13/13 [00:10<00:00,  3.28it/s]100%|| 13/13 [00:10<00:00,  1.26it/s]
=> result
* total: 2,463
* correct: 1,066
* accuracy: 43.3%
* error: 56.7%
* macro_f1: 36.4%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat stanford_cars 1 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/stanford_cars/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/stanford_cars/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:StanfordCars
Loading dataset: StanfordCars
Reading split from /shared/s2/lab01/dataset/clip/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/stanford_cars/split_fewshot_taesup/shot_16-seed_1.pkl
3136 1635 8041
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      1,635
# test     8,041
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/42 [00:00<?, ?it/s]  2%|         | 1/42 [00:07<05:27,  7.98s/it]  5%|         | 2/42 [00:08<02:26,  3.65s/it]  7%|         | 3/42 [00:09<01:27,  2.25s/it] 10%|         | 4/42 [00:09<01:00,  1.58s/it] 12%|        | 5/42 [00:10<00:45,  1.22s/it] 14%|        | 6/42 [00:10<00:36,  1.01s/it] 17%|        | 7/42 [00:11<00:31,  1.11it/s] 19%|        | 8/42 [00:12<00:26,  1.30it/s] 21%|       | 9/42 [00:12<00:22,  1.46it/s] 24%|       | 10/42 [00:13<00:20,  1.57it/s] 26%|       | 11/42 [00:13<00:19,  1.62it/s] 29%|       | 12/42 [00:14<00:18,  1.65it/s] 31%|       | 13/42 [00:14<00:17,  1.69it/s] 33%|      | 14/42 [00:15<00:16,  1.74it/s] 36%|      | 15/42 [00:15<00:15,  1.72it/s] 38%|      | 16/42 [00:16<00:15,  1.73it/s] 40%|      | 17/42 [00:17<00:14,  1.72it/s] 43%|     | 18/42 [00:17<00:13,  1.76it/s] 45%|     | 19/42 [00:18<00:12,  1.79it/s] 48%|     | 20/42 [00:18<00:11,  1.87it/s] 50%|     | 21/42 [00:19<00:11,  1.85it/s] 52%|    | 22/42 [00:19<00:10,  1.96it/s] 55%|    | 23/42 [00:20<00:09,  1.98it/s] 57%|    | 24/42 [00:20<00:08,  2.07it/s] 60%|    | 25/42 [00:20<00:07,  2.22it/s] 62%|   | 26/42 [00:21<00:06,  2.33it/s] 64%|   | 27/42 [00:21<00:06,  2.45it/s] 67%|   | 28/42 [00:22<00:05,  2.54it/s] 69%|   | 29/42 [00:22<00:04,  2.61it/s] 71%|  | 30/42 [00:22<00:04,  2.66it/s] 74%|  | 31/42 [00:23<00:04,  2.70it/s] 76%|  | 32/42 [00:23<00:03,  2.72it/s] 79%|  | 33/42 [00:23<00:03,  2.74it/s] 81%|  | 34/42 [00:24<00:02,  2.76it/s] 83%| | 35/42 [00:24<00:02,  2.77it/s] 86%| | 36/42 [00:24<00:02,  2.77it/s] 88%| | 37/42 [00:25<00:01,  2.77it/s] 90%| | 38/42 [00:25<00:01,  2.78it/s] 93%|| 39/42 [00:26<00:01,  2.76it/s] 95%|| 40/42 [00:26<00:00,  2.77it/s] 98%|| 41/42 [00:26<00:00,  2.78it/s]100%|| 42/42 [00:26<00:00,  3.50it/s]100%|| 42/42 [00:26<00:00,  1.56it/s]
=> result
* total: 8,041
* correct: 3,911
* accuracy: 48.6%
* error: 51.4%
* macro_f1: 46.7%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat stanford_cars 2 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/stanford_cars/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/stanford_cars/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:StanfordCars
Loading dataset: StanfordCars
Reading split from /shared/s2/lab01/dataset/clip/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/stanford_cars/split_fewshot_taesup/shot_16-seed_2.pkl
3136 1635 8041
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      1,635
# test     8,041
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/42 [00:00<?, ?it/s]  2%|         | 1/42 [00:08<05:59,  8.77s/it]  5%|         | 2/42 [00:09<02:37,  3.94s/it]  7%|         | 3/42 [00:09<01:33,  2.39s/it] 10%|         | 4/42 [00:10<01:02,  1.65s/it] 12%|        | 5/42 [00:11<00:47,  1.28s/it] 14%|        | 6/42 [00:11<00:37,  1.05s/it] 17%|        | 7/42 [00:12<00:30,  1.16it/s] 19%|        | 8/42 [00:12<00:25,  1.33it/s] 21%|       | 9/42 [00:13<00:22,  1.46it/s] 24%|       | 10/42 [00:13<00:20,  1.54it/s] 26%|       | 11/42 [00:14<00:19,  1.60it/s] 29%|       | 12/42 [00:14<00:17,  1.70it/s] 31%|       | 13/42 [00:15<00:16,  1.71it/s] 33%|      | 14/42 [00:15<00:16,  1.69it/s] 36%|      | 15/42 [00:16<00:15,  1.76it/s] 38%|      | 16/42 [00:16<00:14,  1.83it/s] 40%|      | 17/42 [00:17<00:13,  1.85it/s] 43%|     | 18/42 [00:18<00:12,  1.85it/s] 45%|     | 19/42 [00:18<00:12,  1.85it/s] 48%|     | 20/42 [00:19<00:11,  1.94it/s] 50%|     | 21/42 [00:19<00:10,  2.02it/s] 52%|    | 22/42 [00:19<00:09,  2.04it/s] 55%|    | 23/42 [00:20<00:08,  2.11it/s] 57%|    | 24/42 [00:20<00:07,  2.28it/s] 60%|    | 25/42 [00:21<00:07,  2.41it/s] 62%|   | 26/42 [00:21<00:06,  2.52it/s] 64%|   | 27/42 [00:21<00:05,  2.60it/s] 67%|   | 28/42 [00:22<00:05,  2.66it/s] 69%|   | 29/42 [00:22<00:04,  2.70it/s] 71%|  | 30/42 [00:22<00:04,  2.73it/s] 74%|  | 31/42 [00:23<00:03,  2.76it/s] 76%|  | 32/42 [00:23<00:03,  2.77it/s] 79%|  | 33/42 [00:23<00:03,  2.78it/s] 81%|  | 34/42 [00:24<00:02,  2.79it/s] 83%| | 35/42 [00:24<00:02,  2.79it/s] 86%| | 36/42 [00:25<00:02,  2.79it/s] 88%| | 37/42 [00:25<00:01,  2.79it/s] 90%| | 38/42 [00:25<00:01,  2.80it/s] 93%|| 39/42 [00:26<00:01,  2.80it/s] 95%|| 40/42 [00:26<00:00,  2.80it/s] 98%|| 41/42 [00:26<00:00,  2.80it/s]100%|| 42/42 [00:26<00:00,  3.53it/s]100%|| 42/42 [00:27<00:00,  1.55it/s]
=> result
* total: 8,041
* correct: 3,371
* accuracy: 41.9%
* error: 58.1%
* macro_f1: 39.6%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat stanford_cars 3 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/stanford_cars/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/stanford_cars/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:StanfordCars
Loading dataset: StanfordCars
Reading split from /shared/s2/lab01/dataset/clip/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/stanford_cars/split_fewshot_taesup/shot_16-seed_3.pkl
3136 1635 8041
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      1,635
# test     8,041
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/42 [00:00<?, ?it/s]  2%|         | 1/42 [00:08<06:02,  8.84s/it]  5%|         | 2/42 [00:09<02:37,  3.95s/it]  7%|         | 3/42 [00:09<01:32,  2.37s/it] 10%|         | 4/42 [00:10<01:05,  1.71s/it] 12%|        | 5/42 [00:11<00:47,  1.27s/it] 14%|        | 6/42 [00:11<00:35,  1.00it/s] 17%|        | 7/42 [00:12<00:29,  1.20it/s] 19%|        | 8/42 [00:12<00:24,  1.37it/s] 21%|       | 9/42 [00:13<00:22,  1.47it/s] 24%|       | 10/42 [00:13<00:20,  1.55it/s] 26%|       | 11/42 [00:14<00:19,  1.59it/s] 29%|       | 12/42 [00:14<00:18,  1.62it/s] 31%|       | 13/42 [00:15<00:17,  1.68it/s] 33%|      | 14/42 [00:15<00:15,  1.77it/s] 36%|      | 15/42 [00:16<00:14,  1.83it/s] 38%|      | 16/42 [00:16<00:13,  1.86it/s] 40%|      | 17/42 [00:17<00:13,  1.90it/s] 43%|     | 18/42 [00:17<00:12,  1.88it/s] 45%|     | 19/42 [00:18<00:12,  1.91it/s] 48%|     | 20/42 [00:18<00:11,  1.96it/s] 50%|     | 21/42 [00:19<00:10,  2.02it/s] 52%|    | 22/42 [00:19<00:09,  2.05it/s] 55%|    | 23/42 [00:20<00:08,  2.12it/s] 57%|    | 24/42 [00:20<00:07,  2.25it/s] 60%|    | 25/42 [00:21<00:07,  2.39it/s] 62%|   | 26/42 [00:21<00:06,  2.50it/s] 64%|   | 27/42 [00:21<00:05,  2.59it/s] 67%|   | 28/42 [00:22<00:05,  2.65it/s] 69%|   | 29/42 [00:22<00:04,  2.69it/s] 71%|  | 30/42 [00:22<00:04,  2.72it/s] 74%|  | 31/42 [00:23<00:04,  2.75it/s] 76%|  | 32/42 [00:23<00:03,  2.75it/s] 79%|  | 33/42 [00:23<00:03,  2.76it/s] 81%|  | 34/42 [00:24<00:02,  2.77it/s] 83%| | 35/42 [00:24<00:02,  2.78it/s] 86%| | 36/42 [00:24<00:02,  2.79it/s] 88%| | 37/42 [00:25<00:01,  2.79it/s] 90%| | 38/42 [00:25<00:01,  2.80it/s] 93%|| 39/42 [00:26<00:01,  2.80it/s] 95%|| 40/42 [00:26<00:00,  2.80it/s] 98%|| 41/42 [00:26<00:00,  2.80it/s]100%|| 42/42 [00:26<00:00,  3.53it/s]100%|| 42/42 [00:26<00:00,  1.56it/s]
=> result
* total: 8,041
* correct: 3,287
* accuracy: 40.9%
* error: 59.1%
* macro_f1: 38.3%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat oxford_pets 1 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/oxford_pets/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/oxford_pets/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:OxfordPets
Loading dataset: OxfordPets
Reading split from /shared/s2/lab01/dataset/clip/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_pets/split_fewshot_taesup/shot_16-seed_1.pkl
592 736 3669
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  37
# train_x  592
# val      736
# test     3,669
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/19 [00:00<?, ?it/s]  5%|         | 1/19 [00:06<01:53,  6.32s/it] 11%|         | 2/19 [00:06<00:47,  2.78s/it] 16%|        | 3/19 [00:06<00:26,  1.64s/it] 21%|        | 4/19 [00:07<00:16,  1.11s/it] 26%|       | 5/19 [00:07<00:11,  1.23it/s] 32%|      | 6/19 [00:07<00:08,  1.57it/s] 37%|      | 7/19 [00:08<00:06,  1.91it/s] 42%|     | 8/19 [00:08<00:04,  2.22it/s] 47%|     | 9/19 [00:08<00:04,  2.50it/s] 53%|    | 10/19 [00:08<00:03,  2.72it/s] 58%|    | 11/19 [00:09<00:02,  2.91it/s] 63%|   | 12/19 [00:09<00:02,  3.05it/s] 68%|   | 13/19 [00:09<00:01,  3.15it/s] 74%|  | 14/19 [00:10<00:01,  3.23it/s] 79%|  | 15/19 [00:10<00:01,  3.25it/s] 84%| | 16/19 [00:10<00:00,  3.30it/s] 89%| | 17/19 [00:11<00:00,  3.34it/s] 95%|| 18/19 [00:11<00:00,  3.36it/s]100%|| 19/19 [00:11<00:00,  3.65it/s]100%|| 19/19 [00:11<00:00,  1.63it/s]
=> result
* total: 3,669
* correct: 2,810
* accuracy: 76.6%
* error: 23.4%
* macro_f1: 74.7%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat oxford_pets 2 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/oxford_pets/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/oxford_pets/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:OxfordPets
Loading dataset: OxfordPets
Reading split from /shared/s2/lab01/dataset/clip/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_pets/split_fewshot_taesup/shot_16-seed_2.pkl
592 736 3669
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  37
# train_x  592
# val      736
# test     3,669
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/19 [00:00<?, ?it/s]  5%|         | 1/19 [00:06<01:58,  6.60s/it] 11%|         | 2/19 [00:06<00:49,  2.89s/it] 16%|        | 3/19 [00:07<00:27,  1.70s/it] 21%|        | 4/19 [00:07<00:17,  1.15s/it] 26%|       | 5/19 [00:07<00:11,  1.19it/s] 32%|      | 6/19 [00:08<00:08,  1.53it/s] 37%|      | 7/19 [00:08<00:06,  1.87it/s] 42%|     | 8/19 [00:08<00:05,  2.18it/s] 47%|     | 9/19 [00:08<00:04,  2.46it/s] 53%|    | 10/19 [00:09<00:03,  2.70it/s] 58%|    | 11/19 [00:09<00:02,  2.89it/s] 63%|   | 12/19 [00:09<00:02,  3.03it/s] 68%|   | 13/19 [00:10<00:01,  3.14it/s] 74%|  | 14/19 [00:10<00:01,  3.22it/s] 79%|  | 15/19 [00:10<00:01,  3.28it/s] 84%| | 16/19 [00:10<00:00,  3.32it/s] 89%| | 17/19 [00:11<00:00,  3.34it/s] 95%|| 18/19 [00:11<00:00,  3.37it/s]100%|| 19/19 [00:11<00:00,  3.65it/s]100%|| 19/19 [00:11<00:00,  1.60it/s]
=> result
* total: 3,669
* correct: 2,652
* accuracy: 72.3%
* error: 27.7%
* macro_f1: 69.7%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat oxford_pets 3 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/oxford_pets/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/oxford_pets/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:OxfordPets
Loading dataset: OxfordPets
Reading split from /shared/s2/lab01/dataset/clip/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_pets/split_fewshot_taesup/shot_16-seed_3.pkl
592 736 3669
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  37
# train_x  592
# val      736
# test     3,669
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/19 [00:00<?, ?it/s]  5%|         | 1/19 [00:06<01:58,  6.58s/it] 11%|         | 2/19 [00:06<00:48,  2.88s/it] 16%|        | 3/19 [00:07<00:27,  1.70s/it] 21%|        | 4/19 [00:07<00:17,  1.14s/it] 26%|       | 5/19 [00:07<00:11,  1.20it/s] 32%|      | 6/19 [00:08<00:08,  1.54it/s] 37%|      | 7/19 [00:08<00:06,  1.88it/s] 42%|     | 8/19 [00:08<00:04,  2.21it/s] 47%|     | 9/19 [00:08<00:04,  2.49it/s] 53%|    | 10/19 [00:09<00:03,  2.72it/s] 58%|    | 11/19 [00:09<00:02,  2.91it/s] 63%|   | 12/19 [00:09<00:02,  3.06it/s] 68%|   | 13/19 [00:10<00:01,  3.18it/s] 74%|  | 14/19 [00:10<00:01,  3.26it/s] 79%|  | 15/19 [00:10<00:01,  3.32it/s] 84%| | 16/19 [00:10<00:00,  3.37it/s] 89%| | 17/19 [00:11<00:00,  3.40it/s] 95%|| 18/19 [00:11<00:00,  3.42it/s]100%|| 19/19 [00:11<00:00,  3.72it/s]100%|| 19/19 [00:11<00:00,  1.61it/s]
=> result
* total: 3,669
* correct: 2,682
* accuracy: 73.1%
* error: 26.9%
* macro_f1: 70.8%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat food101 1 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/food101/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/food101/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:Food101
Loading dataset: Food101
Reading split from /shared/s2/lab01/dataset/clip/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/food-101/split_fewshot_taesup/shot_16-seed_1.pkl
1616 20200 30300
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  101
# train_x  1,616
# val      20,200
# test     30,300
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/155 [00:00<?, ?it/s]  1%|          | 1/155 [00:07<19:30,  7.60s/it]  1%|         | 2/155 [00:07<08:29,  3.33s/it]  2%|         | 3/155 [00:08<05:01,  1.98s/it]  3%|         | 4/155 [00:08<03:23,  1.35s/it]  3%|         | 5/155 [00:09<02:32,  1.02s/it]  4%|         | 6/155 [00:09<01:59,  1.24it/s]  5%|         | 7/155 [00:09<01:41,  1.46it/s]  5%|         | 8/155 [00:10<01:27,  1.68it/s]  6%|         | 9/155 [00:10<01:19,  1.84it/s]  6%|         | 10/155 [00:11<01:13,  1.97it/s]  7%|         | 11/155 [00:11<01:10,  2.05it/s]  8%|         | 12/155 [00:12<01:07,  2.11it/s]  8%|         | 13/155 [00:12<01:05,  2.16it/s]  9%|         | 14/155 [00:12<01:03,  2.21it/s] 10%|         | 15/155 [00:13<01:02,  2.22it/s] 10%|         | 16/155 [00:13<01:01,  2.26it/s] 11%|         | 17/155 [00:14<01:01,  2.24it/s] 12%|        | 18/155 [00:14<01:00,  2.28it/s] 12%|        | 19/155 [00:15<00:59,  2.30it/s] 13%|        | 20/155 [00:15<00:59,  2.28it/s] 14%|        | 21/155 [00:16<00:58,  2.28it/s] 14%|        | 22/155 [00:16<00:58,  2.29it/s] 15%|        | 23/155 [00:16<00:57,  2.29it/s] 15%|        | 24/155 [00:17<00:57,  2.30it/s] 16%|        | 25/155 [00:17<00:56,  2.30it/s] 17%|        | 26/155 [00:18<00:55,  2.31it/s] 17%|        | 27/155 [00:18<00:55,  2.30it/s] 18%|        | 28/155 [00:19<00:55,  2.28it/s] 19%|        | 29/155 [00:19<00:55,  2.28it/s] 19%|        | 30/155 [00:19<00:54,  2.29it/s] 20%|        | 31/155 [00:20<00:53,  2.31it/s] 21%|        | 32/155 [00:20<00:53,  2.31it/s] 21%|       | 33/155 [00:21<00:53,  2.30it/s] 22%|       | 34/155 [00:21<00:52,  2.31it/s] 23%|       | 35/155 [00:22<00:52,  2.28it/s] 23%|       | 36/155 [00:22<00:52,  2.29it/s] 24%|       | 37/155 [00:22<00:51,  2.29it/s] 25%|       | 38/155 [00:23<00:51,  2.29it/s] 25%|       | 39/155 [00:23<00:50,  2.29it/s] 26%|       | 40/155 [00:24<00:49,  2.30it/s] 26%|       | 41/155 [00:24<00:49,  2.30it/s] 27%|       | 42/155 [00:25<00:49,  2.29it/s] 28%|       | 43/155 [00:25<00:47,  2.34it/s] 28%|       | 44/155 [00:26<00:47,  2.34it/s] 29%|       | 45/155 [00:26<00:46,  2.35it/s] 30%|       | 46/155 [00:26<00:47,  2.30it/s] 30%|       | 47/155 [00:27<00:46,  2.31it/s] 31%|       | 48/155 [00:27<00:46,  2.32it/s] 32%|      | 49/155 [00:28<00:44,  2.36it/s] 32%|      | 50/155 [00:28<00:44,  2.34it/s] 33%|      | 51/155 [00:29<00:44,  2.36it/s] 34%|      | 52/155 [00:29<00:43,  2.36it/s] 34%|      | 53/155 [00:29<00:43,  2.32it/s] 35%|      | 54/155 [00:30<00:43,  2.32it/s] 35%|      | 55/155 [00:30<00:43,  2.32it/s] 36%|      | 56/155 [00:31<00:42,  2.31it/s] 37%|      | 57/155 [00:31<00:42,  2.29it/s] 37%|      | 58/155 [00:32<00:42,  2.28it/s] 38%|      | 59/155 [00:32<00:42,  2.25it/s] 39%|      | 60/155 [00:32<00:41,  2.27it/s] 39%|      | 61/155 [00:33<00:40,  2.31it/s] 40%|      | 62/155 [00:33<00:40,  2.30it/s] 41%|      | 63/155 [00:34<00:39,  2.30it/s] 41%|     | 64/155 [00:34<00:39,  2.30it/s] 42%|     | 65/155 [00:35<00:39,  2.31it/s] 43%|     | 66/155 [00:35<00:38,  2.29it/s] 43%|     | 67/155 [00:35<00:38,  2.29it/s] 44%|     | 68/155 [00:36<00:38,  2.28it/s] 45%|     | 69/155 [00:36<00:37,  2.32it/s] 45%|     | 70/155 [00:37<00:36,  2.31it/s] 46%|     | 71/155 [00:37<00:36,  2.31it/s] 46%|     | 72/155 [00:38<00:36,  2.29it/s] 47%|     | 73/155 [00:38<00:36,  2.26it/s] 48%|     | 74/155 [00:39<00:35,  2.26it/s] 48%|     | 75/155 [00:39<00:35,  2.27it/s] 49%|     | 76/155 [00:39<00:34,  2.28it/s] 50%|     | 77/155 [00:40<00:34,  2.26it/s] 50%|     | 78/155 [00:40<00:34,  2.25it/s] 51%|     | 79/155 [00:41<00:33,  2.29it/s] 52%|    | 80/155 [00:41<00:32,  2.28it/s] 52%|    | 81/155 [00:42<00:32,  2.28it/s] 53%|    | 82/155 [00:42<00:31,  2.28it/s] 54%|    | 83/155 [00:42<00:30,  2.34it/s] 54%|    | 84/155 [00:43<00:30,  2.32it/s] 55%|    | 85/155 [00:43<00:30,  2.30it/s] 55%|    | 86/155 [00:44<00:30,  2.28it/s] 56%|    | 87/155 [00:44<00:29,  2.30it/s] 57%|    | 88/155 [00:45<00:29,  2.30it/s] 57%|    | 89/155 [00:45<00:28,  2.31it/s] 58%|    | 90/155 [00:46<00:28,  2.31it/s] 59%|    | 91/155 [00:46<00:27,  2.30it/s] 59%|    | 92/155 [00:46<00:27,  2.30it/s] 60%|    | 93/155 [00:47<00:26,  2.30it/s] 61%|    | 94/155 [00:47<00:26,  2.29it/s] 61%|   | 95/155 [00:48<00:26,  2.29it/s] 62%|   | 96/155 [00:48<00:25,  2.28it/s] 63%|   | 97/155 [00:49<00:25,  2.29it/s] 63%|   | 98/155 [00:49<00:24,  2.29it/s] 64%|   | 99/155 [00:49<00:24,  2.29it/s] 65%|   | 100/155 [00:50<00:24,  2.28it/s] 65%|   | 101/155 [00:50<00:23,  2.28it/s] 66%|   | 102/155 [00:51<00:23,  2.27it/s] 66%|   | 103/155 [00:51<00:22,  2.28it/s] 67%|   | 104/155 [00:52<00:22,  2.30it/s] 68%|   | 105/155 [00:52<00:21,  2.29it/s] 68%|   | 106/155 [00:53<00:21,  2.29it/s] 69%|   | 107/155 [00:53<00:21,  2.28it/s] 70%|   | 108/155 [00:53<00:20,  2.26it/s] 70%|   | 109/155 [00:54<00:20,  2.24it/s] 71%|   | 110/155 [00:54<00:19,  2.27it/s] 72%|  | 111/155 [00:55<00:19,  2.28it/s] 72%|  | 112/155 [00:55<00:18,  2.28it/s] 73%|  | 113/155 [00:56<00:18,  2.28it/s] 74%|  | 114/155 [00:56<00:18,  2.26it/s] 74%|  | 115/155 [00:56<00:17,  2.26it/s] 75%|  | 116/155 [00:57<00:17,  2.26it/s] 75%|  | 117/155 [00:57<00:16,  2.25it/s] 76%|  | 118/155 [00:58<00:16,  2.31it/s] 77%|  | 119/155 [00:58<00:15,  2.30it/s] 77%|  | 120/155 [00:59<00:15,  2.30it/s] 78%|  | 121/155 [00:59<00:14,  2.28it/s] 79%|  | 122/155 [01:00<00:14,  2.27it/s] 79%|  | 123/155 [01:00<00:13,  2.30it/s] 80%|  | 124/155 [01:00<00:13,  2.29it/s] 81%|  | 125/155 [01:01<00:12,  2.31it/s] 81%| | 126/155 [01:01<00:12,  2.31it/s] 82%| | 127/155 [01:02<00:12,  2.30it/s] 83%| | 128/155 [01:02<00:11,  2.31it/s] 83%| | 129/155 [01:03<00:11,  2.31it/s] 84%| | 130/155 [01:03<00:10,  2.30it/s] 85%| | 131/155 [01:03<00:10,  2.29it/s] 85%| | 132/155 [01:04<00:09,  2.32it/s] 86%| | 133/155 [01:04<00:09,  2.42it/s] 86%| | 134/155 [01:05<00:08,  2.44it/s] 87%| | 135/155 [01:05<00:08,  2.45it/s] 88%| | 136/155 [01:05<00:07,  2.57it/s] 88%| | 137/155 [01:06<00:06,  2.72it/s] 89%| | 138/155 [01:06<00:05,  2.85it/s] 90%| | 139/155 [01:06<00:05,  2.94it/s] 90%| | 140/155 [01:07<00:04,  3.01it/s] 91%| | 141/155 [01:07<00:04,  3.06it/s] 92%|| 142/155 [01:07<00:04,  3.10it/s] 92%|| 143/155 [01:08<00:03,  3.12it/s] 93%|| 144/155 [01:08<00:03,  3.14it/s] 94%|| 145/155 [01:08<00:03,  3.15it/s] 94%|| 146/155 [01:09<00:02,  3.16it/s] 95%|| 147/155 [01:09<00:02,  3.17it/s] 95%|| 148/155 [01:09<00:02,  3.17it/s] 96%|| 149/155 [01:09<00:01,  3.18it/s] 97%|| 150/155 [01:10<00:01,  3.18it/s] 97%|| 151/155 [01:10<00:01,  3.18it/s] 98%|| 152/155 [01:10<00:00,  3.18it/s] 99%|| 153/155 [01:11<00:00,  3.18it/s] 99%|| 154/155 [01:11<00:00,  3.18it/s]100%|| 155/155 [01:11<00:00,  3.54it/s]100%|| 155/155 [01:11<00:00,  2.16it/s]
=> result
* total: 30,300
* correct: 19,469
* accuracy: 64.3%
* error: 35.7%
* macro_f1: 64.6%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat food101 2 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/food101/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/food101/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:Food101
Loading dataset: Food101
Reading split from /shared/s2/lab01/dataset/clip/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/food-101/split_fewshot_taesup/shot_16-seed_2.pkl
1616 20200 30300
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  101
# train_x  1,616
# val      20,200
# test     30,300
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/155 [00:00<?, ?it/s]  1%|          | 1/155 [00:06<17:28,  6.81s/it]  1%|         | 2/155 [00:07<07:47,  3.06s/it]  2%|         | 3/155 [00:07<04:41,  1.86s/it]  3%|         | 4/155 [00:08<03:14,  1.29s/it]  3%|         | 5/155 [00:08<02:27,  1.02it/s]  4%|         | 6/155 [00:08<01:59,  1.25it/s]  5%|         | 7/155 [00:09<01:41,  1.46it/s]  5%|         | 8/155 [00:09<01:29,  1.64it/s]  6%|         | 9/155 [00:10<01:20,  1.80it/s]  6%|         | 10/155 [00:10<01:14,  1.94it/s]  7%|         | 11/155 [00:11<01:10,  2.04it/s]  8%|         | 12/155 [00:11<01:07,  2.11it/s]  8%|         | 13/155 [00:12<01:05,  2.16it/s]  9%|         | 14/155 [00:12<01:04,  2.20it/s] 10%|         | 15/155 [00:12<01:03,  2.22it/s] 10%|         | 16/155 [00:13<01:02,  2.22it/s] 11%|         | 17/155 [00:13<01:01,  2.25it/s] 12%|        | 18/155 [00:14<01:00,  2.26it/s] 12%|        | 19/155 [00:14<01:00,  2.25it/s] 13%|        | 20/155 [00:15<00:59,  2.26it/s] 14%|        | 21/155 [00:15<00:58,  2.28it/s] 14%|        | 22/155 [00:15<00:58,  2.28it/s] 15%|        | 23/155 [00:16<00:57,  2.29it/s] 15%|        | 24/155 [00:16<00:57,  2.26it/s] 16%|        | 25/155 [00:17<00:57,  2.26it/s] 17%|        | 26/155 [00:17<00:56,  2.27it/s] 17%|        | 27/155 [00:18<00:56,  2.26it/s] 18%|        | 28/155 [00:18<00:56,  2.27it/s] 19%|        | 29/155 [00:19<00:55,  2.25it/s] 19%|        | 30/155 [00:19<00:55,  2.27it/s] 20%|        | 31/155 [00:19<00:54,  2.27it/s] 21%|        | 32/155 [00:20<00:54,  2.27it/s] 21%|       | 33/155 [00:20<00:53,  2.27it/s] 22%|       | 34/155 [00:21<00:53,  2.27it/s] 23%|       | 35/155 [00:21<00:52,  2.27it/s] 23%|       | 36/155 [00:22<00:52,  2.29it/s] 24%|       | 37/155 [00:22<00:51,  2.29it/s] 25%|       | 38/155 [00:23<00:51,  2.27it/s] 25%|       | 39/155 [00:23<00:51,  2.27it/s] 26%|       | 40/155 [00:23<00:50,  2.26it/s] 26%|       | 41/155 [00:24<00:51,  2.23it/s] 27%|       | 42/155 [00:24<00:50,  2.24it/s] 28%|       | 43/155 [00:25<00:49,  2.25it/s] 28%|       | 44/155 [00:25<00:49,  2.26it/s] 29%|       | 45/155 [00:26<00:48,  2.27it/s] 30%|       | 46/155 [00:26<00:47,  2.28it/s] 30%|       | 47/155 [00:27<00:47,  2.27it/s] 31%|       | 48/155 [00:27<00:47,  2.28it/s] 32%|      | 49/155 [00:27<00:46,  2.28it/s] 32%|      | 50/155 [00:28<00:46,  2.28it/s] 33%|      | 51/155 [00:28<00:45,  2.28it/s] 34%|      | 52/155 [00:29<00:45,  2.28it/s] 34%|      | 53/155 [00:29<00:44,  2.28it/s] 35%|      | 54/155 [00:30<00:44,  2.29it/s] 35%|      | 55/155 [00:30<00:43,  2.27it/s] 36%|      | 56/155 [00:30<00:43,  2.27it/s] 37%|      | 57/155 [00:31<00:42,  2.28it/s] 37%|      | 58/155 [00:31<00:42,  2.28it/s] 38%|      | 59/155 [00:32<00:41,  2.30it/s] 39%|      | 60/155 [00:32<00:41,  2.30it/s] 39%|      | 61/155 [00:33<00:40,  2.31it/s] 40%|      | 62/155 [00:33<00:40,  2.30it/s] 41%|      | 63/155 [00:34<00:40,  2.29it/s] 41%|     | 64/155 [00:34<00:40,  2.27it/s] 42%|     | 65/155 [00:34<00:39,  2.28it/s] 43%|     | 66/155 [00:35<00:39,  2.26it/s] 43%|     | 67/155 [00:35<00:38,  2.27it/s] 44%|     | 68/155 [00:36<00:38,  2.29it/s] 45%|     | 69/155 [00:36<00:37,  2.29it/s] 45%|     | 70/155 [00:37<00:37,  2.30it/s] 46%|     | 71/155 [00:37<00:36,  2.31it/s] 46%|     | 72/155 [00:37<00:36,  2.30it/s] 47%|     | 73/155 [00:38<00:35,  2.30it/s] 48%|     | 74/155 [00:38<00:35,  2.29it/s] 48%|     | 75/155 [00:39<00:34,  2.29it/s] 49%|     | 76/155 [00:39<00:34,  2.27it/s] 50%|     | 77/155 [00:40<00:34,  2.26it/s] 50%|     | 78/155 [00:40<00:33,  2.27it/s] 51%|     | 79/155 [00:41<00:33,  2.27it/s] 52%|    | 80/155 [00:41<00:33,  2.27it/s] 52%|    | 81/155 [00:41<00:32,  2.26it/s] 53%|    | 82/155 [00:42<00:32,  2.24it/s] 54%|    | 83/155 [00:42<00:31,  2.25it/s] 54%|    | 84/155 [00:43<00:31,  2.26it/s] 55%|    | 85/155 [00:43<00:31,  2.25it/s] 55%|    | 86/155 [00:44<00:30,  2.25it/s] 56%|    | 87/155 [00:44<00:30,  2.24it/s] 57%|    | 88/155 [00:45<00:29,  2.27it/s] 57%|    | 89/155 [00:45<00:29,  2.26it/s] 58%|    | 90/155 [00:45<00:28,  2.25it/s] 59%|    | 91/155 [00:46<00:28,  2.27it/s] 59%|    | 92/155 [00:46<00:27,  2.26it/s] 60%|    | 93/155 [00:47<00:27,  2.26it/s] 61%|    | 94/155 [00:47<00:27,  2.25it/s] 61%|   | 95/155 [00:48<00:26,  2.25it/s] 62%|   | 96/155 [00:48<00:26,  2.26it/s] 63%|   | 97/155 [00:49<00:25,  2.26it/s] 63%|   | 98/155 [00:49<00:25,  2.26it/s] 64%|   | 99/155 [00:49<00:24,  2.26it/s] 65%|   | 100/155 [00:50<00:24,  2.27it/s] 65%|   | 101/155 [00:50<00:23,  2.26it/s] 66%|   | 102/155 [00:51<00:23,  2.25it/s] 66%|   | 103/155 [00:51<00:23,  2.26it/s] 67%|   | 104/155 [00:52<00:22,  2.25it/s] 68%|   | 105/155 [00:52<00:22,  2.26it/s] 68%|   | 106/155 [00:53<00:21,  2.26it/s] 69%|   | 107/155 [00:53<00:21,  2.26it/s] 70%|   | 108/155 [00:53<00:20,  2.27it/s] 70%|   | 109/155 [00:54<00:20,  2.26it/s] 71%|   | 110/155 [00:54<00:19,  2.26it/s] 72%|  | 111/155 [00:55<00:19,  2.25it/s] 72%|  | 112/155 [00:55<00:18,  2.27it/s] 73%|  | 113/155 [00:56<00:18,  2.28it/s] 74%|  | 114/155 [00:56<00:18,  2.27it/s] 74%|  | 115/155 [00:56<00:17,  2.28it/s] 75%|  | 116/155 [00:57<00:17,  2.27it/s] 75%|  | 117/155 [00:57<00:16,  2.27it/s] 76%|  | 118/155 [00:58<00:16,  2.29it/s] 77%|  | 119/155 [00:58<00:15,  2.29it/s] 77%|  | 120/155 [00:59<00:15,  2.27it/s] 78%|  | 121/155 [00:59<00:15,  2.26it/s] 79%|  | 122/155 [01:00<00:14,  2.27it/s] 79%|  | 123/155 [01:00<00:14,  2.27it/s] 80%|  | 124/155 [01:00<00:13,  2.29it/s] 81%|  | 125/155 [01:01<00:13,  2.29it/s] 81%| | 126/155 [01:01<00:12,  2.30it/s] 82%| | 127/155 [01:02<00:12,  2.30it/s] 83%| | 128/155 [01:02<00:11,  2.29it/s] 83%| | 129/155 [01:03<00:11,  2.33it/s] 84%| | 130/155 [01:03<00:10,  2.30it/s] 85%| | 131/155 [01:03<00:10,  2.31it/s] 85%| | 132/155 [01:04<00:09,  2.32it/s] 86%| | 133/155 [01:04<00:09,  2.39it/s] 86%| | 134/155 [01:05<00:08,  2.36it/s] 87%| | 135/155 [01:05<00:08,  2.44it/s] 88%| | 136/155 [01:05<00:07,  2.57it/s] 88%| | 137/155 [01:06<00:06,  2.71it/s] 89%| | 138/155 [01:06<00:05,  2.83it/s] 90%| | 139/155 [01:06<00:05,  2.93it/s] 90%| | 140/155 [01:07<00:05,  2.98it/s] 91%| | 141/155 [01:07<00:04,  3.04it/s] 92%|| 142/155 [01:07<00:04,  3.07it/s] 92%|| 143/155 [01:08<00:03,  3.10it/s] 93%|| 144/155 [01:08<00:03,  3.12it/s] 94%|| 145/155 [01:08<00:03,  3.13it/s] 94%|| 146/155 [01:09<00:02,  3.14it/s] 95%|| 147/155 [01:09<00:02,  3.15it/s] 95%|| 148/155 [01:09<00:02,  3.14it/s] 96%|| 149/155 [01:10<00:01,  3.15it/s] 97%|| 150/155 [01:10<00:01,  3.15it/s] 97%|| 151/155 [01:10<00:01,  3.16it/s] 98%|| 152/155 [01:10<00:00,  3.16it/s] 99%|| 153/155 [01:11<00:00,  3.16it/s] 99%|| 154/155 [01:11<00:00,  3.16it/s]100%|| 155/155 [01:11<00:00,  3.52it/s]100%|| 155/155 [01:11<00:00,  2.15it/s]
=> result
* total: 30,300
* correct: 18,165
* accuracy: 60.0%
* error: 40.0%
* macro_f1: 60.4%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat food101 3 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/food101/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/food101/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:Food101
Loading dataset: Food101
Reading split from /shared/s2/lab01/dataset/clip/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/food-101/split_fewshot_taesup/shot_16-seed_3.pkl
1616 20200 30300
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  101
# train_x  1,616
# val      20,200
# test     30,300
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/155 [00:00<?, ?it/s]  1%|          | 1/155 [00:07<18:45,  7.31s/it]  1%|         | 2/155 [00:07<08:19,  3.26s/it]  2%|         | 3/155 [00:08<04:58,  1.97s/it]  3%|         | 4/155 [00:08<03:24,  1.36s/it]  3%|         | 5/155 [00:09<02:32,  1.02s/it]  4%|         | 6/155 [00:09<02:02,  1.22it/s]  5%|         | 7/155 [00:09<01:42,  1.44it/s]  5%|         | 8/155 [00:10<01:30,  1.63it/s]  6%|         | 9/155 [00:10<01:21,  1.79it/s]  6%|         | 10/155 [00:11<01:15,  1.92it/s]  7%|         | 11/155 [00:11<01:11,  2.02it/s]  8%|         | 12/155 [00:12<01:08,  2.08it/s]  8%|         | 13/155 [00:12<01:06,  2.14it/s]  9%|         | 14/155 [00:12<01:04,  2.18it/s] 10%|         | 15/155 [00:13<01:02,  2.22it/s] 10%|         | 16/155 [00:13<01:02,  2.23it/s] 11%|         | 17/155 [00:14<01:02,  2.21it/s] 12%|        | 18/155 [00:14<01:01,  2.23it/s] 12%|        | 19/155 [00:15<01:00,  2.24it/s] 13%|        | 20/155 [00:15<01:00,  2.24it/s] 14%|        | 21/155 [00:16<00:59,  2.25it/s] 14%|        | 22/155 [00:16<00:59,  2.24it/s] 15%|        | 23/155 [00:16<00:58,  2.24it/s] 15%|        | 24/155 [00:17<00:58,  2.25it/s] 16%|        | 25/155 [00:17<00:57,  2.26it/s] 17%|        | 26/155 [00:18<00:56,  2.26it/s] 17%|        | 27/155 [00:18<00:56,  2.26it/s] 18%|        | 28/155 [00:19<00:55,  2.28it/s] 19%|        | 29/155 [00:19<00:55,  2.26it/s] 19%|        | 30/155 [00:20<00:55,  2.25it/s] 20%|        | 31/155 [00:20<00:54,  2.26it/s] 21%|        | 32/155 [00:20<00:54,  2.26it/s] 21%|       | 33/155 [00:21<00:54,  2.26it/s] 22%|       | 34/155 [00:21<00:53,  2.26it/s] 23%|       | 35/155 [00:22<00:52,  2.28it/s] 23%|       | 36/155 [00:22<00:52,  2.28it/s] 24%|       | 37/155 [00:23<00:52,  2.26it/s] 25%|       | 38/155 [00:23<00:51,  2.26it/s] 25%|       | 39/155 [00:24<00:51,  2.25it/s] 26%|       | 40/155 [00:24<00:51,  2.25it/s] 26%|       | 41/155 [00:24<00:50,  2.25it/s] 27%|       | 42/155 [00:25<00:50,  2.23it/s] 28%|       | 43/155 [00:25<00:49,  2.24it/s] 28%|       | 44/155 [00:26<00:48,  2.27it/s] 29%|       | 45/155 [00:26<00:48,  2.27it/s] 30%|       | 46/155 [00:27<00:47,  2.28it/s] 30%|       | 47/155 [00:27<00:47,  2.28it/s] 31%|       | 48/155 [00:27<00:46,  2.28it/s] 32%|      | 49/155 [00:28<00:46,  2.27it/s] 32%|      | 50/155 [00:28<00:46,  2.27it/s] 33%|      | 51/155 [00:29<00:46,  2.25it/s] 34%|      | 52/155 [00:29<00:45,  2.26it/s] 34%|      | 53/155 [00:30<00:45,  2.25it/s] 35%|      | 54/155 [00:30<00:44,  2.25it/s] 35%|      | 55/155 [00:31<00:44,  2.26it/s] 36%|      | 56/155 [00:31<00:43,  2.28it/s] 37%|      | 57/155 [00:31<00:43,  2.27it/s] 37%|      | 58/155 [00:32<00:42,  2.27it/s] 38%|      | 59/155 [00:32<00:42,  2.28it/s] 39%|      | 60/155 [00:33<00:41,  2.28it/s] 39%|      | 61/155 [00:33<00:41,  2.29it/s] 40%|      | 62/155 [00:34<00:40,  2.29it/s] 41%|      | 63/155 [00:34<00:40,  2.28it/s] 41%|     | 64/155 [00:35<00:40,  2.27it/s] 42%|     | 65/155 [00:35<00:39,  2.27it/s] 43%|     | 66/155 [00:35<00:38,  2.29it/s] 43%|     | 67/155 [00:36<00:38,  2.27it/s] 44%|     | 68/155 [00:36<00:38,  2.26it/s] 45%|     | 69/155 [00:37<00:37,  2.27it/s] 45%|     | 70/155 [00:37<00:37,  2.25it/s] 46%|     | 71/155 [00:38<00:37,  2.26it/s] 46%|     | 72/155 [00:38<00:36,  2.27it/s] 47%|     | 73/155 [00:39<00:35,  2.28it/s] 48%|     | 74/155 [00:39<00:35,  2.27it/s] 48%|     | 75/155 [00:39<00:34,  2.29it/s] 49%|     | 76/155 [00:40<00:34,  2.28it/s] 50%|     | 77/155 [00:40<00:34,  2.28it/s] 50%|     | 78/155 [00:41<00:34,  2.26it/s] 51%|     | 79/155 [00:41<00:33,  2.28it/s] 52%|    | 80/155 [00:42<00:32,  2.28it/s] 52%|    | 81/155 [00:42<00:32,  2.28it/s] 53%|    | 82/155 [00:42<00:32,  2.28it/s] 54%|    | 83/155 [00:43<00:31,  2.27it/s] 54%|    | 84/155 [00:43<00:31,  2.26it/s] 55%|    | 85/155 [00:44<00:30,  2.27it/s] 55%|    | 86/155 [00:44<00:30,  2.28it/s] 56%|    | 87/155 [00:45<00:29,  2.27it/s] 57%|    | 88/155 [00:45<00:29,  2.25it/s] 57%|    | 89/155 [00:46<00:29,  2.23it/s] 58%|    | 90/155 [00:46<00:28,  2.25it/s] 59%|    | 91/155 [00:46<00:28,  2.25it/s] 59%|    | 92/155 [00:47<00:27,  2.26it/s] 60%|    | 93/155 [00:47<00:27,  2.26it/s] 61%|    | 94/155 [00:48<00:27,  2.25it/s] 61%|   | 95/155 [00:48<00:26,  2.25it/s] 62%|   | 96/155 [00:49<00:26,  2.24it/s] 63%|   | 97/155 [00:49<00:25,  2.25it/s] 63%|   | 98/155 [00:50<00:25,  2.25it/s] 64%|   | 99/155 [00:50<00:24,  2.26it/s] 65%|   | 100/155 [00:50<00:24,  2.28it/s] 65%|   | 101/155 [00:51<00:23,  2.29it/s] 66%|   | 102/155 [00:51<00:23,  2.29it/s] 66%|   | 103/155 [00:52<00:22,  2.28it/s] 67%|   | 104/155 [00:52<00:22,  2.29it/s] 68%|   | 105/155 [00:53<00:22,  2.27it/s] 68%|   | 106/155 [00:53<00:21,  2.27it/s] 69%|   | 107/155 [00:54<00:21,  2.26it/s] 70%|   | 108/155 [00:54<00:20,  2.25it/s] 70%|   | 109/155 [00:54<00:20,  2.26it/s] 71%|   | 110/155 [00:55<00:19,  2.25it/s] 72%|  | 111/155 [00:55<00:19,  2.26it/s] 72%|  | 112/155 [00:56<00:18,  2.27it/s] 73%|  | 113/155 [00:56<00:18,  2.26it/s] 74%|  | 114/155 [00:57<00:18,  2.26it/s] 74%|  | 115/155 [00:57<00:17,  2.24it/s] 75%|  | 116/155 [00:58<00:17,  2.25it/s] 75%|  | 117/155 [00:58<00:16,  2.26it/s] 76%|  | 118/155 [00:58<00:16,  2.24it/s] 77%|  | 119/155 [00:59<00:15,  2.26it/s] 77%|  | 120/155 [00:59<00:15,  2.27it/s] 78%|  | 121/155 [01:00<00:15,  2.25it/s] 79%|  | 122/155 [01:00<00:14,  2.25it/s] 79%|  | 123/155 [01:01<00:14,  2.27it/s] 80%|  | 124/155 [01:01<00:13,  2.25it/s] 81%|  | 125/155 [01:01<00:13,  2.25it/s] 81%| | 126/155 [01:02<00:12,  2.23it/s] 82%| | 127/155 [01:02<00:12,  2.23it/s] 83%| | 128/155 [01:03<00:12,  2.24it/s] 83%| | 129/155 [01:03<00:11,  2.22it/s] 84%| | 130/155 [01:04<00:11,  2.25it/s] 85%| | 131/155 [01:04<00:10,  2.25it/s] 85%| | 132/155 [01:05<00:10,  2.27it/s] 86%| | 133/155 [01:05<00:09,  2.27it/s] 86%| | 134/155 [01:05<00:09,  2.31it/s] 87%| | 135/155 [01:06<00:07,  2.51it/s] 88%| | 136/155 [01:06<00:07,  2.67it/s] 88%| | 137/155 [01:06<00:06,  2.79it/s] 89%| | 138/155 [01:07<00:05,  2.89it/s] 90%| | 139/155 [01:07<00:05,  2.97it/s] 90%| | 140/155 [01:07<00:04,  3.02it/s] 91%| | 141/155 [01:08<00:04,  3.06it/s] 92%|| 142/155 [01:08<00:04,  3.09it/s] 92%|| 143/155 [01:08<00:03,  3.11it/s] 93%|| 144/155 [01:09<00:03,  3.12it/s] 94%|| 145/155 [01:09<00:03,  3.12it/s] 94%|| 146/155 [01:09<00:02,  3.13it/s] 95%|| 147/155 [01:10<00:02,  3.14it/s] 95%|| 148/155 [01:10<00:02,  3.14it/s] 96%|| 149/155 [01:10<00:01,  3.14it/s] 97%|| 150/155 [01:11<00:01,  3.14it/s] 97%|| 151/155 [01:11<00:01,  3.14it/s] 98%|| 152/155 [01:11<00:00,  3.14it/s] 99%|| 153/155 [01:11<00:00,  3.15it/s] 99%|| 154/155 [01:12<00:00,  3.15it/s]100%|| 155/155 [01:12<00:00,  3.50it/s]100%|| 155/155 [01:12<00:00,  2.13it/s]
=> result
* total: 30,300
* correct: 18,698
* accuracy: 61.7%
* error: 38.3%
* macro_f1: 62.2%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat ucf101 1 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/ucf101/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/ucf101/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:UCF101
Loading dataset: UCF101
Reading split from /shared/s2/lab01/dataset/clip/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/ucf101/split_fewshot_taesup/shot_16-seed_1.pkl
1616 1898 3783
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  101
# train_x  1,616
# val      1,898
# test     3,783
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|         | 1/20 [00:05<01:41,  5.36s/it] 10%|         | 2/20 [00:05<00:43,  2.39s/it] 15%|        | 3/20 [00:05<00:24,  1.44s/it] 20%|        | 4/20 [00:06<00:15,  1.00it/s] 25%|       | 5/20 [00:06<00:11,  1.33it/s] 30%|       | 6/20 [00:06<00:08,  1.66it/s] 35%|      | 7/20 [00:07<00:06,  1.97it/s] 40%|      | 8/20 [00:07<00:05,  2.24it/s] 45%|     | 9/20 [00:07<00:04,  2.46it/s] 50%|     | 10/20 [00:08<00:03,  2.65it/s] 55%|    | 11/20 [00:08<00:03,  2.79it/s] 60%|    | 12/20 [00:08<00:02,  2.89it/s] 65%|   | 13/20 [00:09<00:02,  2.97it/s] 70%|   | 14/20 [00:09<00:01,  3.03it/s] 75%|  | 15/20 [00:09<00:01,  3.07it/s] 80%|  | 16/20 [00:10<00:01,  3.10it/s] 85%| | 17/20 [00:10<00:00,  3.12it/s] 90%| | 18/20 [00:10<00:00,  3.13it/s] 95%|| 19/20 [00:11<00:00,  3.13it/s]100%|| 20/20 [00:11<00:00,  3.79it/s]100%|| 20/20 [00:11<00:00,  1.77it/s]
=> result
* total: 3,783
* correct: 1,936
* accuracy: 51.2%
* error: 48.8%
* macro_f1: 45.9%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat ucf101 2 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/ucf101/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/ucf101/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:UCF101
Loading dataset: UCF101
Reading split from /shared/s2/lab01/dataset/clip/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/ucf101/split_fewshot_taesup/shot_16-seed_2.pkl
1616 1898 3783
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  101
# train_x  1,616
# val      1,898
# test     3,783
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|         | 1/20 [00:05<01:39,  5.23s/it] 10%|         | 2/20 [00:05<00:42,  2.34s/it] 15%|        | 3/20 [00:05<00:24,  1.41s/it] 20%|        | 4/20 [00:06<00:15,  1.02it/s] 25%|       | 5/20 [00:06<00:11,  1.35it/s] 30%|       | 6/20 [00:06<00:08,  1.68it/s] 35%|      | 7/20 [00:07<00:06,  1.99it/s] 40%|      | 8/20 [00:07<00:05,  2.25it/s] 45%|     | 9/20 [00:07<00:04,  2.48it/s] 50%|     | 10/20 [00:08<00:03,  2.66it/s] 55%|    | 11/20 [00:08<00:03,  2.80it/s] 60%|    | 12/20 [00:08<00:02,  2.91it/s] 65%|   | 13/20 [00:08<00:02,  2.99it/s] 70%|   | 14/20 [00:09<00:01,  3.05it/s] 75%|  | 15/20 [00:09<00:01,  3.09it/s] 80%|  | 16/20 [00:09<00:01,  3.12it/s] 85%| | 17/20 [00:10<00:00,  3.13it/s] 90%| | 18/20 [00:10<00:00,  3.15it/s] 95%|| 19/20 [00:10<00:00,  3.15it/s]100%|| 20/20 [00:11<00:00,  3.82it/s]100%|| 20/20 [00:11<00:00,  1.80it/s]
=> result
* total: 3,783
* correct: 1,783
* accuracy: 47.1%
* error: 52.9%
* macro_f1: 42.5%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat ucf101 3 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/ucf101/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/ucf101/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:UCF101
Loading dataset: UCF101
Reading split from /shared/s2/lab01/dataset/clip/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/ucf101/split_fewshot_taesup/shot_16-seed_3.pkl
1616 1898 3783
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  101
# train_x  1,616
# val      1,898
# test     3,783
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|         | 1/20 [00:05<01:37,  5.15s/it] 10%|         | 2/20 [00:05<00:41,  2.30s/it] 15%|        | 3/20 [00:05<00:23,  1.39s/it] 20%|        | 4/20 [00:06<00:15,  1.03it/s] 25%|       | 5/20 [00:06<00:10,  1.37it/s] 30%|       | 6/20 [00:06<00:08,  1.70it/s] 35%|      | 7/20 [00:07<00:06,  2.01it/s] 40%|      | 8/20 [00:07<00:05,  2.28it/s] 45%|     | 9/20 [00:07<00:04,  2.51it/s] 50%|     | 10/20 [00:07<00:03,  2.69it/s] 55%|    | 11/20 [00:08<00:03,  2.83it/s] 60%|    | 12/20 [00:08<00:02,  2.93it/s] 65%|   | 13/20 [00:08<00:02,  3.01it/s] 70%|   | 14/20 [00:09<00:01,  3.07it/s] 75%|  | 15/20 [00:09<00:01,  3.11it/s] 80%|  | 16/20 [00:09<00:01,  3.14it/s] 85%| | 17/20 [00:10<00:00,  3.16it/s] 90%| | 18/20 [00:10<00:00,  3.17it/s] 95%|| 19/20 [00:10<00:00,  3.18it/s]100%|| 20/20 [00:10<00:00,  3.86it/s]100%|| 20/20 [00:11<00:00,  1.82it/s]
=> result
* total: 3,783
* correct: 1,902
* accuracy: 50.3%
* error: 49.7%
* macro_f1: 45.0%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat caltech101 1 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/caltech101/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/caltech101/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:Caltech101
Loading dataset: Caltech101
Reading split from /shared/s2/lab01/dataset/clip/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/caltech-101/split_fewshot_taesup/shot_16-seed_1.pkl
1600 1649 2465
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  1,600
# val      1,649
# test     2,465
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/13 [00:00<?, ?it/s]  8%|         | 1/13 [00:04<00:58,  4.85s/it] 15%|        | 2/13 [00:05<00:24,  2.18s/it] 23%|       | 3/13 [00:05<00:13,  1.33s/it] 31%|       | 4/13 [00:05<00:08,  1.08it/s] 38%|      | 5/13 [00:06<00:05,  1.41it/s] 46%|     | 6/13 [00:06<00:04,  1.75it/s] 54%|    | 7/13 [00:06<00:02,  2.05it/s] 62%|   | 8/13 [00:07<00:02,  2.32it/s] 69%|   | 9/13 [00:07<00:01,  2.54it/s] 77%|  | 10/13 [00:07<00:01,  2.71it/s] 85%| | 11/13 [00:07<00:00,  2.84it/s] 92%|| 12/13 [00:08<00:00,  2.94it/s]100%|| 13/13 [00:08<00:00,  3.34it/s]100%|| 13/13 [00:08<00:00,  1.51it/s]
=> result
* total: 2,465
* correct: 2,128
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 81.3%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat caltech101 2 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/caltech101/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/caltech101/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:Caltech101
Loading dataset: Caltech101
Reading split from /shared/s2/lab01/dataset/clip/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/caltech-101/split_fewshot_taesup/shot_16-seed_2.pkl
1600 1649 2465
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  1,600
# val      1,649
# test     2,465
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/13 [00:00<?, ?it/s]  8%|         | 1/13 [00:04<00:52,  4.38s/it] 15%|        | 2/13 [00:04<00:21,  1.99s/it] 23%|       | 3/13 [00:04<00:12,  1.22s/it] 31%|       | 4/13 [00:05<00:07,  1.16it/s] 38%|      | 5/13 [00:05<00:05,  1.50it/s] 46%|     | 6/13 [00:05<00:03,  1.83it/s] 54%|    | 7/13 [00:06<00:02,  2.13it/s] 62%|   | 8/13 [00:06<00:02,  2.39it/s] 69%|   | 9/13 [00:06<00:01,  2.60it/s] 77%|  | 10/13 [00:07<00:01,  2.76it/s] 85%| | 11/13 [00:07<00:00,  2.89it/s] 92%|| 12/13 [00:07<00:00,  2.98it/s]100%|| 13/13 [00:08<00:00,  3.39it/s]100%|| 13/13 [00:08<00:00,  1.60it/s]
=> result
* total: 2,465
* correct: 2,074
* accuracy: 84.1%
* error: 15.9%
* macro_f1: 78.0%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat caltech101 3 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/caltech101/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/caltech101/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:Caltech101
Loading dataset: Caltech101
Reading split from /shared/s2/lab01/dataset/clip/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/caltech-101/split_fewshot_taesup/shot_16-seed_3.pkl
1600 1649 2465
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  1,600
# val      1,649
# test     2,465
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/13 [00:00<?, ?it/s]  8%|         | 1/13 [00:04<00:55,  4.62s/it] 15%|        | 2/13 [00:04<00:22,  2.09s/it] 23%|       | 3/13 [00:05<00:12,  1.28s/it] 31%|       | 4/13 [00:05<00:08,  1.12it/s] 38%|      | 5/13 [00:05<00:05,  1.46it/s] 46%|     | 6/13 [00:06<00:03,  1.79it/s] 54%|    | 7/13 [00:06<00:02,  2.09it/s] 62%|   | 8/13 [00:06<00:02,  2.36it/s] 69%|   | 9/13 [00:07<00:01,  2.57it/s] 77%|  | 10/13 [00:07<00:01,  2.74it/s] 85%| | 11/13 [00:07<00:00,  2.86it/s] 92%|| 12/13 [00:08<00:00,  2.96it/s]100%|| 13/13 [00:08<00:00,  3.36it/s]100%|| 13/13 [00:08<00:00,  1.55it/s]
=> result
* total: 2,465
* correct: 2,095
* accuracy: 85.0%
* error: 15.0%
* macro_f1: 79.4%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat sun397 1 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/sun397/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/sun397/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:SUN397
Loading dataset: SUN397
Reading split from /shared/s2/lab01/dataset/clip/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/sun397/split_fewshot_taesup/shot_16-seed_1.pkl
6352 3970 19850
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  397
# train_x  6,352
# val      3,970
# test     19,850
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/102 [00:00<?, ?it/s]  1%|          | 1/102 [00:16<27:04, 16.09s/it]  2%|         | 2/102 [00:16<11:45,  7.06s/it]  3%|         | 3/102 [00:17<06:52,  4.17s/it]  4%|         | 4/102 [00:18<04:34,  2.80s/it]  5%|         | 5/102 [00:18<03:18,  2.05s/it]  6%|         | 6/102 [00:19<02:32,  1.59s/it]  7%|         | 7/102 [00:20<02:02,  1.29s/it]  8%|         | 8/102 [00:20<01:41,  1.08s/it]  9%|         | 9/102 [00:21<01:29,  1.04it/s] 10%|         | 10/102 [00:22<01:21,  1.13it/s] 11%|         | 11/102 [00:23<01:15,  1.20it/s] 12%|        | 12/102 [00:23<01:12,  1.25it/s] 13%|        | 13/102 [00:25<01:22,  1.08it/s] 14%|        | 14/102 [00:28<02:24,  1.64s/it] 15%|        | 15/102 [00:29<01:59,  1.37s/it] 16%|        | 16/102 [00:29<01:42,  1.19s/it] 17%|        | 17/102 [00:30<01:29,  1.05s/it] 18%|        | 18/102 [00:31<01:19,  1.06it/s] 19%|        | 19/102 [00:32<01:13,  1.13it/s] 20%|        | 20/102 [00:32<01:08,  1.20it/s] 21%|        | 21/102 [00:33<01:04,  1.25it/s] 22%|       | 22/102 [00:34<01:02,  1.27it/s] 23%|       | 23/102 [00:35<01:02,  1.26it/s] 24%|       | 24/102 [00:35<01:00,  1.29it/s] 25%|       | 25/102 [00:38<01:43,  1.34s/it] 25%|       | 26/102 [00:41<02:24,  1.90s/it] 26%|       | 27/102 [00:42<01:56,  1.55s/it] 27%|       | 28/102 [00:44<02:08,  1.73s/it] 28%|       | 29/102 [00:45<01:45,  1.44s/it] 29%|       | 30/102 [00:46<01:29,  1.24s/it] 30%|       | 31/102 [00:46<01:16,  1.08s/it] 31%|      | 32/102 [00:47<01:08,  1.02it/s] 32%|      | 33/102 [00:48<01:04,  1.07it/s] 33%|      | 34/102 [00:49<01:01,  1.10it/s] 34%|      | 35/102 [00:49<00:57,  1.16it/s] 35%|      | 36/102 [00:50<00:55,  1.19it/s] 36%|      | 37/102 [00:51<00:54,  1.20it/s] 37%|      | 38/102 [00:53<01:17,  1.21s/it] 38%|      | 39/102 [00:54<01:08,  1.08s/it] 39%|      | 40/102 [00:55<01:03,  1.02s/it] 40%|      | 41/102 [00:56<00:58,  1.04it/s] 41%|      | 42/102 [00:56<00:56,  1.07it/s] 42%|     | 43/102 [00:57<00:52,  1.13it/s] 43%|     | 44/102 [00:58<00:49,  1.18it/s] 44%|     | 45/102 [00:59<00:47,  1.20it/s] 45%|     | 46/102 [01:00<00:46,  1.21it/s] 46%|     | 47/102 [01:00<00:44,  1.23it/s] 47%|     | 48/102 [01:01<00:42,  1.26it/s] 48%|     | 49/102 [01:02<00:42,  1.26it/s] 49%|     | 50/102 [01:06<01:30,  1.74s/it] 50%|     | 51/102 [01:07<01:13,  1.45s/it] 51%|     | 52/102 [01:07<01:02,  1.24s/it] 52%|    | 53/102 [01:08<00:54,  1.10s/it] 53%|    | 54/102 [01:09<00:47,  1.00it/s] 54%|    | 55/102 [01:10<00:44,  1.07it/s] 55%|    | 56/102 [01:11<00:41,  1.10it/s] 56%|    | 57/102 [01:11<00:39,  1.14it/s] 57%|    | 58/102 [01:12<00:37,  1.18it/s] 58%|    | 59/102 [01:13<00:35,  1.20it/s] 59%|    | 60/102 [01:14<00:34,  1.21it/s] 60%|    | 61/102 [01:15<00:33,  1.22it/s] 61%|    | 62/102 [01:23<02:05,  3.13s/it] 62%|   | 63/102 [01:24<01:32,  2.37s/it] 63%|   | 64/102 [01:24<01:10,  1.86s/it] 64%|   | 65/102 [01:25<00:54,  1.48s/it] 65%|   | 66/102 [01:26<00:43,  1.22s/it] 66%|   | 67/102 [01:26<00:36,  1.04s/it] 67%|   | 68/102 [01:27<00:31,  1.07it/s] 68%|   | 69/102 [01:28<00:29,  1.13it/s] 69%|   | 70/102 [01:28<00:27,  1.17it/s] 70%|   | 71/102 [01:29<00:25,  1.22it/s] 71%|   | 72/102 [01:30<00:24,  1.22it/s] 72%|  | 73/102 [01:31<00:23,  1.23it/s] 73%|  | 74/102 [01:35<00:50,  1.81s/it] 74%|  | 75/102 [01:36<00:40,  1.48s/it] 75%|  | 76/102 [01:36<00:31,  1.22s/it] 75%|  | 77/102 [01:37<00:26,  1.05s/it] 76%|  | 78/102 [01:38<00:22,  1.08it/s] 77%|  | 79/102 [01:38<00:19,  1.18it/s] 78%|  | 80/102 [01:39<00:17,  1.27it/s] 79%|  | 81/102 [01:39<00:15,  1.36it/s] 80%|  | 82/102 [01:40<00:13,  1.43it/s] 81%| | 83/102 [01:41<00:12,  1.49it/s] 82%| | 84/102 [01:41<00:11,  1.55it/s] 83%| | 85/102 [01:42<00:10,  1.60it/s] 84%| | 86/102 [01:42<00:09,  1.62it/s] 85%| | 87/102 [01:43<00:09,  1.64it/s] 86%| | 88/102 [01:44<00:08,  1.67it/s] 87%| | 89/102 [01:44<00:07,  1.69it/s] 88%| | 90/102 [01:45<00:07,  1.69it/s] 89%| | 91/102 [01:45<00:06,  1.70it/s] 90%| | 92/102 [01:46<00:05,  1.71it/s] 91%| | 93/102 [01:47<00:05,  1.72it/s] 92%|| 94/102 [01:47<00:04,  1.74it/s] 93%|| 95/102 [01:48<00:04,  1.74it/s] 94%|| 96/102 [01:48<00:03,  1.75it/s] 95%|| 97/102 [01:50<00:05,  1.05s/it] 96%|| 98/102 [01:51<00:03,  1.14it/s] 97%|| 99/102 [01:51<00:02,  1.32it/s] 98%|| 100/102 [01:52<00:01,  1.49it/s] 99%|| 101/102 [01:52<00:00,  1.63it/s]100%|| 102/102 [01:53<00:00,  1.94it/s]100%|| 102/102 [01:53<00:00,  1.11s/it]
=> result
* total: 19,850
* correct: 10,577
* accuracy: 53.3%
* error: 46.7%
* macro_f1: 51.7%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat sun397 2 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/sun397/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/sun397/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:SUN397
Loading dataset: SUN397
Reading split from /shared/s2/lab01/dataset/clip/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/sun397/split_fewshot_taesup/shot_16-seed_2.pkl
6352 3970 19850
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  397
# train_x  6,352
# val      3,970
# test     19,850
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/102 [00:00<?, ?it/s]  1%|          | 1/102 [00:13<22:33, 13.40s/it]  2%|         | 2/102 [00:14<10:34,  6.34s/it]  3%|         | 3/102 [00:16<07:16,  4.41s/it]  4%|         | 4/102 [00:17<04:51,  2.97s/it]  5%|         | 5/102 [00:18<03:30,  2.17s/it]  6%|         | 6/102 [00:19<02:39,  1.66s/it]  7%|         | 7/102 [00:19<02:08,  1.35s/it]  8%|         | 8/102 [00:20<01:46,  1.13s/it]  9%|         | 9/102 [00:21<01:32,  1.00it/s] 10%|         | 10/102 [00:21<01:24,  1.08it/s] 11%|         | 11/102 [00:22<01:20,  1.13it/s] 12%|        | 12/102 [00:23<01:16,  1.18it/s] 13%|        | 13/102 [00:24<01:12,  1.22it/s] 14%|        | 14/102 [00:27<02:13,  1.52s/it] 15%|        | 15/102 [00:28<01:54,  1.31s/it] 16%|        | 16/102 [00:28<01:38,  1.14s/it] 17%|        | 17/102 [00:29<01:26,  1.02s/it] 18%|        | 18/102 [00:30<01:18,  1.06it/s] 19%|        | 19/102 [00:31<01:13,  1.13it/s] 20%|        | 20/102 [00:31<01:07,  1.21it/s] 21%|        | 21/102 [00:32<01:04,  1.26it/s] 22%|       | 22/102 [00:33<01:01,  1.29it/s] 23%|       | 23/102 [00:34<01:00,  1.30it/s] 24%|       | 24/102 [00:34<01:00,  1.30it/s] 25%|       | 25/102 [00:35<00:58,  1.31it/s] 25%|       | 26/102 [00:40<02:34,  2.03s/it] 26%|       | 27/102 [00:41<02:05,  1.68s/it] 27%|       | 28/102 [00:43<02:03,  1.67s/it] 28%|       | 29/102 [00:43<01:42,  1.41s/it] 29%|       | 30/102 [00:44<01:27,  1.21s/it] 30%|       | 31/102 [00:45<01:16,  1.08s/it] 31%|      | 32/102 [00:46<01:08,  1.02it/s] 32%|      | 33/102 [00:46<01:03,  1.08it/s] 33%|      | 34/102 [00:47<01:00,  1.13it/s] 34%|      | 35/102 [00:48<00:57,  1.17it/s] 35%|      | 36/102 [00:49<00:56,  1.16it/s] 36%|      | 37/102 [00:50<00:56,  1.14it/s] 37%|      | 38/102 [00:53<01:47,  1.68s/it] 38%|      | 39/102 [00:54<01:30,  1.43s/it] 39%|      | 40/102 [00:55<01:18,  1.26s/it] 40%|      | 41/102 [00:56<01:11,  1.17s/it] 41%|      | 42/102 [00:57<01:04,  1.08s/it] 42%|     | 43/102 [00:58<00:59,  1.02s/it] 43%|     | 44/102 [00:59<00:56,  1.02it/s] 44%|     | 45/102 [01:00<00:55,  1.03it/s] 45%|     | 46/102 [01:01<00:53,  1.05it/s] 46%|     | 47/102 [01:01<00:50,  1.09it/s] 47%|     | 48/102 [01:02<00:48,  1.12it/s] 48%|     | 49/102 [01:03<00:46,  1.13it/s] 49%|     | 50/102 [01:07<01:35,  1.84s/it] 50%|     | 51/102 [01:08<01:16,  1.51s/it] 51%|     | 52/102 [01:09<01:03,  1.27s/it] 52%|    | 53/102 [01:09<00:54,  1.12s/it] 53%|    | 54/102 [01:10<00:48,  1.01s/it] 54%|    | 55/102 [01:11<00:44,  1.06it/s] 55%|    | 56/102 [01:12<00:41,  1.12it/s] 56%|    | 57/102 [01:13<00:39,  1.13it/s] 57%|    | 58/102 [01:13<00:39,  1.13it/s] 58%|    | 59/102 [01:14<00:36,  1.17it/s] 59%|    | 60/102 [01:15<00:35,  1.18it/s] 60%|    | 61/102 [01:16<00:33,  1.21it/s] 61%|    | 62/102 [01:23<01:48,  2.71s/it] 62%|   | 63/102 [01:24<01:20,  2.07s/it] 63%|   | 64/102 [01:24<01:01,  1.63s/it] 64%|   | 65/102 [01:25<00:48,  1.32s/it] 65%|   | 66/102 [01:25<00:39,  1.11s/it] 66%|   | 67/102 [01:26<00:33,  1.04it/s] 67%|   | 68/102 [01:27<00:29,  1.15it/s] 68%|   | 69/102 [01:27<00:27,  1.22it/s] 69%|   | 70/102 [01:28<00:25,  1.25it/s] 70%|   | 71/102 [01:29<00:24,  1.26it/s] 71%|   | 72/102 [01:30<00:24,  1.24it/s] 72%|  | 73/102 [01:30<00:23,  1.25it/s] 73%|  | 74/102 [01:33<00:37,  1.35s/it] 74%|  | 75/102 [01:34<00:32,  1.19s/it] 75%|  | 76/102 [01:35<00:27,  1.07s/it] 75%|  | 77/102 [01:36<00:24,  1.01it/s] 76%|  | 78/102 [01:36<00:21,  1.12it/s] 77%|  | 79/102 [01:37<00:19,  1.19it/s] 78%|  | 80/102 [01:38<00:17,  1.26it/s] 79%|  | 81/102 [01:38<00:16,  1.31it/s] 80%|  | 82/102 [01:39<00:14,  1.39it/s] 81%| | 83/102 [01:39<00:12,  1.47it/s] 82%| | 84/102 [01:40<00:11,  1.53it/s] 83%| | 85/102 [01:41<00:10,  1.58it/s] 84%| | 86/102 [01:41<00:09,  1.62it/s] 85%| | 87/102 [01:42<00:09,  1.65it/s] 86%| | 88/102 [01:42<00:08,  1.67it/s] 87%| | 89/102 [01:43<00:07,  1.69it/s] 88%| | 90/102 [01:44<00:07,  1.71it/s] 89%| | 91/102 [01:44<00:06,  1.71it/s] 90%| | 92/102 [01:45<00:05,  1.71it/s] 91%| | 93/102 [01:45<00:05,  1.72it/s] 92%|| 94/102 [01:46<00:04,  1.73it/s] 93%|| 95/102 [01:46<00:03,  1.77it/s] 94%|| 96/102 [01:47<00:03,  1.78it/s] 95%|| 97/102 [01:47<00:02,  1.81it/s] 96%|| 98/102 [01:48<00:02,  1.53it/s] 97%|| 99/102 [01:49<00:01,  1.66it/s] 98%|| 100/102 [01:49<00:01,  1.77it/s] 99%|| 101/102 [01:50<00:00,  1.86it/s]100%|| 102/102 [01:50<00:00,  2.16it/s]100%|| 102/102 [01:50<00:00,  1.09s/it]
=> result
* total: 19,850
* correct: 9,880
* accuracy: 49.8%
* error: 50.2%
* macro_f1: 47.6%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat sun397 3 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/sun397/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/sun397/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:SUN397
Loading dataset: SUN397
Reading split from /shared/s2/lab01/dataset/clip/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/sun397/split_fewshot_taesup/shot_16-seed_3.pkl
6352 3970 19850
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  397
# train_x  6,352
# val      3,970
# test     19,850
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed3/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/102 [00:00<?, ?it/s]  1%|          | 1/102 [00:14<24:47, 14.73s/it]  2%|         | 2/102 [00:15<10:54,  6.55s/it]  3%|         | 3/102 [00:16<06:28,  3.92s/it]  4%|         | 4/102 [00:17<04:23,  2.69s/it]  5%|         | 5/102 [00:17<03:15,  2.02s/it]  6%|         | 6/102 [00:18<02:32,  1.59s/it]  7%|         | 7/102 [00:19<02:04,  1.31s/it]  8%|         | 8/102 [00:20<01:46,  1.14s/it]  9%|         | 9/102 [00:21<01:36,  1.04s/it] 10%|         | 10/102 [00:21<01:29,  1.03it/s] 11%|         | 11/102 [00:22<01:23,  1.09it/s] 12%|        | 12/102 [00:23<01:21,  1.11it/s] 13%|        | 13/102 [00:24<01:18,  1.14it/s] 14%|        | 14/102 [00:27<02:21,  1.61s/it] 15%|        | 15/102 [00:28<01:59,  1.38s/it] 16%|        | 16/102 [00:29<01:42,  1.20s/it] 17%|        | 17/102 [00:30<01:31,  1.08s/it] 18%|        | 18/102 [00:30<01:22,  1.01it/s] 19%|        | 19/102 [00:31<01:15,  1.10it/s] 20%|        | 20/102 [00:32<01:10,  1.17it/s] 21%|        | 21/102 [00:33<01:06,  1.21it/s] 22%|       | 22/102 [00:33<01:04,  1.23it/s] 23%|       | 23/102 [00:34<01:02,  1.26it/s] 24%|       | 24/102 [00:35<01:01,  1.26it/s] 25%|       | 25/102 [00:37<01:24,  1.10s/it] 25%|       | 26/102 [00:41<02:27,  1.94s/it] 26%|       | 27/102 [00:41<02:01,  1.62s/it] 27%|       | 28/102 [00:42<01:44,  1.41s/it] 28%|       | 29/102 [00:43<01:29,  1.23s/it] 29%|       | 30/102 [00:44<01:17,  1.08s/it] 30%|       | 31/102 [00:45<01:10,  1.01it/s] 31%|      | 32/102 [00:46<01:08,  1.02it/s] 32%|      | 33/102 [00:47<01:05,  1.05it/s] 33%|      | 34/102 [00:48<01:04,  1.05it/s] 34%|      | 35/102 [00:48<01:01,  1.09it/s] 35%|      | 36/102 [00:49<01:00,  1.09it/s] 36%|      | 37/102 [00:50<00:59,  1.10it/s] 37%|      | 38/102 [00:54<01:58,  1.86s/it] 38%|      | 39/102 [00:55<01:38,  1.56s/it] 39%|      | 40/102 [00:56<01:22,  1.33s/it] 40%|      | 41/102 [00:57<01:13,  1.21s/it] 41%|      | 42/102 [00:58<01:05,  1.09s/it] 42%|     | 43/102 [00:58<00:59,  1.01s/it] 43%|     | 44/102 [00:59<00:54,  1.07it/s] 44%|     | 45/102 [01:00<00:52,  1.09it/s] 45%|     | 46/102 [01:01<00:50,  1.11it/s] 46%|     | 47/102 [01:02<00:47,  1.15it/s] 47%|     | 48/102 [01:03<00:46,  1.17it/s] 48%|     | 49/102 [01:03<00:43,  1.21it/s] 49%|     | 50/102 [01:06<01:17,  1.50s/it] 50%|     | 51/102 [01:07<01:06,  1.31s/it] 51%|     | 52/102 [01:08<00:57,  1.16s/it] 52%|    | 53/102 [01:09<00:50,  1.03s/it] 53%|    | 54/102 [01:10<00:44,  1.07it/s] 54%|    | 55/102 [01:10<00:41,  1.13it/s] 55%|    | 56/102 [01:11<00:38,  1.19it/s] 56%|    | 57/102 [01:12<00:38,  1.18it/s] 57%|    | 58/102 [01:13<00:37,  1.18it/s] 58%|    | 59/102 [01:14<00:37,  1.15it/s] 59%|    | 60/102 [01:14<00:35,  1.17it/s] 60%|    | 61/102 [01:15<00:33,  1.21it/s] 61%|    | 62/102 [01:24<02:08,  3.21s/it] 62%|   | 63/102 [01:25<01:34,  2.42s/it] 63%|   | 64/102 [01:25<01:11,  1.87s/it] 64%|   | 65/102 [01:26<00:55,  1.49s/it] 65%|   | 66/102 [01:26<00:43,  1.22s/it] 66%|   | 67/102 [01:27<00:36,  1.04s/it] 67%|   | 68/102 [01:28<00:31,  1.09it/s] 68%|   | 69/102 [01:28<00:28,  1.18it/s] 69%|   | 70/102 [01:29<00:26,  1.22it/s] 70%|   | 71/102 [01:30<00:25,  1.23it/s] 71%|   | 72/102 [01:31<00:23,  1.27it/s] 72%|  | 73/102 [01:31<00:22,  1.29it/s] 73%|  | 74/102 [01:35<00:48,  1.74s/it] 74%|  | 75/102 [01:36<00:38,  1.41s/it] 75%|  | 76/102 [01:37<00:30,  1.19s/it] 75%|  | 77/102 [01:37<00:26,  1.06s/it] 76%|  | 78/102 [01:38<00:22,  1.09it/s] 77%|  | 79/102 [01:39<00:19,  1.20it/s] 78%|  | 80/102 [01:39<00:16,  1.30it/s] 79%|  | 81/102 [01:40<00:15,  1.37it/s] 80%|  | 82/102 [01:40<00:13,  1.44it/s] 81%| | 83/102 [01:41<00:12,  1.51it/s] 82%| | 84/102 [01:42<00:11,  1.56it/s] 83%| | 85/102 [01:42<00:10,  1.60it/s] 84%| | 86/102 [01:43<00:09,  1.63it/s] 85%| | 87/102 [01:43<00:09,  1.66it/s] 86%| | 88/102 [01:44<00:08,  1.69it/s] 87%| | 89/102 [01:45<00:07,  1.71it/s] 88%| | 90/102 [01:45<00:06,  1.72it/s] 89%| | 91/102 [01:46<00:06,  1.74it/s] 90%| | 92/102 [01:46<00:05,  1.75it/s] 91%| | 93/102 [01:47<00:05,  1.73it/s] 92%|| 94/102 [01:47<00:04,  1.75it/s] 93%|| 95/102 [01:48<00:03,  1.77it/s] 94%|| 96/102 [01:48<00:03,  1.77it/s] 95%|| 97/102 [01:49<00:02,  1.79it/s] 96%|| 98/102 [01:50<00:02,  1.74it/s] 97%|| 99/102 [01:50<00:01,  1.83it/s] 98%|| 100/102 [01:51<00:01,  1.91it/s] 99%|| 101/102 [01:51<00:00,  1.96it/s]100%|| 102/102 [01:51<00:00,  2.26it/s]100%|| 102/102 [01:51<00:00,  1.10s/it]
=> result
* total: 19,850
* correct: 9,728
* accuracy: 49.0%
* error: 51.0%
* macro_f1: 47.3%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat imagenet 1 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/imagenet.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/imagenet/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: ImageNet
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/imagenet/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:ImageNet
Loading dataset: ImageNet
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/ImageNet/split_fewshot_taesup/shot_16-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  --------
Dataset    ImageNet
# classes  1,000
# train_x  16,000
# val      50,000
# test     50,000
---------  --------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed1/prompt_learner/model.pth.tar-30" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/256 [00:00<?, ?it/s]  0%|          | 1/256 [00:09<38:47,  9.13s/it]  1%|          | 2/256 [00:09<18:01,  4.26s/it]  1%|          | 3/256 [00:10<11:29,  2.73s/it]  2%|         | 4/256 [00:11<08:23,  2.00s/it]  2%|         | 5/256 [00:12<06:42,  1.60s/it]  2%|         | 6/256 [00:13<05:41,  1.36s/it]  3%|         | 7/256 [00:14<05:02,  1.22s/it]  3%|         | 8/256 [00:15<04:34,  1.11s/it]  4%|         | 9/256 [00:16<04:17,  1.04s/it]  4%|         | 10/256 [00:17<04:04,  1.00it/s]  4%|         | 11/256 [00:18<03:56,  1.04it/s]  5%|         | 12/256 [00:18<03:50,  1.06it/s]  5%|         | 13/256 [00:19<03:46,  1.07it/s]  5%|         | 14/256 [00:20<03:43,  1.08it/s]  6%|         | 15/256 [00:21<03:40,  1.09it/s]  6%|         | 16/256 [00:22<03:36,  1.11it/s]  7%|         | 17/256 [00:23<03:36,  1.11it/s]  7%|         | 18/256 [00:24<03:34,  1.11it/s]  7%|         | 19/256 [00:25<03:33,  1.11it/s]  8%|         | 20/256 [00:26<03:32,  1.11it/s]  8%|         | 21/256 [00:27<03:32,  1.11it/s]  9%|         | 22/256 [00:27<03:31,  1.11it/s]  9%|         | 23/256 [00:28<03:29,  1.11it/s]  9%|         | 24/256 [00:29<03:27,  1.12it/s] 10%|         | 25/256 [00:30<03:26,  1.12it/s] 10%|         | 26/256 [00:31<03:24,  1.12it/s] 11%|         | 27/256 [00:32<03:24,  1.12it/s] 11%|         | 28/256 [00:33<03:24,  1.11it/s] 11%|        | 29/256 [00:34<03:23,  1.12it/s] 12%|        | 30/256 [00:35<03:21,  1.12it/s] 12%|        | 31/256 [00:35<03:21,  1.12it/s] 12%|        | 32/256 [00:36<03:21,  1.11it/s] 13%|        | 33/256 [00:37<03:21,  1.10it/s] 13%|        | 34/256 [00:38<03:20,  1.11it/s] 14%|        | 35/256 [00:39<03:19,  1.11it/s] 14%|        | 36/256 [00:40<03:17,  1.11it/s] 14%|        | 37/256 [00:41<03:16,  1.11it/s] 15%|        | 38/256 [00:42<03:16,  1.11it/s] 15%|        | 39/256 [00:43<03:16,  1.10it/s] 16%|        | 40/256 [00:44<03:15,  1.10it/s] 16%|        | 41/256 [00:44<03:13,  1.11it/s] 16%|        | 42/256 [00:45<03:12,  1.11it/s] 17%|        | 43/256 [00:46<03:11,  1.11it/s] 17%|        | 44/256 [00:47<03:11,  1.11it/s] 18%|        | 45/256 [00:48<03:11,  1.10it/s] 18%|        | 46/256 [00:49<03:11,  1.10it/s] 18%|        | 47/256 [00:50<03:08,  1.11it/s] 19%|        | 48/256 [00:51<03:07,  1.11it/s] 19%|        | 49/256 [00:52<03:05,  1.11it/s] 20%|        | 50/256 [00:53<03:04,  1.11it/s] 20%|        | 51/256 [00:54<03:05,  1.11it/s] 20%|        | 52/256 [00:54<03:05,  1.10it/s] 21%|        | 53/256 [00:55<03:03,  1.11it/s] 21%|        | 54/256 [00:56<03:02,  1.10it/s] 21%|       | 55/256 [00:57<03:03,  1.09it/s] 22%|       | 56/256 [00:58<03:02,  1.09it/s] 22%|       | 57/256 [00:59<03:02,  1.09it/s] 23%|       | 58/256 [01:00<03:00,  1.10it/s] 23%|       | 59/256 [01:01<02:59,  1.10it/s] 23%|       | 60/256 [01:02<02:57,  1.10it/s] 24%|       | 61/256 [01:03<02:57,  1.10it/s] 24%|       | 62/256 [01:04<02:55,  1.11it/s] 25%|       | 63/256 [01:04<02:54,  1.11it/s] 25%|       | 64/256 [01:06<03:08,  1.02it/s] 25%|       | 65/256 [01:07<03:02,  1.05it/s] 26%|       | 66/256 [01:07<02:58,  1.06it/s] 26%|       | 67/256 [01:08<02:54,  1.08it/s] 27%|       | 68/256 [01:09<03:04,  1.02it/s] 27%|       | 69/256 [01:10<02:59,  1.04it/s] 27%|       | 70/256 [01:11<02:54,  1.06it/s] 28%|       | 71/256 [01:12<02:52,  1.07it/s] 28%|       | 72/256 [01:13<02:48,  1.09it/s] 29%|       | 73/256 [01:14<02:58,  1.03it/s] 29%|       | 74/256 [01:15<02:53,  1.05it/s] 29%|       | 75/256 [01:16<02:50,  1.06it/s] 30%|       | 76/256 [01:17<02:47,  1.07it/s] 30%|       | 77/256 [01:18<02:45,  1.08it/s] 30%|       | 78/256 [01:19<02:43,  1.09it/s] 31%|       | 79/256 [01:20<02:42,  1.09it/s] 31%|      | 80/256 [01:20<02:41,  1.09it/s] 32%|      | 81/256 [01:21<02:39,  1.10it/s] 32%|      | 82/256 [01:22<02:38,  1.10it/s] 32%|      | 83/256 [01:23<02:36,  1.10it/s] 33%|      | 84/256 [01:24<02:35,  1.11it/s] 33%|      | 85/256 [01:25<02:34,  1.11it/s] 34%|      | 86/256 [01:26<02:33,  1.11it/s] 34%|      | 87/256 [01:27<02:32,  1.11it/s] 34%|      | 88/256 [01:28<02:30,  1.12it/s] 35%|      | 89/256 [01:29<02:30,  1.11it/s] 35%|      | 90/256 [01:29<02:29,  1.11it/s] 36%|      | 91/256 [01:30<02:27,  1.12it/s] 36%|      | 92/256 [01:31<02:27,  1.11it/s] 36%|      | 93/256 [01:32<02:25,  1.12it/s] 37%|      | 94/256 [01:33<02:25,  1.11it/s] 37%|      | 95/256 [01:34<02:25,  1.11it/s] 38%|      | 96/256 [01:35<02:24,  1.10it/s] 38%|      | 97/256 [01:36<02:23,  1.11it/s] 38%|      | 98/256 [01:37<02:23,  1.10it/s] 39%|      | 99/256 [01:38<02:23,  1.10it/s] 39%|      | 100/256 [01:39<02:21,  1.10it/s] 39%|      | 101/256 [01:39<02:20,  1.10it/s] 40%|      | 102/256 [01:40<02:18,  1.11it/s] 40%|      | 103/256 [01:41<02:17,  1.11it/s] 41%|      | 104/256 [01:42<02:16,  1.11it/s] 41%|      | 105/256 [01:43<02:15,  1.12it/s] 41%|     | 106/256 [01:44<02:14,  1.12it/s] 42%|     | 107/256 [01:45<02:12,  1.12it/s] 42%|     | 108/256 [01:46<02:12,  1.12it/s] 43%|     | 109/256 [01:47<02:11,  1.12it/s] 43%|     | 110/256 [01:47<02:11,  1.11it/s] 43%|     | 111/256 [01:48<02:09,  1.12it/s] 44%|     | 112/256 [01:49<02:09,  1.11it/s] 44%|     | 113/256 [01:50<02:09,  1.11it/s] 45%|     | 114/256 [01:51<02:08,  1.10it/s] 45%|     | 115/256 [01:52<02:06,  1.12it/s] 45%|     | 116/256 [01:53<02:05,  1.11it/s] 46%|     | 117/256 [01:54<02:04,  1.12it/s] 46%|     | 118/256 [01:55<02:03,  1.11it/s] 46%|     | 119/256 [01:56<02:03,  1.11it/s] 47%|     | 120/256 [01:56<02:02,  1.11it/s] 47%|     | 121/256 [01:57<02:00,  1.12it/s] 48%|     | 122/256 [01:58<01:59,  1.12it/s] 48%|     | 123/256 [01:59<01:59,  1.11it/s] 48%|     | 124/256 [02:00<01:59,  1.11it/s] 49%|     | 125/256 [02:01<01:58,  1.11it/s] 49%|     | 126/256 [02:02<01:58,  1.10it/s] 50%|     | 127/256 [02:03<01:56,  1.11it/s] 50%|     | 128/256 [02:04<01:54,  1.11it/s] 50%|     | 129/256 [02:05<01:54,  1.11it/s] 51%|     | 130/256 [02:05<01:53,  1.11it/s] 51%|     | 131/256 [02:06<01:53,  1.10it/s] 52%|    | 132/256 [02:07<01:52,  1.10it/s] 52%|    | 133/256 [02:08<01:52,  1.09it/s] 52%|    | 134/256 [02:09<01:50,  1.10it/s] 53%|    | 135/256 [02:10<01:49,  1.11it/s] 53%|    | 136/256 [02:11<01:49,  1.10it/s] 54%|    | 137/256 [02:12<01:48,  1.10it/s] 54%|    | 138/256 [02:13<01:46,  1.11it/s] 54%|    | 139/256 [02:14<01:45,  1.11it/s] 55%|    | 140/256 [02:15<01:44,  1.12it/s] 55%|    | 141/256 [02:15<01:42,  1.12it/s] 55%|    | 142/256 [02:16<01:41,  1.12it/s] 56%|    | 143/256 [02:17<01:41,  1.12it/s] 56%|    | 144/256 [02:18<01:40,  1.11it/s] 57%|    | 145/256 [02:19<01:39,  1.11it/s] 57%|    | 146/256 [02:20<01:39,  1.11it/s] 57%|    | 147/256 [02:21<01:38,  1.10it/s] 58%|    | 148/256 [02:22<01:38,  1.10it/s] 58%|    | 149/256 [02:23<01:36,  1.10it/s] 59%|    | 150/256 [02:24<01:36,  1.10it/s] 59%|    | 151/256 [02:24<01:34,  1.11it/s] 59%|    | 152/256 [02:25<01:33,  1.11it/s] 60%|    | 153/256 [02:26<01:32,  1.11it/s] 60%|    | 154/256 [02:27<01:32,  1.10it/s] 61%|    | 155/256 [02:28<01:32,  1.10it/s] 61%|    | 156/256 [02:29<01:31,  1.10it/s] 61%|   | 157/256 [02:30<01:29,  1.10it/s] 62%|   | 158/256 [02:31<01:28,  1.10it/s] 62%|   | 159/256 [02:32<01:27,  1.10it/s] 62%|   | 160/256 [02:33<01:27,  1.10it/s] 63%|   | 161/256 [02:34<01:26,  1.10it/s] 63%|   | 162/256 [02:34<01:25,  1.10it/s] 64%|   | 163/256 [02:35<01:24,  1.10it/s] 64%|   | 164/256 [02:36<01:23,  1.11it/s] 64%|   | 165/256 [02:37<01:22,  1.10it/s] 65%|   | 166/256 [02:38<01:21,  1.10it/s] 65%|   | 167/256 [02:39<01:20,  1.10it/s] 66%|   | 168/256 [02:40<01:19,  1.11it/s] 66%|   | 169/256 [02:41<01:18,  1.11it/s] 66%|   | 170/256 [02:42<01:16,  1.12it/s] 67%|   | 171/256 [02:43<01:15,  1.12it/s] 67%|   | 172/256 [02:43<01:15,  1.11it/s] 68%|   | 173/256 [02:44<01:14,  1.11it/s] 68%|   | 174/256 [02:45<01:13,  1.11it/s] 68%|   | 175/256 [02:46<01:13,  1.10it/s] 69%|   | 176/256 [02:47<01:13,  1.09it/s] 69%|   | 177/256 [02:48<01:12,  1.09it/s] 70%|   | 178/256 [02:49<01:10,  1.11it/s] 70%|   | 179/256 [02:50<01:09,  1.10it/s] 70%|   | 180/256 [02:51<01:09,  1.10it/s] 71%|   | 181/256 [02:52<01:07,  1.10it/s] 71%|   | 182/256 [02:53<01:06,  1.11it/s] 71%|  | 183/256 [02:53<01:05,  1.11it/s] 72%|  | 184/256 [02:54<01:04,  1.12it/s] 72%|  | 185/256 [02:55<01:03,  1.12it/s] 73%|  | 186/256 [02:56<01:02,  1.12it/s] 73%|  | 187/256 [02:57<01:02,  1.11it/s] 73%|  | 188/256 [02:58<01:00,  1.12it/s] 74%|  | 189/256 [02:59<00:59,  1.12it/s] 74%|  | 190/256 [03:00<00:59,  1.12it/s] 75%|  | 191/256 [03:01<00:58,  1.12it/s] 75%|  | 192/256 [03:01<00:57,  1.12it/s] 75%|  | 193/256 [03:02<00:56,  1.12it/s] 76%|  | 194/256 [03:03<00:55,  1.11it/s] 76%|  | 195/256 [03:04<00:54,  1.11it/s] 77%|  | 196/256 [03:05<00:53,  1.12it/s] 77%|  | 197/256 [03:06<00:52,  1.12it/s] 77%|  | 198/256 [03:07<00:51,  1.12it/s] 78%|  | 199/256 [03:08<00:51,  1.12it/s] 78%|  | 200/256 [03:09<00:50,  1.11it/s] 79%|  | 201/256 [03:10<00:49,  1.11it/s] 79%|  | 202/256 [03:10<00:48,  1.11it/s] 79%|  | 203/256 [03:11<00:47,  1.11it/s] 80%|  | 204/256 [03:12<00:46,  1.11it/s] 80%|  | 205/256 [03:13<00:45,  1.11it/s] 80%|  | 206/256 [03:14<00:45,  1.10it/s] 81%|  | 207/256 [03:15<00:44,  1.10it/s] 81%| | 208/256 [03:16<00:43,  1.10it/s] 82%| | 209/256 [03:17<00:42,  1.10it/s] 82%| | 210/256 [03:18<00:42,  1.09it/s] 82%| | 211/256 [03:19<00:41,  1.10it/s] 83%| | 212/256 [03:20<00:40,  1.10it/s] 83%| | 213/256 [03:20<00:38,  1.11it/s] 84%| | 214/256 [03:21<00:37,  1.11it/s] 84%| | 215/256 [03:22<00:37,  1.10it/s] 84%| | 216/256 [03:23<00:36,  1.10it/s] 85%| | 217/256 [03:24<00:35,  1.10it/s] 85%| | 218/256 [03:25<00:34,  1.10it/s] 86%| | 219/256 [03:26<00:33,  1.10it/s] 86%| | 220/256 [03:27<00:32,  1.10it/s] 86%| | 221/256 [03:28<00:31,  1.11it/s] 87%| | 222/256 [03:29<00:31,  1.09it/s] 87%| | 223/256 [03:30<00:30,  1.09it/s] 88%| | 224/256 [03:30<00:29,  1.10it/s] 88%| | 225/256 [03:31<00:28,  1.10it/s] 88%| | 226/256 [03:32<00:27,  1.10it/s] 89%| | 227/256 [03:33<00:26,  1.11it/s] 89%| | 228/256 [03:34<00:25,  1.10it/s] 89%| | 229/256 [03:35<00:24,  1.10it/s] 90%| | 230/256 [03:36<00:23,  1.10it/s] 90%| | 231/256 [03:37<00:22,  1.11it/s] 91%| | 232/256 [03:38<00:21,  1.10it/s] 91%| | 233/256 [03:39<00:20,  1.11it/s] 91%|| 234/256 [03:39<00:19,  1.13it/s] 92%|| 235/256 [03:40<00:18,  1.15it/s] 92%|| 236/256 [03:41<00:16,  1.18it/s] 93%|| 237/256 [03:42<00:15,  1.19it/s] 93%|| 238/256 [03:43<00:14,  1.20it/s] 93%|| 239/256 [03:44<00:14,  1.21it/s] 94%|| 240/256 [03:44<00:13,  1.22it/s] 94%|| 241/256 [03:45<00:12,  1.22it/s] 95%|| 242/256 [03:46<00:11,  1.22it/s] 95%|| 243/256 [03:47<00:10,  1.22it/s] 95%|| 244/256 [03:48<00:09,  1.23it/s] 96%|| 245/256 [03:48<00:08,  1.23it/s] 96%|| 246/256 [03:49<00:08,  1.23it/s] 96%|| 247/256 [03:50<00:07,  1.23it/s] 97%|| 248/256 [03:51<00:06,  1.23it/s] 97%|| 249/256 [03:52<00:05,  1.23it/s] 98%|| 250/256 [03:52<00:04,  1.23it/s] 98%|| 251/256 [03:53<00:04,  1.23it/s] 98%|| 252/256 [03:54<00:03,  1.23it/s] 99%|| 253/256 [03:55<00:02,  1.23it/s] 99%|| 254/256 [03:56<00:01,  1.23it/s]100%|| 255/256 [03:57<00:00,  1.23it/s]100%|| 256/256 [03:57<00:00,  1.35it/s]100%|| 256/256 [03:57<00:00,  1.08it/s]
=> result
* total: 50,000
* correct: 25,930
* accuracy: 51.9%
* error: 48.1%
* macro_f1: 51.1%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/xd_test.sh eurosat imagenet 2 0 main_final1212 16 30 RPO_prime
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_final1212.yaml
dataset_config_file: configs/datasets/imagenet.yaml
eval_only: True
head: 
load_epoch: 30
model_dir: output/rpo_prime/crossdataset_1212/train_source/eurosat/shots_16/RPO_prime/main_final1212/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/imagenet/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: ImageNet
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/crossdataset_1212/test_target/source_eurosat/imagenet/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 24
    K2: 0
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:ImageNet
Loading dataset: ImageNet
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/ImageNet/split_fewshot_taesup/shot_16-seed_2.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  --------
Dataset    ImageNet
# classes  1,000
# train_x  16,000
# val      50,000
# test     50,000
---------  --------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Traceback (most recent call last):
  File "train.py", line 236, in <module>
    main(args)
  File "train.py", line 170, in main
    trainer = build_trainer(cfg)
  File "/home/s2/mjoolee/CLIP/KgCoop/Dassl.pytorch/dassl/engine/build.py", line 11, in build_trainer
    return TRAINER_REGISTRY.get(cfg.TRAINER.NAME)(cfg)
  File "/home/s2/mjoolee/CLIP/KgCoop/Dassl.pytorch/dassl/engine/trainer.py", line 325, in __init__
    self.build_model()
  File "/shared/s2/lab01/myungjoo/RPO_v2/trainers/rpo_prime.py", line 304, in build_model
    self.model = CustomCLIP(cfg, classnames, prompt, clip_model)
  File "/shared/s2/lab01/myungjoo/RPO_v2/trainers/rpo_prime.py", line 115, in __init__
    self.prompt_learner = PromptLearner(self.cfg, clipmodel)
  File "/shared/s2/lab01/myungjoo/RPO_v2/trainers/rpo_prime.py", line 49, in __init__
    assert cfg.TRAINER.RPO.K2 >= 1, "K should be bigger than 0"
AssertionError: K should be bigger than 0
