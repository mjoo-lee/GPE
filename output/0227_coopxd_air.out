set -e
set -x

eval "$(conda shell.bash hook)"
++ conda shell.bash hook
+ eval 'export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
export CONDA_EXE='/home/s2/mjoolee/anaconda/bin/conda'
++ export CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
++ CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
export _CE_M=''
++ export _CE_M=
++ _CE_M=
export _CE_CONDA=''
++ export _CE_CONDA=
++ _CE_CONDA=
export CONDA_PYTHON_EXE='/home/s2/mjoolee/anaconda/bin/python'
++ export CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
++ CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We're not allowing PS1 to be unbound. It must at least be set.
    # However, we're not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi
++ '[' -z x ']'

conda activate base
++ conda activate base
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate base
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate base
+++ /home/s2/mjoolee/anaconda/bin/conda shell.posix activate base
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_PREFIX='\''/home/s2/mjoolee/anaconda'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/home/s2/mjoolee/anaconda/envs/dassl'\''
export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_PREFIX='\''/home/s2/mjoolee/anaconda'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/home/s2/mjoolee/anaconda/envs/dassl'\''
export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\'''
PS1='(base) '
+++ PS1='(base) '
export PATH='/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'
+++ export PATH=/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
+++ PATH=/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
export CONDA_PREFIX='/home/s2/mjoolee/anaconda'
+++ export CONDA_PREFIX=/home/s2/mjoolee/anaconda
+++ CONDA_PREFIX=/home/s2/mjoolee/anaconda
export CONDA_SHLVL='3'
+++ export CONDA_SHLVL=3
+++ CONDA_SHLVL=3
export CONDA_DEFAULT_ENV='base'
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
export CONDA_PROMPT_MODIFIER='(base) '
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
export CONDA_PREFIX_2='/home/s2/mjoolee/anaconda/envs/dassl'
+++ export CONDA_PREFIX_2=/home/s2/mjoolee/anaconda/envs/dassl
+++ CONDA_PREFIX_2=/home/s2/mjoolee/anaconda/envs/dassl
export CONDA_EXE='/home/s2/mjoolee/anaconda/bin/conda'
+++ export CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
+++ CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
export _CE_M=''
+++ export _CE_M=
+++ _CE_M=
export _CE_CONDA=''
+++ export _CE_CONDA=
+++ _CE_CONDA=
export CONDA_PYTHON_EXE='/home/s2/mjoolee/anaconda/bin/python'
+++ export CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
+++ CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
conda activate dassl
+ conda activate dassl
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate dassl
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate dassl
++ /home/s2/mjoolee/anaconda/bin/conda shell.posix activate dassl
+ ask_conda='PS1='\''(dassl) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_PREFIX='\''/home/s2/mjoolee/anaconda/envs/dassl'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''dassl'\''
export CONDA_PROMPT_MODIFIER='\''(dassl) '\''
export CONDA_PREFIX_3='\''/home/s2/mjoolee/anaconda'\''
export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\'''
+ eval 'PS1='\''(dassl) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_PREFIX='\''/home/s2/mjoolee/anaconda/envs/dassl'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''dassl'\''
export CONDA_PROMPT_MODIFIER='\''(dassl) '\''
export CONDA_PREFIX_3='\''/home/s2/mjoolee/anaconda'\''
export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\'''
PS1='(dassl) '
++ PS1='(dassl) '
export PATH='/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'
++ export PATH=/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
++ PATH=/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
export CONDA_PREFIX='/home/s2/mjoolee/anaconda/envs/dassl'
++ export CONDA_PREFIX=/home/s2/mjoolee/anaconda/envs/dassl
++ CONDA_PREFIX=/home/s2/mjoolee/anaconda/envs/dassl
export CONDA_SHLVL='4'
++ export CONDA_SHLVL=4
++ CONDA_SHLVL=4
export CONDA_DEFAULT_ENV='dassl'
++ export CONDA_DEFAULT_ENV=dassl
++ CONDA_DEFAULT_ENV=dassl
export CONDA_PROMPT_MODIFIER='(dassl) '
++ export 'CONDA_PROMPT_MODIFIER=(dassl) '
++ CONDA_PROMPT_MODIFIER='(dassl) '
export CONDA_PREFIX_3='/home/s2/mjoolee/anaconda'
++ export CONDA_PREFIX_3=/home/s2/mjoolee/anaconda
++ CONDA_PREFIX_3=/home/s2/mjoolee/anaconda
export CONDA_EXE='/home/s2/mjoolee/anaconda/bin/conda'
++ export CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
++ CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
export _CE_M=''
++ export _CE_M=
++ _CE_M=
export _CE_CONDA=''
++ export _CE_CONDA=
++ _CE_CONDA=
export CONDA_PYTHON_EXE='/home/s2/mjoolee/anaconda/bin/python'
++ export CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
++ CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r

GPU=0
+ GPU=0
SHOT=16
+ SHOT=16
# EPOCH=50
# cfg=vit_b16_ep50_ctxv1
EPOCH=200
+ EPOCH=200
cfg=vit_b16_ctxv1
+ cfg=vit_b16_ctxv1
TRAINER=CoOp
+ TRAINER=CoOp


for seed in 1 2 3
 do
     #training
     sh scripts/coop/crossdataset_train.sh fgvc_aircraft ${seed} ${GPU} ${cfg} ${SHOT} ${TRAINER}
 done         
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_train.sh fgvc_aircraft 1 0 vit_b16_ctxv1 16 CoOp
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_1.pkl
1600 3333 3333
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  100
# train_x  1,600
# val      3,333
# test     3,333
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/200] batch [5/50] time 0.083 (0.782) data 0.000 (0.372) loss 3.2324 (3.1281) acc 15.6250 (18.1250) lr 1.0000e-05 eta 2:10:11
epoch [1/200] batch [10/50] time 0.084 (0.433) data 0.000 (0.186) loss 2.5371 (3.0758) acc 25.0000 (17.8125) lr 1.0000e-05 eta 1:12:03
epoch [1/200] batch [15/50] time 0.087 (0.317) data 0.000 (0.124) loss 3.0566 (3.0720) acc 18.7500 (19.3750) lr 1.0000e-05 eta 0:52:44
epoch [1/200] batch [20/50] time 0.084 (0.259) data 0.000 (0.093) loss 3.1523 (3.0777) acc 6.2500 (18.9062) lr 1.0000e-05 eta 0:43:03
epoch [1/200] batch [25/50] time 0.083 (0.224) data 0.000 (0.075) loss 2.9531 (3.0066) acc 9.3750 (19.6250) lr 1.0000e-05 eta 0:37:15
epoch [1/200] batch [30/50] time 0.084 (0.201) data 0.000 (0.062) loss 3.1309 (2.9891) acc 15.6250 (19.4792) lr 1.0000e-05 eta 0:33:22
epoch [1/200] batch [35/50] time 0.084 (0.184) data 0.001 (0.054) loss 3.5137 (3.0116) acc 15.6250 (19.4643) lr 1.0000e-05 eta 0:30:35
epoch [1/200] batch [40/50] time 0.084 (0.173) data 0.000 (0.048) loss 2.4961 (2.9894) acc 21.8750 (19.7656) lr 1.0000e-05 eta 0:28:40
epoch [1/200] batch [45/50] time 0.096 (0.165) data 0.013 (0.045) loss 3.0684 (2.9964) acc 18.7500 (19.7917) lr 1.0000e-05 eta 0:27:20
epoch [1/200] batch [50/50] time 0.082 (0.157) data 0.000 (0.041) loss 2.9023 (3.0159) acc 21.8750 (19.4375) lr 2.0000e-03 eta 0:26:00
epoch [2/200] batch [5/50] time 0.084 (0.309) data 0.000 (0.225) loss 2.9863 (2.9445) acc 18.7500 (18.1250) lr 2.0000e-03 eta 0:51:16
epoch [2/200] batch [10/50] time 0.085 (0.201) data 0.000 (0.117) loss 2.6562 (2.9703) acc 18.7500 (17.5000) lr 2.0000e-03 eta 0:33:18
epoch [2/200] batch [15/50] time 0.083 (0.165) data 0.000 (0.082) loss 3.2949 (2.9712) acc 15.6250 (18.9583) lr 2.0000e-03 eta 0:27:23
epoch [2/200] batch [20/50] time 0.112 (0.155) data 0.029 (0.071) loss 2.4707 (2.9131) acc 31.2500 (18.5938) lr 2.0000e-03 eta 0:25:37
epoch [2/200] batch [25/50] time 0.085 (0.141) data 0.000 (0.057) loss 2.2129 (2.8662) acc 37.5000 (20.3750) lr 2.0000e-03 eta 0:23:17
epoch [2/200] batch [30/50] time 0.084 (0.139) data 0.000 (0.055) loss 2.6914 (2.8282) acc 15.6250 (21.4583) lr 2.0000e-03 eta 0:22:55
epoch [2/200] batch [35/50] time 0.235 (0.135) data 0.153 (0.051) loss 2.8535 (2.8051) acc 18.7500 (22.0536) lr 2.0000e-03 eta 0:22:20
epoch [2/200] batch [40/50] time 0.083 (0.129) data 0.000 (0.045) loss 2.9531 (2.8443) acc 28.1250 (21.9531) lr 2.0000e-03 eta 0:21:19
epoch [2/200] batch [45/50] time 0.083 (0.127) data 0.000 (0.044) loss 2.7539 (2.8291) acc 15.6250 (22.0139) lr 2.0000e-03 eta 0:21:00
epoch [2/200] batch [50/50] time 0.081 (0.123) data 0.000 (0.039) loss 3.3027 (2.8113) acc 15.6250 (22.6875) lr 1.9999e-03 eta 0:20:15
epoch [3/200] batch [5/50] time 0.082 (0.316) data 0.000 (0.234) loss 2.3301 (2.8625) acc 34.3750 (24.3750) lr 1.9999e-03 eta 0:52:11
epoch [3/200] batch [10/50] time 0.108 (0.204) data 0.025 (0.121) loss 1.8496 (2.5645) acc 56.2500 (31.8750) lr 1.9999e-03 eta 0:33:35
epoch [3/200] batch [15/50] time 0.083 (0.164) data 0.000 (0.081) loss 2.3613 (2.5582) acc 18.7500 (29.5833) lr 1.9999e-03 eta 0:26:56
epoch [3/200] batch [20/50] time 0.083 (0.152) data 0.000 (0.069) loss 2.7695 (2.6065) acc 25.0000 (27.0312) lr 1.9999e-03 eta 0:25:04
epoch [3/200] batch [25/50] time 0.084 (0.138) data 0.001 (0.055) loss 2.8066 (2.5934) acc 18.7500 (26.7500) lr 1.9999e-03 eta 0:22:47
epoch [3/200] batch [30/50] time 0.084 (0.135) data 0.000 (0.052) loss 2.6328 (2.5975) acc 15.6250 (26.0417) lr 1.9999e-03 eta 0:22:08
epoch [3/200] batch [35/50] time 0.084 (0.127) data 0.001 (0.044) loss 2.6152 (2.6030) acc 31.2500 (26.2500) lr 1.9999e-03 eta 0:20:56
epoch [3/200] batch [40/50] time 0.084 (0.122) data 0.000 (0.039) loss 2.2324 (2.6033) acc 40.6250 (26.0156) lr 1.9999e-03 eta 0:20:02
epoch [3/200] batch [45/50] time 0.082 (0.118) data 0.000 (0.035) loss 2.6992 (2.5982) acc 25.0000 (26.0417) lr 1.9999e-03 eta 0:19:18
epoch [3/200] batch [50/50] time 0.082 (0.114) data 0.000 (0.031) loss 2.5000 (2.5945) acc 34.3750 (26.2500) lr 1.9995e-03 eta 0:18:42
epoch [4/200] batch [5/50] time 0.084 (0.431) data 0.000 (0.346) loss 2.2949 (2.2852) acc 21.8750 (33.1250) lr 1.9995e-03 eta 1:10:47
epoch [4/200] batch [10/50] time 0.085 (0.268) data 0.000 (0.184) loss 2.4590 (2.4420) acc 25.0000 (28.7500) lr 1.9995e-03 eta 0:44:01
epoch [4/200] batch [15/50] time 0.084 (0.207) data 0.000 (0.123) loss 2.1230 (2.4234) acc 34.3750 (28.9583) lr 1.9995e-03 eta 0:33:56
epoch [4/200] batch [20/50] time 0.084 (0.186) data 0.000 (0.102) loss 2.4961 (2.4523) acc 34.3750 (28.5938) lr 1.9995e-03 eta 0:30:28
epoch [4/200] batch [25/50] time 0.244 (0.172) data 0.160 (0.088) loss 2.3008 (2.4482) acc 37.5000 (29.1250) lr 1.9995e-03 eta 0:28:11
epoch [4/200] batch [30/50] time 0.084 (0.158) data 0.001 (0.073) loss 2.6172 (2.4594) acc 25.0000 (28.8542) lr 1.9995e-03 eta 0:25:47
epoch [4/200] batch [35/50] time 0.084 (0.152) data 0.000 (0.067) loss 2.9961 (2.4742) acc 21.8750 (28.7500) lr 1.9995e-03 eta 0:24:47
epoch [4/200] batch [40/50] time 0.083 (0.143) data 0.000 (0.059) loss 2.3730 (2.5015) acc 28.1250 (28.1250) lr 1.9995e-03 eta 0:23:22
epoch [4/200] batch [45/50] time 0.082 (0.138) data 0.000 (0.054) loss 2.7910 (2.4945) acc 31.2500 (28.1250) lr 1.9995e-03 eta 0:22:34
epoch [4/200] batch [50/50] time 0.082 (0.133) data 0.000 (0.049) loss 2.6875 (2.4957) acc 18.7500 (27.5625) lr 1.9989e-03 eta 0:21:39
epoch [5/200] batch [5/50] time 0.084 (0.326) data 0.000 (0.241) loss 2.4473 (2.5398) acc 31.2500 (31.8750) lr 1.9989e-03 eta 0:53:12
epoch [5/200] batch [10/50] time 0.085 (0.218) data 0.001 (0.134) loss 2.1699 (2.5098) acc 25.0000 (26.8750) lr 1.9989e-03 eta 0:35:38
epoch [5/200] batch [15/50] time 0.085 (0.174) data 0.000 (0.090) loss 2.4512 (2.5566) acc 15.6250 (26.0417) lr 1.9989e-03 eta 0:28:23
epoch [5/200] batch [20/50] time 0.086 (0.162) data 0.001 (0.077) loss 2.2109 (2.5128) acc 21.8750 (27.0312) lr 1.9989e-03 eta 0:26:22
epoch [5/200] batch [25/50] time 0.248 (0.153) data 0.164 (0.069) loss 2.2656 (2.5109) acc 28.1250 (26.8750) lr 1.9989e-03 eta 0:24:55
epoch [5/200] batch [30/50] time 0.085 (0.142) data 0.000 (0.057) loss 2.1777 (2.5091) acc 40.6250 (27.8125) lr 1.9989e-03 eta 0:23:03
epoch [5/200] batch [35/50] time 0.085 (0.136) data 0.000 (0.052) loss 2.4336 (2.5114) acc 31.2500 (27.3214) lr 1.9989e-03 eta 0:22:09
epoch [5/200] batch [40/50] time 0.089 (0.130) data 0.000 (0.045) loss 2.1816 (2.5000) acc 37.5000 (27.2656) lr 1.9989e-03 eta 0:21:06
epoch [5/200] batch [45/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.2695 (2.4906) acc 31.2500 (27.1528) lr 1.9989e-03 eta 0:20:24
epoch [5/200] batch [50/50] time 0.082 (0.121) data 0.000 (0.037) loss 2.1934 (2.4650) acc 28.1250 (27.7500) lr 1.9980e-03 eta 0:19:41
epoch [6/200] batch [5/50] time 0.083 (0.313) data 0.000 (0.228) loss 1.9648 (2.4098) acc 53.1250 (26.8750) lr 1.9980e-03 eta 0:50:48
epoch [6/200] batch [10/50] time 0.083 (0.209) data 0.000 (0.125) loss 1.9668 (2.4084) acc 31.2500 (27.8125) lr 1.9980e-03 eta 0:33:58
epoch [6/200] batch [15/50] time 0.084 (0.167) data 0.001 (0.084) loss 2.2559 (2.3453) acc 18.7500 (29.7917) lr 1.9980e-03 eta 0:27:09
epoch [6/200] batch [20/50] time 0.084 (0.147) data 0.000 (0.063) loss 2.4199 (2.4148) acc 25.0000 (27.8125) lr 1.9980e-03 eta 0:23:48
epoch [6/200] batch [25/50] time 0.084 (0.134) data 0.000 (0.051) loss 2.5176 (2.3808) acc 18.7500 (28.1250) lr 1.9980e-03 eta 0:21:44
epoch [6/200] batch [30/50] time 0.086 (0.130) data 0.000 (0.046) loss 2.2578 (2.3577) acc 18.7500 (28.2292) lr 1.9980e-03 eta 0:21:03
epoch [6/200] batch [35/50] time 0.085 (0.125) data 0.001 (0.041) loss 2.2812 (2.3615) acc 28.1250 (28.2143) lr 1.9980e-03 eta 0:20:16
epoch [6/200] batch [40/50] time 0.085 (0.120) data 0.000 (0.036) loss 2.3281 (2.4009) acc 28.1250 (27.8125) lr 1.9980e-03 eta 0:19:27
epoch [6/200] batch [45/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.3164 (2.4257) acc 34.3750 (27.7083) lr 1.9980e-03 eta 0:19:19
epoch [6/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.032) loss 2.5566 (2.4236) acc 28.1250 (28.0000) lr 1.9969e-03 eta 0:18:43
epoch [7/200] batch [5/50] time 0.286 (0.334) data 0.201 (0.249) loss 2.4004 (2.3924) acc 34.3750 (26.8750) lr 1.9969e-03 eta 0:53:57
epoch [7/200] batch [10/50] time 0.085 (0.210) data 0.000 (0.125) loss 2.3320 (2.4052) acc 37.5000 (27.5000) lr 1.9969e-03 eta 0:33:51
epoch [7/200] batch [15/50] time 0.085 (0.171) data 0.000 (0.086) loss 2.6094 (2.4553) acc 28.1250 (26.6667) lr 1.9969e-03 eta 0:27:38
epoch [7/200] batch [20/50] time 0.085 (0.150) data 0.000 (0.065) loss 2.6426 (2.4602) acc 31.2500 (27.5000) lr 1.9969e-03 eta 0:24:09
epoch [7/200] batch [25/50] time 0.086 (0.142) data 0.000 (0.057) loss 2.3477 (2.4082) acc 40.6250 (29.6250) lr 1.9969e-03 eta 0:22:54
epoch [7/200] batch [30/50] time 0.228 (0.137) data 0.143 (0.052) loss 2.1719 (2.4174) acc 31.2500 (29.7917) lr 1.9969e-03 eta 0:22:08
epoch [7/200] batch [35/50] time 0.086 (0.130) data 0.001 (0.045) loss 2.6387 (2.4297) acc 21.8750 (29.2857) lr 1.9969e-03 eta 0:20:55
epoch [7/200] batch [40/50] time 0.083 (0.130) data 0.000 (0.045) loss 2.2578 (2.4014) acc 46.8750 (30.3125) lr 1.9969e-03 eta 0:20:54
epoch [7/200] batch [45/50] time 0.082 (0.125) data 0.000 (0.040) loss 2.3613 (2.3939) acc 40.6250 (30.9028) lr 1.9969e-03 eta 0:20:03
epoch [7/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.038) loss 2.2461 (2.3772) acc 31.2500 (31.0000) lr 1.9956e-03 eta 0:19:37
epoch [8/200] batch [5/50] time 0.088 (0.304) data 0.000 (0.218) loss 2.5957 (2.2430) acc 21.8750 (32.5000) lr 1.9956e-03 eta 0:48:52
epoch [8/200] batch [10/50] time 0.085 (0.204) data 0.000 (0.118) loss 2.0879 (2.2682) acc 37.5000 (33.4375) lr 1.9956e-03 eta 0:32:45
epoch [8/200] batch [15/50] time 0.085 (0.164) data 0.000 (0.079) loss 2.4336 (2.2704) acc 25.0000 (33.1250) lr 1.9956e-03 eta 0:26:23
epoch [8/200] batch [20/50] time 0.087 (0.147) data 0.000 (0.062) loss 2.9355 (2.3402) acc 18.7500 (31.0938) lr 1.9956e-03 eta 0:23:35
epoch [8/200] batch [25/50] time 0.183 (0.139) data 0.098 (0.053) loss 2.2617 (2.3454) acc 31.2500 (31.2500) lr 1.9956e-03 eta 0:22:14
epoch [8/200] batch [30/50] time 0.086 (0.130) data 0.000 (0.045) loss 2.6074 (2.3526) acc 34.3750 (31.6667) lr 1.9956e-03 eta 0:20:49
epoch [8/200] batch [35/50] time 0.085 (0.126) data 0.001 (0.041) loss 2.6523 (2.3667) acc 18.7500 (31.0714) lr 1.9956e-03 eta 0:20:13
epoch [8/200] batch [40/50] time 0.086 (0.121) data 0.000 (0.036) loss 2.0859 (2.3611) acc 43.7500 (31.7188) lr 1.9956e-03 eta 0:19:24
epoch [8/200] batch [45/50] time 0.083 (0.119) data 0.000 (0.034) loss 2.2539 (2.3495) acc 34.3750 (31.7361) lr 1.9956e-03 eta 0:19:06
epoch [8/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.031) loss 2.4512 (2.3495) acc 37.5000 (31.6250) lr 1.9940e-03 eta 0:18:30
epoch [9/200] batch [5/50] time 0.085 (0.325) data 0.000 (0.239) loss 2.1055 (2.1990) acc 37.5000 (34.3750) lr 1.9940e-03 eta 0:51:57
epoch [9/200] batch [10/50] time 0.086 (0.205) data 0.000 (0.120) loss 2.3594 (2.2220) acc 46.8750 (34.6875) lr 1.9940e-03 eta 0:32:49
epoch [9/200] batch [15/50] time 0.086 (0.165) data 0.000 (0.080) loss 2.5566 (2.3124) acc 18.7500 (32.2917) lr 1.9940e-03 eta 0:26:26
epoch [9/200] batch [20/50] time 0.208 (0.153) data 0.123 (0.067) loss 2.0938 (2.3244) acc 34.3750 (32.1875) lr 1.9940e-03 eta 0:24:22
epoch [9/200] batch [25/50] time 0.085 (0.139) data 0.001 (0.054) loss 2.1777 (2.3361) acc 28.1250 (31.1250) lr 1.9940e-03 eta 0:22:12
epoch [9/200] batch [30/50] time 0.084 (0.136) data 0.000 (0.051) loss 1.8652 (2.3275) acc 43.7500 (31.0417) lr 1.9940e-03 eta 0:21:45
epoch [9/200] batch [35/50] time 0.103 (0.130) data 0.018 (0.045) loss 2.3867 (2.3232) acc 25.0000 (30.9821) lr 1.9940e-03 eta 0:20:39
epoch [9/200] batch [40/50] time 0.084 (0.129) data 0.000 (0.044) loss 2.1953 (2.3241) acc 34.3750 (30.8594) lr 1.9940e-03 eta 0:20:35
epoch [9/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.3398 (2.3271) acc 25.0000 (30.6944) lr 1.9940e-03 eta 0:19:53
epoch [9/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.036) loss 2.2910 (2.3247) acc 31.2500 (30.6875) lr 1.9921e-03 eta 0:19:12
epoch [10/200] batch [5/50] time 0.086 (0.306) data 0.001 (0.220) loss 1.9629 (2.0162) acc 37.5000 (38.7500) lr 1.9921e-03 eta 0:48:36
epoch [10/200] batch [10/50] time 0.230 (0.210) data 0.145 (0.125) loss 2.8027 (2.1766) acc 25.0000 (36.2500) lr 1.9921e-03 eta 0:33:24
epoch [10/200] batch [15/50] time 0.086 (0.169) data 0.001 (0.083) loss 2.1387 (2.2202) acc 43.7500 (33.5417) lr 1.9921e-03 eta 0:26:47
epoch [10/200] batch [20/50] time 0.085 (0.157) data 0.000 (0.072) loss 2.4492 (2.2415) acc 28.1250 (33.1250) lr 1.9921e-03 eta 0:24:59
epoch [10/200] batch [25/50] time 0.086 (0.143) data 0.001 (0.058) loss 2.0078 (2.2542) acc 40.6250 (33.2500) lr 1.9921e-03 eta 0:22:42
epoch [10/200] batch [30/50] time 0.085 (0.136) data 0.000 (0.051) loss 2.6777 (2.2978) acc 18.7500 (32.2917) lr 1.9921e-03 eta 0:21:35
epoch [10/200] batch [35/50] time 0.087 (0.132) data 0.001 (0.047) loss 2.7129 (2.3153) acc 28.1250 (32.5000) lr 1.9921e-03 eta 0:20:59
epoch [10/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.041) loss 1.9619 (2.3062) acc 40.6250 (32.4219) lr 1.9921e-03 eta 0:20:02
epoch [10/200] batch [45/50] time 0.082 (0.125) data 0.000 (0.040) loss 2.1406 (2.2978) acc 31.2500 (32.1528) lr 1.9921e-03 eta 0:19:46
epoch [10/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.036) loss 2.1230 (2.3073) acc 34.3750 (31.7500) lr 1.9900e-03 eta 0:19:05
epoch [11/200] batch [5/50] time 0.086 (0.337) data 0.001 (0.253) loss 2.5469 (2.4363) acc 34.3750 (28.7500) lr 1.9900e-03 eta 0:53:23
epoch [11/200] batch [10/50] time 0.085 (0.225) data 0.000 (0.141) loss 2.1348 (2.3396) acc 28.1250 (31.5625) lr 1.9900e-03 eta 0:35:37
epoch [11/200] batch [15/50] time 0.086 (0.179) data 0.000 (0.094) loss 2.3164 (2.3628) acc 31.2500 (31.8750) lr 1.9900e-03 eta 0:28:14
epoch [11/200] batch [20/50] time 0.084 (0.156) data 0.000 (0.071) loss 2.0664 (2.3421) acc 34.3750 (32.5000) lr 1.9900e-03 eta 0:24:40
epoch [11/200] batch [25/50] time 0.086 (0.142) data 0.000 (0.057) loss 1.8701 (2.3132) acc 46.8750 (33.3750) lr 1.9900e-03 eta 0:22:25
epoch [11/200] batch [30/50] time 0.086 (0.136) data 0.000 (0.052) loss 2.4805 (2.3201) acc 25.0000 (33.1250) lr 1.9900e-03 eta 0:21:32
epoch [11/200] batch [35/50] time 0.087 (0.134) data 0.002 (0.049) loss 1.9316 (2.2974) acc 40.6250 (33.2143) lr 1.9900e-03 eta 0:21:03
epoch [11/200] batch [40/50] time 0.084 (0.127) data 0.000 (0.043) loss 2.6270 (2.3388) acc 28.1250 (32.1094) lr 1.9900e-03 eta 0:20:05
epoch [11/200] batch [45/50] time 0.082 (0.127) data 0.000 (0.043) loss 2.1172 (2.3282) acc 37.5000 (32.2222) lr 1.9900e-03 eta 0:20:03
epoch [11/200] batch [50/50] time 0.082 (0.123) data 0.000 (0.039) loss 1.8564 (2.3358) acc 37.5000 (31.6875) lr 1.9877e-03 eta 0:19:20
epoch [12/200] batch [5/50] time 0.084 (0.420) data 0.001 (0.334) loss 1.8447 (2.0299) acc 43.7500 (40.0000) lr 1.9877e-03 eta 1:06:06
epoch [12/200] batch [10/50] time 0.161 (0.260) data 0.073 (0.174) loss 2.1875 (2.2106) acc 25.0000 (35.9375) lr 1.9877e-03 eta 0:40:56
epoch [12/200] batch [15/50] time 0.085 (0.202) data 0.001 (0.116) loss 1.9746 (2.2113) acc 43.7500 (36.4583) lr 1.9877e-03 eta 0:31:46
epoch [12/200] batch [20/50] time 0.084 (0.186) data 0.000 (0.100) loss 2.4004 (2.2883) acc 34.3750 (34.3750) lr 1.9877e-03 eta 0:29:12
epoch [12/200] batch [25/50] time 0.085 (0.166) data 0.000 (0.080) loss 2.2578 (2.2656) acc 40.6250 (35.5000) lr 1.9877e-03 eta 0:26:00
epoch [12/200] batch [30/50] time 0.088 (0.157) data 0.000 (0.072) loss 3.1484 (2.2844) acc 28.1250 (35.9375) lr 1.9877e-03 eta 0:24:37
epoch [12/200] batch [35/50] time 0.086 (0.151) data 0.001 (0.066) loss 2.5059 (2.2935) acc 18.7500 (34.5536) lr 1.9877e-03 eta 0:23:39
epoch [12/200] batch [40/50] time 0.083 (0.142) data 0.000 (0.057) loss 2.6348 (2.3142) acc 31.2500 (34.0625) lr 1.9877e-03 eta 0:22:20
epoch [12/200] batch [45/50] time 0.082 (0.137) data 0.000 (0.052) loss 2.1797 (2.3319) acc 31.2500 (33.1944) lr 1.9877e-03 eta 0:21:29
epoch [12/200] batch [50/50] time 0.082 (0.132) data 0.000 (0.047) loss 1.9141 (2.3294) acc 34.3750 (33.0000) lr 1.9851e-03 eta 0:20:37
epoch [13/200] batch [5/50] time 0.085 (0.316) data 0.000 (0.231) loss 2.0742 (2.2930) acc 31.2500 (31.8750) lr 1.9851e-03 eta 0:49:26
epoch [13/200] batch [10/50] time 0.188 (0.211) data 0.103 (0.126) loss 2.3125 (2.3402) acc 28.1250 (31.8750) lr 1.9851e-03 eta 0:32:58
epoch [13/200] batch [15/50] time 0.086 (0.169) data 0.000 (0.084) loss 2.1094 (2.2926) acc 28.1250 (32.9167) lr 1.9851e-03 eta 0:26:25
epoch [13/200] batch [20/50] time 0.085 (0.154) data 0.000 (0.069) loss 2.2227 (2.2828) acc 37.5000 (33.2812) lr 1.9851e-03 eta 0:24:04
epoch [13/200] batch [25/50] time 0.087 (0.140) data 0.000 (0.055) loss 2.1875 (2.2946) acc 28.1250 (33.3750) lr 1.9851e-03 eta 0:21:55
epoch [13/200] batch [30/50] time 0.087 (0.137) data 0.000 (0.052) loss 2.2520 (2.3085) acc 28.1250 (32.5000) lr 1.9851e-03 eta 0:21:20
epoch [13/200] batch [35/50] time 0.086 (0.133) data 0.001 (0.048) loss 2.4023 (2.2990) acc 37.5000 (32.7679) lr 1.9851e-03 eta 0:20:44
epoch [13/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.042) loss 2.3848 (2.3145) acc 28.1250 (32.5781) lr 1.9851e-03 eta 0:19:47
epoch [13/200] batch [45/50] time 0.082 (0.123) data 0.000 (0.038) loss 2.2148 (2.2934) acc 40.6250 (32.8472) lr 1.9851e-03 eta 0:19:08
epoch [13/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.034) loss 2.6055 (2.2978) acc 21.8750 (32.8125) lr 1.9823e-03 eta 0:18:29
epoch [14/200] batch [5/50] time 0.086 (0.337) data 0.000 (0.252) loss 1.9072 (2.3959) acc 40.6250 (32.5000) lr 1.9823e-03 eta 0:52:33
epoch [14/200] batch [10/50] time 0.085 (0.214) data 0.001 (0.129) loss 2.3555 (2.2860) acc 25.0000 (34.0625) lr 1.9823e-03 eta 0:33:23
epoch [14/200] batch [15/50] time 0.085 (0.172) data 0.000 (0.086) loss 2.1250 (2.2399) acc 31.2500 (34.3750) lr 1.9823e-03 eta 0:26:41
epoch [14/200] batch [20/50] time 0.085 (0.160) data 0.000 (0.075) loss 1.8428 (2.2198) acc 43.7500 (34.5312) lr 1.9823e-03 eta 0:24:49
epoch [14/200] batch [25/50] time 0.147 (0.147) data 0.062 (0.062) loss 2.2734 (2.2387) acc 28.1250 (33.5000) lr 1.9823e-03 eta 0:22:54
epoch [14/200] batch [30/50] time 0.086 (0.137) data 0.000 (0.052) loss 2.3945 (2.2469) acc 28.1250 (32.8125) lr 1.9823e-03 eta 0:21:18
epoch [14/200] batch [35/50] time 0.085 (0.132) data 0.001 (0.047) loss 2.3223 (2.2543) acc 25.0000 (32.3214) lr 1.9823e-03 eta 0:20:33
epoch [14/200] batch [40/50] time 0.085 (0.126) data 0.000 (0.041) loss 2.8555 (2.2590) acc 25.0000 (32.8906) lr 1.9823e-03 eta 0:19:37
epoch [14/200] batch [45/50] time 0.082 (0.122) data 0.000 (0.037) loss 2.2832 (2.2682) acc 43.7500 (33.1250) lr 1.9823e-03 eta 0:18:51
epoch [14/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.033) loss 2.2363 (2.2721) acc 28.1250 (32.9375) lr 1.9792e-03 eta 0:18:15
epoch [15/200] batch [5/50] time 0.085 (0.309) data 0.000 (0.225) loss 2.2656 (2.1373) acc 37.5000 (37.5000) lr 1.9792e-03 eta 0:47:55
epoch [15/200] batch [10/50] time 0.152 (0.204) data 0.069 (0.119) loss 2.0996 (2.2445) acc 43.7500 (35.3125) lr 1.9792e-03 eta 0:31:32
epoch [15/200] batch [15/50] time 0.084 (0.164) data 0.000 (0.080) loss 1.8047 (2.2136) acc 43.7500 (35.4167) lr 1.9792e-03 eta 0:25:19
epoch [15/200] batch [20/50] time 0.085 (0.150) data 0.000 (0.066) loss 2.5703 (2.2794) acc 18.7500 (33.1250) lr 1.9792e-03 eta 0:23:08
epoch [15/200] batch [25/50] time 0.084 (0.137) data 0.000 (0.053) loss 2.0273 (2.2849) acc 37.5000 (32.8750) lr 1.9792e-03 eta 0:21:07
epoch [15/200] batch [30/50] time 0.085 (0.130) data 0.000 (0.046) loss 2.3535 (2.2899) acc 34.3750 (33.2292) lr 1.9792e-03 eta 0:20:08
epoch [15/200] batch [35/50] time 0.085 (0.124) data 0.000 (0.040) loss 2.1484 (2.2951) acc 37.5000 (33.3929) lr 1.9792e-03 eta 0:19:07
epoch [15/200] batch [40/50] time 0.083 (0.121) data 0.000 (0.037) loss 2.4277 (2.3057) acc 25.0000 (32.7344) lr 1.9792e-03 eta 0:18:44
epoch [15/200] batch [45/50] time 0.084 (0.117) data 0.000 (0.033) loss 2.3965 (2.3106) acc 34.3750 (32.6389) lr 1.9792e-03 eta 0:18:05
epoch [15/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.030) loss 2.1211 (2.3109) acc 25.0000 (32.4375) lr 1.9759e-03 eta 0:17:33
epoch [16/200] batch [5/50] time 0.085 (0.334) data 0.000 (0.249) loss 2.6543 (2.3572) acc 15.6250 (26.8750) lr 1.9759e-03 eta 0:51:30
epoch [16/200] batch [10/50] time 0.086 (0.213) data 0.000 (0.128) loss 1.8447 (2.2246) acc 53.1250 (31.5625) lr 1.9759e-03 eta 0:32:52
epoch [16/200] batch [15/50] time 0.086 (0.171) data 0.001 (0.086) loss 1.9814 (2.1698) acc 34.3750 (35.0000) lr 1.9759e-03 eta 0:26:20
epoch [16/200] batch [20/50] time 0.086 (0.157) data 0.001 (0.072) loss 2.2988 (2.2285) acc 40.6250 (35.0000) lr 1.9759e-03 eta 0:24:12
epoch [16/200] batch [25/50] time 0.206 (0.148) data 0.121 (0.063) loss 2.8203 (2.2573) acc 31.2500 (34.5000) lr 1.9759e-03 eta 0:22:43
epoch [16/200] batch [30/50] time 0.085 (0.137) data 0.000 (0.052) loss 2.4902 (2.2572) acc 12.5000 (34.1667) lr 1.9759e-03 eta 0:21:06
epoch [16/200] batch [35/50] time 0.085 (0.135) data 0.001 (0.050) loss 2.7090 (2.2748) acc 25.0000 (33.3036) lr 1.9759e-03 eta 0:20:48
epoch [16/200] batch [40/50] time 0.084 (0.129) data 0.000 (0.044) loss 2.8398 (2.2997) acc 15.6250 (32.8906) lr 1.9759e-03 eta 0:19:49
epoch [16/200] batch [45/50] time 0.084 (0.125) data 0.000 (0.040) loss 2.3809 (2.2915) acc 31.2500 (33.3333) lr 1.9759e-03 eta 0:19:10
epoch [16/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.036) loss 1.7412 (2.2839) acc 43.7500 (33.9375) lr 1.9724e-03 eta 0:18:31
epoch [17/200] batch [5/50] time 0.142 (0.272) data 0.058 (0.187) loss 2.1855 (2.1576) acc 43.7500 (42.5000) lr 1.9724e-03 eta 0:41:39
epoch [17/200] batch [10/50] time 0.085 (0.179) data 0.000 (0.094) loss 2.4082 (2.2560) acc 31.2500 (36.5625) lr 1.9724e-03 eta 0:27:23
epoch [17/200] batch [15/50] time 0.085 (0.158) data 0.001 (0.072) loss 2.1094 (2.2665) acc 34.3750 (34.3750) lr 1.9724e-03 eta 0:24:07
epoch [17/200] batch [20/50] time 0.234 (0.147) data 0.150 (0.062) loss 2.2734 (2.2087) acc 18.7500 (34.6875) lr 1.9724e-03 eta 0:22:29
epoch [17/200] batch [25/50] time 0.085 (0.135) data 0.000 (0.050) loss 2.3906 (2.2320) acc 28.1250 (33.6250) lr 1.9724e-03 eta 0:20:35
epoch [17/200] batch [30/50] time 0.087 (0.127) data 0.001 (0.042) loss 2.1133 (2.2107) acc 34.3750 (33.7500) lr 1.9724e-03 eta 0:19:24
epoch [17/200] batch [35/50] time 0.086 (0.122) data 0.001 (0.037) loss 2.1797 (2.2053) acc 46.8750 (34.5536) lr 1.9724e-03 eta 0:18:40
epoch [17/200] batch [40/50] time 0.085 (0.120) data 0.000 (0.034) loss 2.1094 (2.2249) acc 34.3750 (34.3750) lr 1.9724e-03 eta 0:18:14
epoch [17/200] batch [45/50] time 0.083 (0.118) data 0.000 (0.033) loss 1.8457 (2.2138) acc 46.8750 (34.6528) lr 1.9724e-03 eta 0:18:03
epoch [17/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.030) loss 2.2832 (2.2163) acc 34.3750 (34.8125) lr 1.9686e-03 eta 0:17:30
epoch [18/200] batch [5/50] time 0.086 (0.324) data 0.000 (0.239) loss 1.7646 (2.0428) acc 37.5000 (31.8750) lr 1.9686e-03 eta 0:49:27
epoch [18/200] batch [10/50] time 0.085 (0.217) data 0.000 (0.131) loss 2.2188 (2.1274) acc 37.5000 (32.8125) lr 1.9686e-03 eta 0:32:59
epoch [18/200] batch [15/50] time 0.085 (0.173) data 0.000 (0.088) loss 2.3164 (2.1204) acc 15.6250 (34.5833) lr 1.9686e-03 eta 0:26:19
epoch [18/200] batch [20/50] time 0.084 (0.159) data 0.000 (0.074) loss 2.6738 (2.1710) acc 40.6250 (34.8438) lr 1.9686e-03 eta 0:24:13
epoch [18/200] batch [25/50] time 0.086 (0.144) data 0.000 (0.059) loss 2.7520 (2.2141) acc 31.2500 (34.2500) lr 1.9686e-03 eta 0:21:57
epoch [18/200] batch [30/50] time 0.084 (0.135) data 0.000 (0.050) loss 2.1816 (2.2345) acc 43.7500 (34.2708) lr 1.9686e-03 eta 0:20:27
epoch [18/200] batch [35/50] time 0.084 (0.130) data 0.000 (0.045) loss 2.2734 (2.2252) acc 31.2500 (34.7321) lr 1.9686e-03 eta 0:19:40
epoch [18/200] batch [40/50] time 0.083 (0.124) data 0.000 (0.039) loss 2.5996 (2.2444) acc 18.7500 (33.9844) lr 1.9686e-03 eta 0:18:47
epoch [18/200] batch [45/50] time 0.083 (0.119) data 0.001 (0.035) loss 2.1797 (2.2509) acc 34.3750 (34.0972) lr 1.9686e-03 eta 0:18:06
epoch [18/200] batch [50/50] time 0.084 (0.116) data 0.000 (0.031) loss 2.0430 (2.2420) acc 28.1250 (33.8750) lr 1.9646e-03 eta 0:17:33
epoch [19/200] batch [5/50] time 0.084 (0.287) data 0.000 (0.203) loss 2.2031 (2.3203) acc 40.6250 (34.3750) lr 1.9646e-03 eta 0:43:31
epoch [19/200] batch [10/50] time 0.084 (0.185) data 0.000 (0.102) loss 2.2266 (2.1884) acc 46.8750 (37.8125) lr 1.9646e-03 eta 0:28:05
epoch [19/200] batch [15/50] time 0.083 (0.160) data 0.000 (0.077) loss 2.1875 (2.1305) acc 40.6250 (37.7083) lr 1.9646e-03 eta 0:24:17
epoch [19/200] batch [20/50] time 0.212 (0.148) data 0.130 (0.064) loss 2.2656 (2.1564) acc 31.2500 (35.9375) lr 1.9646e-03 eta 0:22:21
epoch [19/200] batch [25/50] time 0.083 (0.135) data 0.000 (0.051) loss 2.2422 (2.1754) acc 25.0000 (34.8750) lr 1.9646e-03 eta 0:20:24
epoch [19/200] batch [30/50] time 0.083 (0.132) data 0.000 (0.049) loss 2.1621 (2.1827) acc 50.0000 (34.7917) lr 1.9646e-03 eta 0:19:58
epoch [19/200] batch [35/50] time 0.083 (0.125) data 0.000 (0.042) loss 2.0840 (2.2101) acc 28.1250 (34.4643) lr 1.9646e-03 eta 0:18:54
epoch [19/200] batch [40/50] time 0.083 (0.121) data 0.000 (0.038) loss 2.1445 (2.1806) acc 31.2500 (35.2344) lr 1.9646e-03 eta 0:18:16
epoch [19/200] batch [45/50] time 0.083 (0.117) data 0.000 (0.034) loss 2.4863 (2.1978) acc 25.0000 (34.5833) lr 1.9646e-03 eta 0:17:38
epoch [19/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.030) loss 2.3164 (2.2051) acc 18.7500 (33.6875) lr 1.9603e-03 eta 0:17:09
epoch [20/200] batch [5/50] time 0.085 (0.290) data 0.000 (0.206) loss 1.9375 (2.3402) acc 31.2500 (30.0000) lr 1.9603e-03 eta 0:43:44
epoch [20/200] batch [10/50] time 0.171 (0.195) data 0.089 (0.112) loss 1.4092 (2.3158) acc 56.2500 (30.9375) lr 1.9603e-03 eta 0:29:26
epoch [20/200] batch [15/50] time 0.083 (0.161) data 0.000 (0.078) loss 2.0176 (2.2152) acc 40.6250 (33.3333) lr 1.9603e-03 eta 0:24:17
epoch [20/200] batch [20/50] time 0.156 (0.146) data 0.073 (0.062) loss 2.4141 (2.1886) acc 40.6250 (35.1562) lr 1.9603e-03 eta 0:21:54
epoch [20/200] batch [25/50] time 0.084 (0.133) data 0.000 (0.050) loss 2.2656 (2.1798) acc 25.0000 (35.2500) lr 1.9603e-03 eta 0:20:02
epoch [20/200] batch [30/50] time 0.084 (0.133) data 0.000 (0.050) loss 2.2637 (2.1662) acc 28.1250 (35.5208) lr 1.9603e-03 eta 0:20:03
epoch [20/200] batch [35/50] time 0.084 (0.126) data 0.001 (0.043) loss 2.4746 (2.1816) acc 31.2500 (34.9107) lr 1.9603e-03 eta 0:18:59
epoch [20/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.043) loss 2.2832 (2.1961) acc 34.3750 (34.7656) lr 1.9603e-03 eta 0:19:00
epoch [20/200] batch [45/50] time 0.084 (0.126) data 0.000 (0.042) loss 2.2168 (2.1927) acc 31.2500 (34.6528) lr 1.9603e-03 eta 0:18:51
epoch [20/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.038) loss 2.0527 (2.1922) acc 50.0000 (35.0000) lr 1.9558e-03 eta 0:18:12
epoch [21/200] batch [5/50] time 0.084 (0.291) data 0.000 (0.207) loss 2.2695 (2.3887) acc 31.2500 (33.7500) lr 1.9558e-03 eta 0:43:41
epoch [21/200] batch [10/50] time 0.197 (0.200) data 0.113 (0.115) loss 2.1289 (2.2773) acc 34.3750 (34.6875) lr 1.9558e-03 eta 0:29:57
epoch [21/200] batch [15/50] time 0.086 (0.162) data 0.000 (0.077) loss 2.2188 (2.3083) acc 37.5000 (33.3333) lr 1.9558e-03 eta 0:24:13
epoch [21/200] batch [20/50] time 0.085 (0.146) data 0.000 (0.061) loss 2.1680 (2.2586) acc 40.6250 (34.6875) lr 1.9558e-03 eta 0:21:51
epoch [21/200] batch [25/50] time 0.136 (0.136) data 0.052 (0.051) loss 2.0195 (2.2547) acc 37.5000 (34.5000) lr 1.9558e-03 eta 0:20:20
epoch [21/200] batch [30/50] time 0.085 (0.128) data 0.000 (0.043) loss 2.5605 (2.2833) acc 28.1250 (34.3750) lr 1.9558e-03 eta 0:19:06
epoch [21/200] batch [35/50] time 0.085 (0.127) data 0.001 (0.042) loss 2.4707 (2.2611) acc 21.8750 (35.0000) lr 1.9558e-03 eta 0:18:56
epoch [21/200] batch [40/50] time 0.085 (0.122) data 0.000 (0.037) loss 1.9658 (2.2438) acc 37.5000 (34.7656) lr 1.9558e-03 eta 0:18:10
epoch [21/200] batch [45/50] time 0.083 (0.121) data 0.000 (0.036) loss 1.9834 (2.2454) acc 25.0000 (33.8194) lr 1.9558e-03 eta 0:18:00
epoch [21/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.032) loss 2.2383 (2.2443) acc 34.3750 (33.5625) lr 1.9511e-03 eta 0:17:26
epoch [22/200] batch [5/50] time 0.084 (0.353) data 0.000 (0.268) loss 2.0957 (2.2969) acc 34.3750 (36.2500) lr 1.9511e-03 eta 0:52:33
epoch [22/200] batch [10/50] time 0.085 (0.219) data 0.000 (0.134) loss 2.4277 (2.2666) acc 25.0000 (34.6875) lr 1.9511e-03 eta 0:32:33
epoch [22/200] batch [15/50] time 0.086 (0.174) data 0.000 (0.089) loss 1.9580 (2.2874) acc 40.6250 (34.7917) lr 1.9511e-03 eta 0:25:56
epoch [22/200] batch [20/50] time 0.086 (0.152) data 0.000 (0.067) loss 1.8604 (2.2493) acc 40.6250 (33.7500) lr 1.9511e-03 eta 0:22:37
epoch [22/200] batch [25/50] time 0.086 (0.139) data 0.000 (0.055) loss 2.5820 (2.2799) acc 9.3750 (33.1250) lr 1.9511e-03 eta 0:20:44
epoch [22/200] batch [30/50] time 0.085 (0.134) data 0.000 (0.049) loss 1.7822 (2.2597) acc 34.3750 (33.5417) lr 1.9511e-03 eta 0:19:53
epoch [22/200] batch [35/50] time 0.085 (0.127) data 0.001 (0.042) loss 2.1074 (2.2302) acc 40.6250 (34.8214) lr 1.9511e-03 eta 0:18:51
epoch [22/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.1094 (2.2318) acc 37.5000 (34.6094) lr 1.9511e-03 eta 0:18:35
epoch [22/200] batch [45/50] time 0.251 (0.124) data 0.169 (0.040) loss 2.1055 (2.2335) acc 28.1250 (34.3750) lr 1.9511e-03 eta 0:18:27
epoch [22/200] batch [50/50] time 0.082 (0.120) data 0.000 (0.036) loss 2.0332 (2.2314) acc 46.8750 (34.3750) lr 1.9461e-03 eta 0:17:49
epoch [23/200] batch [5/50] time 0.086 (0.294) data 0.000 (0.209) loss 1.8281 (1.9764) acc 37.5000 (35.6250) lr 1.9461e-03 eta 0:43:36
epoch [23/200] batch [10/50] time 0.084 (0.189) data 0.000 (0.105) loss 2.0039 (2.1126) acc 46.8750 (33.1250) lr 1.9461e-03 eta 0:28:02
epoch [23/200] batch [15/50] time 0.085 (0.163) data 0.000 (0.079) loss 2.2129 (2.1395) acc 34.3750 (32.0833) lr 1.9461e-03 eta 0:24:11
epoch [23/200] batch [20/50] time 0.162 (0.148) data 0.079 (0.063) loss 2.6113 (2.1641) acc 12.5000 (32.5000) lr 1.9461e-03 eta 0:21:50
epoch [23/200] batch [25/50] time 0.085 (0.135) data 0.000 (0.051) loss 2.5664 (2.1888) acc 25.0000 (32.6250) lr 1.9461e-03 eta 0:19:57
epoch [23/200] batch [30/50] time 0.084 (0.133) data 0.000 (0.048) loss 2.5566 (2.2063) acc 28.1250 (33.0208) lr 1.9461e-03 eta 0:19:35
epoch [23/200] batch [35/50] time 0.084 (0.126) data 0.001 (0.041) loss 2.3867 (2.2097) acc 18.7500 (32.9464) lr 1.9461e-03 eta 0:18:33
epoch [23/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.2949 (2.1990) acc 31.2500 (33.3594) lr 1.9461e-03 eta 0:18:23
epoch [23/200] batch [45/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.9658 (2.1888) acc 40.6250 (33.6111) lr 1.9461e-03 eta 0:17:42
epoch [23/200] batch [50/50] time 0.084 (0.116) data 0.000 (0.032) loss 2.3184 (2.1822) acc 25.0000 (33.7500) lr 1.9409e-03 eta 0:17:09
epoch [24/200] batch [5/50] time 0.083 (0.376) data 0.000 (0.291) loss 2.2676 (2.2008) acc 25.0000 (31.2500) lr 1.9409e-03 eta 0:55:26
epoch [24/200] batch [10/50] time 0.085 (0.246) data 0.000 (0.161) loss 1.8789 (2.2111) acc 37.5000 (31.2500) lr 1.9409e-03 eta 0:36:13
epoch [24/200] batch [15/50] time 0.085 (0.193) data 0.001 (0.108) loss 2.2988 (2.1305) acc 28.1250 (32.0833) lr 1.9409e-03 eta 0:28:20
epoch [24/200] batch [20/50] time 0.087 (0.171) data 0.000 (0.086) loss 2.1055 (2.1787) acc 25.0000 (31.8750) lr 1.9409e-03 eta 0:25:09
epoch [24/200] batch [25/50] time 0.225 (0.160) data 0.141 (0.075) loss 2.1270 (2.1590) acc 43.7500 (33.7500) lr 1.9409e-03 eta 0:23:28
epoch [24/200] batch [30/50] time 0.085 (0.147) data 0.000 (0.062) loss 2.5547 (2.1577) acc 9.3750 (33.5417) lr 1.9409e-03 eta 0:21:38
epoch [24/200] batch [35/50] time 0.084 (0.145) data 0.001 (0.060) loss 2.4082 (2.1580) acc 31.2500 (33.7500) lr 1.9409e-03 eta 0:21:17
epoch [24/200] batch [40/50] time 0.084 (0.137) data 0.000 (0.052) loss 1.9111 (2.1480) acc 40.6250 (34.6094) lr 1.9409e-03 eta 0:20:09
epoch [24/200] batch [45/50] time 0.082 (0.135) data 0.000 (0.050) loss 1.8594 (2.1465) acc 46.8750 (34.8611) lr 1.9409e-03 eta 0:19:49
epoch [24/200] batch [50/50] time 0.082 (0.130) data 0.000 (0.045) loss 2.2148 (2.1795) acc 34.3750 (34.6250) lr 1.9354e-03 eta 0:19:02
epoch [25/200] batch [5/50] time 0.086 (0.332) data 0.000 (0.245) loss 2.4961 (2.3746) acc 31.2500 (33.1250) lr 1.9354e-03 eta 0:48:38
epoch [25/200] batch [10/50] time 0.085 (0.209) data 0.000 (0.123) loss 2.4238 (2.2735) acc 28.1250 (33.7500) lr 1.9354e-03 eta 0:30:34
epoch [25/200] batch [15/50] time 0.086 (0.175) data 0.000 (0.090) loss 2.0605 (2.1904) acc 31.2500 (35.6250) lr 1.9354e-03 eta 0:25:39
epoch [25/200] batch [20/50] time 0.086 (0.162) data 0.001 (0.077) loss 2.2656 (2.1620) acc 34.3750 (36.2500) lr 1.9354e-03 eta 0:23:40
epoch [25/200] batch [25/50] time 0.086 (0.147) data 0.000 (0.061) loss 1.8906 (2.1559) acc 40.6250 (36.0000) lr 1.9354e-03 eta 0:21:26
epoch [25/200] batch [30/50] time 0.085 (0.139) data 0.000 (0.054) loss 2.5352 (2.1917) acc 21.8750 (34.8958) lr 1.9354e-03 eta 0:20:20
epoch [25/200] batch [35/50] time 0.257 (0.136) data 0.174 (0.051) loss 2.3516 (2.1851) acc 31.2500 (34.4643) lr 1.9354e-03 eta 0:19:54
epoch [25/200] batch [40/50] time 0.084 (0.130) data 0.000 (0.045) loss 2.7949 (2.2157) acc 18.7500 (33.7500) lr 1.9354e-03 eta 0:18:56
epoch [25/200] batch [45/50] time 0.084 (0.129) data 0.000 (0.045) loss 2.2617 (2.1986) acc 28.1250 (34.7917) lr 1.9354e-03 eta 0:18:53
epoch [25/200] batch [50/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.1094 (2.1998) acc 37.5000 (34.8125) lr 1.9298e-03 eta 0:18:12
epoch [26/200] batch [5/50] time 0.085 (0.291) data 0.001 (0.206) loss 1.7764 (2.1016) acc 40.6250 (37.5000) lr 1.9298e-03 eta 0:42:27
epoch [26/200] batch [10/50] time 0.085 (0.206) data 0.001 (0.121) loss 2.2090 (2.1008) acc 34.3750 (37.8125) lr 1.9298e-03 eta 0:29:59
epoch [26/200] batch [15/50] time 0.086 (0.166) data 0.000 (0.081) loss 2.2227 (2.0994) acc 43.7500 (38.3333) lr 1.9298e-03 eta 0:24:07
epoch [26/200] batch [20/50] time 0.086 (0.152) data 0.001 (0.067) loss 2.0840 (2.1375) acc 43.7500 (37.0312) lr 1.9298e-03 eta 0:22:07
epoch [26/200] batch [25/50] time 0.245 (0.145) data 0.162 (0.060) loss 2.3418 (2.1798) acc 25.0000 (35.1250) lr 1.9298e-03 eta 0:21:07
epoch [26/200] batch [30/50] time 0.085 (0.135) data 0.000 (0.050) loss 2.0996 (2.1761) acc 34.3750 (35.5208) lr 1.9298e-03 eta 0:19:39
epoch [26/200] batch [35/50] time 0.084 (0.133) data 0.001 (0.048) loss 2.1621 (2.1970) acc 43.7500 (35.2679) lr 1.9298e-03 eta 0:19:17
epoch [26/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.042) loss 2.2852 (2.1922) acc 34.3750 (35.3906) lr 1.9298e-03 eta 0:18:23
epoch [26/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 2.0820 (2.1912) acc 37.5000 (35.4167) lr 1.9298e-03 eta 0:18:10
epoch [26/200] batch [50/50] time 0.085 (0.121) data 0.000 (0.037) loss 1.7715 (2.1847) acc 59.3750 (35.5000) lr 1.9239e-03 eta 0:17:33
epoch [27/200] batch [5/50] time 0.085 (0.282) data 0.000 (0.197) loss 1.6689 (1.9973) acc 65.6250 (44.3750) lr 1.9239e-03 eta 0:40:51
epoch [27/200] batch [10/50] time 0.085 (0.184) data 0.000 (0.098) loss 2.4297 (1.9851) acc 25.0000 (40.3125) lr 1.9239e-03 eta 0:26:34
epoch [27/200] batch [15/50] time 0.084 (0.161) data 0.000 (0.076) loss 2.5156 (2.0559) acc 25.0000 (38.1250) lr 1.9239e-03 eta 0:23:15
epoch [27/200] batch [20/50] time 0.232 (0.149) data 0.148 (0.064) loss 2.5781 (2.0579) acc 28.1250 (36.4062) lr 1.9239e-03 eta 0:21:33
epoch [27/200] batch [25/50] time 0.086 (0.136) data 0.000 (0.052) loss 2.4004 (2.0947) acc 18.7500 (35.5000) lr 1.9239e-03 eta 0:19:41
epoch [27/200] batch [30/50] time 0.084 (0.131) data 0.000 (0.047) loss 2.1777 (2.1277) acc 34.3750 (35.2083) lr 1.9239e-03 eta 0:18:58
epoch [27/200] batch [35/50] time 0.084 (0.125) data 0.001 (0.040) loss 2.0293 (2.1272) acc 37.5000 (35.6250) lr 1.9239e-03 eta 0:17:59
epoch [27/200] batch [40/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.3066 (2.1515) acc 28.1250 (35.0000) lr 1.9239e-03 eta 0:17:51
epoch [27/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.4824 (2.1762) acc 28.1250 (34.3750) lr 1.9239e-03 eta 0:17:41
epoch [27/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.0742 (2.1819) acc 31.2500 (33.9375) lr 1.9178e-03 eta 0:17:06
epoch [28/200] batch [5/50] time 0.086 (0.323) data 0.000 (0.237) loss 2.4844 (2.3541) acc 28.1250 (30.6250) lr 1.9178e-03 eta 0:46:33
epoch [28/200] batch [10/50] time 0.086 (0.214) data 0.000 (0.129) loss 2.1172 (2.2438) acc 34.3750 (34.3750) lr 1.9178e-03 eta 0:30:49
epoch [28/200] batch [15/50] time 0.085 (0.171) data 0.000 (0.086) loss 1.9180 (2.2426) acc 46.8750 (33.5417) lr 1.9178e-03 eta 0:24:38
epoch [28/200] batch [20/50] time 0.087 (0.152) data 0.000 (0.067) loss 2.2520 (2.2244) acc 21.8750 (33.1250) lr 1.9178e-03 eta 0:21:55
epoch [28/200] batch [25/50] time 0.085 (0.139) data 0.000 (0.054) loss 2.2539 (2.2437) acc 34.3750 (33.1250) lr 1.9178e-03 eta 0:20:02
epoch [28/200] batch [30/50] time 0.226 (0.135) data 0.140 (0.050) loss 2.8594 (2.2387) acc 15.6250 (32.7083) lr 1.9178e-03 eta 0:19:25
epoch [28/200] batch [35/50] time 0.085 (0.128) data 0.000 (0.043) loss 1.9160 (2.2358) acc 37.5000 (32.2321) lr 1.9178e-03 eta 0:18:22
epoch [28/200] batch [40/50] time 0.084 (0.128) data 0.000 (0.043) loss 2.3613 (2.2397) acc 31.2500 (32.2656) lr 1.9178e-03 eta 0:18:24
epoch [28/200] batch [45/50] time 0.082 (0.123) data 0.000 (0.038) loss 2.0039 (2.2264) acc 43.7500 (32.7083) lr 1.9178e-03 eta 0:17:41
epoch [28/200] batch [50/50] time 0.082 (0.124) data 0.000 (0.039) loss 1.9580 (2.2123) acc 37.5000 (33.1250) lr 1.9114e-03 eta 0:17:43
epoch [29/200] batch [5/50] time 0.085 (0.327) data 0.000 (0.242) loss 2.7148 (2.2756) acc 28.1250 (33.1250) lr 1.9114e-03 eta 0:46:54
epoch [29/200] batch [10/50] time 0.085 (0.214) data 0.000 (0.129) loss 2.5098 (2.2390) acc 40.6250 (35.0000) lr 1.9114e-03 eta 0:30:38
epoch [29/200] batch [15/50] time 0.086 (0.172) data 0.001 (0.086) loss 2.2754 (2.2019) acc 28.1250 (34.7917) lr 1.9114e-03 eta 0:24:32
epoch [29/200] batch [20/50] time 0.084 (0.153) data 0.000 (0.068) loss 2.1426 (2.1531) acc 37.5000 (35.6250) lr 1.9114e-03 eta 0:21:54
epoch [29/200] batch [25/50] time 0.185 (0.143) data 0.099 (0.058) loss 2.3281 (2.1417) acc 40.6250 (36.1250) lr 1.9114e-03 eta 0:20:30
epoch [29/200] batch [30/50] time 0.085 (0.134) data 0.000 (0.049) loss 2.2520 (2.1522) acc 37.5000 (35.5208) lr 1.9114e-03 eta 0:19:06
epoch [29/200] batch [35/50] time 0.086 (0.131) data 0.001 (0.046) loss 1.7393 (2.1133) acc 40.6250 (36.2500) lr 1.9114e-03 eta 0:18:46
epoch [29/200] batch [40/50] time 0.085 (0.126) data 0.000 (0.041) loss 2.3652 (2.1303) acc 34.3750 (35.9375) lr 1.9114e-03 eta 0:17:55
epoch [29/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.038) loss 2.5039 (2.1525) acc 28.1250 (35.6944) lr 1.9114e-03 eta 0:17:29
epoch [29/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.034) loss 2.2324 (2.1592) acc 40.6250 (35.1250) lr 1.9048e-03 eta 0:16:55
epoch [30/200] batch [5/50] time 0.085 (0.317) data 0.000 (0.232) loss 1.9365 (2.2092) acc 46.8750 (39.3750) lr 1.9048e-03 eta 0:45:10
epoch [30/200] batch [10/50] time 0.085 (0.205) data 0.000 (0.119) loss 1.8076 (2.1369) acc 50.0000 (38.4375) lr 1.9048e-03 eta 0:29:07
epoch [30/200] batch [15/50] time 0.085 (0.167) data 0.000 (0.082) loss 2.0234 (2.0887) acc 43.7500 (40.0000) lr 1.9048e-03 eta 0:23:48
epoch [30/200] batch [20/50] time 0.085 (0.154) data 0.000 (0.069) loss 1.8672 (2.1078) acc 50.0000 (40.3125) lr 1.9048e-03 eta 0:21:55
epoch [30/200] batch [25/50] time 0.098 (0.141) data 0.013 (0.056) loss 2.2070 (2.1243) acc 25.0000 (38.5000) lr 1.9048e-03 eta 0:20:01
epoch [30/200] batch [30/50] time 0.085 (0.136) data 0.000 (0.051) loss 2.3789 (2.1161) acc 37.5000 (39.1667) lr 1.9048e-03 eta 0:19:18
epoch [30/200] batch [35/50] time 0.085 (0.132) data 0.000 (0.047) loss 2.4160 (2.1303) acc 18.7500 (38.1250) lr 1.9048e-03 eta 0:18:40
epoch [30/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.1816 (2.1285) acc 43.7500 (38.1250) lr 1.9048e-03 eta 0:17:50
epoch [30/200] batch [45/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.2227 (2.1266) acc 34.3750 (37.4306) lr 1.9048e-03 eta 0:17:47
epoch [30/200] batch [50/50] time 0.082 (0.121) data 0.000 (0.037) loss 2.1504 (2.1210) acc 37.5000 (37.7500) lr 1.8980e-03 eta 0:17:11
epoch [31/200] batch [5/50] time 0.086 (0.308) data 0.000 (0.222) loss 1.9541 (2.2338) acc 43.7500 (35.0000) lr 1.8980e-03 eta 0:43:32
epoch [31/200] batch [10/50] time 0.089 (0.197) data 0.004 (0.112) loss 1.9941 (2.1497) acc 28.1250 (36.8750) lr 1.8980e-03 eta 0:27:51
epoch [31/200] batch [15/50] time 0.087 (0.160) data 0.000 (0.075) loss 2.6426 (2.1245) acc 21.8750 (36.8750) lr 1.8980e-03 eta 0:22:37
epoch [31/200] batch [20/50] time 0.087 (0.142) data 0.001 (0.056) loss 2.6172 (2.1375) acc 34.3750 (36.5625) lr 1.8980e-03 eta 0:20:00
epoch [31/200] batch [25/50] time 0.085 (0.130) data 0.000 (0.045) loss 2.0605 (2.1570) acc 37.5000 (35.6250) lr 1.8980e-03 eta 0:18:24
epoch [31/200] batch [30/50] time 0.086 (0.127) data 0.000 (0.042) loss 1.9258 (2.1529) acc 43.7500 (35.5208) lr 1.8980e-03 eta 0:17:56
epoch [31/200] batch [35/50] time 0.085 (0.121) data 0.000 (0.036) loss 2.2031 (2.1401) acc 25.0000 (35.2679) lr 1.8980e-03 eta 0:17:04
epoch [31/200] batch [40/50] time 0.157 (0.118) data 0.073 (0.033) loss 2.0508 (2.1327) acc 31.2500 (35.8594) lr 1.8980e-03 eta 0:16:41
epoch [31/200] batch [45/50] time 0.084 (0.117) data 0.000 (0.033) loss 2.1699 (2.1357) acc 37.5000 (36.1806) lr 1.8980e-03 eta 0:16:32
epoch [31/200] batch [50/50] time 0.082 (0.114) data 0.000 (0.029) loss 2.3496 (2.1447) acc 28.1250 (36.1250) lr 1.8910e-03 eta 0:16:02
epoch [32/200] batch [5/50] time 0.085 (0.381) data 0.000 (0.295) loss 1.7666 (1.8906) acc 50.0000 (45.0000) lr 1.8910e-03 eta 0:53:35
epoch [32/200] batch [10/50] time 0.087 (0.233) data 0.000 (0.148) loss 2.0195 (2.0092) acc 37.5000 (41.2500) lr 1.8910e-03 eta 0:32:48
epoch [32/200] batch [15/50] time 0.086 (0.190) data 0.000 (0.105) loss 2.2188 (2.0440) acc 40.6250 (39.7917) lr 1.8910e-03 eta 0:26:43
epoch [32/200] batch [20/50] time 0.085 (0.171) data 0.000 (0.086) loss 1.9590 (2.0835) acc 21.8750 (36.4062) lr 1.8910e-03 eta 0:24:04
epoch [32/200] batch [25/50] time 0.085 (0.154) data 0.000 (0.069) loss 1.9717 (2.0867) acc 53.1250 (37.3750) lr 1.8910e-03 eta 0:21:37
epoch [32/200] batch [30/50] time 0.085 (0.147) data 0.000 (0.061) loss 1.9141 (2.0825) acc 43.7500 (38.0208) lr 1.8910e-03 eta 0:20:34
epoch [32/200] batch [35/50] time 0.292 (0.144) data 0.208 (0.059) loss 2.6406 (2.1271) acc 21.8750 (36.8750) lr 1.8910e-03 eta 0:20:08
epoch [32/200] batch [40/50] time 0.085 (0.136) data 0.000 (0.051) loss 2.3379 (2.1556) acc 34.3750 (36.3281) lr 1.8910e-03 eta 0:19:05
epoch [32/200] batch [45/50] time 0.083 (0.134) data 0.000 (0.050) loss 2.3672 (2.1641) acc 40.6250 (36.3889) lr 1.8910e-03 eta 0:18:49
epoch [32/200] batch [50/50] time 0.083 (0.129) data 0.000 (0.045) loss 2.1621 (2.1602) acc 28.1250 (36.0000) lr 1.8838e-03 eta 0:18:04
epoch [33/200] batch [5/50] time 0.085 (0.300) data 0.000 (0.214) loss 2.1309 (2.0287) acc 37.5000 (44.3750) lr 1.8838e-03 eta 0:41:54
epoch [33/200] batch [10/50] time 0.084 (0.192) data 0.000 (0.107) loss 2.0645 (2.1431) acc 37.5000 (39.3750) lr 1.8838e-03 eta 0:26:54
epoch [33/200] batch [15/50] time 0.085 (0.156) data 0.000 (0.072) loss 2.0820 (2.1140) acc 34.3750 (39.3750) lr 1.8838e-03 eta 0:21:52
epoch [33/200] batch [20/50] time 0.084 (0.139) data 0.000 (0.054) loss 2.1758 (2.1268) acc 28.1250 (38.4375) lr 1.8838e-03 eta 0:19:21
epoch [33/200] batch [25/50] time 0.086 (0.128) data 0.000 (0.043) loss 1.7939 (2.0938) acc 40.6250 (38.2500) lr 1.8838e-03 eta 0:17:51
epoch [33/200] batch [30/50] time 0.087 (0.121) data 0.000 (0.036) loss 2.3457 (2.1185) acc 31.2500 (37.3958) lr 1.8838e-03 eta 0:16:52
epoch [33/200] batch [35/50] time 0.085 (0.116) data 0.001 (0.031) loss 2.0918 (2.1216) acc 28.1250 (36.7857) lr 1.8838e-03 eta 0:16:09
epoch [33/200] batch [40/50] time 0.084 (0.112) data 0.000 (0.027) loss 1.8770 (2.1122) acc 43.7500 (37.4219) lr 1.8838e-03 eta 0:15:35
epoch [33/200] batch [45/50] time 0.083 (0.109) data 0.000 (0.024) loss 2.2930 (2.1337) acc 31.2500 (37.2917) lr 1.8838e-03 eta 0:15:08
epoch [33/200] batch [50/50] time 0.082 (0.106) data 0.000 (0.022) loss 1.9502 (2.1392) acc 37.5000 (36.8125) lr 1.8763e-03 eta 0:14:46
epoch [34/200] batch [5/50] time 0.086 (0.315) data 0.000 (0.229) loss 2.2852 (2.1383) acc 21.8750 (35.0000) lr 1.8763e-03 eta 0:43:52
epoch [34/200] batch [10/50] time 0.084 (0.209) data 0.000 (0.124) loss 2.0039 (2.1988) acc 46.8750 (32.5000) lr 1.8763e-03 eta 0:29:05
epoch [34/200] batch [15/50] time 0.085 (0.168) data 0.000 (0.082) loss 2.0020 (2.1401) acc 34.3750 (35.0000) lr 1.8763e-03 eta 0:23:18
epoch [34/200] batch [20/50] time 0.084 (0.151) data 0.000 (0.067) loss 2.4512 (2.1634) acc 34.3750 (35.1562) lr 1.8763e-03 eta 0:21:01
epoch [34/200] batch [25/50] time 0.084 (0.138) data 0.000 (0.053) loss 1.5967 (2.1491) acc 53.1250 (36.5000) lr 1.8763e-03 eta 0:19:09
epoch [34/200] batch [30/50] time 0.084 (0.129) data 0.000 (0.045) loss 1.9648 (2.1361) acc 40.6250 (36.7708) lr 1.8763e-03 eta 0:17:53
epoch [34/200] batch [35/50] time 0.082 (0.123) data 0.000 (0.039) loss 2.2109 (2.1402) acc 25.0000 (36.6964) lr 1.8763e-03 eta 0:17:06
epoch [34/200] batch [40/50] time 0.083 (0.118) data 0.000 (0.034) loss 2.0195 (2.1537) acc 37.5000 (36.7969) lr 1.8763e-03 eta 0:16:24
epoch [34/200] batch [45/50] time 0.083 (0.117) data 0.000 (0.033) loss 2.1973 (2.1472) acc 31.2500 (36.7361) lr 1.8763e-03 eta 0:16:12
epoch [34/200] batch [50/50] time 0.084 (0.114) data 0.000 (0.030) loss 2.1719 (2.1451) acc 40.6250 (36.8125) lr 1.8686e-03 eta 0:15:44
epoch [35/200] batch [5/50] time 0.083 (0.304) data 0.000 (0.221) loss 2.3984 (2.1072) acc 21.8750 (34.3750) lr 1.8686e-03 eta 0:42:04
epoch [35/200] batch [10/50] time 0.084 (0.206) data 0.000 (0.123) loss 1.7822 (2.0980) acc 46.8750 (36.2500) lr 1.8686e-03 eta 0:28:31
epoch [35/200] batch [15/50] time 0.084 (0.166) data 0.000 (0.082) loss 2.3027 (2.0697) acc 31.2500 (38.1250) lr 1.8686e-03 eta 0:22:52
epoch [35/200] batch [20/50] time 0.085 (0.145) data 0.000 (0.062) loss 2.2539 (2.0261) acc 37.5000 (40.1562) lr 1.8686e-03 eta 0:20:03
epoch [35/200] batch [25/50] time 0.085 (0.133) data 0.000 (0.049) loss 2.1230 (2.0796) acc 40.6250 (38.6250) lr 1.8686e-03 eta 0:18:21
epoch [35/200] batch [30/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.7246 (2.1088) acc 34.3750 (36.9792) lr 1.8686e-03 eta 0:17:12
epoch [35/200] batch [35/50] time 0.083 (0.119) data 0.000 (0.035) loss 1.9316 (2.0838) acc 31.2500 (36.7857) lr 1.8686e-03 eta 0:16:23
epoch [35/200] batch [40/50] time 0.085 (0.116) data 0.000 (0.032) loss 2.0625 (2.0957) acc 34.3750 (36.4844) lr 1.8686e-03 eta 0:15:59
epoch [35/200] batch [45/50] time 0.084 (0.114) data 0.000 (0.030) loss 1.9814 (2.1130) acc 40.6250 (36.1806) lr 1.8686e-03 eta 0:15:39
epoch [35/200] batch [50/50] time 0.083 (0.111) data 0.000 (0.027) loss 1.8994 (2.1125) acc 40.6250 (36.5000) lr 1.8607e-03 eta 0:15:14
epoch [36/200] batch [5/50] time 0.086 (0.351) data 0.000 (0.265) loss 1.8779 (1.9441) acc 46.8750 (43.7500) lr 1.8607e-03 eta 0:48:09
epoch [36/200] batch [10/50] time 0.174 (0.227) data 0.089 (0.142) loss 1.7822 (2.0312) acc 50.0000 (40.6250) lr 1.8607e-03 eta 0:31:11
epoch [36/200] batch [15/50] time 0.085 (0.180) data 0.000 (0.095) loss 2.2012 (2.0850) acc 37.5000 (38.9583) lr 1.8607e-03 eta 0:24:42
epoch [36/200] batch [20/50] time 0.086 (0.167) data 0.000 (0.081) loss 1.5479 (2.0771) acc 50.0000 (37.6562) lr 1.8607e-03 eta 0:22:53
epoch [36/200] batch [25/50] time 0.086 (0.151) data 0.000 (0.065) loss 1.8154 (2.0839) acc 46.8750 (37.7500) lr 1.8607e-03 eta 0:20:39
epoch [36/200] batch [30/50] time 0.086 (0.144) data 0.000 (0.059) loss 1.8672 (2.0643) acc 43.7500 (37.9167) lr 1.8607e-03 eta 0:19:44
epoch [36/200] batch [35/50] time 0.086 (0.139) data 0.001 (0.054) loss 1.6357 (2.0957) acc 40.6250 (37.0536) lr 1.8607e-03 eta 0:19:02
epoch [36/200] batch [40/50] time 0.084 (0.132) data 0.000 (0.047) loss 2.1914 (2.0973) acc 40.6250 (37.4219) lr 1.8607e-03 eta 0:18:06
epoch [36/200] batch [45/50] time 0.083 (0.130) data 0.000 (0.045) loss 2.2070 (2.1026) acc 34.3750 (37.2222) lr 1.8607e-03 eta 0:17:44
epoch [36/200] batch [50/50] time 0.084 (0.125) data 0.000 (0.040) loss 2.4766 (2.1218) acc 21.8750 (36.8750) lr 1.8526e-03 eta 0:17:06
epoch [37/200] batch [5/50] time 0.085 (0.305) data 0.000 (0.220) loss 2.1777 (2.0480) acc 40.6250 (38.1250) lr 1.8526e-03 eta 0:41:38
epoch [37/200] batch [10/50] time 0.091 (0.196) data 0.006 (0.111) loss 2.0410 (2.0217) acc 31.2500 (37.8125) lr 1.8526e-03 eta 0:26:44
epoch [37/200] batch [15/50] time 0.085 (0.159) data 0.000 (0.074) loss 2.0527 (2.0790) acc 40.6250 (36.0417) lr 1.8526e-03 eta 0:21:41
epoch [37/200] batch [20/50] time 0.087 (0.144) data 0.000 (0.058) loss 2.1875 (2.0873) acc 34.3750 (36.8750) lr 1.8526e-03 eta 0:19:36
epoch [37/200] batch [25/50] time 0.097 (0.133) data 0.012 (0.048) loss 2.1035 (2.1015) acc 37.5000 (37.0000) lr 1.8526e-03 eta 0:18:09
epoch [37/200] batch [30/50] time 0.091 (0.130) data 0.000 (0.045) loss 2.1641 (2.1210) acc 43.7500 (37.1875) lr 1.8526e-03 eta 0:17:43
epoch [37/200] batch [35/50] time 0.085 (0.125) data 0.001 (0.040) loss 1.7598 (2.1172) acc 50.0000 (38.2143) lr 1.8526e-03 eta 0:17:01
epoch [37/200] batch [40/50] time 0.086 (0.121) data 0.000 (0.036) loss 2.3066 (2.1262) acc 25.0000 (37.7344) lr 1.8526e-03 eta 0:16:27
epoch [37/200] batch [45/50] time 0.084 (0.117) data 0.000 (0.032) loss 2.2031 (2.1324) acc 34.3750 (37.3611) lr 1.8526e-03 eta 0:15:53
epoch [37/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.031) loss 2.2969 (2.1308) acc 28.1250 (36.7500) lr 1.8443e-03 eta 0:15:45
epoch [38/200] batch [5/50] time 0.318 (0.324) data 0.232 (0.239) loss 1.7588 (2.1070) acc 43.7500 (35.6250) lr 1.8443e-03 eta 0:44:01
epoch [38/200] batch [10/50] time 0.085 (0.212) data 0.000 (0.127) loss 2.2305 (2.1504) acc 31.2500 (34.6875) lr 1.8443e-03 eta 0:28:47
epoch [38/200] batch [15/50] time 0.091 (0.176) data 0.000 (0.090) loss 2.5781 (2.1908) acc 31.2500 (35.8333) lr 1.8443e-03 eta 0:23:48
epoch [38/200] batch [20/50] time 0.086 (0.153) data 0.001 (0.068) loss 2.0117 (2.1777) acc 50.0000 (36.5625) lr 1.8443e-03 eta 0:20:46
epoch [38/200] batch [25/50] time 0.086 (0.146) data 0.000 (0.061) loss 2.0254 (2.1919) acc 37.5000 (36.1250) lr 1.8443e-03 eta 0:19:48
epoch [38/200] batch [30/50] time 0.085 (0.142) data 0.000 (0.057) loss 1.8389 (2.1597) acc 53.1250 (36.7708) lr 1.8443e-03 eta 0:19:11
epoch [38/200] batch [35/50] time 0.085 (0.134) data 0.000 (0.049) loss 1.9824 (2.1504) acc 37.5000 (36.9643) lr 1.8443e-03 eta 0:18:04
epoch [38/200] batch [40/50] time 0.084 (0.131) data 0.000 (0.046) loss 2.1875 (2.1547) acc 40.6250 (36.9531) lr 1.8443e-03 eta 0:17:38
epoch [38/200] batch [45/50] time 0.208 (0.128) data 0.126 (0.043) loss 2.1406 (2.1465) acc 37.5000 (37.2917) lr 1.8443e-03 eta 0:17:18
epoch [38/200] batch [50/50] time 0.083 (0.124) data 0.000 (0.039) loss 1.7754 (2.1312) acc 43.7500 (37.5625) lr 1.8358e-03 eta 0:16:40
epoch [39/200] batch [5/50] time 0.086 (0.302) data 0.001 (0.212) loss 1.6865 (2.0342) acc 40.6250 (39.3750) lr 1.8358e-03 eta 0:40:41
epoch [39/200] batch [10/50] time 0.197 (0.205) data 0.112 (0.117) loss 2.2363 (2.1009) acc 43.7500 (38.7500) lr 1.8358e-03 eta 0:27:35
epoch [39/200] batch [15/50] time 0.086 (0.165) data 0.000 (0.078) loss 2.2891 (2.1052) acc 31.2500 (38.1250) lr 1.8358e-03 eta 0:22:16
epoch [39/200] batch [20/50] time 0.147 (0.150) data 0.061 (0.064) loss 2.2461 (2.0950) acc 34.3750 (38.4375) lr 1.8358e-03 eta 0:20:14
epoch [39/200] batch [25/50] time 0.180 (0.141) data 0.096 (0.055) loss 1.9395 (2.1092) acc 46.8750 (37.7500) lr 1.8358e-03 eta 0:19:00
epoch [39/200] batch [30/50] time 0.084 (0.134) data 0.000 (0.048) loss 2.0195 (2.0989) acc 40.6250 (38.3333) lr 1.8358e-03 eta 0:18:02
epoch [39/200] batch [35/50] time 0.084 (0.129) data 0.000 (0.044) loss 2.0430 (2.0954) acc 65.6250 (39.0179) lr 1.8358e-03 eta 0:17:23
epoch [39/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.041) loss 2.1992 (2.0993) acc 40.6250 (38.9844) lr 1.8358e-03 eta 0:16:57
epoch [39/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.4395 (2.1105) acc 21.8750 (37.8472) lr 1.8358e-03 eta 0:16:42
epoch [39/200] batch [50/50] time 0.084 (0.120) data 0.000 (0.036) loss 2.3203 (2.1212) acc 31.2500 (37.3750) lr 1.8271e-03 eta 0:16:09
epoch [40/200] batch [5/50] time 0.086 (0.291) data 0.000 (0.206) loss 2.1016 (2.1889) acc 43.7500 (34.3750) lr 1.8271e-03 eta 0:39:00
epoch [40/200] batch [10/50] time 0.086 (0.188) data 0.000 (0.103) loss 2.0898 (2.1486) acc 37.5000 (34.3750) lr 1.8271e-03 eta 0:25:14
epoch [40/200] batch [15/50] time 0.087 (0.160) data 0.000 (0.074) loss 1.9365 (2.1125) acc 40.6250 (36.0417) lr 1.8271e-03 eta 0:21:22
epoch [40/200] batch [20/50] time 0.214 (0.153) data 0.129 (0.068) loss 1.8965 (2.0827) acc 46.8750 (37.0312) lr 1.8271e-03 eta 0:20:24
epoch [40/200] batch [25/50] time 0.087 (0.139) data 0.000 (0.054) loss 2.0977 (2.0723) acc 40.6250 (37.8750) lr 1.8271e-03 eta 0:18:37
epoch [40/200] batch [30/50] time 0.086 (0.132) data 0.000 (0.047) loss 2.0332 (2.0593) acc 40.6250 (37.9167) lr 1.8271e-03 eta 0:17:42
epoch [40/200] batch [35/50] time 0.091 (0.126) data 0.001 (0.041) loss 2.1074 (2.1019) acc 43.7500 (37.4107) lr 1.8271e-03 eta 0:16:49
epoch [40/200] batch [40/50] time 0.085 (0.123) data 0.000 (0.038) loss 1.6914 (2.0747) acc 40.6250 (38.0469) lr 1.8271e-03 eta 0:16:28
epoch [40/200] batch [45/50] time 0.084 (0.119) data 0.000 (0.034) loss 2.6953 (2.0924) acc 18.7500 (37.3611) lr 1.8271e-03 eta 0:15:52
epoch [40/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.031) loss 1.9717 (2.0732) acc 34.3750 (37.5625) lr 1.8181e-03 eta 0:15:27
epoch [41/200] batch [5/50] time 0.086 (0.322) data 0.000 (0.236) loss 2.2559 (2.1996) acc 28.1250 (33.7500) lr 1.8181e-03 eta 0:42:56
epoch [41/200] batch [10/50] time 0.085 (0.204) data 0.000 (0.118) loss 2.1699 (2.1478) acc 31.2500 (36.5625) lr 1.8181e-03 eta 0:27:08
epoch [41/200] batch [15/50] time 0.086 (0.173) data 0.000 (0.087) loss 2.0664 (2.0454) acc 46.8750 (40.6250) lr 1.8181e-03 eta 0:22:59
epoch [41/200] batch [20/50] time 0.087 (0.163) data 0.001 (0.078) loss 1.6709 (1.9733) acc 53.1250 (41.0938) lr 1.8181e-03 eta 0:21:43
epoch [41/200] batch [25/50] time 0.086 (0.148) data 0.000 (0.063) loss 2.5879 (1.9889) acc 31.2500 (40.3750) lr 1.8181e-03 eta 0:19:38
epoch [41/200] batch [30/50] time 0.084 (0.142) data 0.000 (0.057) loss 2.2559 (1.9964) acc 28.1250 (40.2083) lr 1.8181e-03 eta 0:18:53
epoch [41/200] batch [35/50] time 0.306 (0.140) data 0.222 (0.055) loss 2.2598 (2.0438) acc 31.2500 (38.8393) lr 1.8181e-03 eta 0:18:37
epoch [41/200] batch [40/50] time 0.084 (0.133) data 0.000 (0.048) loss 1.9375 (2.0734) acc 37.5000 (38.2031) lr 1.8181e-03 eta 0:17:41
epoch [41/200] batch [45/50] time 0.083 (0.130) data 0.000 (0.045) loss 1.7715 (2.0576) acc 40.6250 (38.5417) lr 1.8181e-03 eta 0:17:14
epoch [41/200] batch [50/50] time 0.083 (0.125) data 0.000 (0.041) loss 2.3184 (2.0936) acc 28.1250 (37.5625) lr 1.8090e-03 eta 0:16:36
epoch [42/200] batch [5/50] time 0.085 (0.328) data 0.000 (0.243) loss 1.7900 (1.9352) acc 43.7500 (44.3750) lr 1.8090e-03 eta 0:43:23
epoch [42/200] batch [10/50] time 0.085 (0.213) data 0.001 (0.128) loss 2.3516 (2.0741) acc 31.2500 (39.3750) lr 1.8090e-03 eta 0:28:08
epoch [42/200] batch [15/50] time 0.086 (0.170) data 0.000 (0.085) loss 1.9121 (2.1245) acc 40.6250 (38.5417) lr 1.8090e-03 eta 0:22:29
epoch [42/200] batch [20/50] time 0.084 (0.153) data 0.000 (0.069) loss 2.0879 (2.1190) acc 37.5000 (37.0312) lr 1.8090e-03 eta 0:20:15
epoch [42/200] batch [25/50] time 0.246 (0.146) data 0.162 (0.062) loss 1.9443 (2.1243) acc 43.7500 (37.5000) lr 1.8090e-03 eta 0:19:16
epoch [42/200] batch [30/50] time 0.084 (0.136) data 0.000 (0.051) loss 2.1250 (2.1135) acc 34.3750 (37.8125) lr 1.8090e-03 eta 0:17:54
epoch [42/200] batch [35/50] time 0.083 (0.134) data 0.000 (0.050) loss 2.1816 (2.1092) acc 34.3750 (38.7500) lr 1.8090e-03 eta 0:17:41
epoch [42/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.044) loss 2.8633 (2.1364) acc 15.6250 (37.9688) lr 1.8090e-03 eta 0:16:50
epoch [42/200] batch [45/50] time 0.082 (0.126) data 0.000 (0.043) loss 2.0430 (2.1324) acc 43.7500 (38.0556) lr 1.8090e-03 eta 0:16:39
epoch [42/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.038) loss 2.3496 (2.1310) acc 28.1250 (38.2500) lr 1.7997e-03 eta 0:16:04
epoch [43/200] batch [5/50] time 0.086 (0.340) data 0.001 (0.255) loss 1.5010 (1.9389) acc 50.0000 (38.7500) lr 1.7997e-03 eta 0:44:45
epoch [43/200] batch [10/50] time 0.085 (0.229) data 0.000 (0.143) loss 1.6582 (1.8615) acc 46.8750 (43.1250) lr 1.7997e-03 eta 0:30:03
epoch [43/200] batch [15/50] time 0.085 (0.181) data 0.000 (0.096) loss 2.1270 (1.8885) acc 40.6250 (43.3333) lr 1.7997e-03 eta 0:23:47
epoch [43/200] batch [20/50] time 0.086 (0.166) data 0.000 (0.080) loss 2.0957 (1.9656) acc 34.3750 (41.2500) lr 1.7997e-03 eta 0:21:45
epoch [43/200] batch [25/50] time 0.262 (0.157) data 0.178 (0.071) loss 2.4746 (1.9842) acc 31.2500 (40.7500) lr 1.7997e-03 eta 0:20:35
epoch [43/200] batch [30/50] time 0.085 (0.145) data 0.000 (0.060) loss 2.2480 (2.0308) acc 37.5000 (40.3125) lr 1.7997e-03 eta 0:19:00
epoch [43/200] batch [35/50] time 0.085 (0.140) data 0.001 (0.055) loss 1.9824 (2.0366) acc 46.8750 (39.9107) lr 1.7997e-03 eta 0:18:21
epoch [43/200] batch [40/50] time 0.083 (0.133) data 0.000 (0.048) loss 2.0859 (2.0511) acc 40.6250 (39.5312) lr 1.7997e-03 eta 0:17:25
epoch [43/200] batch [45/50] time 0.083 (0.129) data 0.000 (0.044) loss 2.3359 (2.0537) acc 46.8750 (39.7222) lr 1.7997e-03 eta 0:16:53
epoch [43/200] batch [50/50] time 0.084 (0.124) data 0.000 (0.040) loss 1.8906 (2.0515) acc 37.5000 (39.6250) lr 1.7902e-03 eta 0:16:17
epoch [44/200] batch [5/50] time 0.087 (0.332) data 0.000 (0.247) loss 1.7598 (2.1664) acc 53.1250 (38.7500) lr 1.7902e-03 eta 0:43:27
epoch [44/200] batch [10/50] time 0.126 (0.214) data 0.040 (0.128) loss 1.9053 (2.0729) acc 40.6250 (40.9375) lr 1.7902e-03 eta 0:27:57
epoch [44/200] batch [15/50] time 0.087 (0.171) data 0.000 (0.086) loss 2.3633 (2.0546) acc 31.2500 (40.4167) lr 1.7902e-03 eta 0:22:21
epoch [44/200] batch [20/50] time 0.086 (0.157) data 0.000 (0.072) loss 2.4980 (2.0796) acc 25.0000 (39.0625) lr 1.7902e-03 eta 0:20:33
epoch [44/200] batch [25/50] time 0.088 (0.143) data 0.000 (0.058) loss 1.9238 (2.0893) acc 43.7500 (39.1250) lr 1.7902e-03 eta 0:18:41
epoch [44/200] batch [30/50] time 0.085 (0.140) data 0.000 (0.054) loss 2.1211 (2.0930) acc 43.7500 (39.2708) lr 1.7902e-03 eta 0:18:15
epoch [44/200] batch [35/50] time 0.085 (0.137) data 0.000 (0.051) loss 2.3359 (2.0878) acc 34.3750 (39.6429) lr 1.7902e-03 eta 0:17:49
epoch [44/200] batch [40/50] time 0.085 (0.130) data 0.000 (0.045) loss 2.1172 (2.0953) acc 34.3750 (39.2188) lr 1.7902e-03 eta 0:16:57
epoch [44/200] batch [45/50] time 0.084 (0.127) data 0.000 (0.042) loss 1.6611 (2.0729) acc 46.8750 (39.5139) lr 1.7902e-03 eta 0:16:34
epoch [44/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.038) loss 1.7666 (2.0725) acc 43.7500 (39.1875) lr 1.7804e-03 eta 0:15:59
epoch [45/200] batch [5/50] time 0.086 (0.326) data 0.000 (0.240) loss 1.7461 (1.9289) acc 31.2500 (38.1250) lr 1.7804e-03 eta 0:42:18
epoch [45/200] batch [10/50] time 0.086 (0.206) data 0.000 (0.120) loss 2.1855 (2.0426) acc 31.2500 (35.9375) lr 1.7804e-03 eta 0:26:43
epoch [45/200] batch [15/50] time 0.085 (0.166) data 0.000 (0.080) loss 2.2930 (2.0434) acc 37.5000 (35.6250) lr 1.7804e-03 eta 0:21:32
epoch [45/200] batch [20/50] time 0.085 (0.154) data 0.000 (0.069) loss 2.1230 (2.1114) acc 21.8750 (35.3125) lr 1.7804e-03 eta 0:19:59
epoch [45/200] batch [25/50] time 0.316 (0.150) data 0.230 (0.064) loss 1.9766 (2.1119) acc 31.2500 (35.6250) lr 1.7804e-03 eta 0:19:23
epoch [45/200] batch [30/50] time 0.086 (0.139) data 0.000 (0.054) loss 1.9971 (2.0844) acc 46.8750 (35.9375) lr 1.7804e-03 eta 0:18:00
epoch [45/200] batch [35/50] time 0.085 (0.134) data 0.001 (0.049) loss 1.7012 (2.0914) acc 50.0000 (35.5357) lr 1.7804e-03 eta 0:17:21
epoch [45/200] batch [40/50] time 0.086 (0.128) data 0.000 (0.043) loss 2.1641 (2.0887) acc 37.5000 (35.4688) lr 1.7804e-03 eta 0:16:34
epoch [45/200] batch [45/50] time 0.083 (0.127) data 0.000 (0.042) loss 2.3340 (2.1129) acc 28.1250 (35.0000) lr 1.7804e-03 eta 0:16:25
epoch [45/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.038) loss 1.6338 (2.0868) acc 46.8750 (36.1250) lr 1.7705e-03 eta 0:15:50
epoch [46/200] batch [5/50] time 0.085 (0.279) data 0.000 (0.193) loss 2.2539 (2.0053) acc 34.3750 (38.7500) lr 1.7705e-03 eta 0:36:02
epoch [46/200] batch [10/50] time 0.086 (0.183) data 0.000 (0.097) loss 1.9756 (2.0859) acc 59.3750 (40.9375) lr 1.7705e-03 eta 0:23:34
epoch [46/200] batch [15/50] time 0.085 (0.151) data 0.000 (0.066) loss 2.3145 (2.0942) acc 28.1250 (40.0000) lr 1.7705e-03 eta 0:19:31
epoch [46/200] batch [20/50] time 0.085 (0.136) data 0.001 (0.051) loss 1.9629 (2.0728) acc 40.6250 (39.5312) lr 1.7705e-03 eta 0:17:34
epoch [46/200] batch [25/50] time 0.084 (0.130) data 0.000 (0.046) loss 1.8730 (2.0592) acc 34.3750 (39.0000) lr 1.7705e-03 eta 0:16:46
epoch [46/200] batch [30/50] time 0.084 (0.129) data 0.000 (0.045) loss 1.8477 (2.0539) acc 50.0000 (39.7917) lr 1.7705e-03 eta 0:16:39
epoch [46/200] batch [35/50] time 0.084 (0.123) data 0.000 (0.038) loss 1.9180 (2.0638) acc 56.2500 (39.2857) lr 1.7705e-03 eta 0:15:48
epoch [46/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.040) loss 1.7715 (2.0373) acc 50.0000 (39.6875) lr 1.7705e-03 eta 0:15:59
epoch [46/200] batch [45/50] time 0.084 (0.120) data 0.000 (0.036) loss 1.7969 (2.0392) acc 53.1250 (39.6528) lr 1.7705e-03 eta 0:15:24
epoch [46/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.032) loss 2.5430 (2.0557) acc 21.8750 (39.4375) lr 1.7604e-03 eta 0:14:55
epoch [47/200] batch [5/50] time 0.084 (0.328) data 0.000 (0.243) loss 2.2168 (2.1688) acc 40.6250 (39.3750) lr 1.7604e-03 eta 0:42:03
epoch [47/200] batch [10/50] time 0.195 (0.220) data 0.112 (0.135) loss 2.4316 (2.1548) acc 25.0000 (37.8125) lr 1.7604e-03 eta 0:28:12
epoch [47/200] batch [15/50] time 0.086 (0.175) data 0.000 (0.090) loss 2.1934 (2.1730) acc 28.1250 (36.4583) lr 1.7604e-03 eta 0:22:26
epoch [47/200] batch [20/50] time 0.085 (0.155) data 0.001 (0.070) loss 1.9014 (2.1110) acc 37.5000 (37.8125) lr 1.7604e-03 eta 0:19:49
epoch [47/200] batch [25/50] time 0.196 (0.146) data 0.112 (0.060) loss 1.9678 (2.0770) acc 43.7500 (38.7500) lr 1.7604e-03 eta 0:18:36
epoch [47/200] batch [30/50] time 0.086 (0.135) data 0.000 (0.050) loss 2.2910 (2.0990) acc 37.5000 (37.8125) lr 1.7604e-03 eta 0:17:19
epoch [47/200] batch [35/50] time 0.086 (0.131) data 0.001 (0.046) loss 1.8184 (2.0931) acc 53.1250 (38.0357) lr 1.7604e-03 eta 0:16:43
epoch [47/200] batch [40/50] time 0.085 (0.125) data 0.000 (0.040) loss 2.0449 (2.0990) acc 28.1250 (37.7344) lr 1.7604e-03 eta 0:16:00
epoch [47/200] batch [45/50] time 0.085 (0.122) data 0.000 (0.037) loss 2.4844 (2.1048) acc 31.2500 (38.0556) lr 1.7604e-03 eta 0:15:32
epoch [47/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.033) loss 2.1641 (2.1052) acc 37.5000 (38.3125) lr 1.7501e-03 eta 0:15:02
epoch [48/200] batch [5/50] time 0.086 (0.311) data 0.000 (0.227) loss 2.4512 (2.0666) acc 34.3750 (36.8750) lr 1.7501e-03 eta 0:39:41
epoch [48/200] batch [10/50] time 0.086 (0.209) data 0.000 (0.124) loss 2.1133 (2.0301) acc 43.7500 (40.0000) lr 1.7501e-03 eta 0:26:36
epoch [48/200] batch [15/50] time 0.084 (0.168) data 0.000 (0.083) loss 2.4590 (2.1048) acc 34.3750 (39.1667) lr 1.7501e-03 eta 0:21:20
epoch [48/200] batch [20/50] time 0.085 (0.151) data 0.000 (0.067) loss 2.0957 (2.1089) acc 40.6250 (39.3750) lr 1.7501e-03 eta 0:19:15
epoch [48/200] batch [25/50] time 0.086 (0.138) data 0.000 (0.053) loss 2.2969 (2.1114) acc 46.8750 (39.7500) lr 1.7501e-03 eta 0:17:34
epoch [48/200] batch [30/50] time 0.086 (0.134) data 0.000 (0.049) loss 2.1914 (2.0998) acc 34.3750 (40.1042) lr 1.7501e-03 eta 0:17:02
epoch [48/200] batch [35/50] time 0.087 (0.133) data 0.000 (0.048) loss 2.0742 (2.1089) acc 37.5000 (40.0000) lr 1.7501e-03 eta 0:16:52
epoch [48/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.042) loss 2.2891 (2.1101) acc 34.3750 (39.6094) lr 1.7501e-03 eta 0:16:05
epoch [48/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.039) loss 1.6338 (2.1016) acc 50.0000 (39.5833) lr 1.7501e-03 eta 0:15:37
epoch [48/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.5391 (2.1300) acc 25.0000 (38.9375) lr 1.7396e-03 eta 0:15:05
epoch [49/200] batch [5/50] time 0.085 (0.308) data 0.000 (0.222) loss 1.5713 (1.8135) acc 50.0000 (44.3750) lr 1.7396e-03 eta 0:38:57
epoch [49/200] batch [10/50] time 0.085 (0.197) data 0.000 (0.112) loss 2.0234 (1.9702) acc 34.3750 (41.8750) lr 1.7396e-03 eta 0:24:54
epoch [49/200] batch [15/50] time 0.085 (0.169) data 0.000 (0.084) loss 2.1602 (2.0297) acc 31.2500 (42.0833) lr 1.7396e-03 eta 0:21:20
epoch [49/200] batch [20/50] time 0.086 (0.161) data 0.000 (0.075) loss 2.3730 (2.0485) acc 34.3750 (41.8750) lr 1.7396e-03 eta 0:20:17
epoch [49/200] batch [25/50] time 0.086 (0.146) data 0.001 (0.060) loss 2.4844 (2.0605) acc 31.2500 (40.7500) lr 1.7396e-03 eta 0:18:23
epoch [49/200] batch [30/50] time 0.086 (0.142) data 0.000 (0.057) loss 2.2402 (2.0762) acc 34.3750 (39.5833) lr 1.7396e-03 eta 0:17:57
epoch [49/200] batch [35/50] time 0.261 (0.139) data 0.177 (0.054) loss 2.1074 (2.0921) acc 50.0000 (39.3750) lr 1.7396e-03 eta 0:17:34
epoch [49/200] batch [40/50] time 0.083 (0.133) data 0.000 (0.047) loss 2.0742 (2.0853) acc 50.0000 (39.7656) lr 1.7396e-03 eta 0:16:42
epoch [49/200] batch [45/50] time 0.082 (0.127) data 0.000 (0.042) loss 2.1055 (2.0566) acc 34.3750 (39.9306) lr 1.7396e-03 eta 0:15:59
epoch [49/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.038) loss 2.2305 (2.0621) acc 25.0000 (39.8125) lr 1.7290e-03 eta 0:15:25
epoch [50/200] batch [5/50] time 0.085 (0.277) data 0.000 (0.192) loss 1.8789 (1.8746) acc 46.8750 (43.1250) lr 1.7290e-03 eta 0:34:51
epoch [50/200] batch [10/50] time 0.105 (0.184) data 0.020 (0.098) loss 1.8301 (1.9828) acc 43.7500 (42.5000) lr 1.7290e-03 eta 0:23:03
epoch [50/200] batch [15/50] time 0.085 (0.156) data 0.000 (0.071) loss 2.0645 (2.0513) acc 46.8750 (38.9583) lr 1.7290e-03 eta 0:19:38
epoch [50/200] batch [20/50] time 0.234 (0.146) data 0.150 (0.061) loss 1.6895 (2.0533) acc 53.1250 (38.7500) lr 1.7290e-03 eta 0:18:20
epoch [50/200] batch [25/50] time 0.084 (0.134) data 0.000 (0.049) loss 2.0449 (2.0769) acc 37.5000 (38.5000) lr 1.7290e-03 eta 0:16:48
epoch [50/200] batch [30/50] time 0.085 (0.126) data 0.000 (0.041) loss 2.0508 (2.0649) acc 40.6250 (39.0625) lr 1.7290e-03 eta 0:15:45
epoch [50/200] batch [35/50] time 0.084 (0.120) data 0.000 (0.035) loss 2.2168 (2.0724) acc 31.2500 (38.4821) lr 1.7290e-03 eta 0:15:00
epoch [50/200] batch [40/50] time 0.083 (0.120) data 0.000 (0.035) loss 2.0098 (2.0785) acc 59.3750 (38.4375) lr 1.7290e-03 eta 0:14:58
epoch [50/200] batch [45/50] time 0.084 (0.116) data 0.000 (0.031) loss 2.2344 (2.0986) acc 34.3750 (38.2639) lr 1.7290e-03 eta 0:14:27
epoch [50/200] batch [50/50] time 0.084 (0.116) data 0.000 (0.031) loss 1.7852 (2.0975) acc 50.0000 (38.3125) lr 1.7181e-03 eta 0:14:26
epoch [51/200] batch [5/50] time 0.085 (0.326) data 0.000 (0.241) loss 1.8838 (2.0105) acc 46.8750 (35.0000) lr 1.7181e-03 eta 0:40:45
epoch [51/200] batch [10/50] time 0.086 (0.211) data 0.000 (0.126) loss 2.2656 (2.0796) acc 37.5000 (35.0000) lr 1.7181e-03 eta 0:26:23
epoch [51/200] batch [15/50] time 0.086 (0.169) data 0.000 (0.084) loss 2.1680 (2.0876) acc 37.5000 (36.0417) lr 1.7181e-03 eta 0:21:08
epoch [51/200] batch [20/50] time 0.085 (0.159) data 0.000 (0.073) loss 2.2422 (2.0951) acc 28.1250 (36.4062) lr 1.7181e-03 eta 0:19:45
epoch [51/200] batch [25/50] time 0.086 (0.144) data 0.001 (0.058) loss 2.2305 (2.0928) acc 31.2500 (36.6250) lr 1.7181e-03 eta 0:17:58
epoch [51/200] batch [30/50] time 0.086 (0.140) data 0.000 (0.054) loss 1.8984 (2.0674) acc 50.0000 (37.7083) lr 1.7181e-03 eta 0:17:22
epoch [51/200] batch [35/50] time 0.086 (0.136) data 0.001 (0.050) loss 2.2383 (2.0681) acc 40.6250 (38.7500) lr 1.7181e-03 eta 0:16:51
epoch [51/200] batch [40/50] time 0.084 (0.129) data 0.000 (0.044) loss 1.8047 (2.0722) acc 40.6250 (38.2812) lr 1.7181e-03 eta 0:16:03
epoch [51/200] batch [45/50] time 0.082 (0.126) data 0.000 (0.041) loss 1.9668 (2.0744) acc 46.8750 (38.4722) lr 1.7181e-03 eta 0:15:41
epoch [51/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.037) loss 2.1836 (2.0813) acc 34.3750 (38.0625) lr 1.7071e-03 eta 0:15:08
epoch [52/200] batch [5/50] time 0.087 (0.336) data 0.000 (0.250) loss 2.2676 (2.2232) acc 28.1250 (28.7500) lr 1.7071e-03 eta 0:41:44
epoch [52/200] batch [10/50] time 0.086 (0.222) data 0.000 (0.136) loss 2.3262 (2.1207) acc 43.7500 (35.6250) lr 1.7071e-03 eta 0:27:34
epoch [52/200] batch [15/50] time 0.085 (0.177) data 0.000 (0.091) loss 2.0918 (2.1159) acc 40.6250 (37.2917) lr 1.7071e-03 eta 0:21:54
epoch [52/200] batch [20/50] time 0.086 (0.159) data 0.000 (0.073) loss 2.0254 (2.0872) acc 40.6250 (36.8750) lr 1.7071e-03 eta 0:19:43
epoch [52/200] batch [25/50] time 0.209 (0.150) data 0.122 (0.064) loss 1.8623 (2.0632) acc 56.2500 (37.3750) lr 1.7071e-03 eta 0:18:30
epoch [52/200] batch [30/50] time 0.086 (0.139) data 0.000 (0.053) loss 2.5547 (2.0872) acc 31.2500 (37.2917) lr 1.7071e-03 eta 0:17:10
epoch [52/200] batch [35/50] time 0.086 (0.136) data 0.001 (0.050) loss 2.1797 (2.0817) acc 31.2500 (37.2321) lr 1.7071e-03 eta 0:16:46
epoch [52/200] batch [40/50] time 0.083 (0.129) data 0.000 (0.044) loss 2.7148 (2.0874) acc 25.0000 (37.4219) lr 1.7071e-03 eta 0:15:58
epoch [52/200] batch [45/50] time 0.085 (0.126) data 0.000 (0.041) loss 2.3711 (2.0845) acc 40.6250 (37.5000) lr 1.7071e-03 eta 0:15:34
epoch [52/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.037) loss 2.1641 (2.0875) acc 34.3750 (37.6250) lr 1.6959e-03 eta 0:15:01
epoch [53/200] batch [5/50] time 0.086 (0.328) data 0.000 (0.241) loss 1.7910 (1.9883) acc 37.5000 (38.1250) lr 1.6959e-03 eta 0:40:25
epoch [53/200] batch [10/50] time 0.085 (0.224) data 0.000 (0.138) loss 2.2363 (1.9412) acc 31.2500 (39.6875) lr 1.6959e-03 eta 0:27:36
epoch [53/200] batch [15/50] time 0.085 (0.178) data 0.000 (0.092) loss 2.0469 (2.0102) acc 37.5000 (38.3333) lr 1.6959e-03 eta 0:21:51
epoch [53/200] batch [20/50] time 0.085 (0.166) data 0.001 (0.081) loss 2.3008 (2.0486) acc 31.2500 (37.9688) lr 1.6959e-03 eta 0:20:25
epoch [53/200] batch [25/50] time 0.152 (0.153) data 0.067 (0.067) loss 1.9424 (2.0900) acc 43.7500 (37.5000) lr 1.6959e-03 eta 0:18:45
epoch [53/200] batch [30/50] time 0.084 (0.141) data 0.000 (0.056) loss 1.6006 (2.0810) acc 53.1250 (37.1875) lr 1.6959e-03 eta 0:17:22
epoch [53/200] batch [35/50] time 0.086 (0.137) data 0.000 (0.052) loss 2.1191 (2.0715) acc 43.7500 (37.2321) lr 1.6959e-03 eta 0:16:49
epoch [53/200] batch [40/50] time 0.085 (0.131) data 0.000 (0.045) loss 1.6074 (2.0628) acc 59.3750 (37.8906) lr 1.6959e-03 eta 0:16:01
epoch [53/200] batch [45/50] time 0.085 (0.125) data 0.000 (0.040) loss 2.0078 (2.0523) acc 37.5000 (37.5694) lr 1.6959e-03 eta 0:15:23
epoch [53/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.036) loss 2.1133 (2.0451) acc 31.2500 (37.4375) lr 1.6845e-03 eta 0:14:51
epoch [54/200] batch [5/50] time 0.085 (0.302) data 0.000 (0.217) loss 2.2305 (1.9520) acc 34.3750 (41.2500) lr 1.6845e-03 eta 0:36:55
epoch [54/200] batch [10/50] time 0.240 (0.208) data 0.156 (0.124) loss 2.1816 (1.9089) acc 40.6250 (43.7500) lr 1.6845e-03 eta 0:25:29
epoch [54/200] batch [15/50] time 0.086 (0.167) data 0.000 (0.083) loss 1.9287 (1.8943) acc 28.1250 (42.5000) lr 1.6845e-03 eta 0:20:27
epoch [54/200] batch [20/50] time 0.084 (0.153) data 0.000 (0.068) loss 2.2207 (2.0099) acc 21.8750 (38.4375) lr 1.6845e-03 eta 0:18:40
epoch [54/200] batch [25/50] time 0.084 (0.139) data 0.000 (0.055) loss 1.7539 (1.9923) acc 43.7500 (38.2500) lr 1.6845e-03 eta 0:16:59
epoch [54/200] batch [30/50] time 0.085 (0.135) data 0.000 (0.051) loss 2.2109 (1.9911) acc 31.2500 (38.2292) lr 1.6845e-03 eta 0:16:27
epoch [54/200] batch [35/50] time 0.086 (0.132) data 0.001 (0.048) loss 2.1953 (1.9903) acc 37.5000 (39.1964) lr 1.6845e-03 eta 0:16:09
epoch [54/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.042) loss 2.2363 (1.9973) acc 31.2500 (39.3750) lr 1.6845e-03 eta 0:15:23
epoch [54/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 2.6895 (2.0310) acc 21.8750 (39.1667) lr 1.6845e-03 eta 0:15:11
epoch [54/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 2.1484 (2.0456) acc 40.6250 (38.6875) lr 1.6730e-03 eta 0:14:40
epoch [55/200] batch [5/50] time 0.086 (0.294) data 0.000 (0.208) loss 1.8213 (2.1137) acc 34.3750 (39.3750) lr 1.6730e-03 eta 0:35:44
epoch [55/200] batch [10/50] time 0.091 (0.190) data 0.000 (0.104) loss 2.1992 (2.1102) acc 43.7500 (40.9375) lr 1.6730e-03 eta 0:23:08
epoch [55/200] batch [15/50] time 0.086 (0.160) data 0.000 (0.074) loss 1.8555 (2.0521) acc 40.6250 (42.2917) lr 1.6730e-03 eta 0:19:24
epoch [55/200] batch [20/50] time 0.228 (0.148) data 0.143 (0.063) loss 1.8574 (2.0731) acc 34.3750 (41.2500) lr 1.6730e-03 eta 0:18:00
epoch [55/200] batch [25/50] time 0.085 (0.136) data 0.000 (0.050) loss 2.1875 (2.0612) acc 40.6250 (42.1250) lr 1.6730e-03 eta 0:16:27
epoch [55/200] batch [30/50] time 0.084 (0.132) data 0.000 (0.047) loss 2.0977 (2.0640) acc 31.2500 (41.5625) lr 1.6730e-03 eta 0:16:01
epoch [55/200] batch [35/50] time 0.086 (0.125) data 0.000 (0.040) loss 2.4199 (2.0929) acc 34.3750 (40.6250) lr 1.6730e-03 eta 0:15:11
epoch [55/200] batch [40/50] time 0.084 (0.124) data 0.000 (0.039) loss 2.2461 (2.1057) acc 40.6250 (39.9219) lr 1.6730e-03 eta 0:15:00
epoch [55/200] batch [45/50] time 0.084 (0.121) data 0.000 (0.036) loss 1.8760 (2.0842) acc 50.0000 (40.0694) lr 1.6730e-03 eta 0:14:39
epoch [55/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.033) loss 1.9932 (2.0781) acc 34.3750 (40.1250) lr 1.6613e-03 eta 0:14:11
epoch [56/200] batch [5/50] time 0.084 (0.332) data 0.000 (0.248) loss 1.9990 (2.0891) acc 40.6250 (42.5000) lr 1.6613e-03 eta 0:40:08
epoch [56/200] batch [10/50] time 0.179 (0.218) data 0.096 (0.134) loss 1.8203 (1.9951) acc 43.7500 (44.6875) lr 1.6613e-03 eta 0:26:16
epoch [56/200] batch [15/50] time 0.085 (0.173) data 0.000 (0.089) loss 1.9141 (2.0345) acc 37.5000 (42.7083) lr 1.6613e-03 eta 0:20:53
epoch [56/200] batch [20/50] time 0.084 (0.151) data 0.000 (0.067) loss 2.0469 (2.0079) acc 28.1250 (42.3438) lr 1.6613e-03 eta 0:18:11
epoch [56/200] batch [25/50] time 0.085 (0.138) data 0.001 (0.054) loss 2.1680 (2.0555) acc 25.0000 (40.8750) lr 1.6613e-03 eta 0:16:34
epoch [56/200] batch [30/50] time 0.084 (0.129) data 0.000 (0.045) loss 2.4043 (2.0613) acc 31.2500 (40.9375) lr 1.6613e-03 eta 0:15:29
epoch [56/200] batch [35/50] time 0.258 (0.127) data 0.174 (0.043) loss 2.3633 (2.0776) acc 28.1250 (40.0000) lr 1.6613e-03 eta 0:15:18
epoch [56/200] batch [40/50] time 0.083 (0.122) data 0.000 (0.038) loss 2.3672 (2.0905) acc 34.3750 (39.3750) lr 1.6613e-03 eta 0:14:39
epoch [56/200] batch [45/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.8555 (2.0694) acc 40.6250 (39.3750) lr 1.6613e-03 eta 0:14:30
epoch [56/200] batch [50/50] time 0.082 (0.117) data 0.000 (0.033) loss 2.0156 (2.0718) acc 40.6250 (39.3750) lr 1.6494e-03 eta 0:14:02
epoch [57/200] batch [5/50] time 0.086 (0.337) data 0.000 (0.252) loss 2.0059 (1.9291) acc 46.8750 (43.1250) lr 1.6494e-03 eta 0:40:25
epoch [57/200] batch [10/50] time 0.087 (0.213) data 0.001 (0.128) loss 1.9404 (1.9459) acc 50.0000 (43.7500) lr 1.6494e-03 eta 0:25:32
epoch [57/200] batch [15/50] time 0.086 (0.171) data 0.000 (0.085) loss 1.9600 (1.9897) acc 37.5000 (42.2917) lr 1.6494e-03 eta 0:20:26
epoch [57/200] batch [20/50] time 0.091 (0.156) data 0.000 (0.070) loss 1.9746 (2.0148) acc 46.8750 (42.6562) lr 1.6494e-03 eta 0:18:37
epoch [57/200] batch [25/50] time 0.086 (0.142) data 0.000 (0.056) loss 2.5098 (2.0013) acc 31.2500 (42.6250) lr 1.6494e-03 eta 0:16:56
epoch [57/200] batch [30/50] time 0.084 (0.135) data 0.000 (0.050) loss 2.2441 (2.0050) acc 43.7500 (42.6042) lr 1.6494e-03 eta 0:16:11
epoch [57/200] batch [35/50] time 0.083 (0.133) data 0.000 (0.048) loss 2.4258 (2.0396) acc 21.8750 (41.4286) lr 1.6494e-03 eta 0:15:52
epoch [57/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.042) loss 1.9375 (2.0336) acc 34.3750 (40.8594) lr 1.6494e-03 eta 0:15:07
epoch [57/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.7520 (2.0313) acc 43.7500 (40.4861) lr 1.6494e-03 eta 0:14:57
epoch [57/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.9727 (2.0393) acc 37.5000 (40.2500) lr 1.6374e-03 eta 0:14:26
epoch [58/200] batch [5/50] time 0.086 (0.313) data 0.000 (0.228) loss 2.0488 (1.9701) acc 40.6250 (46.2500) lr 1.6374e-03 eta 0:37:17
epoch [58/200] batch [10/50] time 0.086 (0.200) data 0.000 (0.114) loss 1.7939 (2.0271) acc 50.0000 (43.1250) lr 1.6374e-03 eta 0:23:45
epoch [58/200] batch [15/50] time 0.085 (0.163) data 0.000 (0.078) loss 1.8242 (1.9717) acc 46.8750 (43.5417) lr 1.6374e-03 eta 0:19:23
epoch [58/200] batch [20/50] time 0.086 (0.145) data 0.000 (0.059) loss 2.0391 (2.0398) acc 40.6250 (41.7188) lr 1.6374e-03 eta 0:17:10
epoch [58/200] batch [25/50] time 0.086 (0.133) data 0.001 (0.047) loss 1.6953 (2.0327) acc 40.6250 (41.0000) lr 1.6374e-03 eta 0:15:46
epoch [58/200] batch [30/50] time 0.085 (0.127) data 0.000 (0.042) loss 2.1211 (2.0424) acc 46.8750 (40.2083) lr 1.6374e-03 eta 0:15:06
epoch [58/200] batch [35/50] time 0.086 (0.121) data 0.001 (0.036) loss 1.8936 (2.0418) acc 50.0000 (40.4464) lr 1.6374e-03 eta 0:14:23
epoch [58/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.035) loss 1.8066 (2.0470) acc 43.7500 (39.6875) lr 1.6374e-03 eta 0:14:11
epoch [58/200] batch [45/50] time 0.082 (0.116) data 0.000 (0.031) loss 2.4199 (2.0788) acc 28.1250 (38.9583) lr 1.6374e-03 eta 0:13:42
epoch [58/200] batch [50/50] time 0.082 (0.112) data 0.000 (0.028) loss 2.0977 (2.0678) acc 34.3750 (38.7500) lr 1.6252e-03 eta 0:13:18
epoch [59/200] batch [5/50] time 0.086 (0.328) data 0.000 (0.242) loss 2.1719 (1.9937) acc 40.6250 (43.7500) lr 1.6252e-03 eta 0:38:45
epoch [59/200] batch [10/50] time 0.084 (0.212) data 0.000 (0.127) loss 1.9619 (1.9863) acc 46.8750 (42.8125) lr 1.6252e-03 eta 0:25:02
epoch [59/200] batch [15/50] time 0.085 (0.170) data 0.000 (0.084) loss 1.6963 (1.9520) acc 50.0000 (42.5000) lr 1.6252e-03 eta 0:20:02
epoch [59/200] batch [20/50] time 0.085 (0.156) data 0.000 (0.071) loss 1.8789 (1.9773) acc 40.6250 (40.4688) lr 1.6252e-03 eta 0:18:26
epoch [59/200] batch [25/50] time 0.160 (0.145) data 0.076 (0.060) loss 1.9951 (2.0168) acc 34.3750 (39.1250) lr 1.6252e-03 eta 0:17:05
epoch [59/200] batch [30/50] time 0.085 (0.140) data 0.000 (0.055) loss 1.9404 (2.0464) acc 46.8750 (39.1667) lr 1.6252e-03 eta 0:16:29
epoch [59/200] batch [35/50] time 0.085 (0.135) data 0.000 (0.050) loss 1.7363 (2.0318) acc 53.1250 (39.8214) lr 1.6252e-03 eta 0:15:54
epoch [59/200] batch [40/50] time 0.084 (0.129) data 0.000 (0.044) loss 2.2715 (2.0392) acc 37.5000 (39.7656) lr 1.6252e-03 eta 0:15:08
epoch [59/200] batch [45/50] time 0.083 (0.127) data 0.000 (0.042) loss 1.8643 (2.0351) acc 53.1250 (40.2778) lr 1.6252e-03 eta 0:14:52
epoch [59/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.038) loss 1.9678 (2.0558) acc 34.3750 (39.6875) lr 1.6129e-03 eta 0:14:21
epoch [60/200] batch [5/50] time 0.085 (0.315) data 0.001 (0.230) loss 1.9395 (1.9672) acc 43.7500 (36.2500) lr 1.6129e-03 eta 0:37:02
epoch [60/200] batch [10/50] time 0.148 (0.215) data 0.064 (0.130) loss 2.2676 (2.0348) acc 34.3750 (38.4375) lr 1.6129e-03 eta 0:25:13
epoch [60/200] batch [15/50] time 0.084 (0.172) data 0.000 (0.087) loss 2.1328 (2.0612) acc 43.7500 (38.3333) lr 1.6129e-03 eta 0:20:08
epoch [60/200] batch [20/50] time 0.083 (0.157) data 0.000 (0.072) loss 2.3691 (2.0797) acc 34.3750 (38.7500) lr 1.6129e-03 eta 0:18:22
epoch [60/200] batch [25/50] time 0.084 (0.142) data 0.000 (0.058) loss 2.0859 (2.1030) acc 34.3750 (37.5000) lr 1.6129e-03 eta 0:16:38
epoch [60/200] batch [30/50] time 0.084 (0.137) data 0.000 (0.053) loss 1.9443 (2.0567) acc 43.7500 (38.5417) lr 1.6129e-03 eta 0:16:02
epoch [60/200] batch [35/50] time 0.083 (0.130) data 0.000 (0.045) loss 1.8008 (2.0556) acc 50.0000 (38.7500) lr 1.6129e-03 eta 0:15:08
epoch [60/200] batch [40/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.3418 (2.0518) acc 31.2500 (38.4375) lr 1.6129e-03 eta 0:14:27
epoch [60/200] batch [45/50] time 0.084 (0.119) data 0.000 (0.035) loss 2.2480 (2.0664) acc 28.1250 (37.5000) lr 1.6129e-03 eta 0:13:55
epoch [60/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.032) loss 2.0410 (2.0564) acc 31.2500 (37.6250) lr 1.6004e-03 eta 0:13:30
epoch [61/200] batch [5/50] time 0.085 (0.332) data 0.001 (0.247) loss 2.0605 (2.1125) acc 46.8750 (37.5000) lr 1.6004e-03 eta 0:38:38
epoch [61/200] batch [10/50] time 0.115 (0.222) data 0.029 (0.136) loss 2.1309 (2.1279) acc 37.5000 (35.6250) lr 1.6004e-03 eta 0:25:49
epoch [61/200] batch [15/50] time 0.086 (0.176) data 0.000 (0.091) loss 1.6787 (2.0976) acc 37.5000 (37.0833) lr 1.6004e-03 eta 0:20:32
epoch [61/200] batch [20/50] time 0.086 (0.164) data 0.000 (0.078) loss 1.9980 (2.1186) acc 37.5000 (37.0312) lr 1.6004e-03 eta 0:19:03
epoch [61/200] batch [25/50] time 0.086 (0.148) data 0.001 (0.063) loss 2.5156 (2.0869) acc 28.1250 (38.7500) lr 1.6004e-03 eta 0:17:13
epoch [61/200] batch [30/50] time 0.086 (0.144) data 0.001 (0.059) loss 2.0293 (2.0734) acc 31.2500 (38.5417) lr 1.6004e-03 eta 0:16:46
epoch [61/200] batch [35/50] time 0.086 (0.140) data 0.001 (0.055) loss 2.0020 (2.0723) acc 40.6250 (38.3036) lr 1.6004e-03 eta 0:16:16
epoch [61/200] batch [40/50] time 0.085 (0.133) data 0.000 (0.048) loss 1.9180 (2.0741) acc 43.7500 (39.1406) lr 1.6004e-03 eta 0:15:27
epoch [61/200] batch [45/50] time 0.084 (0.128) data 0.000 (0.043) loss 1.7568 (2.0615) acc 40.6250 (39.6528) lr 1.6004e-03 eta 0:14:48
epoch [61/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.039) loss 1.8984 (2.0573) acc 40.6250 (39.6875) lr 1.5878e-03 eta 0:14:17
epoch [62/200] batch [5/50] time 0.084 (0.370) data 0.000 (0.286) loss 1.8594 (2.0041) acc 50.0000 (41.8750) lr 1.5878e-03 eta 0:42:47
epoch [62/200] batch [10/50] time 0.084 (0.237) data 0.000 (0.153) loss 2.1719 (2.0948) acc 25.0000 (39.0625) lr 1.5878e-03 eta 0:27:23
epoch [62/200] batch [15/50] time 0.084 (0.186) data 0.000 (0.102) loss 2.4883 (2.1012) acc 31.2500 (37.7083) lr 1.5878e-03 eta 0:21:29
epoch [62/200] batch [20/50] time 0.085 (0.169) data 0.000 (0.085) loss 1.9180 (2.1068) acc 34.3750 (37.0312) lr 1.5878e-03 eta 0:19:33
epoch [62/200] batch [25/50] time 0.213 (0.158) data 0.127 (0.073) loss 1.9316 (2.0900) acc 40.6250 (37.6250) lr 1.5878e-03 eta 0:18:11
epoch [62/200] batch [30/50] time 0.085 (0.145) data 0.000 (0.061) loss 2.2910 (2.0818) acc 25.0000 (38.3333) lr 1.5878e-03 eta 0:16:46
epoch [62/200] batch [35/50] time 0.084 (0.142) data 0.000 (0.058) loss 2.2422 (2.1028) acc 28.1250 (37.6786) lr 1.5878e-03 eta 0:16:23
epoch [62/200] batch [40/50] time 0.084 (0.135) data 0.000 (0.051) loss 1.8340 (2.0794) acc 53.1250 (38.4375) lr 1.5878e-03 eta 0:15:33
epoch [62/200] batch [45/50] time 0.083 (0.129) data 0.000 (0.045) loss 2.5449 (2.0708) acc 25.0000 (38.4722) lr 1.5878e-03 eta 0:14:52
epoch [62/200] batch [50/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.8682 (2.0641) acc 40.6250 (38.6250) lr 1.5750e-03 eta 0:14:19
epoch [63/200] batch [5/50] time 0.084 (0.316) data 0.000 (0.231) loss 2.0430 (1.9438) acc 50.0000 (46.2500) lr 1.5750e-03 eta 0:36:16
epoch [63/200] batch [10/50] time 0.086 (0.207) data 0.000 (0.122) loss 2.1855 (1.9487) acc 28.1250 (45.6250) lr 1.5750e-03 eta 0:23:48
epoch [63/200] batch [15/50] time 0.085 (0.167) data 0.000 (0.082) loss 1.7539 (2.0070) acc 53.1250 (43.7500) lr 1.5750e-03 eta 0:19:07
epoch [63/200] batch [20/50] time 0.086 (0.154) data 0.000 (0.069) loss 1.6582 (2.0197) acc 43.7500 (41.4062) lr 1.5750e-03 eta 0:17:36
epoch [63/200] batch [25/50] time 0.228 (0.146) data 0.141 (0.061) loss 2.4316 (2.0325) acc 46.8750 (41.3750) lr 1.5750e-03 eta 0:16:42
epoch [63/200] batch [30/50] time 0.085 (0.136) data 0.000 (0.051) loss 2.5312 (2.0720) acc 21.8750 (40.0000) lr 1.5750e-03 eta 0:15:32
epoch [63/200] batch [35/50] time 0.085 (0.130) data 0.001 (0.044) loss 1.7588 (2.0552) acc 37.5000 (39.8214) lr 1.5750e-03 eta 0:14:49
epoch [63/200] batch [40/50] time 0.085 (0.124) data 0.000 (0.039) loss 1.8701 (2.0519) acc 46.8750 (39.8438) lr 1.5750e-03 eta 0:14:11
epoch [63/200] batch [45/50] time 0.084 (0.121) data 0.000 (0.036) loss 1.5840 (2.0378) acc 53.1250 (40.4861) lr 1.5750e-03 eta 0:13:50
epoch [63/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.033) loss 1.7236 (2.0220) acc 37.5000 (40.3125) lr 1.5621e-03 eta 0:13:23
epoch [64/200] batch [5/50] time 0.084 (0.313) data 0.000 (0.228) loss 1.6797 (2.0307) acc 50.0000 (41.8750) lr 1.5621e-03 eta 0:35:43
epoch [64/200] batch [10/50] time 0.086 (0.217) data 0.000 (0.132) loss 2.0078 (1.9543) acc 28.1250 (42.5000) lr 1.5621e-03 eta 0:24:42
epoch [64/200] batch [15/50] time 0.086 (0.173) data 0.000 (0.088) loss 1.4326 (1.9924) acc 65.6250 (41.4583) lr 1.5621e-03 eta 0:19:43
epoch [64/200] batch [20/50] time 0.085 (0.158) data 0.000 (0.072) loss 2.0820 (2.0172) acc 37.5000 (40.4688) lr 1.5621e-03 eta 0:17:56
epoch [64/200] batch [25/50] time 0.275 (0.151) data 0.190 (0.066) loss 1.8027 (2.0193) acc 50.0000 (40.5000) lr 1.5621e-03 eta 0:17:09
epoch [64/200] batch [30/50] time 0.086 (0.140) data 0.000 (0.055) loss 2.0312 (2.0079) acc 31.2500 (40.2083) lr 1.5621e-03 eta 0:15:54
epoch [64/200] batch [35/50] time 0.085 (0.137) data 0.001 (0.052) loss 2.4609 (2.0092) acc 31.2500 (40.3571) lr 1.5621e-03 eta 0:15:36
epoch [64/200] batch [40/50] time 0.084 (0.131) data 0.000 (0.046) loss 2.2461 (2.0027) acc 46.8750 (40.6250) lr 1.5621e-03 eta 0:14:51
epoch [64/200] batch [45/50] time 0.082 (0.128) data 0.000 (0.043) loss 2.0879 (2.0084) acc 43.7500 (40.5556) lr 1.5621e-03 eta 0:14:29
epoch [64/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.0801 (2.0188) acc 56.2500 (40.6250) lr 1.5490e-03 eta 0:13:57
epoch [65/200] batch [5/50] time 0.086 (0.320) data 0.000 (0.235) loss 1.9873 (1.9016) acc 34.3750 (43.7500) lr 1.5490e-03 eta 0:36:12
epoch [65/200] batch [10/50] time 0.085 (0.203) data 0.001 (0.118) loss 2.1035 (1.9420) acc 43.7500 (43.1250) lr 1.5490e-03 eta 0:22:56
epoch [65/200] batch [15/50] time 0.086 (0.192) data 0.000 (0.107) loss 1.6416 (1.9518) acc 50.0000 (42.2917) lr 1.5490e-03 eta 0:21:43
epoch [65/200] batch [20/50] time 0.087 (0.166) data 0.000 (0.080) loss 2.1172 (1.9439) acc 34.3750 (41.0938) lr 1.5490e-03 eta 0:18:44
epoch [65/200] batch [25/50] time 0.086 (0.151) data 0.000 (0.065) loss 1.6172 (1.9649) acc 62.5000 (41.3750) lr 1.5490e-03 eta 0:17:01
epoch [65/200] batch [30/50] time 0.086 (0.143) data 0.000 (0.058) loss 1.8779 (1.9706) acc 46.8750 (40.9375) lr 1.5490e-03 eta 0:16:09
epoch [65/200] batch [35/50] time 0.086 (0.136) data 0.001 (0.051) loss 2.5273 (1.9847) acc 37.5000 (41.0714) lr 1.5490e-03 eta 0:15:22
epoch [65/200] batch [40/50] time 0.083 (0.135) data 0.000 (0.049) loss 2.3379 (2.0077) acc 40.6250 (40.3125) lr 1.5490e-03 eta 0:15:10
epoch [65/200] batch [45/50] time 0.082 (0.129) data 0.000 (0.044) loss 2.0957 (1.9965) acc 34.3750 (40.8333) lr 1.5490e-03 eta 0:14:31
epoch [65/200] batch [50/50] time 0.083 (0.127) data 0.000 (0.043) loss 1.5342 (2.0049) acc 46.8750 (40.5625) lr 1.5358e-03 eta 0:14:19
epoch [66/200] batch [5/50] time 0.085 (0.307) data 0.000 (0.221) loss 1.7861 (1.8729) acc 53.1250 (42.5000) lr 1.5358e-03 eta 0:34:29
epoch [66/200] batch [10/50] time 0.095 (0.197) data 0.000 (0.111) loss 1.4951 (1.8171) acc 53.1250 (44.0625) lr 1.5358e-03 eta 0:22:08
epoch [66/200] batch [15/50] time 0.085 (0.165) data 0.000 (0.079) loss 2.1953 (1.9175) acc 34.3750 (41.8750) lr 1.5358e-03 eta 0:18:29
epoch [66/200] batch [20/50] time 0.086 (0.150) data 0.001 (0.064) loss 1.7959 (1.9781) acc 46.8750 (40.6250) lr 1.5358e-03 eta 0:16:47
epoch [66/200] batch [25/50] time 0.090 (0.137) data 0.000 (0.051) loss 2.1426 (1.9975) acc 34.3750 (40.1250) lr 1.5358e-03 eta 0:15:23
epoch [66/200] batch [30/50] time 0.086 (0.135) data 0.000 (0.049) loss 1.7988 (1.9822) acc 46.8750 (40.8333) lr 1.5358e-03 eta 0:15:05
epoch [66/200] batch [35/50] time 0.190 (0.132) data 0.108 (0.046) loss 2.0566 (1.9973) acc 40.6250 (40.4464) lr 1.5358e-03 eta 0:14:44
epoch [66/200] batch [40/50] time 0.085 (0.126) data 0.000 (0.040) loss 1.5479 (1.9720) acc 53.1250 (41.0156) lr 1.5358e-03 eta 0:14:03
epoch [66/200] batch [45/50] time 0.085 (0.125) data 0.000 (0.039) loss 2.2773 (1.9763) acc 40.6250 (41.3194) lr 1.5358e-03 eta 0:13:56
epoch [66/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.036) loss 1.8955 (2.0012) acc 46.8750 (40.9375) lr 1.5225e-03 eta 0:13:28
epoch [67/200] batch [5/50] time 0.085 (0.312) data 0.000 (0.227) loss 1.9307 (1.9320) acc 34.3750 (40.0000) lr 1.5225e-03 eta 0:34:52
epoch [67/200] batch [10/50] time 0.088 (0.204) data 0.000 (0.118) loss 1.8574 (2.1091) acc 34.3750 (38.1250) lr 1.5225e-03 eta 0:22:42
epoch [67/200] batch [15/50] time 0.087 (0.166) data 0.000 (0.080) loss 2.2344 (2.0934) acc 31.2500 (38.9583) lr 1.5225e-03 eta 0:18:26
epoch [67/200] batch [20/50] time 0.084 (0.159) data 0.000 (0.074) loss 1.8936 (2.0309) acc 46.8750 (40.4688) lr 1.5225e-03 eta 0:17:42
epoch [67/200] batch [25/50] time 0.085 (0.144) data 0.000 (0.059) loss 1.8496 (2.0231) acc 37.5000 (40.5000) lr 1.5225e-03 eta 0:16:02
epoch [67/200] batch [30/50] time 0.086 (0.142) data 0.000 (0.057) loss 1.9824 (2.0257) acc 40.6250 (39.5833) lr 1.5225e-03 eta 0:15:48
epoch [67/200] batch [35/50] time 0.252 (0.139) data 0.169 (0.054) loss 1.8408 (2.0080) acc 40.6250 (40.0893) lr 1.5225e-03 eta 0:15:25
epoch [67/200] batch [40/50] time 0.084 (0.132) data 0.000 (0.047) loss 2.4316 (2.0463) acc 40.6250 (39.3750) lr 1.5225e-03 eta 0:14:39
epoch [67/200] batch [45/50] time 0.083 (0.129) data 0.000 (0.045) loss 1.7471 (2.0303) acc 50.0000 (39.5139) lr 1.5225e-03 eta 0:14:18
epoch [67/200] batch [50/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.4922 (2.0334) acc 31.2500 (39.1875) lr 1.5090e-03 eta 0:13:47
epoch [68/200] batch [5/50] time 0.085 (0.323) data 0.000 (0.238) loss 1.9746 (1.9773) acc 53.1250 (46.2500) lr 1.5090e-03 eta 0:35:46
epoch [68/200] batch [10/50] time 0.086 (0.215) data 0.000 (0.130) loss 2.0508 (1.8974) acc 43.7500 (46.5625) lr 1.5090e-03 eta 0:23:44
epoch [68/200] batch [15/50] time 0.085 (0.172) data 0.000 (0.087) loss 2.1875 (1.9646) acc 34.3750 (44.3750) lr 1.5090e-03 eta 0:18:58
epoch [68/200] batch [20/50] time 0.085 (0.162) data 0.000 (0.077) loss 1.9678 (1.9993) acc 34.3750 (42.8125) lr 1.5090e-03 eta 0:17:54
epoch [68/200] batch [25/50] time 0.252 (0.153) data 0.167 (0.068) loss 2.0391 (2.0104) acc 46.8750 (43.0000) lr 1.5090e-03 eta 0:16:56
epoch [68/200] batch [30/50] time 0.085 (0.142) data 0.000 (0.057) loss 2.3652 (2.0288) acc 37.5000 (42.9167) lr 1.5090e-03 eta 0:15:41
epoch [68/200] batch [35/50] time 0.084 (0.134) data 0.000 (0.049) loss 2.5371 (2.0572) acc 18.7500 (42.2321) lr 1.5090e-03 eta 0:14:46
epoch [68/200] batch [40/50] time 0.085 (0.128) data 0.000 (0.043) loss 1.4482 (2.0309) acc 71.8750 (42.8125) lr 1.5090e-03 eta 0:14:04
epoch [68/200] batch [45/50] time 0.084 (0.125) data 0.000 (0.040) loss 2.1074 (2.0303) acc 28.1250 (42.4306) lr 1.5090e-03 eta 0:13:42
epoch [68/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 2.2383 (2.0546) acc 31.2500 (41.6250) lr 1.4955e-03 eta 0:13:15
epoch [69/200] batch [5/50] time 0.085 (0.325) data 0.000 (0.240) loss 1.9736 (2.0430) acc 31.2500 (35.0000) lr 1.4955e-03 eta 0:35:43
epoch [69/200] batch [10/50] time 0.085 (0.222) data 0.000 (0.138) loss 1.9805 (2.0351) acc 31.2500 (37.1875) lr 1.4955e-03 eta 0:24:24
epoch [69/200] batch [15/50] time 0.084 (0.176) data 0.000 (0.092) loss 1.5654 (2.0353) acc 65.6250 (39.3750) lr 1.4955e-03 eta 0:19:19
epoch [69/200] batch [20/50] time 0.086 (0.163) data 0.000 (0.079) loss 1.9385 (2.0422) acc 43.7500 (38.1250) lr 1.4955e-03 eta 0:17:51
epoch [69/200] batch [25/50] time 0.162 (0.150) data 0.078 (0.066) loss 1.7236 (2.0121) acc 56.2500 (39.6250) lr 1.4955e-03 eta 0:16:28
epoch [69/200] batch [30/50] time 0.086 (0.142) data 0.000 (0.058) loss 2.2402 (2.0184) acc 31.2500 (39.2708) lr 1.4955e-03 eta 0:15:35
epoch [69/200] batch [35/50] time 0.086 (0.139) data 0.001 (0.055) loss 1.9824 (1.9900) acc 34.3750 (39.9107) lr 1.4955e-03 eta 0:15:14
epoch [69/200] batch [40/50] time 0.083 (0.132) data 0.000 (0.048) loss 2.4727 (2.0292) acc 21.8750 (39.1406) lr 1.4955e-03 eta 0:14:28
epoch [69/200] batch [45/50] time 0.083 (0.132) data 0.000 (0.047) loss 2.5449 (2.0536) acc 34.3750 (38.8194) lr 1.4955e-03 eta 0:14:23
epoch [69/200] batch [50/50] time 0.084 (0.127) data 0.000 (0.043) loss 1.9648 (2.0382) acc 43.7500 (39.5625) lr 1.4818e-03 eta 0:13:51
epoch [70/200] batch [5/50] time 0.085 (0.289) data 0.000 (0.204) loss 1.8213 (1.9402) acc 59.3750 (45.6250) lr 1.4818e-03 eta 0:31:31
epoch [70/200] batch [10/50] time 0.084 (0.188) data 0.000 (0.102) loss 1.9912 (2.0229) acc 31.2500 (40.9375) lr 1.4818e-03 eta 0:20:26
epoch [70/200] batch [15/50] time 0.086 (0.167) data 0.000 (0.082) loss 1.9824 (1.9992) acc 25.0000 (39.5833) lr 1.4818e-03 eta 0:18:12
epoch [70/200] batch [20/50] time 0.085 (0.154) data 0.000 (0.069) loss 2.1191 (2.0017) acc 37.5000 (40.0000) lr 1.4818e-03 eta 0:16:46
epoch [70/200] batch [25/50] time 0.085 (0.140) data 0.000 (0.055) loss 2.1934 (2.0024) acc 34.3750 (40.1250) lr 1.4818e-03 eta 0:15:15
epoch [70/200] batch [30/50] time 0.084 (0.139) data 0.000 (0.054) loss 2.0547 (1.9890) acc 37.5000 (40.3125) lr 1.4818e-03 eta 0:15:09
epoch [70/200] batch [35/50] time 0.234 (0.136) data 0.150 (0.051) loss 1.9824 (1.9927) acc 46.8750 (40.6250) lr 1.4818e-03 eta 0:14:45
epoch [70/200] batch [40/50] time 0.084 (0.130) data 0.000 (0.045) loss 2.4121 (1.9948) acc 28.1250 (40.5469) lr 1.4818e-03 eta 0:14:03
epoch [70/200] batch [45/50] time 0.082 (0.126) data 0.000 (0.041) loss 1.7568 (1.9974) acc 37.5000 (40.4861) lr 1.4818e-03 eta 0:13:40
epoch [70/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.037) loss 1.9424 (2.0227) acc 50.0000 (40.0000) lr 1.4679e-03 eta 0:13:11
epoch [71/200] batch [5/50] time 0.085 (0.361) data 0.000 (0.275) loss 1.9688 (2.0229) acc 34.3750 (40.0000) lr 1.4679e-03 eta 0:39:04
epoch [71/200] batch [10/50] time 0.085 (0.223) data 0.001 (0.138) loss 1.6631 (1.9258) acc 62.5000 (42.8125) lr 1.4679e-03 eta 0:24:09
epoch [71/200] batch [15/50] time 0.086 (0.180) data 0.000 (0.094) loss 1.9004 (1.8689) acc 40.6250 (43.9583) lr 1.4679e-03 eta 0:19:25
epoch [71/200] batch [20/50] time 0.183 (0.164) data 0.098 (0.078) loss 1.6689 (1.8931) acc 53.1250 (42.9688) lr 1.4679e-03 eta 0:17:41
epoch [71/200] batch [25/50] time 0.086 (0.148) data 0.000 (0.063) loss 2.0762 (1.9379) acc 37.5000 (42.3750) lr 1.4679e-03 eta 0:15:58
epoch [71/200] batch [30/50] time 0.086 (0.145) data 0.000 (0.060) loss 1.8672 (1.9513) acc 46.8750 (41.7708) lr 1.4679e-03 eta 0:15:40
epoch [71/200] batch [35/50] time 0.084 (0.137) data 0.001 (0.052) loss 2.3047 (1.9892) acc 28.1250 (40.9821) lr 1.4679e-03 eta 0:14:44
epoch [71/200] batch [40/50] time 0.084 (0.134) data 0.000 (0.049) loss 2.2695 (2.0236) acc 46.8750 (40.7031) lr 1.4679e-03 eta 0:14:27
epoch [71/200] batch [45/50] time 0.083 (0.132) data 0.000 (0.047) loss 1.8457 (2.0226) acc 46.8750 (40.7639) lr 1.4679e-03 eta 0:14:10
epoch [71/200] batch [50/50] time 0.082 (0.127) data 0.000 (0.042) loss 1.9131 (2.0230) acc 46.8750 (40.6875) lr 1.4540e-03 eta 0:13:37
epoch [72/200] batch [5/50] time 0.454 (0.356) data 0.369 (0.271) loss 2.4160 (2.0285) acc 28.1250 (40.6250) lr 1.4540e-03 eta 0:38:13
epoch [72/200] batch [10/50] time 0.086 (0.221) data 0.001 (0.136) loss 2.0410 (2.0663) acc 43.7500 (43.4375) lr 1.4540e-03 eta 0:23:42
epoch [72/200] batch [15/50] time 0.086 (0.181) data 0.000 (0.096) loss 2.4258 (2.0762) acc 34.3750 (41.8750) lr 1.4540e-03 eta 0:19:27
epoch [72/200] batch [20/50] time 0.086 (0.158) data 0.001 (0.072) loss 1.9346 (2.0851) acc 31.2500 (39.6875) lr 1.4540e-03 eta 0:16:53
epoch [72/200] batch [25/50] time 0.085 (0.147) data 0.000 (0.062) loss 1.7764 (2.0248) acc 40.6250 (41.1250) lr 1.4540e-03 eta 0:15:45
epoch [72/200] batch [30/50] time 0.293 (0.144) data 0.209 (0.059) loss 2.1953 (2.0144) acc 28.1250 (41.4583) lr 1.4540e-03 eta 0:15:23
epoch [72/200] batch [35/50] time 0.084 (0.135) data 0.000 (0.050) loss 1.8867 (2.0085) acc 46.8750 (41.2500) lr 1.4540e-03 eta 0:14:28
epoch [72/200] batch [40/50] time 0.082 (0.132) data 0.000 (0.048) loss 2.2090 (2.0332) acc 34.3750 (40.4688) lr 1.4540e-03 eta 0:14:08
epoch [72/200] batch [45/50] time 0.082 (0.127) data 0.000 (0.042) loss 2.0527 (2.0426) acc 40.6250 (39.7222) lr 1.4540e-03 eta 0:13:32
epoch [72/200] batch [50/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.3906 (2.0472) acc 31.2500 (39.3750) lr 1.4399e-03 eta 0:13:17
epoch [73/200] batch [5/50] time 0.086 (0.298) data 0.001 (0.212) loss 2.1855 (2.0289) acc 31.2500 (38.1250) lr 1.4399e-03 eta 0:31:45
epoch [73/200] batch [10/50] time 0.086 (0.194) data 0.001 (0.108) loss 1.9443 (2.0236) acc 40.6250 (39.0625) lr 1.4399e-03 eta 0:20:36
epoch [73/200] batch [15/50] time 0.086 (0.158) data 0.000 (0.072) loss 1.9180 (1.9626) acc 56.2500 (41.6667) lr 1.4399e-03 eta 0:16:47
epoch [73/200] batch [20/50] time 0.086 (0.144) data 0.001 (0.058) loss 1.8066 (1.9364) acc 28.1250 (42.3438) lr 1.4399e-03 eta 0:15:16
epoch [73/200] batch [25/50] time 0.085 (0.135) data 0.000 (0.049) loss 1.9102 (1.9568) acc 37.5000 (41.7500) lr 1.4399e-03 eta 0:14:18
epoch [73/200] batch [30/50] time 0.086 (0.130) data 0.000 (0.045) loss 2.1875 (1.9507) acc 34.3750 (41.5625) lr 1.4399e-03 eta 0:13:47
epoch [73/200] batch [35/50] time 0.087 (0.128) data 0.001 (0.043) loss 2.3203 (1.9583) acc 34.3750 (41.5179) lr 1.4399e-03 eta 0:13:37
epoch [73/200] batch [40/50] time 0.084 (0.124) data 0.000 (0.038) loss 1.9092 (1.9649) acc 34.3750 (40.7812) lr 1.4399e-03 eta 0:13:06
epoch [73/200] batch [45/50] time 0.083 (0.122) data 0.000 (0.037) loss 2.1426 (1.9956) acc 40.6250 (40.0694) lr 1.4399e-03 eta 0:12:57
epoch [73/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.034) loss 1.5439 (1.9927) acc 50.0000 (40.0000) lr 1.4258e-03 eta 0:12:32
epoch [74/200] batch [5/50] time 0.085 (0.315) data 0.000 (0.229) loss 1.8467 (1.9350) acc 40.6250 (38.1250) lr 1.4258e-03 eta 0:33:20
epoch [74/200] batch [10/50] time 0.086 (0.210) data 0.000 (0.125) loss 2.2852 (2.0491) acc 43.7500 (40.0000) lr 1.4258e-03 eta 0:22:14
epoch [74/200] batch [15/50] time 0.086 (0.169) data 0.000 (0.083) loss 1.9678 (2.0250) acc 40.6250 (40.8333) lr 1.4258e-03 eta 0:17:49
epoch [74/200] batch [20/50] time 0.086 (0.156) data 0.001 (0.070) loss 2.3184 (2.0413) acc 31.2500 (40.3125) lr 1.4258e-03 eta 0:16:26
epoch [74/200] batch [25/50] time 0.083 (0.142) data 0.000 (0.056) loss 2.3438 (2.0389) acc 34.3750 (40.2500) lr 1.4258e-03 eta 0:14:54
epoch [74/200] batch [30/50] time 0.084 (0.136) data 0.000 (0.051) loss 1.8047 (2.0346) acc 56.2500 (40.0000) lr 1.4258e-03 eta 0:14:18
epoch [74/200] batch [35/50] time 0.084 (0.131) data 0.001 (0.046) loss 2.0977 (2.0287) acc 31.2500 (40.0893) lr 1.4258e-03 eta 0:13:44
epoch [74/200] batch [40/50] time 0.085 (0.125) data 0.000 (0.040) loss 1.8213 (2.0321) acc 46.8750 (39.8438) lr 1.4258e-03 eta 0:13:07
epoch [74/200] batch [45/50] time 0.083 (0.121) data 0.000 (0.036) loss 1.7324 (2.0343) acc 50.0000 (39.4444) lr 1.4258e-03 eta 0:12:40
epoch [74/200] batch [50/50] time 0.084 (0.117) data 0.000 (0.033) loss 1.9375 (2.0229) acc 43.7500 (40.2500) lr 1.4115e-03 eta 0:12:16
epoch [75/200] batch [5/50] time 0.086 (0.327) data 0.000 (0.240) loss 2.0234 (1.9527) acc 53.1250 (42.5000) lr 1.4115e-03 eta 0:34:16
epoch [75/200] batch [10/50] time 0.085 (0.217) data 0.000 (0.131) loss 2.0215 (2.0133) acc 40.6250 (40.0000) lr 1.4115e-03 eta 0:22:44
epoch [75/200] batch [15/50] time 0.086 (0.173) data 0.000 (0.088) loss 1.9600 (2.0352) acc 46.8750 (40.2083) lr 1.4115e-03 eta 0:18:09
epoch [75/200] batch [20/50] time 0.085 (0.158) data 0.000 (0.073) loss 2.0586 (2.0191) acc 34.3750 (41.7188) lr 1.4115e-03 eta 0:16:34
epoch [75/200] batch [25/50] time 0.281 (0.152) data 0.198 (0.066) loss 2.2988 (2.0732) acc 31.2500 (40.0000) lr 1.4115e-03 eta 0:15:51
epoch [75/200] batch [30/50] time 0.085 (0.141) data 0.000 (0.055) loss 1.8760 (2.0555) acc 50.0000 (40.0000) lr 1.4115e-03 eta 0:14:41
epoch [75/200] batch [35/50] time 0.084 (0.138) data 0.001 (0.053) loss 1.9326 (2.0707) acc 37.5000 (39.5536) lr 1.4115e-03 eta 0:14:25
epoch [75/200] batch [40/50] time 0.084 (0.131) data 0.000 (0.046) loss 2.7480 (2.0886) acc 34.3750 (38.9062) lr 1.4115e-03 eta 0:13:42
epoch [75/200] batch [45/50] time 0.083 (0.127) data 0.000 (0.043) loss 1.7178 (2.0710) acc 40.6250 (39.0278) lr 1.4115e-03 eta 0:13:16
epoch [75/200] batch [50/50] time 0.082 (0.123) data 0.000 (0.038) loss 2.3438 (2.0767) acc 21.8750 (38.5000) lr 1.3971e-03 eta 0:12:48
epoch [76/200] batch [5/50] time 0.083 (0.323) data 0.000 (0.238) loss 1.8037 (2.0951) acc 40.6250 (36.2500) lr 1.3971e-03 eta 0:33:34
epoch [76/200] batch [10/50] time 0.096 (0.204) data 0.012 (0.120) loss 2.3320 (2.1255) acc 31.2500 (35.9375) lr 1.3971e-03 eta 0:21:15
epoch [76/200] batch [15/50] time 0.083 (0.164) data 0.000 (0.080) loss 2.1309 (2.0682) acc 34.3750 (38.9583) lr 1.3971e-03 eta 0:17:02
epoch [76/200] batch [20/50] time 0.084 (0.149) data 0.000 (0.065) loss 1.9639 (2.0599) acc 40.6250 (40.1562) lr 1.3971e-03 eta 0:15:26
epoch [76/200] batch [25/50] time 0.084 (0.136) data 0.000 (0.052) loss 1.9824 (2.0644) acc 40.6250 (41.0000) lr 1.3971e-03 eta 0:14:05
epoch [76/200] batch [30/50] time 0.084 (0.136) data 0.000 (0.052) loss 1.9902 (2.0578) acc 56.2500 (40.9375) lr 1.3971e-03 eta 0:14:07
epoch [76/200] batch [35/50] time 0.085 (0.133) data 0.000 (0.049) loss 2.1094 (2.0616) acc 46.8750 (40.9821) lr 1.3971e-03 eta 0:13:44
epoch [76/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.043) loss 1.7461 (2.0402) acc 46.8750 (41.6406) lr 1.3971e-03 eta 0:13:05
epoch [76/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.041) loss 2.5059 (2.0422) acc 25.0000 (41.1111) lr 1.3971e-03 eta 0:12:50
epoch [76/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.8955 (2.0309) acc 53.1250 (41.3750) lr 1.3827e-03 eta 0:12:24
epoch [77/200] batch [5/50] time 0.087 (0.329) data 0.000 (0.244) loss 1.6914 (1.9643) acc 43.7500 (38.1250) lr 1.3827e-03 eta 0:34:00
epoch [77/200] batch [10/50] time 0.085 (0.211) data 0.001 (0.125) loss 2.0645 (2.0340) acc 43.7500 (38.1250) lr 1.3827e-03 eta 0:21:46
epoch [77/200] batch [15/50] time 0.086 (0.169) data 0.000 (0.084) loss 2.0059 (1.9913) acc 37.5000 (38.9583) lr 1.3827e-03 eta 0:17:26
epoch [77/200] batch [20/50] time 0.086 (0.154) data 0.000 (0.069) loss 1.9668 (1.9717) acc 34.3750 (37.5000) lr 1.3827e-03 eta 0:15:53
epoch [77/200] batch [25/50] time 0.086 (0.141) data 0.001 (0.055) loss 2.3438 (1.9918) acc 34.3750 (37.5000) lr 1.3827e-03 eta 0:14:28
epoch [77/200] batch [30/50] time 0.086 (0.133) data 0.000 (0.047) loss 1.7070 (1.9860) acc 53.1250 (39.6875) lr 1.3827e-03 eta 0:13:37
epoch [77/200] batch [35/50] time 0.304 (0.132) data 0.219 (0.047) loss 2.0449 (1.9970) acc 46.8750 (39.6429) lr 1.3827e-03 eta 0:13:35
epoch [77/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.0742 (2.0034) acc 40.6250 (39.9219) lr 1.3827e-03 eta 0:12:58
epoch [77/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.040) loss 1.9463 (2.0052) acc 40.6250 (40.2083) lr 1.3827e-03 eta 0:12:50
epoch [77/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.036) loss 1.6953 (1.9973) acc 46.8750 (40.6250) lr 1.3681e-03 eta 0:12:23
epoch [78/200] batch [5/50] time 0.085 (0.312) data 0.000 (0.227) loss 1.7715 (2.0709) acc 56.2500 (42.5000) lr 1.3681e-03 eta 0:31:57
epoch [78/200] batch [10/50] time 0.090 (0.199) data 0.005 (0.114) loss 1.8838 (2.0987) acc 53.1250 (41.8750) lr 1.3681e-03 eta 0:20:21
epoch [78/200] batch [15/50] time 0.085 (0.161) data 0.000 (0.076) loss 1.6816 (2.0019) acc 53.1250 (42.5000) lr 1.3681e-03 eta 0:16:27
epoch [78/200] batch [20/50] time 0.086 (0.142) data 0.000 (0.057) loss 2.0762 (1.9970) acc 43.7500 (42.5000) lr 1.3681e-03 eta 0:14:31
epoch [78/200] batch [25/50] time 0.085 (0.131) data 0.000 (0.046) loss 2.1113 (2.0366) acc 37.5000 (42.1250) lr 1.3681e-03 eta 0:13:21
epoch [78/200] batch [30/50] time 0.086 (0.123) data 0.000 (0.038) loss 1.7559 (2.0122) acc 43.7500 (42.9167) lr 1.3681e-03 eta 0:12:34
epoch [78/200] batch [35/50] time 0.085 (0.118) data 0.000 (0.033) loss 1.9336 (2.0025) acc 37.5000 (42.6786) lr 1.3681e-03 eta 0:12:00
epoch [78/200] batch [40/50] time 0.083 (0.114) data 0.000 (0.029) loss 2.1309 (2.0067) acc 40.6250 (42.4219) lr 1.3681e-03 eta 0:11:34
epoch [78/200] batch [45/50] time 0.084 (0.110) data 0.000 (0.026) loss 1.9971 (2.0120) acc 31.2500 (41.8750) lr 1.3681e-03 eta 0:11:13
epoch [78/200] batch [50/50] time 0.083 (0.108) data 0.000 (0.023) loss 2.4160 (2.0122) acc 43.7500 (42.0625) lr 1.3535e-03 eta 0:10:56
epoch [79/200] batch [5/50] time 0.086 (0.302) data 0.000 (0.217) loss 1.8203 (1.9873) acc 34.3750 (35.0000) lr 1.3535e-03 eta 0:30:41
epoch [79/200] batch [10/50] time 0.171 (0.203) data 0.086 (0.117) loss 1.8652 (1.9678) acc 50.0000 (39.3750) lr 1.3535e-03 eta 0:20:33
epoch [79/200] batch [15/50] time 0.086 (0.168) data 0.000 (0.082) loss 2.3867 (1.9901) acc 31.2500 (39.1667) lr 1.3535e-03 eta 0:17:00
epoch [79/200] batch [20/50] time 0.086 (0.157) data 0.001 (0.072) loss 1.9834 (1.9866) acc 37.5000 (39.0625) lr 1.3535e-03 eta 0:15:54
epoch [79/200] batch [25/50] time 0.087 (0.143) data 0.000 (0.057) loss 1.8184 (1.9944) acc 43.7500 (38.8750) lr 1.3535e-03 eta 0:14:27
epoch [79/200] batch [30/50] time 0.086 (0.138) data 0.000 (0.052) loss 2.2012 (2.0212) acc 34.3750 (37.7083) lr 1.3535e-03 eta 0:13:54
epoch [79/200] batch [35/50] time 0.197 (0.133) data 0.113 (0.048) loss 1.8398 (2.0028) acc 40.6250 (38.9286) lr 1.3535e-03 eta 0:13:28
epoch [79/200] batch [40/50] time 0.087 (0.127) data 0.000 (0.042) loss 2.2578 (1.9940) acc 46.8750 (40.0000) lr 1.3535e-03 eta 0:12:51
epoch [79/200] batch [45/50] time 0.084 (0.125) data 0.000 (0.040) loss 1.6885 (2.0021) acc 50.0000 (39.7222) lr 1.3535e-03 eta 0:12:35
epoch [79/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.036) loss 1.8467 (1.9830) acc 34.3750 (40.0625) lr 1.3387e-03 eta 0:12:10
epoch [80/200] batch [5/50] time 0.090 (0.282) data 0.004 (0.196) loss 2.1855 (2.1180) acc 31.2500 (39.3750) lr 1.3387e-03 eta 0:28:26
epoch [80/200] batch [10/50] time 0.086 (0.196) data 0.001 (0.111) loss 2.0332 (1.9863) acc 40.6250 (41.2500) lr 1.3387e-03 eta 0:19:44
epoch [80/200] batch [15/50] time 0.091 (0.160) data 0.005 (0.074) loss 1.8828 (1.9778) acc 43.7500 (41.2500) lr 1.3387e-03 eta 0:16:04
epoch [80/200] batch [20/50] time 0.083 (0.144) data 0.000 (0.058) loss 2.0508 (1.9932) acc 46.8750 (42.0312) lr 1.3387e-03 eta 0:14:25
epoch [80/200] batch [25/50] time 0.083 (0.138) data 0.000 (0.054) loss 1.6396 (1.9834) acc 56.2500 (42.6250) lr 1.3387e-03 eta 0:13:53
epoch [80/200] batch [30/50] time 0.083 (0.130) data 0.000 (0.045) loss 2.2773 (1.9924) acc 34.3750 (42.0833) lr 1.3387e-03 eta 0:13:00
epoch [80/200] batch [35/50] time 0.085 (0.127) data 0.001 (0.043) loss 1.9404 (1.9791) acc 46.8750 (42.4107) lr 1.3387e-03 eta 0:12:44
epoch [80/200] batch [40/50] time 0.084 (0.124) data 0.000 (0.039) loss 2.1289 (1.9855) acc 28.1250 (42.0312) lr 1.3387e-03 eta 0:12:23
epoch [80/200] batch [45/50] time 0.132 (0.120) data 0.049 (0.036) loss 1.9482 (1.9829) acc 37.5000 (41.9444) lr 1.3387e-03 eta 0:12:02
epoch [80/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.034) loss 2.2520 (1.9949) acc 31.2500 (41.6875) lr 1.3239e-03 eta 0:11:48
epoch [81/200] batch [5/50] time 0.085 (0.316) data 0.000 (0.231) loss 1.8525 (1.8207) acc 50.0000 (45.6250) lr 1.3239e-03 eta 0:31:35
epoch [81/200] batch [10/50] time 0.086 (0.212) data 0.000 (0.126) loss 1.9482 (1.8011) acc 46.8750 (49.3750) lr 1.3239e-03 eta 0:21:09
epoch [81/200] batch [15/50] time 0.093 (0.170) data 0.000 (0.084) loss 1.7900 (1.8048) acc 34.3750 (48.7500) lr 1.3239e-03 eta 0:16:59
epoch [81/200] batch [20/50] time 0.085 (0.158) data 0.000 (0.072) loss 1.6934 (1.8387) acc 46.8750 (46.7188) lr 1.3239e-03 eta 0:15:46
epoch [81/200] batch [25/50] time 0.218 (0.149) data 0.133 (0.063) loss 1.9863 (1.8899) acc 37.5000 (44.3750) lr 1.3239e-03 eta 0:14:49
epoch [81/200] batch [30/50] time 0.086 (0.138) data 0.000 (0.053) loss 1.7295 (1.9056) acc 50.0000 (43.9583) lr 1.3239e-03 eta 0:13:45
epoch [81/200] batch [35/50] time 0.088 (0.137) data 0.001 (0.051) loss 1.6621 (1.9228) acc 40.6250 (42.7679) lr 1.3239e-03 eta 0:13:37
epoch [81/200] batch [40/50] time 0.087 (0.131) data 0.000 (0.045) loss 1.9766 (1.9455) acc 34.3750 (42.0312) lr 1.3239e-03 eta 0:12:58
epoch [81/200] batch [45/50] time 0.082 (0.127) data 0.000 (0.041) loss 1.8096 (1.9435) acc 43.7500 (41.5972) lr 1.3239e-03 eta 0:12:34
epoch [81/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.037) loss 2.0254 (1.9527) acc 40.6250 (41.2500) lr 1.3090e-03 eta 0:12:08
epoch [82/200] batch [5/50] time 0.087 (0.316) data 0.001 (0.223) loss 2.0742 (2.0869) acc 40.6250 (37.5000) lr 1.3090e-03 eta 0:31:17
epoch [82/200] batch [10/50] time 0.087 (0.201) data 0.001 (0.112) loss 1.9277 (2.0158) acc 37.5000 (39.0625) lr 1.3090e-03 eta 0:19:53
epoch [82/200] batch [15/50] time 0.089 (0.174) data 0.000 (0.086) loss 1.9453 (2.0546) acc 31.2500 (39.5833) lr 1.3090e-03 eta 0:17:12
epoch [82/200] batch [20/50] time 0.235 (0.159) data 0.151 (0.072) loss 2.0586 (2.0455) acc 43.7500 (40.3125) lr 1.3090e-03 eta 0:15:44
epoch [82/200] batch [25/50] time 0.084 (0.145) data 0.000 (0.058) loss 1.6299 (2.0145) acc 56.2500 (41.2500) lr 1.3090e-03 eta 0:14:16
epoch [82/200] batch [30/50] time 0.084 (0.140) data 0.000 (0.054) loss 1.7695 (2.0126) acc 40.6250 (40.6250) lr 1.3090e-03 eta 0:13:49
epoch [82/200] batch [35/50] time 0.085 (0.132) data 0.000 (0.046) loss 2.2129 (2.0323) acc 31.2500 (40.5357) lr 1.3090e-03 eta 0:13:02
epoch [82/200] batch [40/50] time 0.085 (0.129) data 0.000 (0.043) loss 2.0098 (2.0213) acc 43.7500 (40.7031) lr 1.3090e-03 eta 0:12:43
epoch [82/200] batch [45/50] time 0.083 (0.127) data 0.000 (0.042) loss 1.9961 (2.0034) acc 46.8750 (40.4167) lr 1.3090e-03 eta 0:12:31
epoch [82/200] batch [50/50] time 0.082 (0.123) data 0.000 (0.038) loss 1.9688 (2.0134) acc 37.5000 (40.1875) lr 1.2940e-03 eta 0:12:04
epoch [83/200] batch [5/50] time 0.085 (0.303) data 0.000 (0.218) loss 1.9170 (1.8432) acc 40.6250 (46.2500) lr 1.2940e-03 eta 0:29:47
epoch [83/200] batch [10/50] time 0.085 (0.195) data 0.000 (0.109) loss 1.7803 (1.9170) acc 40.6250 (44.0625) lr 1.2940e-03 eta 0:19:05
epoch [83/200] batch [15/50] time 0.086 (0.158) data 0.001 (0.073) loss 1.7754 (1.9099) acc 53.1250 (45.8333) lr 1.2940e-03 eta 0:15:31
epoch [83/200] batch [20/50] time 0.085 (0.140) data 0.000 (0.055) loss 2.3379 (1.9828) acc 34.3750 (44.2188) lr 1.2940e-03 eta 0:13:44
epoch [83/200] batch [25/50] time 0.097 (0.130) data 0.012 (0.044) loss 2.3457 (2.0025) acc 40.6250 (43.0000) lr 1.2940e-03 eta 0:12:42
epoch [83/200] batch [30/50] time 0.085 (0.123) data 0.000 (0.037) loss 2.8809 (2.0210) acc 25.0000 (42.5000) lr 1.2940e-03 eta 0:11:59
epoch [83/200] batch [35/50] time 0.086 (0.121) data 0.001 (0.036) loss 1.8262 (2.0244) acc 37.5000 (41.9643) lr 1.2940e-03 eta 0:11:51
epoch [83/200] batch [40/50] time 0.083 (0.120) data 0.000 (0.035) loss 1.5156 (2.0058) acc 50.0000 (42.2656) lr 1.2940e-03 eta 0:11:44
epoch [83/200] batch [45/50] time 0.083 (0.116) data 0.000 (0.031) loss 2.3340 (2.0167) acc 21.8750 (41.9444) lr 1.2940e-03 eta 0:11:20
epoch [83/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.031) loss 1.9561 (2.0123) acc 37.5000 (41.5000) lr 1.2790e-03 eta 0:11:17
epoch [84/200] batch [5/50] time 0.086 (0.320) data 0.001 (0.234) loss 1.9648 (1.9004) acc 28.1250 (40.0000) lr 1.2790e-03 eta 0:31:08
epoch [84/200] batch [10/50] time 0.085 (0.219) data 0.000 (0.134) loss 1.4189 (1.8557) acc 53.1250 (44.6875) lr 1.2790e-03 eta 0:21:19
epoch [84/200] batch [15/50] time 0.086 (0.175) data 0.000 (0.090) loss 2.0449 (1.8645) acc 34.3750 (44.5833) lr 1.2790e-03 eta 0:16:58
epoch [84/200] batch [20/50] time 0.086 (0.162) data 0.000 (0.077) loss 1.9678 (1.8829) acc 46.8750 (43.2812) lr 1.2790e-03 eta 0:15:42
epoch [84/200] batch [25/50] time 0.252 (0.154) data 0.168 (0.068) loss 2.2676 (1.9205) acc 40.6250 (43.1250) lr 1.2790e-03 eta 0:14:54
epoch [84/200] batch [30/50] time 0.086 (0.142) data 0.000 (0.057) loss 1.9678 (1.9286) acc 34.3750 (43.2292) lr 1.2790e-03 eta 0:13:47
epoch [84/200] batch [35/50] time 0.087 (0.141) data 0.001 (0.055) loss 2.4141 (1.9595) acc 40.6250 (42.4107) lr 1.2790e-03 eta 0:13:38
epoch [84/200] batch [40/50] time 0.084 (0.134) data 0.000 (0.049) loss 2.2324 (1.9738) acc 34.3750 (42.0312) lr 1.2790e-03 eta 0:12:56
epoch [84/200] batch [45/50] time 0.084 (0.128) data 0.000 (0.043) loss 2.0840 (1.9892) acc 34.3750 (41.8750) lr 1.2790e-03 eta 0:12:23
epoch [84/200] batch [50/50] time 0.082 (0.124) data 0.000 (0.039) loss 2.2480 (1.9928) acc 37.5000 (41.6875) lr 1.2639e-03 eta 0:11:56
epoch [85/200] batch [5/50] time 0.085 (0.297) data 0.000 (0.212) loss 1.8506 (1.8805) acc 37.5000 (42.5000) lr 1.2639e-03 eta 0:28:40
epoch [85/200] batch [10/50] time 0.087 (0.197) data 0.000 (0.112) loss 2.2754 (1.9938) acc 34.3750 (39.3750) lr 1.2639e-03 eta 0:18:59
epoch [85/200] batch [15/50] time 0.085 (0.165) data 0.000 (0.080) loss 1.8516 (2.0599) acc 56.2500 (40.8333) lr 1.2639e-03 eta 0:15:52
epoch [85/200] batch [20/50] time 0.095 (0.150) data 0.012 (0.065) loss 2.0352 (2.0132) acc 40.6250 (41.2500) lr 1.2639e-03 eta 0:14:28
epoch [85/200] batch [25/50] time 0.283 (0.145) data 0.200 (0.060) loss 2.2070 (1.9890) acc 31.2500 (41.8750) lr 1.2639e-03 eta 0:13:57
epoch [85/200] batch [30/50] time 0.085 (0.135) data 0.000 (0.050) loss 2.1484 (1.9983) acc 43.7500 (42.1875) lr 1.2639e-03 eta 0:12:58
epoch [85/200] batch [35/50] time 0.095 (0.129) data 0.012 (0.044) loss 2.2246 (2.0043) acc 28.1250 (41.6071) lr 1.2639e-03 eta 0:12:22
epoch [85/200] batch [40/50] time 0.084 (0.124) data 0.000 (0.040) loss 1.7217 (2.0139) acc 43.7500 (41.5625) lr 1.2639e-03 eta 0:11:55
epoch [85/200] batch [45/50] time 0.084 (0.122) data 0.000 (0.037) loss 1.4932 (1.9862) acc 43.7500 (41.5278) lr 1.2639e-03 eta 0:11:39
epoch [85/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.034) loss 2.0020 (1.9845) acc 28.1250 (41.2500) lr 1.2487e-03 eta 0:11:16
epoch [86/200] batch [5/50] time 0.086 (0.318) data 0.000 (0.233) loss 1.8213 (1.9262) acc 28.1250 (36.2500) lr 1.2487e-03 eta 0:30:27
epoch [86/200] batch [10/50] time 0.086 (0.215) data 0.001 (0.130) loss 2.0176 (2.0465) acc 37.5000 (35.6250) lr 1.2487e-03 eta 0:20:36
epoch [86/200] batch [15/50] time 0.086 (0.172) data 0.000 (0.087) loss 2.1484 (1.9934) acc 31.2500 (38.3333) lr 1.2487e-03 eta 0:16:26
epoch [86/200] batch [20/50] time 0.086 (0.164) data 0.001 (0.079) loss 2.1348 (1.9996) acc 31.2500 (38.4375) lr 1.2487e-03 eta 0:15:40
epoch [86/200] batch [25/50] time 0.128 (0.150) data 0.043 (0.065) loss 1.8145 (1.9750) acc 56.2500 (40.0000) lr 1.2487e-03 eta 0:14:19
epoch [86/200] batch [30/50] time 0.085 (0.142) data 0.000 (0.057) loss 1.9902 (1.9934) acc 46.8750 (39.8958) lr 1.2487e-03 eta 0:13:33
epoch [86/200] batch [35/50] time 0.085 (0.139) data 0.001 (0.053) loss 1.7441 (1.9995) acc 53.1250 (39.8214) lr 1.2487e-03 eta 0:13:12
epoch [86/200] batch [40/50] time 0.084 (0.132) data 0.000 (0.047) loss 1.9932 (2.0008) acc 46.8750 (40.3125) lr 1.2487e-03 eta 0:12:33
epoch [86/200] batch [45/50] time 0.084 (0.128) data 0.000 (0.043) loss 1.9424 (2.0275) acc 53.1250 (40.0000) lr 1.2487e-03 eta 0:12:09
epoch [86/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.3516 (2.0236) acc 34.3750 (39.8750) lr 1.2334e-03 eta 0:11:42
epoch [87/200] batch [5/50] time 0.086 (0.293) data 0.001 (0.208) loss 1.5664 (1.9773) acc 56.2500 (42.5000) lr 1.2334e-03 eta 0:27:48
epoch [87/200] batch [10/50] time 0.231 (0.204) data 0.148 (0.119) loss 2.0938 (1.9501) acc 50.0000 (44.0625) lr 1.2334e-03 eta 0:19:19
epoch [87/200] batch [15/50] time 0.085 (0.164) data 0.000 (0.079) loss 2.1367 (1.9792) acc 43.7500 (45.2083) lr 1.2334e-03 eta 0:15:32
epoch [87/200] batch [20/50] time 0.085 (0.148) data 0.000 (0.064) loss 2.1758 (1.9626) acc 31.2500 (45.0000) lr 1.2334e-03 eta 0:14:01
epoch [87/200] batch [25/50] time 0.084 (0.135) data 0.000 (0.051) loss 1.9473 (1.9712) acc 43.7500 (44.3750) lr 1.2334e-03 eta 0:12:48
epoch [87/200] batch [30/50] time 0.085 (0.127) data 0.000 (0.042) loss 2.2539 (2.0072) acc 28.1250 (42.3958) lr 1.2334e-03 eta 0:12:00
epoch [87/200] batch [35/50] time 0.086 (0.121) data 0.000 (0.036) loss 2.0586 (2.0171) acc 28.1250 (42.0536) lr 1.2334e-03 eta 0:11:26
epoch [87/200] batch [40/50] time 0.084 (0.117) data 0.000 (0.032) loss 2.1914 (2.0099) acc 50.0000 (42.1875) lr 1.2334e-03 eta 0:10:59
epoch [87/200] batch [45/50] time 0.083 (0.113) data 0.000 (0.028) loss 1.9746 (2.0130) acc 34.3750 (41.9444) lr 1.2334e-03 eta 0:10:38
epoch [87/200] batch [50/50] time 0.083 (0.110) data 0.000 (0.026) loss 2.3242 (2.0165) acc 28.1250 (41.7500) lr 1.2181e-03 eta 0:10:21
epoch [88/200] batch [5/50] time 0.085 (0.327) data 0.000 (0.241) loss 1.5508 (2.0090) acc 37.5000 (40.0000) lr 1.2181e-03 eta 0:30:45
epoch [88/200] batch [10/50] time 0.086 (0.213) data 0.000 (0.127) loss 2.1758 (2.0227) acc 40.6250 (42.5000) lr 1.2181e-03 eta 0:19:59
epoch [88/200] batch [15/50] time 0.085 (0.170) data 0.000 (0.085) loss 1.8037 (2.0111) acc 53.1250 (41.4583) lr 1.2181e-03 eta 0:15:58
epoch [88/200] batch [20/50] time 0.085 (0.155) data 0.000 (0.069) loss 1.9424 (1.9763) acc 50.0000 (43.7500) lr 1.2181e-03 eta 0:14:30
epoch [88/200] batch [25/50] time 0.242 (0.147) data 0.157 (0.062) loss 2.6094 (2.0055) acc 34.3750 (42.3750) lr 1.2181e-03 eta 0:13:47
epoch [88/200] batch [30/50] time 0.091 (0.137) data 0.006 (0.052) loss 2.4355 (2.0383) acc 25.0000 (40.8333) lr 1.2181e-03 eta 0:12:50
epoch [88/200] batch [35/50] time 0.085 (0.132) data 0.000 (0.046) loss 2.2051 (2.0321) acc 43.7500 (41.2500) lr 1.2181e-03 eta 0:12:18
epoch [88/200] batch [40/50] time 0.085 (0.126) data 0.000 (0.041) loss 2.0547 (2.0294) acc 43.7500 (42.1875) lr 1.2181e-03 eta 0:11:44
epoch [88/200] batch [45/50] time 0.084 (0.124) data 0.000 (0.039) loss 2.3574 (2.0514) acc 40.6250 (41.5972) lr 1.2181e-03 eta 0:11:32
epoch [88/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.035) loss 1.9873 (2.0555) acc 34.3750 (41.0000) lr 1.2028e-03 eta 0:11:09
epoch [89/200] batch [5/50] time 0.085 (0.306) data 0.000 (0.221) loss 1.8066 (2.0316) acc 46.8750 (43.1250) lr 1.2028e-03 eta 0:28:31
epoch [89/200] batch [10/50] time 0.086 (0.196) data 0.001 (0.111) loss 2.3125 (1.9583) acc 31.2500 (42.1875) lr 1.2028e-03 eta 0:18:14
epoch [89/200] batch [15/50] time 0.086 (0.159) data 0.000 (0.074) loss 2.2207 (1.9761) acc 28.1250 (41.8750) lr 1.2028e-03 eta 0:14:49
epoch [89/200] batch [20/50] time 0.085 (0.142) data 0.000 (0.056) loss 2.1250 (1.9839) acc 37.5000 (42.0312) lr 1.2028e-03 eta 0:13:10
epoch [89/200] batch [25/50] time 0.085 (0.130) data 0.000 (0.045) loss 1.7520 (2.0155) acc 43.7500 (41.3750) lr 1.2028e-03 eta 0:12:06
epoch [89/200] batch [30/50] time 0.109 (0.129) data 0.024 (0.043) loss 2.2637 (2.0063) acc 34.3750 (42.0833) lr 1.2028e-03 eta 0:11:55
epoch [89/200] batch [35/50] time 0.226 (0.126) data 0.141 (0.041) loss 2.1367 (2.0057) acc 31.2500 (41.6964) lr 1.2028e-03 eta 0:11:43
epoch [89/200] batch [40/50] time 0.084 (0.122) data 0.000 (0.037) loss 1.4160 (1.9925) acc 46.8750 (41.5625) lr 1.2028e-03 eta 0:11:17
epoch [89/200] batch [45/50] time 0.082 (0.118) data 0.000 (0.033) loss 2.1992 (2.0105) acc 34.3750 (40.9722) lr 1.2028e-03 eta 0:10:53
epoch [89/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.034) loss 1.5977 (1.9932) acc 46.8750 (41.3125) lr 1.1874e-03 eta 0:10:56
epoch [90/200] batch [5/50] time 0.086 (0.293) data 0.000 (0.207) loss 1.6367 (1.7582) acc 62.5000 (50.6250) lr 1.1874e-03 eta 0:27:05
epoch [90/200] batch [10/50] time 0.086 (0.194) data 0.000 (0.108) loss 2.1562 (1.8849) acc 40.6250 (46.8750) lr 1.1874e-03 eta 0:17:53
epoch [90/200] batch [15/50] time 0.086 (0.161) data 0.000 (0.076) loss 1.5918 (1.8604) acc 56.2500 (46.0417) lr 1.1874e-03 eta 0:14:52
epoch [90/200] batch [20/50] time 0.086 (0.151) data 0.000 (0.065) loss 1.9629 (1.9194) acc 40.6250 (44.5312) lr 1.1874e-03 eta 0:13:53
epoch [90/200] batch [25/50] time 0.085 (0.138) data 0.000 (0.052) loss 2.0273 (1.9374) acc 40.6250 (44.0000) lr 1.1874e-03 eta 0:12:40
epoch [90/200] batch [30/50] time 0.086 (0.135) data 0.000 (0.050) loss 2.0078 (1.9403) acc 28.1250 (43.1250) lr 1.1874e-03 eta 0:12:27
epoch [90/200] batch [35/50] time 0.190 (0.131) data 0.105 (0.046) loss 2.0273 (1.9622) acc 43.7500 (42.8571) lr 1.1874e-03 eta 0:12:04
epoch [90/200] batch [40/50] time 0.085 (0.126) data 0.000 (0.040) loss 2.1602 (1.9920) acc 31.2500 (41.7188) lr 1.1874e-03 eta 0:11:32
epoch [90/200] batch [45/50] time 0.084 (0.124) data 0.000 (0.039) loss 2.4082 (2.0016) acc 25.0000 (42.0139) lr 1.1874e-03 eta 0:11:24
epoch [90/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.035) loss 2.2090 (2.0146) acc 40.6250 (41.4375) lr 1.1719e-03 eta 0:11:01
epoch [91/200] batch [5/50] time 0.086 (0.335) data 0.000 (0.250) loss 2.0742 (1.8143) acc 40.6250 (40.0000) lr 1.1719e-03 eta 0:30:38
epoch [91/200] batch [10/50] time 0.085 (0.210) data 0.000 (0.125) loss 1.8359 (1.8138) acc 46.8750 (44.6875) lr 1.1719e-03 eta 0:19:14
epoch [91/200] batch [15/50] time 0.085 (0.169) data 0.000 (0.083) loss 1.9102 (1.8648) acc 37.5000 (43.9583) lr 1.1719e-03 eta 0:15:25
epoch [91/200] batch [20/50] time 0.086 (0.154) data 0.000 (0.069) loss 2.4863 (1.9176) acc 34.3750 (43.4375) lr 1.1719e-03 eta 0:14:05
epoch [91/200] batch [25/50] time 0.085 (0.141) data 0.000 (0.055) loss 1.8906 (1.9300) acc 50.0000 (42.8750) lr 1.1719e-03 eta 0:12:49
epoch [91/200] batch [30/50] time 0.085 (0.137) data 0.000 (0.052) loss 1.7500 (1.9577) acc 43.7500 (42.1875) lr 1.1719e-03 eta 0:12:31
epoch [91/200] batch [35/50] time 0.085 (0.134) data 0.001 (0.049) loss 2.0000 (1.9885) acc 31.2500 (40.9821) lr 1.1719e-03 eta 0:12:13
epoch [91/200] batch [40/50] time 0.086 (0.128) data 0.000 (0.043) loss 2.3398 (2.0121) acc 40.6250 (40.7812) lr 1.1719e-03 eta 0:11:39
epoch [91/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.040) loss 1.9053 (2.0054) acc 40.6250 (40.0694) lr 1.1719e-03 eta 0:11:21
epoch [91/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.036) loss 2.2656 (2.0173) acc 37.5000 (39.8125) lr 1.1564e-03 eta 0:10:57
epoch [92/200] batch [5/50] time 0.084 (0.321) data 0.000 (0.237) loss 2.0469 (1.9404) acc 43.7500 (41.8750) lr 1.1564e-03 eta 0:29:09
epoch [92/200] batch [10/50] time 0.085 (0.217) data 0.000 (0.133) loss 2.2461 (1.9959) acc 28.1250 (40.0000) lr 1.1564e-03 eta 0:19:40
epoch [92/200] batch [15/50] time 0.085 (0.173) data 0.000 (0.089) loss 1.8438 (1.9924) acc 46.8750 (42.7083) lr 1.1564e-03 eta 0:15:39
epoch [92/200] batch [20/50] time 0.085 (0.161) data 0.000 (0.077) loss 1.5527 (1.9823) acc 59.3750 (41.4062) lr 1.1564e-03 eta 0:14:35
epoch [92/200] batch [25/50] time 0.241 (0.152) data 0.158 (0.068) loss 1.7119 (1.9845) acc 46.8750 (41.3750) lr 1.1564e-03 eta 0:13:45
epoch [92/200] batch [30/50] time 0.086 (0.141) data 0.001 (0.057) loss 1.6943 (1.9727) acc 53.1250 (41.7708) lr 1.1564e-03 eta 0:12:44
epoch [92/200] batch [35/50] time 0.084 (0.139) data 0.000 (0.055) loss 1.9766 (1.9809) acc 50.0000 (41.6071) lr 1.1564e-03 eta 0:12:34
epoch [92/200] batch [40/50] time 0.084 (0.132) data 0.000 (0.048) loss 2.3145 (1.9885) acc 34.3750 (41.4844) lr 1.1564e-03 eta 0:11:56
epoch [92/200] batch [45/50] time 0.082 (0.129) data 0.000 (0.045) loss 2.1699 (1.9852) acc 40.6250 (42.0139) lr 1.1564e-03 eta 0:11:36
epoch [92/200] batch [50/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.2559 (1.9911) acc 37.5000 (41.6250) lr 1.1409e-03 eta 0:11:10
epoch [93/200] batch [5/50] time 0.255 (0.301) data 0.169 (0.216) loss 2.2344 (2.0273) acc 43.7500 (40.0000) lr 1.1409e-03 eta 0:27:06
epoch [93/200] batch [10/50] time 0.085 (0.193) data 0.001 (0.108) loss 2.1270 (2.0381) acc 37.5000 (38.1250) lr 1.1409e-03 eta 0:17:22
epoch [93/200] batch [15/50] time 0.085 (0.160) data 0.000 (0.075) loss 2.1836 (2.0215) acc 37.5000 (41.0417) lr 1.1409e-03 eta 0:14:21
epoch [93/200] batch [20/50] time 0.084 (0.141) data 0.000 (0.056) loss 2.0000 (1.9915) acc 34.3750 (40.9375) lr 1.1409e-03 eta 0:12:39
epoch [93/200] batch [25/50] time 0.084 (0.134) data 0.000 (0.049) loss 1.6328 (1.9920) acc 50.0000 (40.7500) lr 1.1409e-03 eta 0:12:00
epoch [93/200] batch [30/50] time 0.084 (0.133) data 0.000 (0.048) loss 2.1367 (1.9908) acc 28.1250 (40.4167) lr 1.1409e-03 eta 0:11:52
epoch [93/200] batch [35/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.0137 (1.9909) acc 37.5000 (41.0714) lr 1.1409e-03 eta 0:11:14
epoch [93/200] batch [40/50] time 0.085 (0.125) data 0.000 (0.041) loss 1.8740 (1.9849) acc 46.8750 (41.7188) lr 1.1409e-03 eta 0:11:10
epoch [93/200] batch [45/50] time 0.256 (0.124) data 0.172 (0.040) loss 1.6279 (1.9692) acc 46.8750 (41.9444) lr 1.1409e-03 eta 0:11:05
epoch [93/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 2.0801 (1.9711) acc 43.7500 (41.8750) lr 1.1253e-03 eta 0:10:42
epoch [94/200] batch [5/50] time 0.085 (0.301) data 0.000 (0.215) loss 2.0723 (1.8426) acc 37.5000 (45.0000) lr 1.1253e-03 eta 0:26:46
epoch [94/200] batch [10/50] time 0.165 (0.201) data 0.080 (0.116) loss 2.0293 (1.8473) acc 31.2500 (43.4375) lr 1.1253e-03 eta 0:17:52
epoch [94/200] batch [15/50] time 0.086 (0.166) data 0.000 (0.081) loss 1.7285 (1.8465) acc 43.7500 (44.3750) lr 1.1253e-03 eta 0:14:45
epoch [94/200] batch [20/50] time 0.085 (0.155) data 0.000 (0.070) loss 2.0566 (1.8810) acc 34.3750 (42.1875) lr 1.1253e-03 eta 0:13:45
epoch [94/200] batch [25/50] time 0.086 (0.141) data 0.001 (0.056) loss 1.8555 (1.9041) acc 53.1250 (42.7500) lr 1.1253e-03 eta 0:12:30
epoch [94/200] batch [30/50] time 0.086 (0.135) data 0.000 (0.050) loss 1.7256 (1.9205) acc 53.1250 (42.5000) lr 1.1253e-03 eta 0:11:58
epoch [94/200] batch [35/50] time 0.086 (0.130) data 0.000 (0.045) loss 2.0840 (1.9285) acc 34.3750 (42.0536) lr 1.1253e-03 eta 0:11:29
epoch [94/200] batch [40/50] time 0.086 (0.124) data 0.000 (0.039) loss 1.6748 (1.9368) acc 50.0000 (42.4219) lr 1.1253e-03 eta 0:10:59
epoch [94/200] batch [45/50] time 0.082 (0.124) data 0.000 (0.039) loss 1.9766 (1.9403) acc 40.6250 (42.3611) lr 1.1253e-03 eta 0:10:59
epoch [94/200] batch [50/50] time 0.082 (0.120) data 0.000 (0.035) loss 2.1211 (1.9587) acc 46.8750 (42.1250) lr 1.1097e-03 eta 0:10:36
epoch [95/200] batch [5/50] time 0.086 (0.304) data 0.001 (0.218) loss 1.9688 (1.8687) acc 34.3750 (41.2500) lr 1.1097e-03 eta 0:26:47
epoch [95/200] batch [10/50] time 0.150 (0.201) data 0.065 (0.116) loss 2.0020 (1.9189) acc 37.5000 (39.6875) lr 1.1097e-03 eta 0:17:42
epoch [95/200] batch [15/50] time 0.085 (0.164) data 0.000 (0.079) loss 1.7217 (1.9171) acc 53.1250 (42.2917) lr 1.1097e-03 eta 0:14:25
epoch [95/200] batch [20/50] time 0.086 (0.153) data 0.000 (0.068) loss 2.3301 (1.9571) acc 37.5000 (41.7188) lr 1.1097e-03 eta 0:13:28
epoch [95/200] batch [25/50] time 0.085 (0.140) data 0.000 (0.055) loss 2.1895 (1.9923) acc 43.7500 (39.7500) lr 1.1097e-03 eta 0:12:16
epoch [95/200] batch [30/50] time 0.085 (0.135) data 0.000 (0.050) loss 1.8838 (1.9772) acc 37.5000 (40.5208) lr 1.1097e-03 eta 0:11:50
epoch [95/200] batch [35/50] time 0.086 (0.131) data 0.001 (0.046) loss 1.7344 (1.9818) acc 56.2500 (40.9821) lr 1.1097e-03 eta 0:11:28
epoch [95/200] batch [40/50] time 0.086 (0.125) data 0.000 (0.040) loss 1.4551 (1.9717) acc 50.0000 (41.2500) lr 1.1097e-03 eta 0:10:58
epoch [95/200] batch [45/50] time 0.084 (0.121) data 0.000 (0.036) loss 2.3965 (1.9606) acc 34.3750 (41.1111) lr 1.1097e-03 eta 0:10:34
epoch [95/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.032) loss 1.9854 (1.9729) acc 37.5000 (41.0000) lr 1.0941e-03 eta 0:10:14
epoch [96/200] batch [5/50] time 0.085 (0.304) data 0.000 (0.218) loss 2.0469 (1.8746) acc 40.6250 (43.7500) lr 1.0941e-03 eta 0:26:33
epoch [96/200] batch [10/50] time 0.086 (0.195) data 0.001 (0.109) loss 2.2324 (1.9563) acc 40.6250 (42.1875) lr 1.0941e-03 eta 0:17:00
epoch [96/200] batch [15/50] time 0.087 (0.168) data 0.000 (0.083) loss 1.6211 (1.9152) acc 56.2500 (43.3333) lr 1.0941e-03 eta 0:14:41
epoch [96/200] batch [20/50] time 0.085 (0.153) data 0.000 (0.068) loss 1.6348 (1.9048) acc 43.7500 (44.3750) lr 1.0941e-03 eta 0:13:20
epoch [96/200] batch [25/50] time 0.086 (0.140) data 0.000 (0.054) loss 2.1426 (1.9248) acc 31.2500 (43.7500) lr 1.0941e-03 eta 0:12:09
epoch [96/200] batch [30/50] time 0.086 (0.134) data 0.000 (0.048) loss 2.0020 (1.9310) acc 37.5000 (43.6458) lr 1.0941e-03 eta 0:11:37
epoch [96/200] batch [35/50] time 0.086 (0.127) data 0.001 (0.042) loss 1.6338 (1.9272) acc 56.2500 (43.8393) lr 1.0941e-03 eta 0:11:02
epoch [96/200] batch [40/50] time 0.084 (0.127) data 0.000 (0.042) loss 1.9502 (1.9296) acc 43.7500 (43.7500) lr 1.0941e-03 eta 0:11:00
epoch [96/200] batch [45/50] time 0.085 (0.124) data 0.000 (0.039) loss 2.0234 (1.9382) acc 34.3750 (43.0556) lr 1.0941e-03 eta 0:10:43
epoch [96/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.035) loss 1.8359 (1.9310) acc 46.8750 (43.0000) lr 1.0785e-03 eta 0:10:22
epoch [97/200] batch [5/50] time 0.085 (0.307) data 0.000 (0.222) loss 1.5225 (1.7957) acc 46.8750 (46.2500) lr 1.0785e-03 eta 0:26:33
epoch [97/200] batch [10/50] time 0.085 (0.197) data 0.001 (0.112) loss 1.7168 (1.8224) acc 53.1250 (46.5625) lr 1.0785e-03 eta 0:17:03
epoch [97/200] batch [15/50] time 0.086 (0.160) data 0.000 (0.075) loss 1.8643 (1.8750) acc 53.1250 (46.2500) lr 1.0785e-03 eta 0:13:50
epoch [97/200] batch [20/50] time 0.085 (0.149) data 0.000 (0.064) loss 1.9111 (1.8874) acc 37.5000 (45.9375) lr 1.0785e-03 eta 0:12:50
epoch [97/200] batch [25/50] time 0.335 (0.146) data 0.251 (0.061) loss 2.0176 (1.9101) acc 43.7500 (44.8750) lr 1.0785e-03 eta 0:12:36
epoch [97/200] batch [30/50] time 0.085 (0.136) data 0.000 (0.051) loss 2.0625 (1.9294) acc 40.6250 (44.5833) lr 1.0785e-03 eta 0:11:43
epoch [97/200] batch [35/50] time 0.085 (0.131) data 0.001 (0.047) loss 1.7451 (1.9563) acc 50.0000 (43.3929) lr 1.0785e-03 eta 0:11:19
epoch [97/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.4375 (1.9897) acc 34.3750 (42.6562) lr 1.0785e-03 eta 0:10:47
epoch [97/200] batch [45/50] time 0.082 (0.124) data 0.000 (0.039) loss 1.6543 (1.9697) acc 56.2500 (42.9861) lr 1.0785e-03 eta 0:10:39
epoch [97/200] batch [50/50] time 0.082 (0.120) data 0.000 (0.036) loss 1.9453 (1.9770) acc 37.5000 (42.2500) lr 1.0628e-03 eta 0:10:17
epoch [98/200] batch [5/50] time 0.086 (0.307) data 0.000 (0.221) loss 2.3418 (1.8146) acc 40.6250 (49.3750) lr 1.0628e-03 eta 0:26:19
epoch [98/200] batch [10/50] time 0.085 (0.196) data 0.000 (0.111) loss 1.9072 (1.8184) acc 46.8750 (46.8750) lr 1.0628e-03 eta 0:16:46
epoch [98/200] batch [15/50] time 0.085 (0.165) data 0.000 (0.080) loss 1.9355 (1.8967) acc 46.8750 (43.5417) lr 1.0628e-03 eta 0:14:08
epoch [98/200] batch [20/50] time 0.114 (0.147) data 0.028 (0.062) loss 1.7441 (1.9267) acc 50.0000 (42.8125) lr 1.0628e-03 eta 0:12:32
epoch [98/200] batch [25/50] time 0.085 (0.135) data 0.000 (0.050) loss 1.9053 (1.9343) acc 53.1250 (43.1250) lr 1.0628e-03 eta 0:11:31
epoch [98/200] batch [30/50] time 0.113 (0.132) data 0.028 (0.046) loss 1.5371 (1.9198) acc 50.0000 (43.6458) lr 1.0628e-03 eta 0:11:13
epoch [98/200] batch [35/50] time 0.086 (0.125) data 0.001 (0.040) loss 1.6152 (1.9031) acc 53.1250 (43.9286) lr 1.0628e-03 eta 0:10:38
epoch [98/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.038) loss 1.7969 (1.9222) acc 50.0000 (43.2031) lr 1.0628e-03 eta 0:10:28
epoch [98/200] batch [45/50] time 0.084 (0.119) data 0.000 (0.034) loss 1.7217 (1.9321) acc 43.7500 (42.4306) lr 1.0628e-03 eta 0:10:05
epoch [98/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.033) loss 2.3418 (1.9593) acc 34.3750 (41.6875) lr 1.0471e-03 eta 0:10:00
epoch [99/200] batch [5/50] time 0.085 (0.350) data 0.000 (0.265) loss 1.7734 (1.8811) acc 46.8750 (41.8750) lr 1.0471e-03 eta 0:29:42
epoch [99/200] batch [10/50] time 0.085 (0.226) data 0.001 (0.140) loss 1.9893 (1.8937) acc 40.6250 (42.1875) lr 1.0471e-03 eta 0:19:08
epoch [99/200] batch [15/50] time 0.085 (0.179) data 0.000 (0.094) loss 1.9141 (1.8247) acc 53.1250 (44.5833) lr 1.0471e-03 eta 0:15:09
epoch [99/200] batch [20/50] time 0.086 (0.162) data 0.000 (0.077) loss 2.3223 (1.8659) acc 31.2500 (43.7500) lr 1.0471e-03 eta 0:13:44
epoch [99/200] batch [25/50] time 0.249 (0.153) data 0.164 (0.068) loss 1.9424 (1.9092) acc 37.5000 (42.8750) lr 1.0471e-03 eta 0:12:58
epoch [99/200] batch [30/50] time 0.086 (0.142) data 0.001 (0.057) loss 1.5918 (1.8966) acc 50.0000 (43.1250) lr 1.0471e-03 eta 0:12:00
epoch [99/200] batch [35/50] time 0.087 (0.140) data 0.001 (0.054) loss 2.1660 (1.9265) acc 34.3750 (43.0357) lr 1.0471e-03 eta 0:11:47
epoch [99/200] batch [40/50] time 0.084 (0.133) data 0.000 (0.048) loss 2.1562 (1.9248) acc 40.6250 (43.4375) lr 1.0471e-03 eta 0:11:11
epoch [99/200] batch [45/50] time 0.083 (0.127) data 0.000 (0.042) loss 1.4287 (1.9307) acc 50.0000 (43.0556) lr 1.0471e-03 eta 0:10:43
epoch [99/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.038) loss 1.7217 (1.9433) acc 46.8750 (42.8750) lr 1.0314e-03 eta 0:10:20
epoch [100/200] batch [5/50] time 0.086 (0.316) data 0.000 (0.231) loss 2.2129 (1.9891) acc 31.2500 (40.0000) lr 1.0314e-03 eta 0:26:33
epoch [100/200] batch [10/50] time 0.086 (0.201) data 0.001 (0.116) loss 1.8066 (1.9995) acc 43.7500 (43.4375) lr 1.0314e-03 eta 0:16:52
epoch [100/200] batch [15/50] time 0.085 (0.170) data 0.000 (0.084) loss 1.9365 (1.9895) acc 43.7500 (43.9583) lr 1.0314e-03 eta 0:14:13
epoch [100/200] batch [20/50] time 0.086 (0.156) data 0.000 (0.071) loss 1.6758 (1.9760) acc 50.0000 (44.2188) lr 1.0314e-03 eta 0:13:06
epoch [100/200] batch [25/50] time 0.086 (0.142) data 0.000 (0.057) loss 1.9014 (1.9373) acc 40.6250 (44.6250) lr 1.0314e-03 eta 0:11:54
epoch [100/200] batch [30/50] time 0.086 (0.137) data 0.000 (0.051) loss 1.6152 (1.9437) acc 40.6250 (43.8542) lr 1.0314e-03 eta 0:11:26
epoch [100/200] batch [35/50] time 0.269 (0.134) data 0.183 (0.049) loss 1.8662 (1.9519) acc 43.7500 (43.3929) lr 1.0314e-03 eta 0:11:14
epoch [100/200] batch [40/50] time 0.084 (0.128) data 0.000 (0.043) loss 2.2539 (1.9583) acc 31.2500 (42.8125) lr 1.0314e-03 eta 0:10:42
epoch [100/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.040) loss 1.5898 (1.9758) acc 46.8750 (42.4306) lr 1.0314e-03 eta 0:10:26
epoch [100/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.036) loss 1.8730 (1.9865) acc 37.5000 (42.0625) lr 1.0157e-03 eta 0:10:04
epoch [101/200] batch [5/50] time 0.170 (0.300) data 0.086 (0.215) loss 1.7607 (1.8830) acc 43.7500 (46.2500) lr 1.0157e-03 eta 0:24:56
epoch [101/200] batch [10/50] time 0.086 (0.193) data 0.000 (0.108) loss 1.8086 (1.9494) acc 40.6250 (40.3125) lr 1.0157e-03 eta 0:16:02
epoch [101/200] batch [15/50] time 0.121 (0.160) data 0.035 (0.074) loss 1.9746 (1.9824) acc 31.2500 (40.4167) lr 1.0157e-03 eta 0:13:15
epoch [101/200] batch [20/50] time 0.089 (0.150) data 0.000 (0.065) loss 2.1133 (1.9519) acc 34.3750 (42.1875) lr 1.0157e-03 eta 0:12:28
epoch [101/200] batch [25/50] time 0.086 (0.139) data 0.000 (0.054) loss 2.3867 (1.9659) acc 21.8750 (41.7500) lr 1.0157e-03 eta 0:11:30
epoch [101/200] batch [30/50] time 0.086 (0.132) data 0.000 (0.047) loss 2.4277 (1.9841) acc 28.1250 (42.0833) lr 1.0157e-03 eta 0:10:57
epoch [101/200] batch [35/50] time 0.090 (0.128) data 0.005 (0.043) loss 1.9678 (1.9818) acc 53.1250 (42.6786) lr 1.0157e-03 eta 0:10:36
epoch [101/200] batch [40/50] time 0.084 (0.127) data 0.000 (0.042) loss 1.9365 (1.9756) acc 43.7500 (42.3438) lr 1.0157e-03 eta 0:10:30
epoch [101/200] batch [45/50] time 0.083 (0.122) data 0.000 (0.038) loss 1.9072 (1.9565) acc 34.3750 (42.3611) lr 1.0157e-03 eta 0:10:05
epoch [101/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.034) loss 1.9727 (1.9666) acc 40.6250 (42.3750) lr 1.0000e-03 eta 0:09:48
epoch [102/200] batch [5/50] time 0.087 (0.317) data 0.000 (0.232) loss 1.7480 (1.8115) acc 46.8750 (40.0000) lr 1.0000e-03 eta 0:26:08
epoch [102/200] batch [10/50] time 0.085 (0.202) data 0.000 (0.116) loss 1.3828 (1.8655) acc 59.3750 (42.1875) lr 1.0000e-03 eta 0:16:35
epoch [102/200] batch [15/50] time 0.086 (0.163) data 0.000 (0.078) loss 2.0547 (1.9513) acc 50.0000 (41.2500) lr 1.0000e-03 eta 0:13:23
epoch [102/200] batch [20/50] time 0.084 (0.143) data 0.000 (0.058) loss 2.0020 (1.9409) acc 43.7500 (41.8750) lr 1.0000e-03 eta 0:11:46
epoch [102/200] batch [25/50] time 0.085 (0.138) data 0.000 (0.053) loss 2.3926 (1.9813) acc 31.2500 (41.7500) lr 1.0000e-03 eta 0:11:18
epoch [102/200] batch [30/50] time 0.207 (0.133) data 0.123 (0.048) loss 1.7803 (1.9978) acc 40.6250 (41.8750) lr 1.0000e-03 eta 0:10:54
epoch [102/200] batch [35/50] time 0.086 (0.126) data 0.000 (0.041) loss 2.2695 (1.9833) acc 43.7500 (41.5179) lr 1.0000e-03 eta 0:10:20
epoch [102/200] batch [40/50] time 0.086 (0.123) data 0.000 (0.038) loss 1.9688 (1.9736) acc 34.3750 (41.7188) lr 1.0000e-03 eta 0:10:03
epoch [102/200] batch [45/50] time 0.083 (0.119) data 0.000 (0.034) loss 2.0840 (1.9783) acc 31.2500 (41.4583) lr 1.0000e-03 eta 0:09:42
epoch [102/200] batch [50/50] time 0.082 (0.118) data 0.000 (0.034) loss 2.0996 (1.9659) acc 40.6250 (41.7500) lr 9.8429e-04 eta 0:09:40
epoch [103/200] batch [5/50] time 0.086 (0.318) data 0.001 (0.233) loss 2.2832 (2.1316) acc 28.1250 (38.7500) lr 9.8429e-04 eta 0:25:54
epoch [103/200] batch [10/50] time 0.280 (0.221) data 0.196 (0.136) loss 1.3330 (2.0466) acc 65.6250 (41.8750) lr 9.8429e-04 eta 0:18:00
epoch [103/200] batch [15/50] time 0.085 (0.176) data 0.000 (0.091) loss 1.6748 (2.0354) acc 56.2500 (41.8750) lr 9.8429e-04 eta 0:14:18
epoch [103/200] batch [20/50] time 0.086 (0.159) data 0.000 (0.074) loss 1.5488 (2.0096) acc 46.8750 (42.5000) lr 9.8429e-04 eta 0:12:56
epoch [103/200] batch [25/50] time 0.086 (0.146) data 0.000 (0.061) loss 1.9004 (1.9669) acc 43.7500 (43.2500) lr 9.8429e-04 eta 0:11:50
epoch [103/200] batch [30/50] time 0.086 (0.141) data 0.000 (0.056) loss 2.1758 (1.9843) acc 31.2500 (43.6458) lr 9.8429e-04 eta 0:11:25
epoch [103/200] batch [35/50] time 0.085 (0.134) data 0.001 (0.049) loss 1.6416 (1.9768) acc 50.0000 (43.6607) lr 9.8429e-04 eta 0:10:50
epoch [103/200] batch [40/50] time 0.084 (0.130) data 0.000 (0.045) loss 1.9033 (1.9638) acc 46.8750 (44.0625) lr 9.8429e-04 eta 0:10:31
epoch [103/200] batch [45/50] time 0.213 (0.128) data 0.130 (0.043) loss 1.7695 (1.9632) acc 56.2500 (44.0972) lr 9.8429e-04 eta 0:10:20
epoch [103/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.039) loss 1.5410 (1.9690) acc 50.0000 (43.4375) lr 9.6859e-04 eta 0:09:57
epoch [104/200] batch [5/50] time 0.084 (0.320) data 0.000 (0.236) loss 1.3838 (1.7082) acc 59.3750 (46.2500) lr 9.6859e-04 eta 0:25:51
epoch [104/200] batch [10/50] time 0.084 (0.202) data 0.000 (0.118) loss 1.8965 (1.7925) acc 50.0000 (43.7500) lr 9.6859e-04 eta 0:16:18
epoch [104/200] batch [15/50] time 0.085 (0.163) data 0.000 (0.079) loss 2.6387 (1.8972) acc 31.2500 (43.3333) lr 9.6859e-04 eta 0:13:08
epoch [104/200] batch [20/50] time 0.085 (0.143) data 0.000 (0.059) loss 1.7861 (1.8781) acc 43.7500 (45.0000) lr 9.6859e-04 eta 0:11:32
epoch [104/200] batch [25/50] time 0.235 (0.138) data 0.151 (0.053) loss 1.9814 (1.8743) acc 31.2500 (44.2500) lr 9.6859e-04 eta 0:11:03
epoch [104/200] batch [30/50] time 0.084 (0.129) data 0.000 (0.045) loss 2.1934 (1.8725) acc 28.1250 (43.8542) lr 9.6859e-04 eta 0:10:20
epoch [104/200] batch [35/50] time 0.084 (0.123) data 0.000 (0.039) loss 2.1309 (1.8914) acc 43.7500 (43.3929) lr 9.6859e-04 eta 0:09:54
epoch [104/200] batch [40/50] time 0.083 (0.118) data 0.000 (0.034) loss 1.8809 (1.8966) acc 43.7500 (43.6719) lr 9.6859e-04 eta 0:09:29
epoch [104/200] batch [45/50] time 0.083 (0.115) data 0.000 (0.031) loss 1.8418 (1.9005) acc 40.6250 (43.8194) lr 9.6859e-04 eta 0:09:10
epoch [104/200] batch [50/50] time 0.083 (0.112) data 0.000 (0.028) loss 2.0840 (1.9271) acc 40.6250 (42.7500) lr 9.5289e-04 eta 0:08:55
epoch [105/200] batch [5/50] time 0.086 (0.319) data 0.000 (0.234) loss 1.8721 (1.9947) acc 46.8750 (43.1250) lr 9.5289e-04 eta 0:25:29
epoch [105/200] batch [10/50] time 0.083 (0.216) data 0.000 (0.131) loss 1.9727 (1.9873) acc 28.1250 (40.6250) lr 9.5289e-04 eta 0:17:15
epoch [105/200] batch [15/50] time 0.083 (0.172) data 0.000 (0.088) loss 1.9189 (1.9669) acc 31.2500 (41.4583) lr 9.5289e-04 eta 0:13:42
epoch [105/200] batch [20/50] time 0.084 (0.154) data 0.000 (0.070) loss 2.5645 (1.9843) acc 37.5000 (42.1875) lr 9.5289e-04 eta 0:12:17
epoch [105/200] batch [25/50] time 0.224 (0.146) data 0.140 (0.062) loss 1.9131 (1.9923) acc 50.0000 (42.0000) lr 9.5289e-04 eta 0:11:36
epoch [105/200] batch [30/50] time 0.084 (0.136) data 0.000 (0.052) loss 1.5703 (1.9759) acc 53.1250 (42.6042) lr 9.5289e-04 eta 0:10:47
epoch [105/200] batch [35/50] time 0.084 (0.133) data 0.000 (0.048) loss 2.3691 (1.9825) acc 34.3750 (42.3214) lr 9.5289e-04 eta 0:10:31
epoch [105/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.042) loss 1.9443 (1.9736) acc 43.7500 (42.2656) lr 9.5289e-04 eta 0:10:01
epoch [105/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 2.4707 (1.9868) acc 21.8750 (41.8056) lr 9.5289e-04 eta 0:09:54
epoch [105/200] batch [50/50] time 0.084 (0.121) data 0.000 (0.037) loss 1.8623 (1.9679) acc 43.7500 (42.1875) lr 9.3721e-04 eta 0:09:34
epoch [106/200] batch [5/50] time 0.085 (0.327) data 0.000 (0.242) loss 2.0938 (1.9762) acc 43.7500 (43.7500) lr 9.3721e-04 eta 0:25:50
epoch [106/200] batch [10/50] time 0.083 (0.212) data 0.000 (0.128) loss 1.9004 (1.9982) acc 40.6250 (40.9375) lr 9.3721e-04 eta 0:16:43
epoch [106/200] batch [15/50] time 0.084 (0.169) data 0.000 (0.085) loss 1.8496 (2.0183) acc 37.5000 (40.2083) lr 9.3721e-04 eta 0:13:22
epoch [106/200] batch [20/50] time 0.086 (0.148) data 0.000 (0.064) loss 1.9854 (2.0075) acc 34.3750 (40.7812) lr 9.3721e-04 eta 0:11:42
epoch [106/200] batch [25/50] time 0.085 (0.136) data 0.000 (0.051) loss 2.1953 (2.0445) acc 34.3750 (40.5000) lr 9.3721e-04 eta 0:10:41
epoch [106/200] batch [30/50] time 0.102 (0.128) data 0.016 (0.043) loss 1.9980 (2.0263) acc 37.5000 (39.7917) lr 9.3721e-04 eta 0:10:03
epoch [106/200] batch [35/50] time 0.085 (0.122) data 0.000 (0.037) loss 1.6465 (2.0074) acc 50.0000 (40.6250) lr 9.3721e-04 eta 0:09:34
epoch [106/200] batch [40/50] time 0.083 (0.121) data 0.000 (0.036) loss 1.6641 (1.9953) acc 50.0000 (41.6406) lr 9.3721e-04 eta 0:09:29
epoch [106/200] batch [45/50] time 0.083 (0.117) data 0.000 (0.032) loss 1.7822 (2.0034) acc 46.8750 (41.3194) lr 9.3721e-04 eta 0:09:09
epoch [106/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 1.8750 (2.0116) acc 53.1250 (40.3750) lr 9.2154e-04 eta 0:09:00
epoch [107/200] batch [5/50] time 0.084 (0.335) data 0.000 (0.251) loss 1.8203 (1.8674) acc 53.1250 (48.7500) lr 9.2154e-04 eta 0:26:13
epoch [107/200] batch [10/50] time 0.084 (0.225) data 0.000 (0.141) loss 1.9053 (1.9159) acc 46.8750 (46.5625) lr 9.2154e-04 eta 0:17:33
epoch [107/200] batch [15/50] time 0.084 (0.178) data 0.000 (0.094) loss 1.7734 (1.9005) acc 50.0000 (45.8333) lr 9.2154e-04 eta 0:13:53
epoch [107/200] batch [20/50] time 0.084 (0.162) data 0.000 (0.079) loss 1.7783 (1.8771) acc 46.8750 (46.4062) lr 9.2154e-04 eta 0:12:40
epoch [107/200] batch [25/50] time 0.318 (0.156) data 0.233 (0.072) loss 1.8809 (1.9201) acc 46.8750 (45.1250) lr 9.2154e-04 eta 0:12:10
epoch [107/200] batch [30/50] time 0.085 (0.144) data 0.000 (0.060) loss 1.9971 (1.9283) acc 37.5000 (44.6875) lr 9.2154e-04 eta 0:11:13
epoch [107/200] batch [35/50] time 0.084 (0.139) data 0.001 (0.055) loss 1.9375 (1.9538) acc 25.0000 (43.0357) lr 9.2154e-04 eta 0:10:47
epoch [107/200] batch [40/50] time 0.083 (0.132) data 0.000 (0.048) loss 2.2109 (1.9720) acc 37.5000 (42.2656) lr 9.2154e-04 eta 0:10:15
epoch [107/200] batch [45/50] time 0.084 (0.127) data 0.000 (0.043) loss 1.9492 (1.9682) acc 37.5000 (42.3611) lr 9.2154e-04 eta 0:09:49
epoch [107/200] batch [50/50] time 0.082 (0.122) data 0.000 (0.039) loss 1.7070 (1.9724) acc 56.2500 (42.4375) lr 9.0589e-04 eta 0:09:28
epoch [108/200] batch [5/50] time 0.086 (0.319) data 0.001 (0.234) loss 2.3594 (2.1326) acc 21.8750 (36.8750) lr 9.0589e-04 eta 0:24:41
epoch [108/200] batch [10/50] time 0.086 (0.202) data 0.000 (0.117) loss 1.7959 (2.0216) acc 46.8750 (44.3750) lr 9.0589e-04 eta 0:15:38
epoch [108/200] batch [15/50] time 0.085 (0.163) data 0.000 (0.078) loss 1.5908 (1.9714) acc 53.1250 (45.0000) lr 9.0589e-04 eta 0:12:37
epoch [108/200] batch [20/50] time 0.086 (0.144) data 0.000 (0.059) loss 2.6328 (1.9968) acc 40.6250 (44.2188) lr 9.0589e-04 eta 0:11:06
epoch [108/200] batch [25/50] time 0.084 (0.132) data 0.000 (0.047) loss 2.3301 (1.9968) acc 40.6250 (43.8750) lr 9.0589e-04 eta 0:10:11
epoch [108/200] batch [30/50] time 0.084 (0.127) data 0.000 (0.042) loss 2.5586 (2.0089) acc 31.2500 (44.2708) lr 9.0589e-04 eta 0:09:48
epoch [108/200] batch [35/50] time 0.085 (0.121) data 0.001 (0.036) loss 1.7471 (1.9884) acc 40.6250 (43.4821) lr 9.0589e-04 eta 0:09:20
epoch [108/200] batch [40/50] time 0.084 (0.121) data 0.000 (0.037) loss 2.0801 (1.9847) acc 43.7500 (42.9688) lr 9.0589e-04 eta 0:09:19
epoch [108/200] batch [45/50] time 0.192 (0.120) data 0.110 (0.035) loss 2.4824 (1.9796) acc 31.2500 (43.0556) lr 9.0589e-04 eta 0:09:10
epoch [108/200] batch [50/50] time 0.083 (0.116) data 0.001 (0.031) loss 1.9365 (1.9698) acc 34.3750 (42.8125) lr 8.9027e-04 eta 0:08:53
epoch [109/200] batch [5/50] time 0.085 (0.281) data 0.000 (0.196) loss 1.7949 (1.9021) acc 40.6250 (45.0000) lr 8.9027e-04 eta 0:21:31
epoch [109/200] batch [10/50] time 0.086 (0.183) data 0.000 (0.098) loss 2.0957 (2.0663) acc 37.5000 (40.3125) lr 8.9027e-04 eta 0:14:01
epoch [109/200] batch [15/50] time 0.085 (0.151) data 0.000 (0.066) loss 2.2910 (2.0583) acc 46.8750 (40.8333) lr 8.9027e-04 eta 0:11:31
epoch [109/200] batch [20/50] time 0.086 (0.134) data 0.000 (0.049) loss 1.5840 (1.9973) acc 50.0000 (41.5625) lr 8.9027e-04 eta 0:10:15
epoch [109/200] batch [25/50] time 0.084 (0.125) data 0.000 (0.040) loss 2.3809 (1.9875) acc 34.3750 (42.1250) lr 8.9027e-04 eta 0:09:30
epoch [109/200] batch [30/50] time 0.193 (0.123) data 0.110 (0.039) loss 1.8320 (1.9569) acc 43.7500 (43.1250) lr 8.9027e-04 eta 0:09:24
epoch [109/200] batch [35/50] time 0.083 (0.118) data 0.001 (0.033) loss 2.1543 (1.9300) acc 46.8750 (44.4643) lr 8.9027e-04 eta 0:08:58
epoch [109/200] batch [40/50] time 0.085 (0.114) data 0.000 (0.029) loss 2.2988 (1.9709) acc 21.8750 (42.7344) lr 8.9027e-04 eta 0:08:38
epoch [109/200] batch [45/50] time 0.083 (0.110) data 0.000 (0.026) loss 2.0059 (1.9824) acc 50.0000 (42.7083) lr 8.9027e-04 eta 0:08:22
epoch [109/200] batch [50/50] time 0.083 (0.108) data 0.000 (0.023) loss 1.7578 (1.9721) acc 43.7500 (42.8750) lr 8.7467e-04 eta 0:08:09
epoch [110/200] batch [5/50] time 0.087 (0.290) data 0.000 (0.204) loss 2.6348 (2.0273) acc 31.2500 (42.5000) lr 8.7467e-04 eta 0:21:56
epoch [110/200] batch [10/50] time 0.086 (0.188) data 0.000 (0.102) loss 2.4316 (1.9550) acc 31.2500 (44.0625) lr 8.7467e-04 eta 0:14:13
epoch [110/200] batch [15/50] time 0.086 (0.154) data 0.000 (0.068) loss 2.4160 (1.9945) acc 46.8750 (42.5000) lr 8.7467e-04 eta 0:11:38
epoch [110/200] batch [20/50] time 0.087 (0.137) data 0.001 (0.051) loss 1.6572 (1.9697) acc 50.0000 (44.0625) lr 8.7467e-04 eta 0:10:21
epoch [110/200] batch [25/50] time 0.084 (0.127) data 0.000 (0.041) loss 1.7842 (1.9550) acc 46.8750 (43.8750) lr 8.7467e-04 eta 0:09:33
epoch [110/200] batch [30/50] time 0.085 (0.120) data 0.000 (0.034) loss 1.5166 (1.9608) acc 50.0000 (44.3750) lr 8.7467e-04 eta 0:09:01
epoch [110/200] batch [35/50] time 0.085 (0.115) data 0.001 (0.029) loss 2.2715 (1.9701) acc 31.2500 (43.8393) lr 8.7467e-04 eta 0:08:38
epoch [110/200] batch [40/50] time 0.083 (0.111) data 0.000 (0.026) loss 1.8096 (1.9803) acc 50.0000 (43.2031) lr 8.7467e-04 eta 0:08:20
epoch [110/200] batch [45/50] time 0.083 (0.108) data 0.000 (0.023) loss 1.7686 (1.9581) acc 53.1250 (43.6111) lr 8.7467e-04 eta 0:08:06
epoch [110/200] batch [50/50] time 0.083 (0.106) data 0.000 (0.021) loss 1.4951 (1.9508) acc 59.3750 (43.9375) lr 8.5910e-04 eta 0:07:54
epoch [111/200] batch [5/50] time 0.086 (0.288) data 0.001 (0.202) loss 2.1641 (2.0467) acc 40.6250 (38.1250) lr 8.5910e-04 eta 0:21:33
epoch [111/200] batch [10/50] time 0.085 (0.187) data 0.000 (0.101) loss 2.2812 (2.1022) acc 37.5000 (35.6250) lr 8.5910e-04 eta 0:13:58
epoch [111/200] batch [15/50] time 0.084 (0.153) data 0.000 (0.068) loss 2.0293 (1.9971) acc 31.2500 (37.9167) lr 8.5910e-04 eta 0:11:24
epoch [111/200] batch [20/50] time 0.084 (0.140) data 0.000 (0.055) loss 1.9688 (1.9977) acc 37.5000 (37.9688) lr 8.5910e-04 eta 0:10:28
epoch [111/200] batch [25/50] time 0.085 (0.129) data 0.000 (0.044) loss 2.3379 (2.0451) acc 21.8750 (36.7500) lr 8.5910e-04 eta 0:09:37
epoch [111/200] batch [30/50] time 0.084 (0.129) data 0.000 (0.044) loss 2.4883 (2.0465) acc 28.1250 (37.6042) lr 8.5910e-04 eta 0:09:35
epoch [111/200] batch [35/50] time 0.084 (0.122) data 0.000 (0.038) loss 1.8789 (2.0074) acc 37.5000 (38.6607) lr 8.5910e-04 eta 0:09:06
epoch [111/200] batch [40/50] time 0.083 (0.118) data 0.000 (0.034) loss 3.0391 (2.0423) acc 31.2500 (39.1406) lr 8.5910e-04 eta 0:08:46
epoch [111/200] batch [45/50] time 0.083 (0.114) data 0.000 (0.030) loss 2.0840 (2.0234) acc 31.2500 (39.6528) lr 8.5910e-04 eta 0:08:28
epoch [111/200] batch [50/50] time 0.084 (0.113) data 0.000 (0.028) loss 2.3438 (2.0226) acc 31.2500 (39.5000) lr 8.4357e-04 eta 0:08:20
epoch [112/200] batch [5/50] time 0.084 (0.297) data 0.000 (0.213) loss 2.2930 (2.1063) acc 34.3750 (36.8750) lr 8.4357e-04 eta 0:21:58
epoch [112/200] batch [10/50] time 0.307 (0.213) data 0.223 (0.129) loss 2.0410 (2.1473) acc 28.1250 (35.6250) lr 8.4357e-04 eta 0:15:45
epoch [112/200] batch [15/50] time 0.085 (0.170) data 0.000 (0.086) loss 1.8525 (2.0620) acc 50.0000 (38.9583) lr 8.4357e-04 eta 0:12:33
epoch [112/200] batch [20/50] time 0.083 (0.148) data 0.000 (0.065) loss 1.8604 (1.9875) acc 37.5000 (40.1562) lr 8.4357e-04 eta 0:10:57
epoch [112/200] batch [25/50] time 0.084 (0.136) data 0.000 (0.052) loss 1.7578 (1.9893) acc 37.5000 (40.2500) lr 8.4357e-04 eta 0:10:00
epoch [112/200] batch [30/50] time 0.084 (0.127) data 0.000 (0.043) loss 1.6904 (1.9880) acc 40.6250 (39.7917) lr 8.4357e-04 eta 0:09:21
epoch [112/200] batch [35/50] time 0.084 (0.121) data 0.000 (0.037) loss 1.7412 (1.9619) acc 46.8750 (39.9107) lr 8.4357e-04 eta 0:08:53
epoch [112/200] batch [40/50] time 0.085 (0.116) data 0.000 (0.032) loss 1.9170 (1.9615) acc 46.8750 (40.3906) lr 8.4357e-04 eta 0:08:32
epoch [112/200] batch [45/50] time 0.083 (0.113) data 0.000 (0.029) loss 1.9941 (1.9624) acc 34.3750 (40.5556) lr 8.4357e-04 eta 0:08:16
epoch [112/200] batch [50/50] time 0.083 (0.110) data 0.000 (0.026) loss 1.6826 (1.9671) acc 46.8750 (40.5625) lr 8.2807e-04 eta 0:08:02
epoch [113/200] batch [5/50] time 0.084 (0.301) data 0.000 (0.216) loss 1.9941 (1.9652) acc 37.5000 (44.3750) lr 8.2807e-04 eta 0:22:00
epoch [113/200] batch [10/50] time 0.164 (0.200) data 0.080 (0.116) loss 1.7568 (1.8939) acc 50.0000 (46.8750) lr 8.2807e-04 eta 0:14:39
epoch [113/200] batch [15/50] time 0.084 (0.162) data 0.000 (0.078) loss 2.1543 (1.9379) acc 40.6250 (45.8333) lr 8.2807e-04 eta 0:11:49
epoch [113/200] batch [20/50] time 0.084 (0.155) data 0.000 (0.071) loss 2.0137 (1.9201) acc 43.7500 (45.1562) lr 8.2807e-04 eta 0:11:17
epoch [113/200] batch [25/50] time 0.085 (0.141) data 0.000 (0.057) loss 1.9307 (1.9376) acc 46.8750 (45.0000) lr 8.2807e-04 eta 0:10:15
epoch [113/200] batch [30/50] time 0.084 (0.138) data 0.000 (0.055) loss 1.9297 (1.9691) acc 43.7500 (43.2292) lr 8.2807e-04 eta 0:10:04
epoch [113/200] batch [35/50] time 0.224 (0.136) data 0.141 (0.052) loss 1.6064 (1.9343) acc 53.1250 (43.2143) lr 8.2807e-04 eta 0:09:54
epoch [113/200] batch [40/50] time 0.083 (0.130) data 0.000 (0.046) loss 1.7148 (1.9192) acc 34.3750 (43.4375) lr 8.2807e-04 eta 0:09:25
epoch [113/200] batch [45/50] time 0.085 (0.127) data 0.000 (0.043) loss 1.8945 (1.9310) acc 50.0000 (43.1944) lr 8.2807e-04 eta 0:09:12
epoch [113/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.6172 (1.9496) acc 31.2500 (42.6875) lr 8.1262e-04 eta 0:08:53
epoch [114/200] batch [5/50] time 0.086 (0.322) data 0.000 (0.237) loss 1.7773 (1.9271) acc 56.2500 (48.1250) lr 8.1262e-04 eta 0:23:18
epoch [114/200] batch [10/50] time 0.085 (0.203) data 0.000 (0.118) loss 1.8145 (1.9335) acc 56.2500 (47.8125) lr 8.1262e-04 eta 0:14:42
epoch [114/200] batch [15/50] time 0.084 (0.164) data 0.000 (0.079) loss 2.0703 (1.9128) acc 25.0000 (46.2500) lr 8.1262e-04 eta 0:11:51
epoch [114/200] batch [20/50] time 0.084 (0.144) data 0.000 (0.059) loss 2.1230 (1.9315) acc 34.3750 (46.0938) lr 8.1262e-04 eta 0:10:24
epoch [114/200] batch [25/50] time 0.086 (0.132) data 0.000 (0.048) loss 1.3730 (1.9533) acc 59.3750 (45.1250) lr 8.1262e-04 eta 0:09:32
epoch [114/200] batch [30/50] time 0.084 (0.124) data 0.000 (0.040) loss 1.9375 (1.9711) acc 43.7500 (44.1667) lr 8.1262e-04 eta 0:08:57
epoch [114/200] batch [35/50] time 0.084 (0.119) data 0.000 (0.034) loss 2.1777 (1.9658) acc 31.2500 (43.0357) lr 8.1262e-04 eta 0:08:32
epoch [114/200] batch [40/50] time 0.084 (0.114) data 0.000 (0.030) loss 1.9297 (1.9910) acc 40.6250 (42.1094) lr 8.1262e-04 eta 0:08:12
epoch [114/200] batch [45/50] time 0.083 (0.111) data 0.000 (0.027) loss 1.8242 (1.9853) acc 43.7500 (42.2917) lr 8.1262e-04 eta 0:07:57
epoch [114/200] batch [50/50] time 0.084 (0.108) data 0.000 (0.024) loss 2.0020 (1.9806) acc 46.8750 (42.1250) lr 7.9721e-04 eta 0:07:45
epoch [115/200] batch [5/50] time 0.085 (0.318) data 0.001 (0.234) loss 1.8496 (1.9457) acc 50.0000 (44.3750) lr 7.9721e-04 eta 0:22:43
epoch [115/200] batch [10/50] time 0.084 (0.216) data 0.000 (0.133) loss 2.2344 (1.9670) acc 43.7500 (43.7500) lr 7.9721e-04 eta 0:15:27
epoch [115/200] batch [15/50] time 0.085 (0.173) data 0.000 (0.089) loss 1.5674 (1.9137) acc 50.0000 (44.5833) lr 7.9721e-04 eta 0:12:19
epoch [115/200] batch [20/50] time 0.085 (0.157) data 0.000 (0.073) loss 2.4355 (1.9577) acc 34.3750 (44.2188) lr 7.9721e-04 eta 0:11:12
epoch [115/200] batch [25/50] time 0.085 (0.143) data 0.000 (0.058) loss 2.2949 (1.9176) acc 37.5000 (45.5000) lr 7.9721e-04 eta 0:10:09
epoch [115/200] batch [30/50] time 0.101 (0.134) data 0.000 (0.049) loss 2.5137 (1.9413) acc 31.2500 (45.0000) lr 7.9721e-04 eta 0:09:30
epoch [115/200] batch [35/50] time 0.109 (0.129) data 0.001 (0.042) loss 2.5410 (1.9460) acc 34.3750 (44.4643) lr 7.9721e-04 eta 0:09:08
epoch [115/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.037) loss 1.6846 (1.9321) acc 53.1250 (44.4531) lr 7.9721e-04 eta 0:08:44
epoch [115/200] batch [45/50] time 0.082 (0.119) data 0.000 (0.033) loss 2.1992 (1.9526) acc 37.5000 (43.9583) lr 7.9721e-04 eta 0:08:24
epoch [115/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.030) loss 2.1113 (1.9648) acc 31.2500 (43.6875) lr 7.8186e-04 eta 0:08:08
epoch [116/200] batch [5/50] time 0.083 (0.309) data 0.000 (0.225) loss 1.8613 (1.9559) acc 34.3750 (41.2500) lr 7.8186e-04 eta 0:21:49
epoch [116/200] batch [10/50] time 0.209 (0.209) data 0.126 (0.125) loss 2.1094 (1.9952) acc 43.7500 (42.1875) lr 7.8186e-04 eta 0:14:45
epoch [116/200] batch [15/50] time 0.085 (0.167) data 0.000 (0.083) loss 1.8076 (1.9307) acc 43.7500 (44.7917) lr 7.8186e-04 eta 0:11:47
epoch [116/200] batch [20/50] time 0.085 (0.157) data 0.000 (0.074) loss 2.0645 (1.9618) acc 34.3750 (43.4375) lr 7.8186e-04 eta 0:11:06
epoch [116/200] batch [25/50] time 0.085 (0.143) data 0.000 (0.059) loss 1.9824 (1.9875) acc 40.6250 (42.2500) lr 7.8186e-04 eta 0:10:03
epoch [116/200] batch [30/50] time 0.084 (0.138) data 0.000 (0.054) loss 1.7471 (1.9818) acc 53.1250 (41.9792) lr 7.8186e-04 eta 0:09:40
epoch [116/200] batch [35/50] time 0.084 (0.132) data 0.000 (0.048) loss 1.9062 (1.9698) acc 43.7500 (42.3214) lr 7.8186e-04 eta 0:09:14
epoch [116/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.042) loss 2.1230 (1.9494) acc 40.6250 (43.2812) lr 7.8186e-04 eta 0:08:49
epoch [116/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.040) loss 2.0859 (1.9541) acc 37.5000 (43.2639) lr 7.8186e-04 eta 0:08:38
epoch [116/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.036) loss 2.1641 (1.9768) acc 37.5000 (42.3750) lr 7.6655e-04 eta 0:08:21
epoch [117/200] batch [5/50] time 0.243 (0.299) data 0.156 (0.213) loss 1.5938 (1.8875) acc 53.1250 (41.2500) lr 7.6655e-04 eta 0:20:54
epoch [117/200] batch [10/50] time 0.086 (0.192) data 0.000 (0.107) loss 1.7803 (1.8649) acc 59.3750 (43.4375) lr 7.6655e-04 eta 0:13:25
epoch [117/200] batch [15/50] time 0.086 (0.166) data 0.000 (0.081) loss 2.2324 (1.9430) acc 43.7500 (42.5000) lr 7.6655e-04 eta 0:11:36
epoch [117/200] batch [20/50] time 0.086 (0.146) data 0.000 (0.061) loss 2.0195 (1.9475) acc 31.2500 (41.5625) lr 7.6655e-04 eta 0:10:11
epoch [117/200] batch [25/50] time 0.085 (0.143) data 0.000 (0.058) loss 2.0312 (1.9948) acc 34.3750 (41.5000) lr 7.6655e-04 eta 0:09:57
epoch [117/200] batch [30/50] time 0.085 (0.139) data 0.000 (0.054) loss 2.4316 (1.9831) acc 31.2500 (41.5625) lr 7.6655e-04 eta 0:09:39
epoch [117/200] batch [35/50] time 0.084 (0.131) data 0.000 (0.046) loss 1.7842 (1.9683) acc 43.7500 (41.7857) lr 7.6655e-04 eta 0:09:06
epoch [117/200] batch [40/50] time 0.083 (0.130) data 0.000 (0.045) loss 1.8652 (1.9639) acc 46.8750 (42.1094) lr 7.6655e-04 eta 0:08:59
epoch [117/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.040) loss 1.6514 (1.9506) acc 46.8750 (42.5000) lr 7.6655e-04 eta 0:08:37
epoch [117/200] batch [50/50] time 0.084 (0.120) data 0.000 (0.036) loss 2.1660 (1.9612) acc 28.1250 (41.9375) lr 7.5131e-04 eta 0:08:19
epoch [118/200] batch [5/50] time 0.087 (0.300) data 0.000 (0.213) loss 2.1348 (1.9432) acc 28.1250 (43.1250) lr 7.5131e-04 eta 0:20:41
epoch [118/200] batch [10/50] time 0.085 (0.193) data 0.001 (0.107) loss 2.1172 (1.9175) acc 43.7500 (46.2500) lr 7.5131e-04 eta 0:13:17
epoch [118/200] batch [15/50] time 0.084 (0.157) data 0.000 (0.071) loss 2.2539 (1.9351) acc 40.6250 (46.2500) lr 7.5131e-04 eta 0:10:47
epoch [118/200] batch [20/50] time 0.103 (0.140) data 0.020 (0.055) loss 2.2207 (1.9315) acc 37.5000 (45.4688) lr 7.5131e-04 eta 0:09:36
epoch [118/200] batch [25/50] time 0.083 (0.128) data 0.000 (0.044) loss 2.1328 (1.9377) acc 37.5000 (45.2500) lr 7.5131e-04 eta 0:08:49
epoch [118/200] batch [30/50] time 0.084 (0.121) data 0.000 (0.037) loss 2.1230 (1.9418) acc 34.3750 (45.4167) lr 7.5131e-04 eta 0:08:18
epoch [118/200] batch [35/50] time 0.083 (0.116) data 0.000 (0.031) loss 1.8672 (1.9496) acc 37.5000 (45.2679) lr 7.5131e-04 eta 0:07:56
epoch [118/200] batch [40/50] time 0.083 (0.116) data 0.000 (0.032) loss 2.0586 (1.9499) acc 46.8750 (44.6875) lr 7.5131e-04 eta 0:07:56
epoch [118/200] batch [45/50] time 0.083 (0.112) data 0.000 (0.028) loss 2.0723 (1.9583) acc 40.6250 (43.6806) lr 7.5131e-04 eta 0:07:40
epoch [118/200] batch [50/50] time 0.083 (0.113) data 0.000 (0.029) loss 1.7129 (1.9635) acc 43.7500 (43.6875) lr 7.3613e-04 eta 0:07:42
epoch [119/200] batch [5/50] time 0.084 (0.320) data 0.000 (0.236) loss 1.9883 (1.9250) acc 40.6250 (41.8750) lr 7.3613e-04 eta 0:21:49
epoch [119/200] batch [10/50] time 0.084 (0.202) data 0.001 (0.118) loss 2.0059 (1.9576) acc 50.0000 (42.5000) lr 7.3613e-04 eta 0:13:47
epoch [119/200] batch [15/50] time 0.086 (0.163) data 0.000 (0.079) loss 1.8457 (2.0069) acc 50.0000 (42.2917) lr 7.3613e-04 eta 0:11:05
epoch [119/200] batch [20/50] time 0.084 (0.143) data 0.000 (0.059) loss 1.6104 (1.9867) acc 46.8750 (41.5625) lr 7.3613e-04 eta 0:09:44
epoch [119/200] batch [25/50] time 0.085 (0.132) data 0.000 (0.048) loss 2.2832 (2.0006) acc 40.6250 (41.6250) lr 7.3613e-04 eta 0:08:56
epoch [119/200] batch [30/50] time 0.086 (0.124) data 0.000 (0.040) loss 2.1641 (1.9922) acc 31.2500 (41.7708) lr 7.3613e-04 eta 0:08:24
epoch [119/200] batch [35/50] time 0.084 (0.118) data 0.000 (0.034) loss 1.6680 (1.9723) acc 40.6250 (42.4107) lr 7.3613e-04 eta 0:08:00
epoch [119/200] batch [40/50] time 0.092 (0.114) data 0.008 (0.030) loss 1.6934 (1.9525) acc 37.5000 (42.6562) lr 7.3613e-04 eta 0:07:43
epoch [119/200] batch [45/50] time 0.172 (0.113) data 0.088 (0.029) loss 2.1250 (1.9693) acc 37.5000 (42.2222) lr 7.3613e-04 eta 0:07:37
epoch [119/200] batch [50/50] time 0.084 (0.110) data 0.000 (0.026) loss 1.6670 (1.9548) acc 53.1250 (42.7500) lr 7.2101e-04 eta 0:07:24
epoch [120/200] batch [5/50] time 0.083 (0.336) data 0.000 (0.252) loss 1.5400 (1.8383) acc 53.1250 (47.5000) lr 7.2101e-04 eta 0:22:39
epoch [120/200] batch [10/50] time 0.084 (0.217) data 0.000 (0.133) loss 1.8604 (1.9247) acc 56.2500 (49.3750) lr 7.2101e-04 eta 0:14:36
epoch [120/200] batch [15/50] time 0.084 (0.173) data 0.000 (0.089) loss 1.5625 (1.9484) acc 53.1250 (47.0833) lr 7.2101e-04 eta 0:11:36
epoch [120/200] batch [20/50] time 0.084 (0.154) data 0.000 (0.070) loss 2.4961 (1.9782) acc 28.1250 (45.4688) lr 7.2101e-04 eta 0:10:19
epoch [120/200] batch [25/50] time 0.084 (0.140) data 0.000 (0.056) loss 1.8936 (1.9525) acc 46.8750 (45.2500) lr 7.2101e-04 eta 0:09:23
epoch [120/200] batch [30/50] time 0.086 (0.137) data 0.000 (0.052) loss 2.3652 (1.9710) acc 34.3750 (44.5833) lr 7.2101e-04 eta 0:09:09
epoch [120/200] batch [35/50] time 0.086 (0.135) data 0.001 (0.050) loss 1.6182 (1.9401) acc 59.3750 (45.1786) lr 7.2101e-04 eta 0:09:00
epoch [120/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.044) loss 2.0273 (1.9590) acc 50.0000 (44.7656) lr 7.2101e-04 eta 0:08:34
epoch [120/200] batch [45/50] time 0.083 (0.127) data 0.000 (0.043) loss 1.6924 (1.9437) acc 53.1250 (45.0000) lr 7.2101e-04 eta 0:08:28
epoch [120/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.1309 (1.9276) acc 34.3750 (45.0625) lr 7.0596e-04 eta 0:08:10
epoch [121/200] batch [5/50] time 0.086 (0.271) data 0.000 (0.185) loss 1.6084 (1.8791) acc 40.6250 (43.7500) lr 7.0596e-04 eta 0:18:01
epoch [121/200] batch [10/50] time 0.213 (0.191) data 0.129 (0.105) loss 1.9824 (1.8751) acc 37.5000 (46.2500) lr 7.0596e-04 eta 0:12:41
epoch [121/200] batch [15/50] time 0.086 (0.156) data 0.000 (0.070) loss 1.8418 (1.9615) acc 46.8750 (41.2500) lr 7.0596e-04 eta 0:10:20
epoch [121/200] batch [20/50] time 0.172 (0.143) data 0.088 (0.058) loss 1.6787 (1.9233) acc 56.2500 (42.3438) lr 7.0596e-04 eta 0:09:30
epoch [121/200] batch [25/50] time 0.170 (0.135) data 0.086 (0.050) loss 1.7881 (1.8975) acc 34.3750 (43.0000) lr 7.0596e-04 eta 0:08:56
epoch [121/200] batch [30/50] time 0.085 (0.130) data 0.000 (0.045) loss 1.9814 (1.9078) acc 50.0000 (43.2292) lr 7.0596e-04 eta 0:08:34
epoch [121/200] batch [35/50] time 0.085 (0.123) data 0.000 (0.038) loss 1.8701 (1.9432) acc 43.7500 (43.1250) lr 7.0596e-04 eta 0:08:08
epoch [121/200] batch [40/50] time 0.085 (0.124) data 0.000 (0.039) loss 1.8838 (1.9295) acc 46.8750 (43.1250) lr 7.0596e-04 eta 0:08:10
epoch [121/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.038) loss 1.7041 (1.9411) acc 40.6250 (42.9167) lr 7.0596e-04 eta 0:08:05
epoch [121/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.4883 (1.9641) acc 28.1250 (42.3125) lr 6.9098e-04 eta 0:07:49
epoch [122/200] batch [5/50] time 0.086 (0.278) data 0.000 (0.192) loss 1.6904 (1.8580) acc 53.1250 (48.1250) lr 6.9098e-04 eta 0:18:18
epoch [122/200] batch [10/50] time 0.183 (0.192) data 0.092 (0.106) loss 1.9375 (1.8740) acc 50.0000 (47.5000) lr 6.9098e-04 eta 0:12:34
epoch [122/200] batch [15/50] time 0.086 (0.156) data 0.001 (0.071) loss 1.4951 (1.8207) acc 59.3750 (49.1667) lr 6.9098e-04 eta 0:10:13
epoch [122/200] batch [20/50] time 0.084 (0.144) data 0.000 (0.059) loss 1.9375 (1.9121) acc 43.7500 (45.7812) lr 6.9098e-04 eta 0:09:25
epoch [122/200] batch [25/50] time 0.084 (0.132) data 0.000 (0.047) loss 1.8115 (1.8916) acc 37.5000 (46.2500) lr 6.9098e-04 eta 0:08:38
epoch [122/200] batch [30/50] time 0.085 (0.128) data 0.001 (0.043) loss 1.9619 (1.9448) acc 40.6250 (44.3750) lr 6.9098e-04 eta 0:08:22
epoch [122/200] batch [35/50] time 0.085 (0.122) data 0.001 (0.037) loss 2.2070 (1.9475) acc 40.6250 (44.5536) lr 6.9098e-04 eta 0:07:57
epoch [122/200] batch [40/50] time 0.083 (0.118) data 0.000 (0.034) loss 2.4004 (1.9569) acc 37.5000 (43.7500) lr 6.9098e-04 eta 0:07:42
epoch [122/200] batch [45/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.0977 (1.9624) acc 53.1250 (43.6111) lr 6.9098e-04 eta 0:07:45
epoch [122/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.031) loss 1.7275 (1.9593) acc 56.2500 (43.1875) lr 6.7608e-04 eta 0:07:30
epoch [123/200] batch [5/50] time 0.085 (0.308) data 0.000 (0.223) loss 1.9375 (1.8912) acc 46.8750 (43.1250) lr 6.7608e-04 eta 0:20:01
epoch [123/200] batch [10/50] time 0.087 (0.197) data 0.001 (0.112) loss 2.0645 (1.8499) acc 46.8750 (43.7500) lr 6.7608e-04 eta 0:12:47
epoch [123/200] batch [15/50] time 0.084 (0.167) data 0.000 (0.082) loss 2.3496 (1.9341) acc 34.3750 (41.8750) lr 6.7608e-04 eta 0:10:50
epoch [123/200] batch [20/50] time 0.260 (0.155) data 0.176 (0.071) loss 1.9346 (1.9291) acc 34.3750 (42.3438) lr 6.7608e-04 eta 0:10:02
epoch [123/200] batch [25/50] time 0.083 (0.142) data 0.000 (0.058) loss 1.7734 (1.9324) acc 43.7500 (42.3750) lr 6.7608e-04 eta 0:09:11
epoch [123/200] batch [30/50] time 0.226 (0.137) data 0.145 (0.053) loss 1.2100 (1.8962) acc 56.2500 (43.0208) lr 6.7608e-04 eta 0:08:51
epoch [123/200] batch [35/50] time 0.085 (0.130) data 0.001 (0.046) loss 1.9053 (1.8679) acc 43.7500 (44.0179) lr 6.7608e-04 eta 0:08:22
epoch [123/200] batch [40/50] time 0.084 (0.127) data 0.000 (0.043) loss 1.8779 (1.8981) acc 40.6250 (43.7500) lr 6.7608e-04 eta 0:08:10
epoch [123/200] batch [45/50] time 0.083 (0.122) data 0.000 (0.038) loss 1.6318 (1.9102) acc 53.1250 (43.3333) lr 6.7608e-04 eta 0:07:50
epoch [123/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.034) loss 2.0176 (1.9280) acc 31.2500 (43.0000) lr 6.6126e-04 eta 0:07:35
epoch [124/200] batch [5/50] time 0.083 (0.291) data 0.000 (0.208) loss 2.0254 (1.9492) acc 50.0000 (46.2500) lr 6.6126e-04 eta 0:18:40
epoch [124/200] batch [10/50] time 0.149 (0.199) data 0.065 (0.116) loss 1.5186 (1.9089) acc 53.1250 (45.3125) lr 6.6126e-04 eta 0:12:44
epoch [124/200] batch [15/50] time 0.085 (0.161) data 0.000 (0.077) loss 1.7598 (1.8510) acc 37.5000 (48.3333) lr 6.6126e-04 eta 0:10:17
epoch [124/200] batch [20/50] time 0.084 (0.149) data 0.000 (0.065) loss 1.4717 (1.8356) acc 59.3750 (48.7500) lr 6.6126e-04 eta 0:09:29
epoch [124/200] batch [25/50] time 0.239 (0.142) data 0.156 (0.058) loss 2.1934 (1.8544) acc 31.2500 (47.2500) lr 6.6126e-04 eta 0:09:03
epoch [124/200] batch [30/50] time 0.084 (0.133) data 0.000 (0.049) loss 1.8154 (1.8985) acc 40.6250 (45.8333) lr 6.6126e-04 eta 0:08:26
epoch [124/200] batch [35/50] time 0.084 (0.131) data 0.001 (0.047) loss 1.5762 (1.8976) acc 46.8750 (46.1607) lr 6.6126e-04 eta 0:08:19
epoch [124/200] batch [40/50] time 0.084 (0.125) data 0.000 (0.041) loss 1.8242 (1.9086) acc 53.1250 (46.0938) lr 6.6126e-04 eta 0:07:56
epoch [124/200] batch [45/50] time 0.083 (0.122) data 0.000 (0.039) loss 1.8096 (1.9166) acc 40.6250 (45.3472) lr 6.6126e-04 eta 0:07:45
epoch [124/200] batch [50/50] time 0.084 (0.119) data 0.000 (0.035) loss 1.6904 (1.9111) acc 56.2500 (45.4375) lr 6.4653e-04 eta 0:07:30
epoch [125/200] batch [5/50] time 0.084 (0.312) data 0.000 (0.228) loss 1.9268 (2.1389) acc 37.5000 (39.3750) lr 6.4653e-04 eta 0:19:42
epoch [125/200] batch [10/50] time 0.083 (0.216) data 0.000 (0.133) loss 1.9141 (1.9479) acc 34.3750 (42.1875) lr 6.4653e-04 eta 0:13:39
epoch [125/200] batch [15/50] time 0.083 (0.172) data 0.000 (0.089) loss 1.9170 (1.8964) acc 40.6250 (43.3333) lr 6.4653e-04 eta 0:10:52
epoch [125/200] batch [20/50] time 0.084 (0.159) data 0.000 (0.075) loss 1.9834 (1.8713) acc 46.8750 (45.1562) lr 6.4653e-04 eta 0:10:01
epoch [125/200] batch [25/50] time 0.206 (0.149) data 0.120 (0.065) loss 2.0879 (1.8735) acc 37.5000 (44.2500) lr 6.4653e-04 eta 0:09:22
epoch [125/200] batch [30/50] time 0.086 (0.138) data 0.000 (0.054) loss 1.7539 (1.8787) acc 43.7500 (44.2708) lr 6.4653e-04 eta 0:08:41
epoch [125/200] batch [35/50] time 0.085 (0.134) data 0.001 (0.050) loss 1.8535 (1.9112) acc 43.7500 (43.8393) lr 6.4653e-04 eta 0:08:24
epoch [125/200] batch [40/50] time 0.084 (0.128) data 0.000 (0.044) loss 2.2324 (1.9246) acc 53.1250 (44.2188) lr 6.4653e-04 eta 0:08:00
epoch [125/200] batch [45/50] time 0.084 (0.125) data 0.000 (0.041) loss 1.4062 (1.9283) acc 62.5000 (44.1667) lr 6.4653e-04 eta 0:07:48
epoch [125/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 2.0684 (1.9368) acc 31.2500 (44.0625) lr 6.3188e-04 eta 0:07:31
epoch [126/200] batch [5/50] time 0.087 (0.325) data 0.001 (0.240) loss 2.2891 (1.9582) acc 28.1250 (46.2500) lr 6.3188e-04 eta 0:20:18
epoch [126/200] batch [10/50] time 0.086 (0.206) data 0.000 (0.120) loss 1.7246 (1.9242) acc 43.7500 (45.3125) lr 6.3188e-04 eta 0:12:49
epoch [126/200] batch [15/50] time 0.086 (0.173) data 0.000 (0.087) loss 1.8037 (1.9430) acc 34.3750 (44.3750) lr 6.3188e-04 eta 0:10:44
epoch [126/200] batch [20/50] time 0.086 (0.158) data 0.000 (0.073) loss 1.4697 (1.9114) acc 50.0000 (43.5938) lr 6.3188e-04 eta 0:09:50
epoch [126/200] batch [25/50] time 0.086 (0.144) data 0.001 (0.058) loss 1.9795 (1.9125) acc 28.1250 (43.0000) lr 6.3188e-04 eta 0:08:56
epoch [126/200] batch [30/50] time 0.085 (0.138) data 0.000 (0.053) loss 1.9277 (1.9213) acc 37.5000 (42.9167) lr 6.3188e-04 eta 0:08:34
epoch [126/200] batch [35/50] time 0.201 (0.134) data 0.117 (0.049) loss 1.7471 (1.9206) acc 53.1250 (43.3929) lr 6.3188e-04 eta 0:08:17
epoch [126/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.043) loss 1.6514 (1.9065) acc 53.1250 (43.7500) lr 6.3188e-04 eta 0:07:53
epoch [126/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.041) loss 1.8701 (1.9077) acc 56.2500 (43.8889) lr 6.3188e-04 eta 0:07:47
epoch [126/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.037) loss 1.8574 (1.9005) acc 43.7500 (44.0625) lr 6.1732e-04 eta 0:07:30
epoch [127/200] batch [5/50] time 0.085 (0.284) data 0.000 (0.198) loss 1.9629 (1.8875) acc 43.7500 (43.1250) lr 6.1732e-04 eta 0:17:28
epoch [127/200] batch [10/50] time 0.084 (0.184) data 0.000 (0.099) loss 1.7217 (1.8458) acc 56.2500 (45.9375) lr 6.1732e-04 eta 0:11:19
epoch [127/200] batch [15/50] time 0.085 (0.152) data 0.000 (0.067) loss 1.9912 (1.8867) acc 40.6250 (44.7917) lr 6.1732e-04 eta 0:09:19
epoch [127/200] batch [20/50] time 0.175 (0.140) data 0.090 (0.055) loss 2.1250 (1.9114) acc 50.0000 (44.5312) lr 6.1732e-04 eta 0:08:34
epoch [127/200] batch [25/50] time 0.085 (0.129) data 0.000 (0.044) loss 2.0566 (1.9223) acc 37.5000 (44.6250) lr 6.1732e-04 eta 0:07:54
epoch [127/200] batch [30/50] time 0.116 (0.128) data 0.032 (0.043) loss 2.0742 (1.9186) acc 43.7500 (44.1667) lr 6.1732e-04 eta 0:07:49
epoch [127/200] batch [35/50] time 0.086 (0.122) data 0.000 (0.037) loss 2.0820 (1.9381) acc 34.3750 (43.2143) lr 6.1732e-04 eta 0:07:26
epoch [127/200] batch [40/50] time 0.083 (0.122) data 0.000 (0.037) loss 1.8652 (1.9220) acc 34.3750 (42.5781) lr 6.1732e-04 eta 0:07:24
epoch [127/200] batch [45/50] time 0.082 (0.121) data 0.000 (0.036) loss 1.9258 (1.9259) acc 40.6250 (42.3611) lr 6.1732e-04 eta 0:07:20
epoch [127/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.033) loss 1.8721 (1.9305) acc 37.5000 (42.5000) lr 6.0285e-04 eta 0:07:06
epoch [128/200] batch [5/50] time 0.085 (0.288) data 0.000 (0.202) loss 2.2090 (1.9035) acc 43.7500 (44.3750) lr 6.0285e-04 eta 0:17:29
epoch [128/200] batch [10/50] time 0.085 (0.187) data 0.000 (0.101) loss 1.7900 (1.9279) acc 31.2500 (41.5625) lr 6.0285e-04 eta 0:11:18
epoch [128/200] batch [15/50] time 0.085 (0.153) data 0.000 (0.067) loss 1.7188 (1.8620) acc 53.1250 (44.3750) lr 6.0285e-04 eta 0:09:14
epoch [128/200] batch [20/50] time 0.085 (0.136) data 0.000 (0.051) loss 2.1133 (1.9193) acc 28.1250 (42.9688) lr 6.0285e-04 eta 0:08:12
epoch [128/200] batch [25/50] time 0.086 (0.128) data 0.000 (0.043) loss 2.0762 (1.8944) acc 40.6250 (44.1250) lr 6.0285e-04 eta 0:07:44
epoch [128/200] batch [30/50] time 0.086 (0.121) data 0.000 (0.036) loss 1.9570 (1.9150) acc 53.1250 (43.2292) lr 6.0285e-04 eta 0:07:18
epoch [128/200] batch [35/50] time 0.085 (0.118) data 0.001 (0.033) loss 2.1328 (1.9104) acc 46.8750 (43.3036) lr 6.0285e-04 eta 0:07:07
epoch [128/200] batch [40/50] time 0.085 (0.117) data 0.000 (0.032) loss 2.4258 (1.9032) acc 28.1250 (43.9844) lr 6.0285e-04 eta 0:07:02
epoch [128/200] batch [45/50] time 0.082 (0.113) data 0.000 (0.029) loss 1.5303 (1.9121) acc 62.5000 (43.6806) lr 6.0285e-04 eta 0:06:48
epoch [128/200] batch [50/50] time 0.082 (0.110) data 0.000 (0.026) loss 2.0234 (1.9136) acc 40.6250 (43.4375) lr 5.8849e-04 eta 0:06:36
epoch [129/200] batch [5/50] time 0.085 (0.281) data 0.000 (0.196) loss 2.2891 (1.9871) acc 37.5000 (41.8750) lr 5.8849e-04 eta 0:16:51
epoch [129/200] batch [10/50] time 0.133 (0.188) data 0.049 (0.103) loss 1.5742 (1.8668) acc 56.2500 (43.4375) lr 5.8849e-04 eta 0:11:15
epoch [129/200] batch [15/50] time 0.086 (0.156) data 0.000 (0.070) loss 2.4043 (1.9190) acc 40.6250 (45.0000) lr 5.8849e-04 eta 0:09:17
epoch [129/200] batch [20/50] time 0.085 (0.147) data 0.000 (0.062) loss 1.7949 (1.9179) acc 50.0000 (45.0000) lr 5.8849e-04 eta 0:08:46
epoch [129/200] batch [25/50] time 0.084 (0.134) data 0.000 (0.050) loss 1.7217 (1.9038) acc 56.2500 (45.6250) lr 5.8849e-04 eta 0:08:00
epoch [129/200] batch [30/50] time 0.085 (0.131) data 0.001 (0.047) loss 1.6260 (1.8941) acc 43.7500 (44.5833) lr 5.8849e-04 eta 0:07:48
epoch [129/200] batch [35/50] time 0.269 (0.130) data 0.187 (0.045) loss 2.0039 (1.8951) acc 50.0000 (44.5536) lr 5.8849e-04 eta 0:07:42
epoch [129/200] batch [40/50] time 0.085 (0.124) data 0.000 (0.040) loss 1.8076 (1.8876) acc 43.7500 (44.5312) lr 5.8849e-04 eta 0:07:21
epoch [129/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.1035 (1.8976) acc 40.6250 (44.4444) lr 5.8849e-04 eta 0:07:20
epoch [129/200] batch [50/50] time 0.082 (0.120) data 0.000 (0.036) loss 1.9385 (1.9060) acc 50.0000 (44.2500) lr 5.7422e-04 eta 0:07:05
epoch [130/200] batch [5/50] time 0.086 (0.320) data 0.000 (0.235) loss 1.9482 (1.9686) acc 50.0000 (45.6250) lr 5.7422e-04 eta 0:18:55
epoch [130/200] batch [10/50] time 0.085 (0.212) data 0.000 (0.126) loss 1.6250 (1.8739) acc 43.7500 (44.6875) lr 5.7422e-04 eta 0:12:28
epoch [130/200] batch [15/50] time 0.086 (0.170) data 0.000 (0.084) loss 1.6504 (1.8537) acc 40.6250 (44.1667) lr 5.7422e-04 eta 0:09:59
epoch [130/200] batch [20/50] time 0.085 (0.155) data 0.000 (0.070) loss 2.2539 (1.8700) acc 34.3750 (43.4375) lr 5.7422e-04 eta 0:09:07
epoch [130/200] batch [25/50] time 0.338 (0.151) data 0.253 (0.066) loss 1.7656 (1.8886) acc 46.8750 (43.3750) lr 5.7422e-04 eta 0:08:53
epoch [130/200] batch [30/50] time 0.084 (0.140) data 0.000 (0.055) loss 1.8105 (1.8775) acc 40.6250 (43.8542) lr 5.7422e-04 eta 0:08:13
epoch [130/200] batch [35/50] time 0.085 (0.136) data 0.000 (0.051) loss 2.1641 (1.8878) acc 46.8750 (44.6429) lr 5.7422e-04 eta 0:07:57
epoch [130/200] batch [40/50] time 0.084 (0.129) data 0.000 (0.045) loss 2.2734 (1.9234) acc 28.1250 (43.2812) lr 5.7422e-04 eta 0:07:34
epoch [130/200] batch [45/50] time 0.082 (0.126) data 0.000 (0.042) loss 2.0938 (1.9247) acc 37.5000 (42.6389) lr 5.7422e-04 eta 0:07:22
epoch [130/200] batch [50/50] time 0.082 (0.122) data 0.000 (0.038) loss 2.1641 (1.9576) acc 25.0000 (42.3125) lr 5.6006e-04 eta 0:07:06
epoch [131/200] batch [5/50] time 0.085 (0.295) data 0.000 (0.210) loss 2.0352 (1.8266) acc 34.3750 (43.1250) lr 5.6006e-04 eta 0:17:10
epoch [131/200] batch [10/50] time 0.085 (0.190) data 0.000 (0.105) loss 1.6572 (1.8163) acc 46.8750 (44.3750) lr 5.6006e-04 eta 0:11:02
epoch [131/200] batch [15/50] time 0.084 (0.162) data 0.000 (0.077) loss 1.6797 (1.8665) acc 53.1250 (44.5833) lr 5.6006e-04 eta 0:09:23
epoch [131/200] batch [20/50] time 0.108 (0.149) data 0.023 (0.064) loss 1.7793 (1.8702) acc 46.8750 (44.0625) lr 5.6006e-04 eta 0:08:36
epoch [131/200] batch [25/50] time 0.084 (0.136) data 0.000 (0.051) loss 1.7373 (1.8909) acc 37.5000 (43.0000) lr 5.6006e-04 eta 0:07:52
epoch [131/200] batch [30/50] time 0.084 (0.135) data 0.000 (0.051) loss 1.8750 (1.9080) acc 50.0000 (43.5417) lr 5.6006e-04 eta 0:07:49
epoch [131/200] batch [35/50] time 0.185 (0.131) data 0.100 (0.046) loss 1.9277 (1.9281) acc 46.8750 (43.3036) lr 5.6006e-04 eta 0:07:33
epoch [131/200] batch [40/50] time 0.084 (0.128) data 0.000 (0.044) loss 2.0273 (1.9214) acc 43.7500 (43.6719) lr 5.6006e-04 eta 0:07:23
epoch [131/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 2.0449 (1.9308) acc 31.2500 (42.9167) lr 5.6006e-04 eta 0:07:13
epoch [131/200] batch [50/50] time 0.084 (0.121) data 0.000 (0.037) loss 2.3848 (1.9444) acc 34.3750 (42.2500) lr 5.4601e-04 eta 0:06:58
epoch [132/200] batch [5/50] time 0.084 (0.319) data 0.000 (0.235) loss 1.9092 (2.0855) acc 50.0000 (38.1250) lr 5.4601e-04 eta 0:18:20
epoch [132/200] batch [10/50] time 0.201 (0.214) data 0.118 (0.130) loss 2.0410 (2.0063) acc 50.0000 (42.8125) lr 5.4601e-04 eta 0:12:15
epoch [132/200] batch [15/50] time 0.084 (0.171) data 0.000 (0.086) loss 2.0977 (1.9954) acc 40.6250 (43.1250) lr 5.4601e-04 eta 0:09:45
epoch [132/200] batch [20/50] time 0.084 (0.155) data 0.000 (0.071) loss 2.1875 (1.9851) acc 37.5000 (42.6562) lr 5.4601e-04 eta 0:08:50
epoch [132/200] batch [25/50] time 0.085 (0.141) data 0.000 (0.057) loss 2.3516 (1.9963) acc 28.1250 (41.8750) lr 5.4601e-04 eta 0:08:01
epoch [132/200] batch [30/50] time 0.084 (0.139) data 0.000 (0.055) loss 1.5801 (1.9603) acc 53.1250 (43.4375) lr 5.4601e-04 eta 0:07:54
epoch [132/200] batch [35/50] time 0.086 (0.135) data 0.001 (0.051) loss 1.8838 (1.9714) acc 50.0000 (43.2143) lr 5.4601e-04 eta 0:07:41
epoch [132/200] batch [40/50] time 0.084 (0.129) data 0.000 (0.045) loss 1.7988 (1.9642) acc 46.8750 (43.2812) lr 5.4601e-04 eta 0:07:18
epoch [132/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.3242 (1.9635) acc 34.3750 (43.1250) lr 5.4601e-04 eta 0:07:03
epoch [132/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.7822 (1.9531) acc 50.0000 (43.6250) lr 5.3207e-04 eta 0:06:48
epoch [133/200] batch [5/50] time 0.086 (0.277) data 0.000 (0.192) loss 1.7412 (1.8980) acc 53.1250 (45.0000) lr 5.3207e-04 eta 0:15:40
epoch [133/200] batch [10/50] time 0.085 (0.181) data 0.000 (0.096) loss 1.9580 (1.9451) acc 43.7500 (43.7500) lr 5.3207e-04 eta 0:10:15
epoch [133/200] batch [15/50] time 0.084 (0.153) data 0.000 (0.068) loss 1.6699 (1.9405) acc 46.8750 (42.9167) lr 5.3207e-04 eta 0:08:37
epoch [133/200] batch [20/50] time 0.085 (0.137) data 0.000 (0.052) loss 1.6406 (1.9322) acc 56.2500 (44.2188) lr 5.3207e-04 eta 0:07:42
epoch [133/200] batch [25/50] time 0.085 (0.134) data 0.001 (0.049) loss 2.1348 (1.9359) acc 40.6250 (43.1250) lr 5.3207e-04 eta 0:07:30
epoch [133/200] batch [30/50] time 0.093 (0.126) data 0.009 (0.041) loss 1.9014 (1.9411) acc 40.6250 (42.6042) lr 5.3207e-04 eta 0:07:03
epoch [133/200] batch [35/50] time 0.083 (0.120) data 0.000 (0.035) loss 2.5664 (1.9400) acc 31.2500 (42.3214) lr 5.3207e-04 eta 0:06:42
epoch [133/200] batch [40/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.8193 (1.9303) acc 37.5000 (42.5000) lr 5.3207e-04 eta 0:06:46
epoch [133/200] batch [45/50] time 0.083 (0.117) data 0.000 (0.033) loss 1.9072 (1.9462) acc 37.5000 (42.2222) lr 5.3207e-04 eta 0:06:32
epoch [133/200] batch [50/50] time 0.084 (0.115) data 0.000 (0.031) loss 1.7432 (1.9500) acc 50.0000 (41.8750) lr 5.1825e-04 eta 0:06:25
epoch [134/200] batch [5/50] time 0.086 (0.287) data 0.001 (0.202) loss 1.6162 (1.7545) acc 53.1250 (50.6250) lr 5.1825e-04 eta 0:16:00
epoch [134/200] batch [10/50] time 0.086 (0.187) data 0.000 (0.101) loss 2.4941 (1.9124) acc 25.0000 (47.1875) lr 5.1825e-04 eta 0:10:23
epoch [134/200] batch [15/50] time 0.085 (0.165) data 0.000 (0.079) loss 1.8838 (1.9367) acc 53.1250 (45.8333) lr 5.1825e-04 eta 0:09:10
epoch [134/200] batch [20/50] time 0.085 (0.151) data 0.000 (0.066) loss 1.5547 (1.9201) acc 56.2500 (46.4062) lr 5.1825e-04 eta 0:08:24
epoch [134/200] batch [25/50] time 0.084 (0.138) data 0.000 (0.053) loss 1.7979 (1.9212) acc 40.6250 (45.2500) lr 5.1825e-04 eta 0:07:38
epoch [134/200] batch [30/50] time 0.083 (0.132) data 0.000 (0.047) loss 1.9717 (1.9106) acc 37.5000 (45.4167) lr 5.1825e-04 eta 0:07:19
epoch [134/200] batch [35/50] time 0.292 (0.131) data 0.209 (0.047) loss 1.9404 (1.9091) acc 40.6250 (45.2679) lr 5.1825e-04 eta 0:07:15
epoch [134/200] batch [40/50] time 0.082 (0.125) data 0.000 (0.041) loss 1.6445 (1.9119) acc 53.1250 (44.6875) lr 5.1825e-04 eta 0:06:54
epoch [134/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.9053 (1.9193) acc 53.1250 (44.9306) lr 5.1825e-04 eta 0:06:54
epoch [134/200] batch [50/50] time 0.084 (0.121) data 0.000 (0.037) loss 1.8877 (1.9418) acc 43.7500 (44.6875) lr 5.0454e-04 eta 0:06:39
epoch [135/200] batch [5/50] time 0.084 (0.318) data 0.000 (0.234) loss 1.6084 (1.9133) acc 50.0000 (43.7500) lr 5.0454e-04 eta 0:17:26
epoch [135/200] batch [10/50] time 0.083 (0.213) data 0.000 (0.130) loss 2.0664 (1.8662) acc 53.1250 (44.3750) lr 5.0454e-04 eta 0:11:40
epoch [135/200] batch [15/50] time 0.083 (0.170) data 0.000 (0.086) loss 1.9307 (1.8380) acc 46.8750 (44.5833) lr 5.0454e-04 eta 0:09:17
epoch [135/200] batch [20/50] time 0.083 (0.158) data 0.000 (0.075) loss 1.9932 (1.8419) acc 37.5000 (44.0625) lr 5.0454e-04 eta 0:08:39
epoch [135/200] batch [25/50] time 0.084 (0.143) data 0.000 (0.060) loss 2.1641 (1.8960) acc 34.3750 (42.8750) lr 5.0454e-04 eta 0:07:49
epoch [135/200] batch [30/50] time 0.085 (0.139) data 0.000 (0.056) loss 1.8223 (1.8833) acc 50.0000 (43.6458) lr 5.0454e-04 eta 0:07:35
epoch [135/200] batch [35/50] time 0.085 (0.139) data 0.000 (0.056) loss 2.2754 (1.8880) acc 40.6250 (44.5536) lr 5.0454e-04 eta 0:07:35
epoch [135/200] batch [40/50] time 0.083 (0.132) data 0.000 (0.049) loss 2.2832 (1.8999) acc 34.3750 (44.8438) lr 5.0454e-04 eta 0:07:11
epoch [135/200] batch [45/50] time 0.083 (0.127) data 0.000 (0.043) loss 2.1992 (1.9078) acc 40.6250 (44.6528) lr 5.0454e-04 eta 0:06:53
epoch [135/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.039) loss 1.6338 (1.9066) acc 59.3750 (44.8125) lr 4.9096e-04 eta 0:06:38
epoch [136/200] batch [5/50] time 0.083 (0.322) data 0.000 (0.238) loss 1.5254 (1.8223) acc 46.8750 (43.7500) lr 4.9096e-04 eta 0:17:24
epoch [136/200] batch [10/50] time 0.099 (0.210) data 0.015 (0.126) loss 1.8027 (1.8771) acc 56.2500 (47.1875) lr 4.9096e-04 eta 0:11:19
epoch [136/200] batch [15/50] time 0.083 (0.168) data 0.000 (0.084) loss 2.0195 (1.8436) acc 53.1250 (47.2917) lr 4.9096e-04 eta 0:09:02
epoch [136/200] batch [20/50] time 0.083 (0.156) data 0.000 (0.072) loss 2.1543 (1.8662) acc 34.3750 (47.1875) lr 4.9096e-04 eta 0:08:22
epoch [136/200] batch [25/50] time 0.084 (0.141) data 0.000 (0.058) loss 1.6992 (1.9123) acc 50.0000 (45.3750) lr 4.9096e-04 eta 0:07:35
epoch [136/200] batch [30/50] time 0.084 (0.137) data 0.000 (0.054) loss 1.8896 (1.9009) acc 53.1250 (45.6250) lr 4.9096e-04 eta 0:07:21
epoch [136/200] batch [35/50] time 0.086 (0.135) data 0.001 (0.051) loss 2.4160 (1.9153) acc 37.5000 (45.2679) lr 4.9096e-04 eta 0:07:12
epoch [136/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.044) loss 2.0469 (1.9258) acc 31.2500 (44.7656) lr 4.9096e-04 eta 0:06:51
epoch [136/200] batch [45/50] time 0.082 (0.127) data 0.000 (0.043) loss 2.1992 (1.9167) acc 37.5000 (45.0694) lr 4.9096e-04 eta 0:06:45
epoch [136/200] batch [50/50] time 0.082 (0.122) data 0.000 (0.039) loss 2.4043 (1.9210) acc 18.7500 (44.1875) lr 4.7750e-04 eta 0:06:31
epoch [137/200] batch [5/50] time 0.086 (0.264) data 0.001 (0.178) loss 1.8691 (1.8668) acc 43.7500 (48.7500) lr 4.7750e-04 eta 0:14:02
epoch [137/200] batch [10/50] time 0.085 (0.175) data 0.000 (0.089) loss 1.8486 (1.9021) acc 53.1250 (46.8750) lr 4.7750e-04 eta 0:09:17
epoch [137/200] batch [15/50] time 0.085 (0.151) data 0.000 (0.066) loss 1.7852 (1.9257) acc 53.1250 (45.4167) lr 4.7750e-04 eta 0:08:00
epoch [137/200] batch [20/50] time 0.106 (0.139) data 0.021 (0.054) loss 2.2852 (1.9252) acc 40.6250 (45.3125) lr 4.7750e-04 eta 0:07:22
epoch [137/200] batch [25/50] time 0.298 (0.137) data 0.213 (0.052) loss 1.7256 (1.8985) acc 50.0000 (45.6250) lr 4.7750e-04 eta 0:07:14
epoch [137/200] batch [30/50] time 0.086 (0.130) data 0.000 (0.045) loss 1.5508 (1.8586) acc 56.2500 (46.4583) lr 4.7750e-04 eta 0:06:52
epoch [137/200] batch [35/50] time 0.084 (0.125) data 0.000 (0.040) loss 1.9043 (1.8408) acc 46.8750 (46.8750) lr 4.7750e-04 eta 0:06:37
epoch [137/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.038) loss 2.2988 (1.8779) acc 21.8750 (46.4844) lr 4.7750e-04 eta 0:06:28
epoch [137/200] batch [45/50] time 0.083 (0.122) data 0.000 (0.037) loss 1.8975 (1.8973) acc 37.5000 (45.9028) lr 4.7750e-04 eta 0:06:23
epoch [137/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.033) loss 1.5889 (1.9131) acc 43.7500 (45.1250) lr 4.6417e-04 eta 0:06:10
epoch [138/200] batch [5/50] time 0.085 (0.317) data 0.000 (0.232) loss 2.6738 (2.0723) acc 31.2500 (41.2500) lr 4.6417e-04 eta 0:16:38
epoch [138/200] batch [10/50] time 0.086 (0.208) data 0.000 (0.123) loss 1.6543 (2.0636) acc 59.3750 (42.1875) lr 4.6417e-04 eta 0:10:52
epoch [138/200] batch [15/50] time 0.090 (0.167) data 0.000 (0.082) loss 2.2480 (1.9837) acc 31.2500 (43.3333) lr 4.6417e-04 eta 0:08:45
epoch [138/200] batch [20/50] time 0.085 (0.147) data 0.000 (0.062) loss 1.9873 (1.9638) acc 40.6250 (43.7500) lr 4.6417e-04 eta 0:07:40
epoch [138/200] batch [25/50] time 0.085 (0.135) data 0.000 (0.049) loss 2.0840 (1.9780) acc 31.2500 (42.1250) lr 4.6417e-04 eta 0:07:01
epoch [138/200] batch [30/50] time 0.084 (0.129) data 0.000 (0.044) loss 2.0781 (1.9973) acc 43.7500 (41.7708) lr 4.6417e-04 eta 0:06:42
epoch [138/200] batch [35/50] time 0.085 (0.123) data 0.001 (0.038) loss 1.9160 (1.9897) acc 43.7500 (41.6071) lr 4.6417e-04 eta 0:06:22
epoch [138/200] batch [40/50] time 0.083 (0.121) data 0.000 (0.036) loss 2.1914 (1.9905) acc 37.5000 (41.7188) lr 4.6417e-04 eta 0:06:15
epoch [138/200] batch [45/50] time 0.083 (0.121) data 0.000 (0.036) loss 2.1699 (1.9971) acc 37.5000 (41.8056) lr 4.6417e-04 eta 0:06:15
epoch [138/200] batch [50/50] time 0.085 (0.117) data 0.000 (0.033) loss 2.1914 (1.9786) acc 37.5000 (42.3125) lr 4.5098e-04 eta 0:06:03
epoch [139/200] batch [5/50] time 0.087 (0.300) data 0.000 (0.214) loss 1.9980 (1.9998) acc 37.5000 (46.2500) lr 4.5098e-04 eta 0:15:28
epoch [139/200] batch [10/50] time 0.086 (0.199) data 0.000 (0.113) loss 1.9971 (2.0040) acc 40.6250 (43.1250) lr 4.5098e-04 eta 0:10:14
epoch [139/200] batch [15/50] time 0.085 (0.161) data 0.000 (0.076) loss 1.4697 (1.9337) acc 59.3750 (45.8333) lr 4.5098e-04 eta 0:08:17
epoch [139/200] batch [20/50] time 0.085 (0.147) data 0.001 (0.061) loss 1.4824 (1.9199) acc 56.2500 (45.9375) lr 4.5098e-04 eta 0:07:31
epoch [139/200] batch [25/50] time 0.085 (0.134) data 0.000 (0.049) loss 2.0508 (1.9193) acc 34.3750 (45.3750) lr 4.5098e-04 eta 0:06:53
epoch [139/200] batch [30/50] time 0.085 (0.126) data 0.001 (0.041) loss 1.9844 (1.8980) acc 50.0000 (45.7292) lr 4.5098e-04 eta 0:06:27
epoch [139/200] batch [35/50] time 0.086 (0.121) data 0.001 (0.035) loss 2.0762 (1.9126) acc 37.5000 (45.0893) lr 4.5098e-04 eta 0:06:09
epoch [139/200] batch [40/50] time 0.085 (0.116) data 0.000 (0.031) loss 1.7119 (1.9021) acc 53.1250 (44.8438) lr 4.5098e-04 eta 0:05:55
epoch [139/200] batch [45/50] time 0.085 (0.113) data 0.000 (0.027) loss 2.1172 (1.9039) acc 40.6250 (44.3750) lr 4.5098e-04 eta 0:05:44
epoch [139/200] batch [50/50] time 0.084 (0.111) data 0.000 (0.026) loss 1.9111 (1.9059) acc 46.8750 (44.0625) lr 4.3792e-04 eta 0:05:39
epoch [140/200] batch [5/50] time 0.086 (0.293) data 0.000 (0.208) loss 1.6621 (1.8568) acc 46.8750 (48.1250) lr 4.3792e-04 eta 0:14:53
epoch [140/200] batch [10/50] time 0.086 (0.190) data 0.000 (0.104) loss 1.8115 (1.9523) acc 46.8750 (43.1250) lr 4.3792e-04 eta 0:09:37
epoch [140/200] batch [15/50] time 0.087 (0.161) data 0.000 (0.075) loss 1.9707 (1.8948) acc 34.3750 (43.5417) lr 4.3792e-04 eta 0:08:07
epoch [140/200] batch [20/50] time 0.085 (0.142) data 0.001 (0.056) loss 1.9814 (1.8840) acc 40.6250 (43.9062) lr 4.3792e-04 eta 0:07:10
epoch [140/200] batch [25/50] time 0.086 (0.136) data 0.000 (0.050) loss 1.9443 (1.8889) acc 40.6250 (43.7500) lr 4.3792e-04 eta 0:06:50
epoch [140/200] batch [30/50] time 0.240 (0.132) data 0.155 (0.047) loss 1.9316 (1.8797) acc 40.6250 (43.8542) lr 4.3792e-04 eta 0:06:40
epoch [140/200] batch [35/50] time 0.086 (0.126) data 0.001 (0.040) loss 1.5820 (1.8746) acc 56.2500 (43.7500) lr 4.3792e-04 eta 0:06:19
epoch [140/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.038) loss 1.7627 (1.8857) acc 50.0000 (43.9844) lr 4.3792e-04 eta 0:06:10
epoch [140/200] batch [45/50] time 0.082 (0.119) data 0.000 (0.034) loss 2.1191 (1.9221) acc 37.5000 (42.8472) lr 4.3792e-04 eta 0:05:56
epoch [140/200] batch [50/50] time 0.082 (0.118) data 0.000 (0.033) loss 1.9316 (1.9479) acc 53.1250 (42.7500) lr 4.2499e-04 eta 0:05:53
epoch [141/200] batch [5/50] time 0.325 (0.312) data 0.241 (0.227) loss 2.2324 (1.9404) acc 50.0000 (47.5000) lr 4.2499e-04 eta 0:15:33
epoch [141/200] batch [10/50] time 0.085 (0.198) data 0.000 (0.114) loss 1.8359 (1.9347) acc 53.1250 (46.5625) lr 4.2499e-04 eta 0:09:52
epoch [141/200] batch [15/50] time 0.085 (0.172) data 0.001 (0.087) loss 1.8115 (1.9238) acc 37.5000 (46.0417) lr 4.2499e-04 eta 0:08:31
epoch [141/200] batch [20/50] time 0.084 (0.150) data 0.000 (0.065) loss 1.7451 (1.8815) acc 56.2500 (47.3438) lr 4.2499e-04 eta 0:07:26
epoch [141/200] batch [25/50] time 0.084 (0.144) data 0.000 (0.060) loss 1.7236 (1.9112) acc 53.1250 (46.3750) lr 4.2499e-04 eta 0:07:08
epoch [141/200] batch [30/50] time 0.085 (0.139) data 0.000 (0.055) loss 1.8633 (1.9514) acc 53.1250 (45.1042) lr 4.2499e-04 eta 0:06:52
epoch [141/200] batch [35/50] time 0.086 (0.131) data 0.001 (0.047) loss 1.7363 (1.9453) acc 43.7500 (44.5536) lr 4.2499e-04 eta 0:06:29
epoch [141/200] batch [40/50] time 0.083 (0.130) data 0.000 (0.045) loss 2.1875 (1.9986) acc 37.5000 (43.0469) lr 4.2499e-04 eta 0:06:24
epoch [141/200] batch [45/50] time 0.233 (0.128) data 0.150 (0.044) loss 1.7920 (2.0018) acc 46.8750 (43.2639) lr 4.2499e-04 eta 0:06:17
epoch [141/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.039) loss 1.6982 (1.9671) acc 37.5000 (43.9375) lr 4.1221e-04 eta 0:06:03
epoch [142/200] batch [5/50] time 0.085 (0.326) data 0.000 (0.241) loss 1.5811 (1.7779) acc 56.2500 (45.0000) lr 4.1221e-04 eta 0:16:00
epoch [142/200] batch [10/50] time 0.085 (0.206) data 0.000 (0.121) loss 1.9033 (1.8088) acc 46.8750 (47.1875) lr 4.1221e-04 eta 0:10:05
epoch [142/200] batch [15/50] time 0.086 (0.174) data 0.000 (0.089) loss 2.0566 (1.8431) acc 37.5000 (46.8750) lr 4.1221e-04 eta 0:08:31
epoch [142/200] batch [20/50] time 0.086 (0.158) data 0.000 (0.072) loss 1.6455 (1.8183) acc 50.0000 (47.1875) lr 4.1221e-04 eta 0:07:41
epoch [142/200] batch [25/50] time 0.085 (0.143) data 0.000 (0.058) loss 2.0215 (1.8350) acc 37.5000 (46.0000) lr 4.1221e-04 eta 0:06:58
epoch [142/200] batch [30/50] time 0.086 (0.138) data 0.000 (0.053) loss 2.2910 (1.8851) acc 37.5000 (45.1042) lr 4.1221e-04 eta 0:06:42
epoch [142/200] batch [35/50] time 0.221 (0.134) data 0.138 (0.049) loss 1.6973 (1.8887) acc 46.8750 (45.0893) lr 4.1221e-04 eta 0:06:31
epoch [142/200] batch [40/50] time 0.084 (0.128) data 0.000 (0.043) loss 2.1270 (1.8665) acc 40.6250 (45.8594) lr 4.1221e-04 eta 0:06:12
epoch [142/200] batch [45/50] time 0.082 (0.127) data 0.000 (0.043) loss 2.3359 (1.9106) acc 40.6250 (44.5833) lr 4.1221e-04 eta 0:06:10
epoch [142/200] batch [50/50] time 0.082 (0.123) data 0.000 (0.039) loss 1.8066 (1.9121) acc 43.7500 (44.3125) lr 3.9958e-04 eta 0:05:56
epoch [143/200] batch [5/50] time 0.085 (0.315) data 0.000 (0.229) loss 1.7139 (1.8127) acc 46.8750 (53.1250) lr 3.9958e-04 eta 0:15:11
epoch [143/200] batch [10/50] time 0.157 (0.207) data 0.073 (0.122) loss 2.2793 (1.9406) acc 37.5000 (46.8750) lr 3.9958e-04 eta 0:09:59
epoch [143/200] batch [15/50] time 0.086 (0.167) data 0.000 (0.081) loss 2.0781 (1.9365) acc 37.5000 (45.2083) lr 3.9958e-04 eta 0:08:00
epoch [143/200] batch [20/50] time 0.085 (0.152) data 0.000 (0.067) loss 1.9453 (1.9427) acc 46.8750 (45.3125) lr 3.9958e-04 eta 0:07:16
epoch [143/200] batch [25/50] time 0.084 (0.138) data 0.001 (0.053) loss 1.7148 (1.9172) acc 53.1250 (46.0000) lr 3.9958e-04 eta 0:06:37
epoch [143/200] batch [30/50] time 0.085 (0.138) data 0.000 (0.053) loss 2.4395 (1.9536) acc 37.5000 (44.5833) lr 3.9958e-04 eta 0:06:36
epoch [143/200] batch [35/50] time 0.086 (0.135) data 0.001 (0.050) loss 1.4531 (1.9543) acc 50.0000 (44.1071) lr 3.9958e-04 eta 0:06:26
epoch [143/200] batch [40/50] time 0.084 (0.129) data 0.000 (0.044) loss 1.8906 (1.9442) acc 43.7500 (44.7656) lr 3.9958e-04 eta 0:06:07
epoch [143/200] batch [45/50] time 0.082 (0.124) data 0.000 (0.040) loss 1.5830 (1.9293) acc 53.1250 (45.0000) lr 3.9958e-04 eta 0:05:55
epoch [143/200] batch [50/50] time 0.085 (0.120) data 0.000 (0.036) loss 2.3945 (1.9248) acc 34.3750 (44.9375) lr 3.8709e-04 eta 0:05:42
epoch [144/200] batch [5/50] time 0.085 (0.317) data 0.000 (0.232) loss 2.5000 (2.1781) acc 34.3750 (40.0000) lr 3.8709e-04 eta 0:15:02
epoch [144/200] batch [10/50] time 0.084 (0.219) data 0.000 (0.133) loss 1.4688 (2.0381) acc 59.3750 (42.5000) lr 3.8709e-04 eta 0:10:22
epoch [144/200] batch [15/50] time 0.085 (0.174) data 0.000 (0.089) loss 2.2031 (1.9990) acc 31.2500 (41.8750) lr 3.8709e-04 eta 0:08:14
epoch [144/200] batch [20/50] time 0.084 (0.154) data 0.000 (0.069) loss 1.7334 (1.9805) acc 53.1250 (43.1250) lr 3.8709e-04 eta 0:07:14
epoch [144/200] batch [25/50] time 0.296 (0.148) data 0.213 (0.064) loss 2.3164 (1.9518) acc 28.1250 (44.0000) lr 3.8709e-04 eta 0:06:58
epoch [144/200] batch [30/50] time 0.084 (0.138) data 0.000 (0.053) loss 1.5322 (1.9088) acc 59.3750 (45.3125) lr 3.8709e-04 eta 0:06:28
epoch [144/200] batch [35/50] time 0.084 (0.134) data 0.001 (0.050) loss 1.9229 (1.9565) acc 46.8750 (44.1964) lr 3.8709e-04 eta 0:06:18
epoch [144/200] batch [40/50] time 0.085 (0.128) data 0.000 (0.044) loss 1.6406 (1.9352) acc 59.3750 (44.9219) lr 3.8709e-04 eta 0:06:00
epoch [144/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 1.7002 (1.9394) acc 53.1250 (44.8611) lr 3.8709e-04 eta 0:05:49
epoch [144/200] batch [50/50] time 0.082 (0.120) data 0.000 (0.036) loss 1.9941 (1.9282) acc 43.7500 (45.0000) lr 3.7476e-04 eta 0:05:36
epoch [145/200] batch [5/50] time 0.085 (0.320) data 0.000 (0.235) loss 2.0938 (1.9852) acc 37.5000 (39.3750) lr 3.7476e-04 eta 0:14:54
epoch [145/200] batch [10/50] time 0.261 (0.220) data 0.176 (0.135) loss 1.5986 (1.8687) acc 46.8750 (43.1250) lr 3.7476e-04 eta 0:10:14
epoch [145/200] batch [15/50] time 0.084 (0.175) data 0.000 (0.090) loss 1.7246 (1.8880) acc 50.0000 (43.9583) lr 3.7476e-04 eta 0:08:07
epoch [145/200] batch [20/50] time 0.086 (0.160) data 0.001 (0.075) loss 2.0312 (1.9049) acc 28.1250 (43.4375) lr 3.7476e-04 eta 0:07:25
epoch [145/200] batch [25/50] time 0.085 (0.145) data 0.000 (0.060) loss 1.7939 (1.8891) acc 50.0000 (44.1250) lr 3.7476e-04 eta 0:06:42
epoch [145/200] batch [30/50] time 0.084 (0.139) data 0.000 (0.054) loss 1.5078 (1.8513) acc 59.3750 (45.4167) lr 3.7476e-04 eta 0:06:24
epoch [145/200] batch [35/50] time 0.083 (0.135) data 0.001 (0.051) loss 2.1367 (1.8591) acc 28.1250 (45.4464) lr 3.7476e-04 eta 0:06:13
epoch [145/200] batch [40/50] time 0.084 (0.129) data 0.000 (0.045) loss 1.8867 (1.8973) acc 53.1250 (44.2188) lr 3.7476e-04 eta 0:05:55
epoch [145/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 1.7441 (1.9087) acc 43.7500 (43.7500) lr 3.7476e-04 eta 0:05:40
epoch [145/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.9355 (1.9003) acc 40.6250 (44.0625) lr 3.6258e-04 eta 0:05:29
epoch [146/200] batch [5/50] time 0.084 (0.310) data 0.000 (0.226) loss 1.8877 (1.7842) acc 56.2500 (50.6250) lr 3.6258e-04 eta 0:14:12
epoch [146/200] batch [10/50] time 0.083 (0.209) data 0.000 (0.125) loss 2.2324 (1.9099) acc 34.3750 (44.3750) lr 3.6258e-04 eta 0:09:32
epoch [146/200] batch [15/50] time 0.084 (0.167) data 0.000 (0.083) loss 2.0645 (1.9013) acc 25.0000 (42.9167) lr 3.6258e-04 eta 0:07:37
epoch [146/200] batch [20/50] time 0.084 (0.160) data 0.000 (0.076) loss 1.4980 (1.8993) acc 50.0000 (43.2812) lr 3.6258e-04 eta 0:07:16
epoch [146/200] batch [25/50] time 0.158 (0.148) data 0.075 (0.064) loss 2.2754 (1.9323) acc 40.6250 (42.7500) lr 3.6258e-04 eta 0:06:42
epoch [146/200] batch [30/50] time 0.085 (0.137) data 0.000 (0.053) loss 2.0117 (1.9345) acc 37.5000 (43.1250) lr 3.6258e-04 eta 0:06:13
epoch [146/200] batch [35/50] time 0.084 (0.130) data 0.000 (0.046) loss 1.5996 (1.9078) acc 43.7500 (43.8393) lr 3.6258e-04 eta 0:05:52
epoch [146/200] batch [40/50] time 0.085 (0.124) data 0.000 (0.040) loss 2.0762 (1.9081) acc 46.8750 (44.2969) lr 3.6258e-04 eta 0:05:36
epoch [146/200] batch [45/50] time 0.083 (0.120) data 0.000 (0.036) loss 2.0898 (1.9148) acc 40.6250 (44.2361) lr 3.6258e-04 eta 0:05:23
epoch [146/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.032) loss 1.7158 (1.9184) acc 46.8750 (43.9375) lr 3.5055e-04 eta 0:05:13
epoch [147/200] batch [5/50] time 0.085 (0.338) data 0.000 (0.253) loss 1.9736 (1.9992) acc 40.6250 (44.3750) lr 3.5055e-04 eta 0:15:11
epoch [147/200] batch [10/50] time 0.086 (0.215) data 0.000 (0.130) loss 1.5811 (1.9563) acc 50.0000 (43.7500) lr 3.5055e-04 eta 0:09:39
epoch [147/200] batch [15/50] time 0.085 (0.172) data 0.000 (0.087) loss 1.7510 (1.9365) acc 40.6250 (43.1250) lr 3.5055e-04 eta 0:07:42
epoch [147/200] batch [20/50] time 0.085 (0.155) data 0.000 (0.070) loss 1.8047 (1.9384) acc 50.0000 (43.9062) lr 3.5055e-04 eta 0:06:55
epoch [147/200] batch [25/50] time 0.276 (0.149) data 0.190 (0.064) loss 1.8125 (1.9021) acc 53.1250 (45.5000) lr 3.5055e-04 eta 0:06:38
epoch [147/200] batch [30/50] time 0.084 (0.138) data 0.000 (0.053) loss 1.9248 (1.9125) acc 37.5000 (45.4167) lr 3.5055e-04 eta 0:06:08
epoch [147/200] batch [35/50] time 0.083 (0.135) data 0.000 (0.050) loss 1.5420 (1.9118) acc 50.0000 (45.1786) lr 3.5055e-04 eta 0:05:59
epoch [147/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.044) loss 2.0117 (1.9156) acc 40.6250 (44.5312) lr 3.5055e-04 eta 0:05:41
epoch [147/200] batch [45/50] time 0.084 (0.125) data 0.000 (0.040) loss 1.7168 (1.9118) acc 53.1250 (44.4444) lr 3.5055e-04 eta 0:05:31
epoch [147/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.036) loss 2.1055 (1.9166) acc 37.5000 (44.7500) lr 3.3869e-04 eta 0:05:19
epoch [148/200] batch [5/50] time 0.086 (0.299) data 0.000 (0.213) loss 1.4775 (2.0129) acc 53.1250 (37.5000) lr 3.3869e-04 eta 0:13:09
epoch [148/200] batch [10/50] time 0.093 (0.193) data 0.007 (0.107) loss 2.1191 (1.9724) acc 37.5000 (40.3125) lr 3.3869e-04 eta 0:08:29
epoch [148/200] batch [15/50] time 0.084 (0.170) data 0.000 (0.084) loss 1.9570 (1.8850) acc 46.8750 (43.7500) lr 3.3869e-04 eta 0:07:27
epoch [148/200] batch [20/50] time 0.085 (0.154) data 0.000 (0.069) loss 2.0195 (1.9335) acc 50.0000 (44.0625) lr 3.3869e-04 eta 0:06:45
epoch [148/200] batch [25/50] time 0.085 (0.140) data 0.000 (0.055) loss 2.0312 (1.9557) acc 40.6250 (43.6250) lr 3.3869e-04 eta 0:06:08
epoch [148/200] batch [30/50] time 0.085 (0.134) data 0.000 (0.049) loss 1.5527 (1.9211) acc 50.0000 (43.6458) lr 3.3869e-04 eta 0:05:52
epoch [148/200] batch [35/50] time 0.085 (0.127) data 0.001 (0.042) loss 1.6992 (1.9027) acc 50.0000 (44.2857) lr 3.3869e-04 eta 0:05:33
epoch [148/200] batch [40/50] time 0.084 (0.122) data 0.000 (0.037) loss 1.6270 (1.8824) acc 56.2500 (44.9219) lr 3.3869e-04 eta 0:05:18
epoch [148/200] batch [45/50] time 0.083 (0.118) data 0.000 (0.033) loss 1.6680 (1.8796) acc 50.0000 (44.5139) lr 3.3869e-04 eta 0:05:06
epoch [148/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.030) loss 1.9014 (1.8812) acc 40.6250 (44.7500) lr 3.2699e-04 eta 0:04:57
epoch [149/200] batch [5/50] time 0.086 (0.314) data 0.000 (0.229) loss 2.4180 (1.9826) acc 31.2500 (41.8750) lr 3.2699e-04 eta 0:13:35
epoch [149/200] batch [10/50] time 0.087 (0.208) data 0.001 (0.123) loss 1.9893 (1.9319) acc 40.6250 (39.6875) lr 3.2699e-04 eta 0:08:58
epoch [149/200] batch [15/50] time 0.085 (0.167) data 0.000 (0.082) loss 2.3652 (1.9115) acc 46.8750 (42.0833) lr 3.2699e-04 eta 0:07:11
epoch [149/200] batch [20/50] time 0.084 (0.159) data 0.000 (0.074) loss 1.7354 (1.8951) acc 40.6250 (42.6562) lr 3.2699e-04 eta 0:06:50
epoch [149/200] batch [25/50] time 0.259 (0.151) data 0.175 (0.066) loss 2.0918 (1.8800) acc 37.5000 (42.8750) lr 3.2699e-04 eta 0:06:29
epoch [149/200] batch [30/50] time 0.084 (0.140) data 0.000 (0.055) loss 1.8750 (1.8881) acc 53.1250 (43.5417) lr 3.2699e-04 eta 0:06:00
epoch [149/200] batch [35/50] time 0.085 (0.136) data 0.000 (0.051) loss 2.0410 (1.8948) acc 46.8750 (43.7500) lr 3.2699e-04 eta 0:05:48
epoch [149/200] batch [40/50] time 0.084 (0.129) data 0.000 (0.045) loss 1.5596 (1.9084) acc 62.5000 (43.7500) lr 3.2699e-04 eta 0:05:31
epoch [149/200] batch [45/50] time 0.083 (0.129) data 0.000 (0.045) loss 2.2637 (1.9038) acc 46.8750 (44.4444) lr 3.2699e-04 eta 0:05:30
epoch [149/200] batch [50/50] time 0.082 (0.125) data 0.000 (0.040) loss 2.2695 (1.8987) acc 25.0000 (44.3125) lr 3.1545e-04 eta 0:05:17
epoch [150/200] batch [5/50] time 0.084 (0.357) data 0.000 (0.272) loss 1.6641 (1.8051) acc 50.0000 (43.1250) lr 3.1545e-04 eta 0:15:09
epoch [150/200] batch [10/50] time 0.086 (0.223) data 0.000 (0.138) loss 2.3906 (1.9776) acc 31.2500 (41.8750) lr 3.1545e-04 eta 0:09:25
epoch [150/200] batch [15/50] time 0.084 (0.177) data 0.000 (0.092) loss 2.0996 (1.9503) acc 53.1250 (44.1667) lr 3.1545e-04 eta 0:07:28
epoch [150/200] batch [20/50] time 0.085 (0.162) data 0.000 (0.077) loss 1.9473 (1.9479) acc 53.1250 (44.0625) lr 3.1545e-04 eta 0:06:49
epoch [150/200] batch [25/50] time 0.281 (0.154) data 0.198 (0.070) loss 2.1836 (1.9291) acc 37.5000 (43.8750) lr 3.1545e-04 eta 0:06:29
epoch [150/200] batch [30/50] time 0.086 (0.143) data 0.000 (0.058) loss 2.2910 (1.9321) acc 34.3750 (44.2708) lr 3.1545e-04 eta 0:06:00
epoch [150/200] batch [35/50] time 0.084 (0.139) data 0.000 (0.054) loss 1.9893 (1.9334) acc 43.7500 (43.8393) lr 3.1545e-04 eta 0:05:49
epoch [150/200] batch [40/50] time 0.084 (0.132) data 0.000 (0.047) loss 1.8828 (1.9333) acc 56.2500 (44.1406) lr 3.1545e-04 eta 0:05:31
epoch [150/200] batch [45/50] time 0.082 (0.129) data 0.000 (0.044) loss 1.8008 (1.9320) acc 50.0000 (44.1667) lr 3.1545e-04 eta 0:05:22
epoch [150/200] batch [50/50] time 0.083 (0.124) data 0.000 (0.040) loss 1.8359 (1.9324) acc 40.6250 (43.7500) lr 3.0409e-04 eta 0:05:10
epoch [151/200] batch [5/50] time 0.085 (0.312) data 0.000 (0.227) loss 1.9229 (1.8465) acc 46.8750 (48.1250) lr 3.0409e-04 eta 0:12:57
epoch [151/200] batch [10/50] time 0.217 (0.211) data 0.132 (0.127) loss 2.0039 (1.7936) acc 40.6250 (48.7500) lr 3.0409e-04 eta 0:08:46
epoch [151/200] batch [15/50] time 0.084 (0.169) data 0.000 (0.085) loss 1.8223 (1.8418) acc 37.5000 (47.2917) lr 3.0409e-04 eta 0:07:00
epoch [151/200] batch [20/50] time 0.084 (0.157) data 0.000 (0.073) loss 1.8760 (1.8534) acc 43.7500 (45.6250) lr 3.0409e-04 eta 0:06:30
epoch [151/200] batch [25/50] time 0.085 (0.143) data 0.000 (0.058) loss 1.6406 (1.8649) acc 50.0000 (45.6250) lr 3.0409e-04 eta 0:05:53
epoch [151/200] batch [30/50] time 0.085 (0.138) data 0.000 (0.053) loss 1.9639 (1.8668) acc 40.6250 (45.0000) lr 3.0409e-04 eta 0:05:40
epoch [151/200] batch [35/50] time 0.083 (0.136) data 0.001 (0.051) loss 2.0664 (1.9005) acc 37.5000 (43.3036) lr 3.0409e-04 eta 0:05:34
epoch [151/200] batch [40/50] time 0.083 (0.129) data 0.000 (0.045) loss 2.0664 (1.9016) acc 37.5000 (43.5938) lr 3.0409e-04 eta 0:05:17
epoch [151/200] batch [45/50] time 0.083 (0.129) data 0.000 (0.044) loss 2.0039 (1.9158) acc 43.7500 (43.5417) lr 3.0409e-04 eta 0:05:15
epoch [151/200] batch [50/50] time 0.083 (0.124) data 0.000 (0.040) loss 1.8340 (1.9065) acc 37.5000 (43.6875) lr 2.9289e-04 eta 0:05:04
epoch [152/200] batch [5/50] time 0.085 (0.314) data 0.000 (0.229) loss 2.0938 (2.0182) acc 31.2500 (40.6250) lr 2.9289e-04 eta 0:12:48
epoch [152/200] batch [10/50] time 0.085 (0.217) data 0.001 (0.132) loss 1.9053 (1.9240) acc 50.0000 (43.7500) lr 2.9289e-04 eta 0:08:50
epoch [152/200] batch [15/50] time 0.083 (0.173) data 0.000 (0.088) loss 1.4951 (1.8401) acc 56.2500 (45.0000) lr 2.9289e-04 eta 0:07:01
epoch [152/200] batch [20/50] time 0.083 (0.159) data 0.000 (0.075) loss 1.9043 (1.8969) acc 40.6250 (43.5938) lr 2.9289e-04 eta 0:06:27
epoch [152/200] batch [25/50] time 0.084 (0.144) data 0.000 (0.060) loss 2.1055 (1.9190) acc 31.2500 (42.6250) lr 2.9289e-04 eta 0:05:50
epoch [152/200] batch [30/50] time 0.085 (0.135) data 0.000 (0.051) loss 1.4199 (1.8801) acc 56.2500 (43.6458) lr 2.9289e-04 eta 0:05:27
epoch [152/200] batch [35/50] time 0.085 (0.131) data 0.000 (0.047) loss 2.2383 (1.8755) acc 31.2500 (43.2143) lr 2.9289e-04 eta 0:05:16
epoch [152/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.041) loss 2.1641 (1.8980) acc 37.5000 (42.9688) lr 2.9289e-04 eta 0:05:01
epoch [152/200] batch [45/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.6885 (1.8996) acc 50.0000 (43.2639) lr 2.9289e-04 eta 0:04:49
epoch [152/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.033) loss 1.7861 (1.8930) acc 53.1250 (43.5000) lr 2.8187e-04 eta 0:04:40
epoch [153/200] batch [5/50] time 0.086 (0.317) data 0.001 (0.231) loss 1.7168 (1.8687) acc 34.3750 (41.8750) lr 2.8187e-04 eta 0:12:38
epoch [153/200] batch [10/50] time 0.085 (0.211) data 0.000 (0.126) loss 2.4297 (1.9550) acc 40.6250 (43.7500) lr 2.8187e-04 eta 0:08:24
epoch [153/200] batch [15/50] time 0.085 (0.169) data 0.000 (0.084) loss 1.7842 (1.8671) acc 46.8750 (45.6250) lr 2.8187e-04 eta 0:06:43
epoch [153/200] batch [20/50] time 0.086 (0.157) data 0.000 (0.072) loss 2.1328 (1.8498) acc 34.3750 (46.0938) lr 2.8187e-04 eta 0:06:14
epoch [153/200] batch [25/50] time 0.201 (0.148) data 0.117 (0.063) loss 2.1738 (1.8668) acc 40.6250 (45.3750) lr 2.8187e-04 eta 0:05:50
epoch [153/200] batch [30/50] time 0.086 (0.137) data 0.000 (0.052) loss 1.8271 (1.8406) acc 37.5000 (46.3542) lr 2.8187e-04 eta 0:05:25
epoch [153/200] batch [35/50] time 0.086 (0.133) data 0.001 (0.047) loss 1.8975 (1.8606) acc 53.1250 (45.2679) lr 2.8187e-04 eta 0:05:14
epoch [153/200] batch [40/50] time 0.085 (0.127) data 0.000 (0.042) loss 1.6670 (1.8914) acc 56.2500 (44.7656) lr 2.8187e-04 eta 0:04:59
epoch [153/200] batch [45/50] time 0.082 (0.126) data 0.000 (0.041) loss 2.0527 (1.8774) acc 40.6250 (45.2083) lr 2.8187e-04 eta 0:04:55
epoch [153/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.8555 (1.8896) acc 43.7500 (44.7500) lr 2.7103e-04 eta 0:04:44
epoch [154/200] batch [5/50] time 0.085 (0.324) data 0.000 (0.239) loss 1.6885 (1.6996) acc 53.1250 (48.7500) lr 2.7103e-04 eta 0:12:40
epoch [154/200] batch [10/50] time 0.085 (0.217) data 0.001 (0.132) loss 1.6875 (1.7362) acc 43.7500 (49.3750) lr 2.7103e-04 eta 0:08:27
epoch [154/200] batch [15/50] time 0.086 (0.173) data 0.000 (0.088) loss 1.6094 (1.7633) acc 62.5000 (50.6250) lr 2.7103e-04 eta 0:06:44
epoch [154/200] batch [20/50] time 0.085 (0.151) data 0.000 (0.066) loss 1.8496 (1.7661) acc 46.8750 (49.6875) lr 2.7103e-04 eta 0:05:52
epoch [154/200] batch [25/50] time 0.084 (0.138) data 0.000 (0.053) loss 1.7891 (1.8064) acc 40.6250 (48.1250) lr 2.7103e-04 eta 0:05:20
epoch [154/200] batch [30/50] time 0.085 (0.129) data 0.001 (0.044) loss 1.7295 (1.8198) acc 53.1250 (47.6042) lr 2.7103e-04 eta 0:04:59
epoch [154/200] batch [35/50] time 0.084 (0.123) data 0.001 (0.038) loss 2.3281 (1.8429) acc 43.7500 (47.3214) lr 2.7103e-04 eta 0:04:44
epoch [154/200] batch [40/50] time 0.084 (0.118) data 0.000 (0.033) loss 2.3496 (1.8526) acc 40.6250 (47.4219) lr 2.7103e-04 eta 0:04:32
epoch [154/200] batch [45/50] time 0.085 (0.114) data 0.000 (0.030) loss 1.5410 (1.8431) acc 56.2500 (47.8472) lr 2.7103e-04 eta 0:04:23
epoch [154/200] batch [50/50] time 0.083 (0.111) data 0.000 (0.027) loss 2.0176 (1.8581) acc 43.7500 (47.1875) lr 2.6037e-04 eta 0:04:15
epoch [155/200] batch [5/50] time 0.086 (0.299) data 0.000 (0.213) loss 2.3125 (2.0748) acc 31.2500 (40.0000) lr 2.6037e-04 eta 0:11:25
epoch [155/200] batch [10/50] time 0.086 (0.193) data 0.000 (0.107) loss 1.8242 (2.0128) acc 43.7500 (43.1250) lr 2.6037e-04 eta 0:07:21
epoch [155/200] batch [15/50] time 0.085 (0.157) data 0.000 (0.071) loss 2.0996 (1.9691) acc 43.7500 (44.5833) lr 2.6037e-04 eta 0:05:58
epoch [155/200] batch [20/50] time 0.086 (0.140) data 0.000 (0.054) loss 1.8350 (1.9646) acc 43.7500 (44.8438) lr 2.6037e-04 eta 0:05:19
epoch [155/200] batch [25/50] time 0.086 (0.130) data 0.000 (0.044) loss 1.7314 (1.9443) acc 46.8750 (44.8750) lr 2.6037e-04 eta 0:04:55
epoch [155/200] batch [30/50] time 0.086 (0.122) data 0.000 (0.036) loss 1.9863 (1.9595) acc 46.8750 (44.2708) lr 2.6037e-04 eta 0:04:37
epoch [155/200] batch [35/50] time 0.087 (0.117) data 0.001 (0.031) loss 2.3262 (1.9796) acc 34.3750 (43.5714) lr 2.6037e-04 eta 0:04:25
epoch [155/200] batch [40/50] time 0.084 (0.113) data 0.000 (0.027) loss 1.8643 (1.9606) acc 50.0000 (43.5156) lr 2.6037e-04 eta 0:04:15
epoch [155/200] batch [45/50] time 0.083 (0.110) data 0.000 (0.024) loss 1.7109 (1.9614) acc 46.8750 (43.5417) lr 2.6037e-04 eta 0:04:07
epoch [155/200] batch [50/50] time 0.084 (0.107) data 0.000 (0.022) loss 1.6045 (1.9372) acc 46.8750 (44.3125) lr 2.4989e-04 eta 0:04:01
epoch [156/200] batch [5/50] time 0.087 (0.316) data 0.000 (0.231) loss 1.6123 (1.8100) acc 46.8750 (41.8750) lr 2.4989e-04 eta 0:11:49
epoch [156/200] batch [10/50] time 0.270 (0.219) data 0.187 (0.135) loss 1.1348 (1.7984) acc 68.7500 (43.4375) lr 2.4989e-04 eta 0:08:09
epoch [156/200] batch [15/50] time 0.085 (0.174) data 0.000 (0.090) loss 1.9316 (1.8754) acc 43.7500 (42.2917) lr 2.4989e-04 eta 0:06:28
epoch [156/200] batch [20/50] time 0.086 (0.160) data 0.000 (0.076) loss 1.5801 (1.8771) acc 53.1250 (42.6562) lr 2.4989e-04 eta 0:05:56
epoch [156/200] batch [25/50] time 0.084 (0.145) data 0.000 (0.061) loss 1.8740 (1.9080) acc 34.3750 (42.0000) lr 2.4989e-04 eta 0:05:22
epoch [156/200] batch [30/50] time 0.084 (0.141) data 0.000 (0.057) loss 1.6738 (1.9144) acc 53.1250 (42.2917) lr 2.4989e-04 eta 0:05:13
epoch [156/200] batch [35/50] time 0.084 (0.137) data 0.001 (0.053) loss 1.7578 (1.9270) acc 46.8750 (42.6786) lr 2.4989e-04 eta 0:05:04
epoch [156/200] batch [40/50] time 0.083 (0.131) data 0.000 (0.047) loss 1.6514 (1.9080) acc 62.5000 (43.4375) lr 2.4989e-04 eta 0:04:48
epoch [156/200] batch [45/50] time 0.083 (0.128) data 0.000 (0.044) loss 1.6260 (1.8914) acc 46.8750 (43.8194) lr 2.4989e-04 eta 0:04:42
epoch [156/200] batch [50/50] time 0.083 (0.124) data 0.000 (0.040) loss 1.9297 (1.8898) acc 40.6250 (44.0000) lr 2.3959e-04 eta 0:04:31
epoch [157/200] batch [5/50] time 0.086 (0.271) data 0.000 (0.185) loss 1.9062 (1.8725) acc 40.6250 (45.6250) lr 2.3959e-04 eta 0:09:55
epoch [157/200] batch [10/50] time 0.086 (0.179) data 0.000 (0.093) loss 2.0176 (1.9229) acc 43.7500 (45.6250) lr 2.3959e-04 eta 0:06:31
epoch [157/200] batch [15/50] time 0.086 (0.151) data 0.000 (0.066) loss 2.2285 (1.9243) acc 34.3750 (47.2917) lr 2.3959e-04 eta 0:05:30
epoch [157/200] batch [20/50] time 0.218 (0.142) data 0.133 (0.056) loss 2.1777 (1.9356) acc 34.3750 (45.9375) lr 2.3959e-04 eta 0:05:08
epoch [157/200] batch [25/50] time 0.086 (0.130) data 0.000 (0.045) loss 2.0684 (1.9149) acc 37.5000 (44.6250) lr 2.3959e-04 eta 0:04:43
epoch [157/200] batch [30/50] time 0.085 (0.127) data 0.000 (0.041) loss 1.7588 (1.9024) acc 40.6250 (44.2708) lr 2.3959e-04 eta 0:04:34
epoch [157/200] batch [35/50] time 0.086 (0.121) data 0.001 (0.035) loss 1.9121 (1.9032) acc 40.6250 (44.9107) lr 2.3959e-04 eta 0:04:21
epoch [157/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.035) loss 2.0508 (1.8987) acc 40.6250 (44.6094) lr 2.3959e-04 eta 0:04:19
epoch [157/200] batch [45/50] time 0.083 (0.119) data 0.000 (0.034) loss 2.2812 (1.9143) acc 25.0000 (44.2361) lr 2.3959e-04 eta 0:04:16
epoch [157/200] batch [50/50] time 0.082 (0.115) data 0.000 (0.031) loss 1.7090 (1.9038) acc 46.8750 (44.5000) lr 2.2949e-04 eta 0:04:08
epoch [158/200] batch [5/50] time 0.084 (0.296) data 0.000 (0.211) loss 2.1504 (2.0055) acc 31.2500 (37.5000) lr 2.2949e-04 eta 0:10:35
epoch [158/200] batch [10/50] time 0.235 (0.206) data 0.153 (0.122) loss 2.1875 (1.9472) acc 37.5000 (39.3750) lr 2.2949e-04 eta 0:07:21
epoch [158/200] batch [15/50] time 0.085 (0.166) data 0.000 (0.081) loss 2.0723 (2.0031) acc 43.7500 (40.4167) lr 2.2949e-04 eta 0:05:53
epoch [158/200] batch [20/50] time 0.085 (0.146) data 0.000 (0.062) loss 1.8896 (1.9652) acc 59.3750 (42.5000) lr 2.2949e-04 eta 0:05:11
epoch [158/200] batch [25/50] time 0.168 (0.137) data 0.083 (0.053) loss 1.6699 (1.9152) acc 56.2500 (44.5000) lr 2.2949e-04 eta 0:04:51
epoch [158/200] batch [30/50] time 0.086 (0.129) data 0.000 (0.044) loss 1.8887 (1.8934) acc 43.7500 (45.5208) lr 2.2949e-04 eta 0:04:32
epoch [158/200] batch [35/50] time 0.085 (0.129) data 0.000 (0.044) loss 1.9404 (1.9105) acc 43.7500 (45.4464) lr 2.2949e-04 eta 0:04:31
epoch [158/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.039) loss 1.8906 (1.9120) acc 53.1250 (45.0000) lr 2.2949e-04 eta 0:04:19
epoch [158/200] batch [45/50] time 0.083 (0.120) data 0.000 (0.036) loss 2.2422 (1.9355) acc 37.5000 (44.3750) lr 2.2949e-04 eta 0:04:12
epoch [158/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.032) loss 2.2051 (1.9380) acc 34.3750 (43.9375) lr 2.1957e-04 eta 0:04:04
epoch [159/200] batch [5/50] time 0.086 (0.296) data 0.000 (0.211) loss 1.7148 (1.9340) acc 59.3750 (45.0000) lr 2.1957e-04 eta 0:10:20
epoch [159/200] batch [10/50] time 0.085 (0.191) data 0.000 (0.106) loss 2.1406 (1.8422) acc 50.0000 (46.8750) lr 2.1957e-04 eta 0:06:38
epoch [159/200] batch [15/50] time 0.085 (0.156) data 0.000 (0.071) loss 1.9521 (1.9020) acc 43.7500 (45.4167) lr 2.1957e-04 eta 0:05:25
epoch [159/200] batch [20/50] time 0.180 (0.145) data 0.095 (0.060) loss 1.7295 (1.8712) acc 59.3750 (45.4688) lr 2.1957e-04 eta 0:05:01
epoch [159/200] batch [25/50] time 0.204 (0.138) data 0.120 (0.054) loss 1.9766 (1.8762) acc 46.8750 (45.0000) lr 2.1957e-04 eta 0:04:47
epoch [159/200] batch [30/50] time 0.083 (0.129) data 0.000 (0.045) loss 1.7793 (1.8798) acc 46.8750 (44.8958) lr 2.1957e-04 eta 0:04:27
epoch [159/200] batch [35/50] time 0.085 (0.129) data 0.000 (0.045) loss 1.9395 (1.8854) acc 40.6250 (44.7321) lr 2.1957e-04 eta 0:04:27
epoch [159/200] batch [40/50] time 0.084 (0.124) data 0.000 (0.039) loss 1.6709 (1.8948) acc 53.1250 (44.6875) lr 2.1957e-04 eta 0:04:15
epoch [159/200] batch [45/50] time 0.084 (0.123) data 0.000 (0.038) loss 1.7412 (1.8792) acc 46.8750 (44.9306) lr 2.1957e-04 eta 0:04:12
epoch [159/200] batch [50/50] time 0.084 (0.119) data 0.000 (0.034) loss 1.5615 (1.8644) acc 50.0000 (45.3750) lr 2.0984e-04 eta 0:04:03
epoch [160/200] batch [5/50] time 0.083 (0.343) data 0.000 (0.260) loss 1.9941 (1.8592) acc 28.1250 (38.1250) lr 2.0984e-04 eta 0:11:41
epoch [160/200] batch [10/50] time 0.084 (0.214) data 0.000 (0.130) loss 1.4600 (1.7905) acc 56.2500 (43.1250) lr 2.0984e-04 eta 0:07:16
epoch [160/200] batch [15/50] time 0.086 (0.171) data 0.000 (0.087) loss 2.1973 (1.8437) acc 34.3750 (42.7083) lr 2.0984e-04 eta 0:05:47
epoch [160/200] batch [20/50] time 0.085 (0.153) data 0.000 (0.069) loss 1.9893 (1.8860) acc 31.2500 (41.8750) lr 2.0984e-04 eta 0:05:09
epoch [160/200] batch [25/50] time 0.085 (0.139) data 0.000 (0.055) loss 2.0859 (1.9222) acc 34.3750 (41.6250) lr 2.0984e-04 eta 0:04:41
epoch [160/200] batch [30/50] time 0.085 (0.135) data 0.000 (0.051) loss 1.2988 (1.8960) acc 62.5000 (43.0208) lr 2.0984e-04 eta 0:04:32
epoch [160/200] batch [35/50] time 0.085 (0.131) data 0.001 (0.047) loss 1.9629 (1.9011) acc 46.8750 (43.3036) lr 2.0984e-04 eta 0:04:24
epoch [160/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.041) loss 2.1445 (1.9063) acc 40.6250 (43.7500) lr 2.0984e-04 eta 0:04:11
epoch [160/200] batch [45/50] time 0.082 (0.123) data 0.000 (0.039) loss 2.2285 (1.8952) acc 43.7500 (44.0278) lr 2.0984e-04 eta 0:04:07
epoch [160/200] batch [50/50] time 0.084 (0.119) data 0.000 (0.035) loss 2.2324 (1.8987) acc 37.5000 (44.1875) lr 2.0032e-04 eta 0:03:58
epoch [161/200] batch [5/50] time 0.085 (0.292) data 0.000 (0.208) loss 2.1094 (1.8867) acc 37.5000 (45.6250) lr 2.0032e-04 eta 0:09:43
epoch [161/200] batch [10/50] time 0.085 (0.188) data 0.000 (0.104) loss 2.0977 (1.9228) acc 40.6250 (44.3750) lr 2.0032e-04 eta 0:06:15
epoch [161/200] batch [15/50] time 0.086 (0.154) data 0.000 (0.069) loss 1.5117 (1.9396) acc 62.5000 (45.8333) lr 2.0032e-04 eta 0:05:06
epoch [161/200] batch [20/50] time 0.084 (0.137) data 0.000 (0.052) loss 1.7529 (1.8793) acc 53.1250 (46.5625) lr 2.0032e-04 eta 0:04:31
epoch [161/200] batch [25/50] time 0.086 (0.127) data 0.000 (0.042) loss 2.2227 (1.9155) acc 37.5000 (44.8750) lr 2.0032e-04 eta 0:04:10
epoch [161/200] batch [30/50] time 0.087 (0.120) data 0.000 (0.035) loss 2.2578 (1.8911) acc 40.6250 (45.8333) lr 2.0032e-04 eta 0:03:56
epoch [161/200] batch [35/50] time 0.086 (0.115) data 0.001 (0.030) loss 1.7686 (1.8886) acc 43.7500 (44.9107) lr 2.0032e-04 eta 0:03:45
epoch [161/200] batch [40/50] time 0.083 (0.111) data 0.000 (0.026) loss 2.2539 (1.9000) acc 31.2500 (44.5312) lr 2.0032e-04 eta 0:03:37
epoch [161/200] batch [45/50] time 0.083 (0.108) data 0.000 (0.023) loss 1.9375 (1.8946) acc 34.3750 (44.5139) lr 2.0032e-04 eta 0:03:31
epoch [161/200] batch [50/50] time 0.085 (0.106) data 0.000 (0.021) loss 1.7695 (1.8973) acc 50.0000 (44.5000) lr 1.9098e-04 eta 0:03:26
epoch [162/200] batch [5/50] time 0.084 (0.278) data 0.000 (0.194) loss 2.1230 (1.9109) acc 37.5000 (46.8750) lr 1.9098e-04 eta 0:09:00
epoch [162/200] batch [10/50] time 0.084 (0.181) data 0.000 (0.097) loss 1.7041 (1.8818) acc 56.2500 (47.1875) lr 1.9098e-04 eta 0:05:51
epoch [162/200] batch [15/50] time 0.085 (0.158) data 0.000 (0.073) loss 1.8311 (1.9503) acc 40.6250 (44.1667) lr 1.9098e-04 eta 0:05:04
epoch [162/200] batch [20/50] time 0.252 (0.148) data 0.168 (0.064) loss 1.9932 (1.9228) acc 59.3750 (46.0938) lr 1.9098e-04 eta 0:04:44
epoch [162/200] batch [25/50] time 0.085 (0.135) data 0.000 (0.051) loss 1.6367 (1.9104) acc 50.0000 (46.0000) lr 1.9098e-04 eta 0:04:19
epoch [162/200] batch [30/50] time 0.084 (0.134) data 0.000 (0.050) loss 1.7207 (1.9189) acc 40.6250 (45.3125) lr 1.9098e-04 eta 0:04:16
epoch [162/200] batch [35/50] time 0.087 (0.127) data 0.000 (0.042) loss 1.8574 (1.9247) acc 46.8750 (45.5357) lr 1.9098e-04 eta 0:04:02
epoch [162/200] batch [40/50] time 0.084 (0.125) data 0.000 (0.041) loss 1.6240 (1.8996) acc 53.1250 (46.0156) lr 1.9098e-04 eta 0:03:58
epoch [162/200] batch [45/50] time 0.083 (0.122) data 0.000 (0.038) loss 2.2500 (1.9155) acc 40.6250 (45.6250) lr 1.9098e-04 eta 0:03:53
epoch [162/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.035) loss 2.1465 (1.9069) acc 40.6250 (45.6875) lr 1.8185e-04 eta 0:03:45
epoch [163/200] batch [5/50] time 0.084 (0.380) data 0.000 (0.295) loss 2.3906 (2.1504) acc 25.0000 (36.2500) lr 1.8185e-04 eta 0:11:59
epoch [163/200] batch [10/50] time 0.085 (0.244) data 0.000 (0.159) loss 1.9326 (1.9884) acc 50.0000 (41.2500) lr 1.8185e-04 eta 0:07:40
epoch [163/200] batch [15/50] time 0.084 (0.191) data 0.000 (0.106) loss 2.3906 (2.0204) acc 31.2500 (39.1667) lr 1.8185e-04 eta 0:05:59
epoch [163/200] batch [20/50] time 0.084 (0.170) data 0.000 (0.085) loss 1.8389 (1.9502) acc 43.7500 (41.0938) lr 1.8185e-04 eta 0:05:18
epoch [163/200] batch [25/50] time 0.239 (0.159) data 0.156 (0.074) loss 1.8682 (1.9268) acc 46.8750 (43.0000) lr 1.8185e-04 eta 0:04:57
epoch [163/200] batch [30/50] time 0.085 (0.146) data 0.000 (0.062) loss 1.8574 (1.9551) acc 53.1250 (43.6458) lr 1.8185e-04 eta 0:04:33
epoch [163/200] batch [35/50] time 0.086 (0.143) data 0.000 (0.059) loss 2.4473 (1.9715) acc 34.3750 (42.8571) lr 1.8185e-04 eta 0:04:26
epoch [163/200] batch [40/50] time 0.083 (0.136) data 0.000 (0.051) loss 1.6826 (1.9490) acc 56.2500 (43.4375) lr 1.8185e-04 eta 0:04:12
epoch [163/200] batch [45/50] time 0.083 (0.131) data 0.000 (0.047) loss 1.7451 (1.9470) acc 46.8750 (43.5417) lr 1.8185e-04 eta 0:04:03
epoch [163/200] batch [50/50] time 0.083 (0.126) data 0.000 (0.043) loss 1.5869 (1.9338) acc 59.3750 (43.7500) lr 1.7292e-04 eta 0:03:54
epoch [164/200] batch [5/50] time 0.086 (0.310) data 0.001 (0.225) loss 1.8584 (2.0514) acc 53.1250 (44.3750) lr 1.7292e-04 eta 0:09:32
epoch [164/200] batch [10/50] time 0.148 (0.204) data 0.064 (0.119) loss 2.2070 (2.0202) acc 34.3750 (40.9375) lr 1.7292e-04 eta 0:06:15
epoch [164/200] batch [15/50] time 0.085 (0.165) data 0.000 (0.080) loss 1.5146 (1.9278) acc 56.2500 (43.3333) lr 1.7292e-04 eta 0:05:02
epoch [164/200] batch [20/50] time 0.085 (0.152) data 0.000 (0.067) loss 1.9658 (1.9120) acc 40.6250 (42.6562) lr 1.7292e-04 eta 0:04:37
epoch [164/200] batch [25/50] time 0.086 (0.138) data 0.001 (0.054) loss 1.9385 (1.9061) acc 40.6250 (43.0000) lr 1.7292e-04 eta 0:04:12
epoch [164/200] batch [30/50] time 0.085 (0.136) data 0.000 (0.052) loss 2.3984 (1.9179) acc 40.6250 (43.4375) lr 1.7292e-04 eta 0:04:08
epoch [164/200] batch [35/50] time 0.084 (0.133) data 0.001 (0.048) loss 1.6748 (1.8968) acc 53.1250 (44.1071) lr 1.7292e-04 eta 0:04:01
epoch [164/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.042) loss 2.0547 (1.8936) acc 50.0000 (44.6094) lr 1.7292e-04 eta 0:03:49
epoch [164/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.8311 (1.9001) acc 53.1250 (44.3750) lr 1.7292e-04 eta 0:03:45
epoch [164/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 2.1055 (1.9058) acc 31.2500 (43.8750) lr 1.6419e-04 eta 0:03:37
epoch [165/200] batch [5/50] time 0.087 (0.333) data 0.000 (0.246) loss 2.0586 (2.0941) acc 46.8750 (41.8750) lr 1.6419e-04 eta 0:09:57
epoch [165/200] batch [10/50] time 0.086 (0.209) data 0.000 (0.123) loss 1.9482 (2.0610) acc 46.8750 (42.1875) lr 1.6419e-04 eta 0:06:14
epoch [165/200] batch [15/50] time 0.087 (0.168) data 0.000 (0.082) loss 1.8252 (2.0169) acc 50.0000 (43.1250) lr 1.6419e-04 eta 0:05:00
epoch [165/200] batch [20/50] time 0.085 (0.151) data 0.000 (0.065) loss 1.8438 (1.9946) acc 40.6250 (42.9688) lr 1.6419e-04 eta 0:04:27
epoch [165/200] batch [25/50] time 0.238 (0.144) data 0.153 (0.058) loss 1.7500 (1.9809) acc 53.1250 (42.3750) lr 1.6419e-04 eta 0:04:15
epoch [165/200] batch [30/50] time 0.085 (0.134) data 0.001 (0.048) loss 1.5117 (1.9106) acc 50.0000 (44.5833) lr 1.6419e-04 eta 0:03:57
epoch [165/200] batch [35/50] time 0.084 (0.132) data 0.001 (0.046) loss 1.9766 (1.9022) acc 50.0000 (45.0000) lr 1.6419e-04 eta 0:03:52
epoch [165/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.041) loss 1.9199 (1.8935) acc 46.8750 (45.2344) lr 1.6419e-04 eta 0:03:41
epoch [165/200] batch [45/50] time 0.084 (0.124) data 0.000 (0.039) loss 1.5566 (1.8973) acc 43.7500 (44.7917) lr 1.6419e-04 eta 0:03:37
epoch [165/200] batch [50/50] time 0.084 (0.121) data 0.000 (0.036) loss 1.9297 (1.8997) acc 53.1250 (44.8750) lr 1.5567e-04 eta 0:03:31
epoch [166/200] batch [5/50] time 0.086 (0.323) data 0.000 (0.237) loss 1.5352 (1.9189) acc 56.2500 (50.6250) lr 1.5567e-04 eta 0:09:23
epoch [166/200] batch [10/50] time 0.086 (0.214) data 0.000 (0.128) loss 1.8760 (1.9373) acc 40.6250 (45.9375) lr 1.5567e-04 eta 0:06:11
epoch [166/200] batch [15/50] time 0.083 (0.171) data 0.000 (0.085) loss 1.6523 (1.8856) acc 50.0000 (46.4583) lr 1.5567e-04 eta 0:04:56
epoch [166/200] batch [20/50] time 0.085 (0.156) data 0.000 (0.071) loss 2.1777 (1.9287) acc 43.7500 (45.6250) lr 1.5567e-04 eta 0:04:30
epoch [166/200] batch [25/50] time 0.085 (0.142) data 0.000 (0.057) loss 2.1406 (1.9663) acc 37.5000 (44.6250) lr 1.5567e-04 eta 0:04:04
epoch [166/200] batch [30/50] time 0.084 (0.132) data 0.000 (0.048) loss 1.6152 (1.9319) acc 50.0000 (45.2083) lr 1.5567e-04 eta 0:03:47
epoch [166/200] batch [35/50] time 0.085 (0.125) data 0.000 (0.041) loss 1.4717 (1.9131) acc 50.0000 (45.3571) lr 1.5567e-04 eta 0:03:35
epoch [166/200] batch [40/50] time 0.083 (0.121) data 0.000 (0.036) loss 2.2598 (1.9388) acc 31.2500 (44.8438) lr 1.5567e-04 eta 0:03:26
epoch [166/200] batch [45/50] time 0.254 (0.120) data 0.171 (0.036) loss 1.7578 (1.9103) acc 50.0000 (44.7917) lr 1.5567e-04 eta 0:03:24
epoch [166/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.032) loss 1.9727 (1.8979) acc 50.0000 (45.6250) lr 1.4736e-04 eta 0:03:18
epoch [167/200] batch [5/50] time 0.085 (0.304) data 0.000 (0.219) loss 1.6660 (1.9590) acc 46.8750 (41.8750) lr 1.4736e-04 eta 0:08:35
epoch [167/200] batch [10/50] time 0.160 (0.202) data 0.074 (0.117) loss 1.7598 (1.8401) acc 40.6250 (45.0000) lr 1.4736e-04 eta 0:05:41
epoch [167/200] batch [15/50] time 0.084 (0.163) data 0.000 (0.078) loss 1.8994 (1.8831) acc 53.1250 (45.4167) lr 1.4736e-04 eta 0:04:35
epoch [167/200] batch [20/50] time 0.091 (0.151) data 0.008 (0.066) loss 1.9316 (1.8956) acc 46.8750 (44.3750) lr 1.4736e-04 eta 0:04:13
epoch [167/200] batch [25/50] time 0.177 (0.141) data 0.095 (0.057) loss 1.7109 (1.9013) acc 37.5000 (43.1250) lr 1.4736e-04 eta 0:03:56
epoch [167/200] batch [30/50] time 0.087 (0.136) data 0.001 (0.052) loss 2.2070 (1.9219) acc 40.6250 (43.7500) lr 1.4736e-04 eta 0:03:47
epoch [167/200] batch [35/50] time 0.085 (0.131) data 0.001 (0.046) loss 1.7686 (1.9134) acc 50.0000 (44.1964) lr 1.4736e-04 eta 0:03:37
epoch [167/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.042) loss 2.1504 (1.9295) acc 37.5000 (43.4375) lr 1.4736e-04 eta 0:03:28
epoch [167/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 1.8066 (1.9586) acc 43.7500 (42.7083) lr 1.4736e-04 eta 0:03:25
epoch [167/200] batch [50/50] time 0.084 (0.120) data 0.000 (0.036) loss 1.8750 (1.9495) acc 46.8750 (42.5625) lr 1.3926e-04 eta 0:03:18
epoch [168/200] batch [5/50] time 0.083 (0.312) data 0.000 (0.228) loss 2.2324 (1.7699) acc 43.7500 (52.5000) lr 1.3926e-04 eta 0:08:33
epoch [168/200] batch [10/50] time 0.107 (0.200) data 0.020 (0.116) loss 1.5654 (1.7915) acc 53.1250 (51.8750) lr 1.3926e-04 eta 0:05:28
epoch [168/200] batch [15/50] time 0.083 (0.161) data 0.000 (0.078) loss 1.9912 (1.8209) acc 46.8750 (49.7917) lr 1.3926e-04 eta 0:04:23
epoch [168/200] batch [20/50] time 0.083 (0.154) data 0.000 (0.071) loss 2.0957 (1.8618) acc 40.6250 (47.6562) lr 1.3926e-04 eta 0:04:11
epoch [168/200] batch [25/50] time 0.084 (0.140) data 0.000 (0.057) loss 1.4629 (1.8675) acc 56.2500 (47.3750) lr 1.3926e-04 eta 0:03:47
epoch [168/200] batch [30/50] time 0.085 (0.133) data 0.000 (0.049) loss 1.5332 (1.8588) acc 53.1250 (47.6042) lr 1.3926e-04 eta 0:03:34
epoch [168/200] batch [35/50] time 0.185 (0.129) data 0.102 (0.045) loss 1.6484 (1.8590) acc 65.6250 (47.8571) lr 1.3926e-04 eta 0:03:27
epoch [168/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.039) loss 1.5254 (1.8659) acc 53.1250 (47.8906) lr 1.3926e-04 eta 0:03:18
epoch [168/200] batch [45/50] time 0.083 (0.122) data 0.000 (0.038) loss 1.8398 (1.8506) acc 46.8750 (48.1944) lr 1.3926e-04 eta 0:03:15
epoch [168/200] batch [50/50] time 0.084 (0.118) data 0.000 (0.034) loss 1.4854 (1.8428) acc 62.5000 (48.0000) lr 1.3137e-04 eta 0:03:08
epoch [169/200] batch [5/50] time 0.084 (0.318) data 0.000 (0.234) loss 1.7402 (1.7504) acc 46.8750 (47.5000) lr 1.3137e-04 eta 0:08:27
epoch [169/200] batch [10/50] time 0.084 (0.216) data 0.000 (0.132) loss 1.9971 (1.7944) acc 31.2500 (46.2500) lr 1.3137e-04 eta 0:05:43
epoch [169/200] batch [15/50] time 0.086 (0.172) data 0.000 (0.088) loss 2.2207 (1.8048) acc 40.6250 (45.8333) lr 1.3137e-04 eta 0:04:32
epoch [169/200] batch [20/50] time 0.085 (0.156) data 0.000 (0.072) loss 1.9863 (1.8444) acc 43.7500 (44.3750) lr 1.3137e-04 eta 0:04:07
epoch [169/200] batch [25/50] time 0.247 (0.149) data 0.164 (0.064) loss 1.4932 (1.8561) acc 59.3750 (44.1250) lr 1.3137e-04 eta 0:03:54
epoch [169/200] batch [30/50] time 0.084 (0.138) data 0.000 (0.054) loss 1.8623 (1.8337) acc 37.5000 (44.3750) lr 1.3137e-04 eta 0:03:36
epoch [169/200] batch [35/50] time 0.084 (0.135) data 0.000 (0.051) loss 1.7598 (1.8358) acc 46.8750 (44.6429) lr 1.3137e-04 eta 0:03:31
epoch [169/200] batch [40/50] time 0.083 (0.129) data 0.000 (0.045) loss 2.0137 (1.8269) acc 40.6250 (45.9375) lr 1.3137e-04 eta 0:03:21
epoch [169/200] batch [45/50] time 0.083 (0.127) data 0.000 (0.043) loss 2.2148 (1.8578) acc 40.6250 (45.3472) lr 1.3137e-04 eta 0:03:17
epoch [169/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.039) loss 1.7373 (1.8540) acc 46.8750 (45.0000) lr 1.2369e-04 eta 0:03:09
epoch [170/200] batch [5/50] time 0.086 (0.287) data 0.000 (0.201) loss 2.1074 (1.8512) acc 37.5000 (46.8750) lr 1.2369e-04 eta 0:07:23
epoch [170/200] batch [10/50] time 0.085 (0.187) data 0.001 (0.101) loss 2.1934 (1.8944) acc 40.6250 (47.1875) lr 1.2369e-04 eta 0:04:47
epoch [170/200] batch [15/50] time 0.085 (0.156) data 0.001 (0.070) loss 1.7695 (1.8913) acc 43.7500 (46.6667) lr 1.2369e-04 eta 0:03:59
epoch [170/200] batch [20/50] time 0.085 (0.147) data 0.000 (0.061) loss 2.0293 (1.8897) acc 46.8750 (46.5625) lr 1.2369e-04 eta 0:03:44
epoch [170/200] batch [25/50] time 0.085 (0.135) data 0.000 (0.049) loss 2.0332 (1.8873) acc 40.6250 (46.3750) lr 1.2369e-04 eta 0:03:25
epoch [170/200] batch [30/50] time 0.085 (0.132) data 0.000 (0.047) loss 2.2051 (1.8906) acc 31.2500 (45.4167) lr 1.2369e-04 eta 0:03:20
epoch [170/200] batch [35/50] time 0.261 (0.130) data 0.177 (0.045) loss 2.0586 (1.8931) acc 46.8750 (45.3571) lr 1.2369e-04 eta 0:03:17
epoch [170/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.0898 (1.9181) acc 53.1250 (45.3906) lr 1.2369e-04 eta 0:03:08
epoch [170/200] batch [45/50] time 0.082 (0.123) data 0.000 (0.039) loss 1.7178 (1.9269) acc 50.0000 (45.2083) lr 1.2369e-04 eta 0:03:05
epoch [170/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 1.6816 (1.9093) acc 50.0000 (45.3750) lr 1.1623e-04 eta 0:02:58
epoch [171/200] batch [5/50] time 0.086 (0.299) data 0.000 (0.213) loss 2.1387 (2.2506) acc 43.7500 (39.3750) lr 1.1623e-04 eta 0:07:26
epoch [171/200] batch [10/50] time 0.085 (0.192) data 0.000 (0.107) loss 2.4590 (2.1692) acc 37.5000 (40.9375) lr 1.1623e-04 eta 0:04:46
epoch [171/200] batch [15/50] time 0.086 (0.170) data 0.000 (0.084) loss 1.5713 (2.0620) acc 56.2500 (41.6667) lr 1.1623e-04 eta 0:04:11
epoch [171/200] batch [20/50] time 0.086 (0.154) data 0.000 (0.069) loss 1.7734 (2.0220) acc 53.1250 (42.8125) lr 1.1623e-04 eta 0:03:47
epoch [171/200] batch [25/50] time 0.085 (0.140) data 0.000 (0.055) loss 1.7344 (1.9662) acc 53.1250 (45.0000) lr 1.1623e-04 eta 0:03:26
epoch [171/200] batch [30/50] time 0.084 (0.135) data 0.000 (0.050) loss 1.9053 (1.9613) acc 56.2500 (44.6875) lr 1.1623e-04 eta 0:03:18
epoch [171/200] batch [35/50] time 0.252 (0.132) data 0.168 (0.048) loss 1.8828 (1.9228) acc 50.0000 (45.5357) lr 1.1623e-04 eta 0:03:13
epoch [171/200] batch [40/50] time 0.082 (0.126) data 0.000 (0.042) loss 2.0039 (1.9095) acc 40.6250 (46.2500) lr 1.1623e-04 eta 0:03:04
epoch [171/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 1.6318 (1.9070) acc 50.0000 (45.9722) lr 1.1623e-04 eta 0:03:01
epoch [171/200] batch [50/50] time 0.082 (0.120) data 0.000 (0.036) loss 1.6025 (1.8967) acc 56.2500 (46.1875) lr 1.0899e-04 eta 0:02:54
epoch [172/200] batch [5/50] time 0.084 (0.316) data 0.000 (0.232) loss 1.9814 (2.0514) acc 34.3750 (40.0000) lr 1.0899e-04 eta 0:07:36
epoch [172/200] batch [10/50] time 0.084 (0.218) data 0.000 (0.134) loss 1.9609 (2.0102) acc 50.0000 (40.9375) lr 1.0899e-04 eta 0:05:14
epoch [172/200] batch [15/50] time 0.085 (0.174) data 0.000 (0.090) loss 1.8691 (2.0185) acc 46.8750 (41.0417) lr 1.0899e-04 eta 0:04:09
epoch [172/200] batch [20/50] time 0.084 (0.151) data 0.000 (0.067) loss 1.8691 (1.9927) acc 46.8750 (40.6250) lr 1.0899e-04 eta 0:03:36
epoch [172/200] batch [25/50] time 0.085 (0.138) data 0.000 (0.054) loss 1.6299 (1.9544) acc 46.8750 (42.2500) lr 1.0899e-04 eta 0:03:16
epoch [172/200] batch [30/50] time 0.085 (0.129) data 0.000 (0.045) loss 1.9980 (1.9683) acc 46.8750 (41.9792) lr 1.0899e-04 eta 0:03:03
epoch [172/200] batch [35/50] time 0.085 (0.123) data 0.001 (0.039) loss 2.0098 (1.9265) acc 43.7500 (43.2143) lr 1.0899e-04 eta 0:02:53
epoch [172/200] batch [40/50] time 0.084 (0.118) data 0.000 (0.034) loss 2.1777 (1.9365) acc 34.3750 (43.1250) lr 1.0899e-04 eta 0:02:46
epoch [172/200] batch [45/50] time 0.083 (0.114) data 0.000 (0.030) loss 1.8506 (1.9465) acc 37.5000 (42.2222) lr 1.0899e-04 eta 0:02:40
epoch [172/200] batch [50/50] time 0.083 (0.111) data 0.000 (0.027) loss 2.1211 (1.9326) acc 34.3750 (42.4375) lr 1.0197e-04 eta 0:02:35
epoch [173/200] batch [5/50] time 0.102 (0.270) data 0.018 (0.185) loss 2.2520 (1.9430) acc 37.5000 (40.6250) lr 1.0197e-04 eta 0:06:16
epoch [173/200] batch [10/50] time 0.086 (0.184) data 0.000 (0.100) loss 2.0918 (1.9089) acc 34.3750 (40.9375) lr 1.0197e-04 eta 0:04:16
epoch [173/200] batch [15/50] time 0.085 (0.152) data 0.000 (0.067) loss 1.7500 (1.9222) acc 50.0000 (41.8750) lr 1.0197e-04 eta 0:03:29
epoch [173/200] batch [20/50] time 0.084 (0.143) data 0.000 (0.058) loss 1.9502 (1.9167) acc 34.3750 (41.8750) lr 1.0197e-04 eta 0:03:17
epoch [173/200] batch [25/50] time 0.166 (0.135) data 0.081 (0.050) loss 1.7744 (1.9309) acc 53.1250 (41.8750) lr 1.0197e-04 eta 0:03:05
epoch [173/200] batch [30/50] time 0.086 (0.127) data 0.000 (0.042) loss 1.8691 (1.8854) acc 50.0000 (43.9583) lr 1.0197e-04 eta 0:02:53
epoch [173/200] batch [35/50] time 0.085 (0.125) data 0.001 (0.041) loss 1.6836 (1.8484) acc 56.2500 (45.0893) lr 1.0197e-04 eta 0:02:51
epoch [173/200] batch [40/50] time 0.083 (0.120) data 0.000 (0.036) loss 2.0234 (1.8641) acc 46.8750 (44.8438) lr 1.0197e-04 eta 0:02:43
epoch [173/200] batch [45/50] time 0.083 (0.120) data 0.000 (0.035) loss 1.8262 (1.8499) acc 43.7500 (45.2778) lr 1.0197e-04 eta 0:02:42
epoch [173/200] batch [50/50] time 0.082 (0.116) data 0.000 (0.032) loss 1.8115 (1.8434) acc 46.8750 (45.7500) lr 9.5173e-05 eta 0:02:36
epoch [174/200] batch [5/50] time 0.085 (0.276) data 0.000 (0.190) loss 1.7373 (1.8414) acc 59.3750 (50.0000) lr 9.5173e-05 eta 0:06:11
epoch [174/200] batch [10/50] time 0.086 (0.181) data 0.001 (0.095) loss 1.9541 (1.8148) acc 46.8750 (50.3125) lr 9.5173e-05 eta 0:04:02
epoch [174/200] batch [15/50] time 0.086 (0.156) data 0.000 (0.070) loss 1.7129 (1.8064) acc 50.0000 (48.7500) lr 9.5173e-05 eta 0:03:27
epoch [174/200] batch [20/50] time 0.264 (0.147) data 0.180 (0.062) loss 1.6670 (1.7994) acc 46.8750 (47.3438) lr 9.5173e-05 eta 0:03:15
epoch [174/200] batch [25/50] time 0.086 (0.135) data 0.000 (0.050) loss 1.1904 (1.7541) acc 65.6250 (48.8750) lr 9.5173e-05 eta 0:02:58
epoch [174/200] batch [30/50] time 0.084 (0.130) data 0.000 (0.045) loss 1.7344 (1.7595) acc 50.0000 (48.2292) lr 9.5173e-05 eta 0:02:51
epoch [174/200] batch [35/50] time 0.086 (0.123) data 0.001 (0.039) loss 2.1582 (1.7669) acc 31.2500 (48.4821) lr 9.5173e-05 eta 0:02:42
epoch [174/200] batch [40/50] time 0.084 (0.122) data 0.000 (0.037) loss 2.0977 (1.7828) acc 46.8750 (48.0469) lr 9.5173e-05 eta 0:02:39
epoch [174/200] batch [45/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.8564 (1.8059) acc 43.7500 (47.5694) lr 9.5173e-05 eta 0:02:36
epoch [174/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.032) loss 1.9248 (1.8091) acc 43.7500 (47.3750) lr 8.8597e-05 eta 0:02:31
epoch [175/200] batch [5/50] time 0.086 (0.278) data 0.000 (0.193) loss 1.6768 (1.9898) acc 37.5000 (36.8750) lr 8.8597e-05 eta 0:06:00
epoch [175/200] batch [10/50] time 0.086 (0.182) data 0.000 (0.097) loss 1.9688 (1.9227) acc 34.3750 (40.3125) lr 8.8597e-05 eta 0:03:54
epoch [175/200] batch [15/50] time 0.085 (0.154) data 0.000 (0.069) loss 1.6240 (1.9705) acc 56.2500 (40.2083) lr 8.8597e-05 eta 0:03:18
epoch [175/200] batch [20/50] time 0.234 (0.144) data 0.151 (0.059) loss 1.3936 (1.9183) acc 71.8750 (43.2812) lr 8.8597e-05 eta 0:03:04
epoch [175/200] batch [25/50] time 0.086 (0.133) data 0.001 (0.048) loss 1.7764 (1.9039) acc 59.3750 (44.6250) lr 8.8597e-05 eta 0:02:49
epoch [175/200] batch [30/50] time 0.085 (0.131) data 0.000 (0.046) loss 1.6523 (1.8679) acc 43.7500 (45.4167) lr 8.8597e-05 eta 0:02:46
epoch [175/200] batch [35/50] time 0.084 (0.124) data 0.000 (0.039) loss 1.7578 (1.8802) acc 40.6250 (44.7321) lr 8.8597e-05 eta 0:02:37
epoch [175/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.038) loss 1.5146 (1.8913) acc 56.2500 (44.6875) lr 8.8597e-05 eta 0:02:34
epoch [175/200] batch [45/50] time 0.085 (0.122) data 0.000 (0.037) loss 2.2227 (1.8847) acc 28.1250 (44.5833) lr 8.8597e-05 eta 0:02:32
epoch [175/200] batch [50/50] time 0.082 (0.118) data 0.000 (0.033) loss 1.8848 (1.8829) acc 46.8750 (44.6875) lr 8.2245e-05 eta 0:02:27
epoch [176/200] batch [5/50] time 0.085 (0.273) data 0.000 (0.188) loss 2.0137 (1.9299) acc 43.7500 (44.3750) lr 8.2245e-05 eta 0:05:39
epoch [176/200] batch [10/50] time 0.226 (0.193) data 0.142 (0.108) loss 1.9238 (1.8368) acc 53.1250 (49.0625) lr 8.2245e-05 eta 0:03:59
epoch [176/200] batch [15/50] time 0.085 (0.157) data 0.000 (0.072) loss 2.0605 (1.8477) acc 43.7500 (46.8750) lr 8.2245e-05 eta 0:03:14
epoch [176/200] batch [20/50] time 0.085 (0.146) data 0.000 (0.062) loss 1.9375 (1.8488) acc 40.6250 (46.5625) lr 8.2245e-05 eta 0:03:00
epoch [176/200] batch [25/50] time 0.085 (0.134) data 0.000 (0.049) loss 1.9453 (1.8309) acc 28.1250 (45.6250) lr 8.2245e-05 eta 0:02:44
epoch [176/200] batch [30/50] time 0.084 (0.131) data 0.000 (0.047) loss 1.7891 (1.8249) acc 53.1250 (45.8333) lr 8.2245e-05 eta 0:02:40
epoch [176/200] batch [35/50] time 0.086 (0.129) data 0.001 (0.044) loss 1.9541 (1.8242) acc 43.7500 (45.8929) lr 8.2245e-05 eta 0:02:36
epoch [176/200] batch [40/50] time 0.085 (0.124) data 0.000 (0.039) loss 1.5527 (1.8296) acc 53.1250 (46.1719) lr 8.2245e-05 eta 0:02:29
epoch [176/200] batch [45/50] time 0.083 (0.122) data 0.000 (0.038) loss 2.3945 (1.8464) acc 34.3750 (46.0417) lr 8.2245e-05 eta 0:02:27
epoch [176/200] batch [50/50] time 0.086 (0.119) data 0.003 (0.034) loss 1.6367 (1.8403) acc 53.1250 (46.2500) lr 7.6120e-05 eta 0:02:22
epoch [177/200] batch [5/50] time 0.085 (0.261) data 0.000 (0.176) loss 2.1055 (1.8043) acc 43.7500 (46.2500) lr 7.6120e-05 eta 0:05:11
epoch [177/200] batch [10/50] time 0.085 (0.173) data 0.000 (0.088) loss 2.0176 (1.8544) acc 43.7500 (46.5625) lr 7.6120e-05 eta 0:03:25
epoch [177/200] batch [15/50] time 0.085 (0.144) data 0.000 (0.059) loss 2.4355 (1.8729) acc 28.1250 (45.4167) lr 7.6120e-05 eta 0:02:50
epoch [177/200] batch [20/50] time 0.105 (0.130) data 0.019 (0.045) loss 2.0781 (1.8916) acc 37.5000 (44.6875) lr 7.6120e-05 eta 0:02:33
epoch [177/200] batch [25/50] time 0.084 (0.121) data 0.001 (0.036) loss 2.1973 (1.8876) acc 40.6250 (44.6250) lr 7.6120e-05 eta 0:02:21
epoch [177/200] batch [30/50] time 0.084 (0.120) data 0.000 (0.036) loss 2.1836 (1.9198) acc 43.7500 (44.2708) lr 7.6120e-05 eta 0:02:20
epoch [177/200] batch [35/50] time 0.085 (0.115) data 0.000 (0.031) loss 1.3447 (1.9144) acc 53.1250 (44.3750) lr 7.6120e-05 eta 0:02:14
epoch [177/200] batch [40/50] time 0.082 (0.117) data 0.000 (0.033) loss 1.4326 (1.8969) acc 59.3750 (44.9219) lr 7.6120e-05 eta 0:02:15
epoch [177/200] batch [45/50] time 0.083 (0.115) data 0.000 (0.031) loss 1.9492 (1.8980) acc 43.7500 (45.5556) lr 7.6120e-05 eta 0:02:13
epoch [177/200] batch [50/50] time 0.082 (0.112) data 0.000 (0.028) loss 2.1270 (1.9163) acc 46.8750 (45.2500) lr 7.0224e-05 eta 0:02:08
epoch [178/200] batch [5/50] time 0.084 (0.273) data 0.000 (0.188) loss 1.5166 (1.8152) acc 53.1250 (42.5000) lr 7.0224e-05 eta 0:05:12
epoch [178/200] batch [10/50] time 0.124 (0.183) data 0.040 (0.098) loss 1.6914 (1.8412) acc 59.3750 (45.3125) lr 7.0224e-05 eta 0:03:28
epoch [178/200] batch [15/50] time 0.085 (0.155) data 0.000 (0.070) loss 1.8730 (1.8664) acc 40.6250 (44.3750) lr 7.0224e-05 eta 0:02:56
epoch [178/200] batch [20/50] time 0.263 (0.147) data 0.179 (0.062) loss 2.1133 (1.8438) acc 37.5000 (45.7812) lr 7.0224e-05 eta 0:02:45
epoch [178/200] batch [25/50] time 0.087 (0.134) data 0.001 (0.050) loss 2.3574 (1.8704) acc 34.3750 (44.8750) lr 7.0224e-05 eta 0:02:31
epoch [178/200] batch [30/50] time 0.084 (0.134) data 0.001 (0.049) loss 1.6426 (1.8738) acc 62.5000 (45.2083) lr 7.0224e-05 eta 0:02:30
epoch [178/200] batch [35/50] time 0.084 (0.127) data 0.000 (0.042) loss 1.9434 (1.8765) acc 43.7500 (45.2679) lr 7.0224e-05 eta 0:02:21
epoch [178/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.0078 (1.9078) acc 43.7500 (44.2188) lr 7.0224e-05 eta 0:02:19
epoch [178/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.039) loss 1.4629 (1.8942) acc 59.3750 (45.0000) lr 7.0224e-05 eta 0:02:15
epoch [178/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 1.7246 (1.9075) acc 53.1250 (44.7500) lr 6.4556e-05 eta 0:02:10
epoch [179/200] batch [5/50] time 0.085 (0.308) data 0.000 (0.223) loss 2.0430 (1.7049) acc 37.5000 (43.7500) lr 6.4556e-05 eta 0:05:37
epoch [179/200] batch [10/50] time 0.085 (0.196) data 0.000 (0.112) loss 1.8408 (1.7272) acc 50.0000 (48.1250) lr 6.4556e-05 eta 0:03:34
epoch [179/200] batch [15/50] time 0.085 (0.159) data 0.000 (0.075) loss 1.7949 (1.7101) acc 53.1250 (49.1667) lr 6.4556e-05 eta 0:02:52
epoch [179/200] batch [20/50] time 0.153 (0.144) data 0.069 (0.060) loss 1.5762 (1.7470) acc 46.8750 (48.2812) lr 6.4556e-05 eta 0:02:35
epoch [179/200] batch [25/50] time 0.084 (0.132) data 0.000 (0.048) loss 2.0918 (1.7887) acc 21.8750 (46.3750) lr 6.4556e-05 eta 0:02:21
epoch [179/200] batch [30/50] time 0.084 (0.127) data 0.000 (0.043) loss 1.9893 (1.8354) acc 43.7500 (45.1042) lr 6.4556e-05 eta 0:02:16
epoch [179/200] batch [35/50] time 0.086 (0.121) data 0.001 (0.037) loss 1.8047 (1.8437) acc 50.0000 (45.1786) lr 6.4556e-05 eta 0:02:09
epoch [179/200] batch [40/50] time 0.084 (0.117) data 0.000 (0.032) loss 2.2363 (1.8587) acc 40.6250 (44.9219) lr 6.4556e-05 eta 0:02:03
epoch [179/200] batch [45/50] time 0.083 (0.113) data 0.000 (0.029) loss 2.0801 (1.8695) acc 37.5000 (45.1389) lr 6.4556e-05 eta 0:01:59
epoch [179/200] batch [50/50] time 0.084 (0.110) data 0.000 (0.026) loss 1.4902 (1.8590) acc 46.8750 (45.0625) lr 5.9119e-05 eta 0:01:55
epoch [180/200] batch [5/50] time 0.084 (0.282) data 0.000 (0.198) loss 1.8984 (1.9336) acc 43.7500 (41.2500) lr 5.9119e-05 eta 0:04:54
epoch [180/200] batch [10/50] time 0.085 (0.183) data 0.000 (0.099) loss 2.1270 (1.9644) acc 34.3750 (42.1875) lr 5.9119e-05 eta 0:03:10
epoch [180/200] batch [15/50] time 0.084 (0.150) data 0.000 (0.066) loss 1.9922 (1.9747) acc 40.6250 (41.2500) lr 5.9119e-05 eta 0:02:35
epoch [180/200] batch [20/50] time 0.084 (0.134) data 0.000 (0.050) loss 1.8916 (1.9337) acc 34.3750 (42.9688) lr 5.9119e-05 eta 0:02:17
epoch [180/200] batch [25/50] time 0.085 (0.128) data 0.000 (0.044) loss 1.7891 (1.9100) acc 50.0000 (44.1250) lr 5.9119e-05 eta 0:02:10
epoch [180/200] batch [30/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.0781 (1.9105) acc 40.6250 (45.4167) lr 5.9119e-05 eta 0:02:05
epoch [180/200] batch [35/50] time 0.084 (0.123) data 0.000 (0.040) loss 2.1016 (1.8979) acc 46.8750 (45.6250) lr 5.9119e-05 eta 0:02:05
epoch [180/200] batch [40/50] time 0.083 (0.120) data 0.000 (0.037) loss 2.0312 (1.9256) acc 31.2500 (44.6094) lr 5.9119e-05 eta 0:02:01
epoch [180/200] batch [45/50] time 0.084 (0.116) data 0.000 (0.033) loss 1.4951 (1.9111) acc 40.6250 (44.5833) lr 5.9119e-05 eta 0:01:56
epoch [180/200] batch [50/50] time 0.084 (0.113) data 0.000 (0.029) loss 1.3369 (1.8912) acc 62.5000 (44.9375) lr 5.3915e-05 eta 0:01:52
epoch [181/200] batch [5/50] time 0.084 (0.333) data 0.000 (0.249) loss 1.9541 (1.8486) acc 43.7500 (50.0000) lr 5.3915e-05 eta 0:05:31
epoch [181/200] batch [10/50] time 0.085 (0.220) data 0.000 (0.136) loss 1.8887 (1.7434) acc 46.8750 (50.3125) lr 5.3915e-05 eta 0:03:37
epoch [181/200] batch [15/50] time 0.084 (0.175) data 0.000 (0.091) loss 1.5732 (1.7799) acc 59.3750 (49.1667) lr 5.3915e-05 eta 0:02:52
epoch [181/200] batch [20/50] time 0.084 (0.152) data 0.000 (0.068) loss 1.9297 (1.8014) acc 40.6250 (46.8750) lr 5.3915e-05 eta 0:02:29
epoch [181/200] batch [25/50] time 0.085 (0.139) data 0.000 (0.055) loss 1.5654 (1.8279) acc 53.1250 (47.2500) lr 5.3915e-05 eta 0:02:15
epoch [181/200] batch [30/50] time 0.085 (0.132) data 0.000 (0.047) loss 2.2773 (1.8388) acc 40.6250 (46.9792) lr 5.3915e-05 eta 0:02:07
epoch [181/200] batch [35/50] time 0.086 (0.125) data 0.001 (0.041) loss 1.5195 (1.8435) acc 46.8750 (46.2500) lr 5.3915e-05 eta 0:02:00
epoch [181/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.039) loss 1.9014 (1.8450) acc 43.7500 (46.4062) lr 5.3915e-05 eta 0:01:58
epoch [181/200] batch [45/50] time 0.082 (0.123) data 0.000 (0.039) loss 2.0254 (1.8616) acc 50.0000 (46.1111) lr 5.3915e-05 eta 0:01:57
epoch [181/200] batch [50/50] time 0.082 (0.119) data 0.000 (0.035) loss 1.8037 (1.8601) acc 46.8750 (46.0625) lr 4.8943e-05 eta 0:01:52
epoch [182/200] batch [5/50] time 0.085 (0.286) data 0.000 (0.201) loss 1.9434 (1.8229) acc 40.6250 (43.7500) lr 4.8943e-05 eta 0:04:30
epoch [182/200] batch [10/50] time 0.087 (0.186) data 0.000 (0.101) loss 2.1621 (1.7979) acc 31.2500 (43.7500) lr 4.8943e-05 eta 0:02:54
epoch [182/200] batch [15/50] time 0.085 (0.152) data 0.000 (0.067) loss 1.7549 (1.8285) acc 46.8750 (44.3750) lr 4.8943e-05 eta 0:02:22
epoch [182/200] batch [20/50] time 0.146 (0.138) data 0.062 (0.054) loss 1.7871 (1.7964) acc 43.7500 (46.0938) lr 4.8943e-05 eta 0:02:08
epoch [182/200] batch [25/50] time 0.085 (0.128) data 0.000 (0.043) loss 2.1270 (1.8348) acc 50.0000 (45.1250) lr 4.8943e-05 eta 0:01:57
epoch [182/200] batch [30/50] time 0.085 (0.120) data 0.000 (0.036) loss 1.8145 (1.8348) acc 37.5000 (45.0000) lr 4.8943e-05 eta 0:01:50
epoch [182/200] batch [35/50] time 0.084 (0.117) data 0.001 (0.032) loss 1.5742 (1.8457) acc 62.5000 (45.8036) lr 4.8943e-05 eta 0:01:46
epoch [182/200] batch [40/50] time 0.085 (0.113) data 0.000 (0.028) loss 1.9082 (1.8731) acc 50.0000 (44.5312) lr 4.8943e-05 eta 0:01:42
epoch [182/200] batch [45/50] time 0.083 (0.110) data 0.000 (0.025) loss 2.0117 (1.8648) acc 43.7500 (45.0000) lr 4.8943e-05 eta 0:01:39
epoch [182/200] batch [50/50] time 0.083 (0.107) data 0.000 (0.023) loss 1.7441 (1.8635) acc 53.1250 (45.1250) lr 4.4207e-05 eta 0:01:36
epoch [183/200] batch [5/50] time 0.084 (0.311) data 0.000 (0.227) loss 1.4131 (1.6779) acc 53.1250 (48.7500) lr 4.4207e-05 eta 0:04:38
epoch [183/200] batch [10/50] time 0.085 (0.205) data 0.000 (0.122) loss 2.5137 (1.9333) acc 40.6250 (45.9375) lr 4.4207e-05 eta 0:03:02
epoch [183/200] batch [15/50] time 0.088 (0.165) data 0.000 (0.081) loss 1.9385 (1.9274) acc 37.5000 (45.0000) lr 4.4207e-05 eta 0:02:26
epoch [183/200] batch [20/50] time 0.084 (0.153) data 0.000 (0.069) loss 1.7363 (1.8503) acc 46.8750 (46.5625) lr 4.4207e-05 eta 0:02:14
epoch [183/200] batch [25/50] time 0.249 (0.146) data 0.165 (0.062) loss 2.1387 (1.8774) acc 34.3750 (45.1250) lr 4.4207e-05 eta 0:02:07
epoch [183/200] batch [30/50] time 0.086 (0.136) data 0.000 (0.052) loss 2.1094 (1.8494) acc 40.6250 (45.8333) lr 4.4207e-05 eta 0:01:58
epoch [183/200] batch [35/50] time 0.084 (0.132) data 0.000 (0.048) loss 1.9727 (1.8643) acc 37.5000 (45.1786) lr 4.4207e-05 eta 0:01:53
epoch [183/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.042) loss 2.3594 (1.8750) acc 34.3750 (44.8438) lr 4.4207e-05 eta 0:01:48
epoch [183/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.6807 (1.8679) acc 46.8750 (45.3472) lr 4.4207e-05 eta 0:01:46
epoch [183/200] batch [50/50] time 0.084 (0.121) data 0.000 (0.037) loss 1.8760 (1.8573) acc 43.7500 (45.5625) lr 3.9706e-05 eta 0:01:42
epoch [184/200] batch [5/50] time 0.086 (0.290) data 0.000 (0.204) loss 1.5947 (1.8748) acc 46.8750 (41.8750) lr 3.9706e-05 eta 0:04:05
epoch [184/200] batch [10/50] time 0.235 (0.203) data 0.148 (0.117) loss 2.0508 (1.8376) acc 46.8750 (45.3125) lr 3.9706e-05 eta 0:02:50
epoch [184/200] batch [15/50] time 0.087 (0.164) data 0.000 (0.078) loss 1.6807 (1.8382) acc 50.0000 (45.4167) lr 3.9706e-05 eta 0:02:16
epoch [184/200] batch [20/50] time 0.085 (0.144) data 0.000 (0.059) loss 2.1445 (1.8625) acc 40.6250 (44.5312) lr 3.9706e-05 eta 0:01:59
epoch [184/200] batch [25/50] time 0.086 (0.132) data 0.000 (0.047) loss 1.9658 (1.8574) acc 37.5000 (44.1250) lr 3.9706e-05 eta 0:01:49
epoch [184/200] batch [30/50] time 0.088 (0.127) data 0.002 (0.041) loss 1.4248 (1.8495) acc 62.5000 (45.5208) lr 3.9706e-05 eta 0:01:43
epoch [184/200] batch [35/50] time 0.085 (0.121) data 0.001 (0.035) loss 1.8623 (1.8432) acc 43.7500 (45.8929) lr 3.9706e-05 eta 0:01:38
epoch [184/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.035) loss 2.0469 (1.8328) acc 40.6250 (45.9375) lr 3.9706e-05 eta 0:01:37
epoch [184/200] batch [45/50] time 0.255 (0.120) data 0.171 (0.035) loss 1.8359 (1.8481) acc 43.7500 (45.4167) lr 3.9706e-05 eta 0:01:36
epoch [184/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.031) loss 1.8232 (1.8605) acc 25.0000 (44.8750) lr 3.5443e-05 eta 0:01:32
epoch [185/200] batch [5/50] time 0.085 (0.289) data 0.000 (0.204) loss 1.9004 (1.7098) acc 43.7500 (49.3750) lr 3.5443e-05 eta 0:03:49
epoch [185/200] batch [10/50] time 0.085 (0.187) data 0.000 (0.102) loss 1.8184 (1.6515) acc 43.7500 (50.9375) lr 3.5443e-05 eta 0:02:27
epoch [185/200] batch [15/50] time 0.085 (0.163) data 0.000 (0.078) loss 1.5371 (1.7299) acc 53.1250 (48.7500) lr 3.5443e-05 eta 0:02:07
epoch [185/200] batch [20/50] time 0.098 (0.147) data 0.006 (0.059) loss 1.7598 (1.7487) acc 56.2500 (48.2812) lr 3.5443e-05 eta 0:01:54
epoch [185/200] batch [25/50] time 0.104 (0.136) data 0.004 (0.048) loss 1.7402 (1.7524) acc 50.0000 (48.1250) lr 3.5443e-05 eta 0:01:45
epoch [185/200] batch [30/50] time 0.085 (0.128) data 0.000 (0.040) loss 1.9541 (1.7881) acc 56.2500 (47.8125) lr 3.5443e-05 eta 0:01:38
epoch [185/200] batch [35/50] time 0.085 (0.122) data 0.001 (0.034) loss 2.1758 (1.8225) acc 28.1250 (46.6071) lr 3.5443e-05 eta 0:01:33
epoch [185/200] batch [40/50] time 0.086 (0.117) data 0.000 (0.030) loss 1.6572 (1.8231) acc 46.8750 (46.1719) lr 3.5443e-05 eta 0:01:28
epoch [185/200] batch [45/50] time 0.083 (0.113) data 0.000 (0.027) loss 2.2363 (1.8522) acc 46.8750 (45.5556) lr 3.5443e-05 eta 0:01:25
epoch [185/200] batch [50/50] time 0.083 (0.110) data 0.000 (0.024) loss 1.3848 (1.8639) acc 53.1250 (45.0000) lr 3.1417e-05 eta 0:01:22
epoch [186/200] batch [5/50] time 0.085 (0.284) data 0.001 (0.198) loss 2.5508 (1.9277) acc 40.6250 (46.2500) lr 3.1417e-05 eta 0:03:31
epoch [186/200] batch [10/50] time 0.086 (0.188) data 0.001 (0.102) loss 1.8574 (1.9327) acc 50.0000 (45.6250) lr 3.1417e-05 eta 0:02:19
epoch [186/200] batch [15/50] time 0.086 (0.156) data 0.000 (0.070) loss 2.0273 (1.9378) acc 34.3750 (43.7500) lr 3.1417e-05 eta 0:01:54
epoch [186/200] batch [20/50] time 0.092 (0.145) data 0.007 (0.059) loss 1.8213 (1.9376) acc 53.1250 (44.8438) lr 3.1417e-05 eta 0:01:45
epoch [186/200] batch [25/50] time 0.255 (0.140) data 0.172 (0.054) loss 2.1426 (1.9533) acc 46.8750 (45.1250) lr 3.1417e-05 eta 0:01:41
epoch [186/200] batch [30/50] time 0.084 (0.130) data 0.000 (0.045) loss 1.5361 (1.9296) acc 50.0000 (45.2083) lr 3.1417e-05 eta 0:01:33
epoch [186/200] batch [35/50] time 0.084 (0.124) data 0.000 (0.039) loss 1.8379 (1.9059) acc 46.8750 (45.0893) lr 3.1417e-05 eta 0:01:28
epoch [186/200] batch [40/50] time 0.085 (0.121) data 0.000 (0.036) loss 1.7266 (1.8854) acc 59.3750 (45.7812) lr 3.1417e-05 eta 0:01:25
epoch [186/200] batch [45/50] time 0.157 (0.118) data 0.074 (0.034) loss 1.6777 (1.8787) acc 53.1250 (45.8333) lr 3.1417e-05 eta 0:01:23
epoch [186/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.030) loss 2.0977 (1.8827) acc 34.3750 (45.3750) lr 2.7630e-05 eta 0:01:20
epoch [187/200] batch [5/50] time 0.084 (0.291) data 0.000 (0.206) loss 2.0156 (1.9234) acc 40.6250 (44.3750) lr 2.7630e-05 eta 0:03:22
epoch [187/200] batch [10/50] time 0.126 (0.192) data 0.041 (0.107) loss 1.4473 (1.9150) acc 56.2500 (45.0000) lr 2.7630e-05 eta 0:02:12
epoch [187/200] batch [15/50] time 0.085 (0.156) data 0.000 (0.072) loss 1.8223 (1.8814) acc 56.2500 (45.8333) lr 2.7630e-05 eta 0:01:46
epoch [187/200] batch [20/50] time 0.085 (0.138) data 0.001 (0.054) loss 2.0449 (1.8908) acc 34.3750 (44.6875) lr 2.7630e-05 eta 0:01:33
epoch [187/200] batch [25/50] time 0.085 (0.128) data 0.000 (0.043) loss 2.1738 (1.9334) acc 34.3750 (43.3750) lr 2.7630e-05 eta 0:01:26
epoch [187/200] batch [30/50] time 0.084 (0.120) data 0.000 (0.036) loss 1.7275 (1.9122) acc 53.1250 (43.9583) lr 2.7630e-05 eta 0:01:20
epoch [187/200] batch [35/50] time 0.085 (0.115) data 0.001 (0.031) loss 1.4023 (1.9181) acc 53.1250 (44.1964) lr 2.7630e-05 eta 0:01:16
epoch [187/200] batch [40/50] time 0.084 (0.115) data 0.000 (0.031) loss 1.9111 (1.9013) acc 46.8750 (44.6094) lr 2.7630e-05 eta 0:01:16
epoch [187/200] batch [45/50] time 0.084 (0.113) data 0.000 (0.029) loss 1.3760 (1.8945) acc 56.2500 (45.0694) lr 2.7630e-05 eta 0:01:14
epoch [187/200] batch [50/50] time 0.084 (0.111) data 0.000 (0.027) loss 1.8477 (1.8772) acc 56.2500 (46.0625) lr 2.4083e-05 eta 0:01:11
epoch [188/200] batch [5/50] time 0.085 (0.294) data 0.001 (0.209) loss 1.7725 (1.9789) acc 40.6250 (41.8750) lr 2.4083e-05 eta 0:03:09
epoch [188/200] batch [10/50] time 0.286 (0.210) data 0.202 (0.125) loss 1.7090 (1.8188) acc 40.6250 (44.6875) lr 2.4083e-05 eta 0:02:14
epoch [188/200] batch [15/50] time 0.085 (0.168) data 0.000 (0.084) loss 2.1250 (1.8432) acc 37.5000 (45.2083) lr 2.4083e-05 eta 0:01:46
epoch [188/200] batch [20/50] time 0.085 (0.148) data 0.001 (0.063) loss 2.4082 (1.9153) acc 31.2500 (43.9062) lr 2.4083e-05 eta 0:01:33
epoch [188/200] batch [25/50] time 0.085 (0.135) data 0.000 (0.050) loss 1.5146 (1.8745) acc 53.1250 (45.6250) lr 2.4083e-05 eta 0:01:24
epoch [188/200] batch [30/50] time 0.084 (0.127) data 0.000 (0.042) loss 1.4082 (1.8420) acc 62.5000 (46.3542) lr 2.4083e-05 eta 0:01:18
epoch [188/200] batch [35/50] time 0.084 (0.121) data 0.000 (0.036) loss 1.5225 (1.8322) acc 56.2500 (47.0536) lr 2.4083e-05 eta 0:01:14
epoch [188/200] batch [40/50] time 0.083 (0.116) data 0.000 (0.032) loss 2.0176 (1.8667) acc 50.0000 (46.6406) lr 2.4083e-05 eta 0:01:10
epoch [188/200] batch [45/50] time 0.083 (0.113) data 0.000 (0.028) loss 2.0273 (1.8748) acc 40.6250 (46.8056) lr 2.4083e-05 eta 0:01:08
epoch [188/200] batch [50/50] time 0.083 (0.110) data 0.000 (0.025) loss 1.8311 (1.8797) acc 43.7500 (46.1250) lr 2.0777e-05 eta 0:01:05
epoch [189/200] batch [5/50] time 0.083 (0.328) data 0.000 (0.245) loss 2.0625 (1.8602) acc 31.2500 (40.6250) lr 2.0777e-05 eta 0:03:15
epoch [189/200] batch [10/50] time 0.084 (0.216) data 0.000 (0.132) loss 1.4619 (1.8647) acc 53.1250 (41.2500) lr 2.0777e-05 eta 0:02:07
epoch [189/200] batch [15/50] time 0.083 (0.172) data 0.000 (0.088) loss 1.8906 (1.8231) acc 46.8750 (44.1667) lr 2.0777e-05 eta 0:01:40
epoch [189/200] batch [20/50] time 0.083 (0.152) data 0.000 (0.069) loss 1.5449 (1.8409) acc 53.1250 (44.0625) lr 2.0777e-05 eta 0:01:28
epoch [189/200] batch [25/50] time 0.083 (0.139) data 0.000 (0.055) loss 2.0332 (1.8671) acc 43.7500 (44.2500) lr 2.0777e-05 eta 0:01:19
epoch [189/200] batch [30/50] time 0.083 (0.134) data 0.000 (0.051) loss 1.7197 (1.8729) acc 50.0000 (44.2708) lr 2.0777e-05 eta 0:01:16
epoch [189/200] batch [35/50] time 0.118 (0.133) data 0.034 (0.050) loss 1.6934 (1.8846) acc 53.1250 (44.8214) lr 2.0777e-05 eta 0:01:15
epoch [189/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.044) loss 2.0332 (1.8694) acc 31.2500 (45.7812) lr 2.0777e-05 eta 0:01:11
epoch [189/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.043) loss 1.7256 (1.8574) acc 53.1250 (46.0417) lr 2.0777e-05 eta 0:01:09
epoch [189/200] batch [50/50] time 0.084 (0.122) data 0.000 (0.038) loss 1.7393 (1.8507) acc 50.0000 (46.1250) lr 1.7713e-05 eta 0:01:06
epoch [190/200] batch [5/50] time 0.085 (0.385) data 0.000 (0.301) loss 1.6992 (1.6881) acc 50.0000 (50.0000) lr 1.7713e-05 eta 0:03:30
epoch [190/200] batch [10/50] time 0.086 (0.247) data 0.000 (0.162) loss 2.0078 (1.8452) acc 50.0000 (48.7500) lr 1.7713e-05 eta 0:02:13
epoch [190/200] batch [15/50] time 0.084 (0.193) data 0.000 (0.108) loss 1.8770 (1.8620) acc 46.8750 (46.2500) lr 1.7713e-05 eta 0:01:43
epoch [190/200] batch [20/50] time 0.086 (0.173) data 0.000 (0.088) loss 2.1641 (1.8348) acc 28.1250 (46.2500) lr 1.7713e-05 eta 0:01:31
epoch [190/200] batch [25/50] time 0.197 (0.160) data 0.112 (0.075) loss 1.8516 (1.8078) acc 53.1250 (47.3750) lr 1.7713e-05 eta 0:01:24
epoch [190/200] batch [30/50] time 0.084 (0.148) data 0.000 (0.063) loss 1.9238 (1.8327) acc 43.7500 (47.1875) lr 1.7713e-05 eta 0:01:16
epoch [190/200] batch [35/50] time 0.084 (0.141) data 0.001 (0.057) loss 1.8193 (1.8426) acc 46.8750 (46.7857) lr 1.7713e-05 eta 0:01:12
epoch [190/200] batch [40/50] time 0.083 (0.134) data 0.000 (0.050) loss 1.7754 (1.8508) acc 53.1250 (46.0938) lr 1.7713e-05 eta 0:01:08
epoch [190/200] batch [45/50] time 0.083 (0.132) data 0.000 (0.047) loss 1.8730 (1.8567) acc 31.2500 (45.9028) lr 1.7713e-05 eta 0:01:06
epoch [190/200] batch [50/50] time 0.083 (0.127) data 0.000 (0.043) loss 1.4688 (1.8516) acc 50.0000 (45.3750) lr 1.4891e-05 eta 0:01:03
epoch [191/200] batch [5/50] time 0.085 (0.375) data 0.000 (0.290) loss 1.9004 (1.8826) acc 50.0000 (46.2500) lr 1.4891e-05 eta 0:03:05
epoch [191/200] batch [10/50] time 0.085 (0.230) data 0.000 (0.145) loss 1.7031 (1.9107) acc 50.0000 (44.0625) lr 1.4891e-05 eta 0:01:52
epoch [191/200] batch [15/50] time 0.084 (0.193) data 0.000 (0.109) loss 1.5762 (1.8829) acc 46.8750 (44.5833) lr 1.4891e-05 eta 0:01:33
epoch [191/200] batch [20/50] time 0.084 (0.174) data 0.000 (0.090) loss 1.6562 (1.8308) acc 46.8750 (45.9375) lr 1.4891e-05 eta 0:01:23
epoch [191/200] batch [25/50] time 0.084 (0.156) data 0.000 (0.072) loss 2.0996 (1.8402) acc 37.5000 (45.6250) lr 1.4891e-05 eta 0:01:14
epoch [191/200] batch [30/50] time 0.085 (0.151) data 0.000 (0.067) loss 1.6426 (1.8138) acc 43.7500 (45.5208) lr 1.4891e-05 eta 0:01:10
epoch [191/200] batch [35/50] time 0.193 (0.144) data 0.109 (0.060) loss 1.8330 (1.8263) acc 53.1250 (46.0714) lr 1.4891e-05 eta 0:01:07
epoch [191/200] batch [40/50] time 0.083 (0.137) data 0.000 (0.053) loss 1.6699 (1.8204) acc 53.1250 (46.1719) lr 1.4891e-05 eta 0:01:02
epoch [191/200] batch [45/50] time 0.083 (0.134) data 0.000 (0.050) loss 1.6055 (1.8194) acc 53.1250 (46.2500) lr 1.4891e-05 eta 0:01:00
epoch [191/200] batch [50/50] time 0.085 (0.129) data 0.000 (0.045) loss 2.0430 (1.8390) acc 56.2500 (46.1250) lr 1.2312e-05 eta 0:00:57
epoch [192/200] batch [5/50] time 0.083 (0.313) data 0.000 (0.228) loss 1.6143 (1.7701) acc 50.0000 (42.5000) lr 1.2312e-05 eta 0:02:19
epoch [192/200] batch [10/50] time 0.084 (0.199) data 0.000 (0.114) loss 1.8613 (1.7580) acc 53.1250 (45.9375) lr 1.2312e-05 eta 0:01:27
epoch [192/200] batch [15/50] time 0.085 (0.161) data 0.000 (0.076) loss 1.6836 (1.7781) acc 53.1250 (48.1250) lr 1.2312e-05 eta 0:01:09
epoch [192/200] batch [20/50] time 0.085 (0.142) data 0.000 (0.057) loss 2.0352 (1.8147) acc 46.8750 (47.8125) lr 1.2312e-05 eta 0:01:00
epoch [192/200] batch [25/50] time 0.084 (0.130) data 0.000 (0.046) loss 1.8350 (1.8516) acc 53.1250 (46.6250) lr 1.2312e-05 eta 0:00:55
epoch [192/200] batch [30/50] time 0.084 (0.128) data 0.000 (0.043) loss 1.6855 (1.8465) acc 40.6250 (45.3125) lr 1.2312e-05 eta 0:00:53
epoch [192/200] batch [35/50] time 0.084 (0.121) data 0.001 (0.037) loss 1.9023 (1.8361) acc 53.1250 (45.6250) lr 1.2312e-05 eta 0:00:50
epoch [192/200] batch [40/50] time 0.085 (0.119) data 0.000 (0.035) loss 1.6807 (1.8325) acc 43.7500 (45.5469) lr 1.2312e-05 eta 0:00:48
epoch [192/200] batch [45/50] time 0.153 (0.119) data 0.070 (0.035) loss 1.5000 (1.8216) acc 59.3750 (46.1806) lr 1.2312e-05 eta 0:00:48
epoch [192/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 2.1074 (1.8295) acc 37.5000 (45.9375) lr 9.9763e-06 eta 0:00:46
epoch [193/200] batch [5/50] time 0.084 (0.289) data 0.000 (0.204) loss 2.0762 (1.8633) acc 46.8750 (45.0000) lr 9.9763e-06 eta 0:01:53
epoch [193/200] batch [10/50] time 0.084 (0.192) data 0.000 (0.108) loss 1.5410 (1.8804) acc 53.1250 (45.9375) lr 9.9763e-06 eta 0:01:14
epoch [193/200] batch [15/50] time 0.084 (0.156) data 0.000 (0.072) loss 2.3770 (1.8916) acc 37.5000 (45.8333) lr 9.9763e-06 eta 0:01:00
epoch [193/200] batch [20/50] time 0.084 (0.142) data 0.000 (0.058) loss 2.0820 (1.8771) acc 37.5000 (45.6250) lr 9.9763e-06 eta 0:00:53
epoch [193/200] batch [25/50] time 0.084 (0.130) data 0.000 (0.046) loss 1.7129 (1.8555) acc 46.8750 (46.3750) lr 9.9763e-06 eta 0:00:48
epoch [193/200] batch [30/50] time 0.084 (0.130) data 0.000 (0.046) loss 1.8945 (1.8580) acc 43.7500 (46.7708) lr 9.9763e-06 eta 0:00:47
epoch [193/200] batch [35/50] time 0.084 (0.128) data 0.000 (0.044) loss 1.8721 (1.8570) acc 40.6250 (46.7857) lr 9.9763e-06 eta 0:00:46
epoch [193/200] batch [40/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.0000 (1.8744) acc 37.5000 (45.6250) lr 9.9763e-06 eta 0:00:44
epoch [193/200] batch [45/50] time 0.084 (0.121) data 0.000 (0.037) loss 2.4180 (1.8926) acc 37.5000 (45.3472) lr 9.9763e-06 eta 0:00:42
epoch [193/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.034) loss 1.9209 (1.8786) acc 34.3750 (45.8125) lr 7.8853e-06 eta 0:00:41
epoch [194/200] batch [5/50] time 0.085 (0.280) data 0.000 (0.196) loss 2.0527 (1.9168) acc 34.3750 (41.8750) lr 7.8853e-06 eta 0:01:36
epoch [194/200] batch [10/50] time 0.084 (0.182) data 0.000 (0.098) loss 2.1016 (1.8899) acc 37.5000 (43.1250) lr 7.8853e-06 eta 0:01:02
epoch [194/200] batch [15/50] time 0.084 (0.156) data 0.000 (0.072) loss 2.1250 (1.8553) acc 37.5000 (45.2083) lr 7.8853e-06 eta 0:00:52
epoch [194/200] batch [20/50] time 0.085 (0.138) data 0.000 (0.054) loss 1.7451 (1.8374) acc 46.8750 (46.2500) lr 7.8853e-06 eta 0:00:45
epoch [194/200] batch [25/50] time 0.084 (0.134) data 0.000 (0.050) loss 2.0078 (1.8782) acc 31.2500 (44.5000) lr 7.8853e-06 eta 0:00:43
epoch [194/200] batch [30/50] time 0.084 (0.131) data 0.000 (0.047) loss 2.3398 (1.8952) acc 28.1250 (43.7500) lr 7.8853e-06 eta 0:00:41
epoch [194/200] batch [35/50] time 0.085 (0.124) data 0.000 (0.040) loss 1.8652 (1.8721) acc 37.5000 (43.6607) lr 7.8853e-06 eta 0:00:39
epoch [194/200] batch [40/50] time 0.083 (0.120) data 0.000 (0.037) loss 1.9316 (1.8878) acc 43.7500 (43.5156) lr 7.8853e-06 eta 0:00:37
epoch [194/200] batch [45/50] time 0.216 (0.119) data 0.133 (0.035) loss 1.5977 (1.8757) acc 50.0000 (44.0972) lr 7.8853e-06 eta 0:00:36
epoch [194/200] batch [50/50] time 0.084 (0.116) data 0.000 (0.032) loss 2.2500 (1.8843) acc 37.5000 (44.3125) lr 6.0390e-06 eta 0:00:34
epoch [195/200] batch [5/50] time 0.084 (0.295) data 0.000 (0.211) loss 2.1641 (2.1133) acc 40.6250 (38.7500) lr 6.0390e-06 eta 0:01:26
epoch [195/200] batch [10/50] time 0.084 (0.189) data 0.000 (0.106) loss 2.1445 (2.0024) acc 31.2500 (42.1875) lr 6.0390e-06 eta 0:00:54
epoch [195/200] batch [15/50] time 0.085 (0.160) data 0.000 (0.077) loss 1.8750 (1.9083) acc 46.8750 (46.8750) lr 6.0390e-06 eta 0:00:45
epoch [195/200] batch [20/50] time 0.084 (0.142) data 0.000 (0.058) loss 2.2754 (1.9309) acc 37.5000 (46.7188) lr 6.0390e-06 eta 0:00:39
epoch [195/200] batch [25/50] time 0.084 (0.134) data 0.000 (0.050) loss 1.8086 (1.9274) acc 46.8750 (46.6250) lr 6.0390e-06 eta 0:00:36
epoch [195/200] batch [30/50] time 0.084 (0.131) data 0.000 (0.047) loss 2.5215 (1.9351) acc 31.2500 (46.2500) lr 6.0390e-06 eta 0:00:35
epoch [195/200] batch [35/50] time 0.085 (0.124) data 0.001 (0.040) loss 1.9648 (1.8932) acc 53.1250 (47.6786) lr 6.0390e-06 eta 0:00:32
epoch [195/200] batch [40/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.1250 (1.8685) acc 43.7500 (48.0469) lr 6.0390e-06 eta 0:00:32
epoch [195/200] batch [45/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.2207 (1.8488) acc 43.7500 (48.4028) lr 6.0390e-06 eta 0:00:30
epoch [195/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 1.6445 (1.8510) acc 62.5000 (48.0000) lr 4.4380e-06 eta 0:00:28
epoch [196/200] batch [5/50] time 0.085 (0.309) data 0.000 (0.225) loss 1.5820 (1.7434) acc 59.3750 (49.3750) lr 4.4380e-06 eta 0:01:15
epoch [196/200] batch [10/50] time 0.210 (0.209) data 0.124 (0.125) loss 1.9688 (1.9229) acc 46.8750 (45.0000) lr 4.4380e-06 eta 0:00:50
epoch [196/200] batch [15/50] time 0.084 (0.168) data 0.000 (0.083) loss 1.6221 (1.8619) acc 50.0000 (45.0000) lr 4.4380e-06 eta 0:00:39
epoch [196/200] batch [20/50] time 0.084 (0.154) data 0.000 (0.070) loss 1.6699 (1.8838) acc 62.5000 (45.4688) lr 4.4380e-06 eta 0:00:35
epoch [196/200] batch [25/50] time 0.084 (0.140) data 0.000 (0.056) loss 1.4404 (1.8579) acc 56.2500 (46.1250) lr 4.4380e-06 eta 0:00:31
epoch [196/200] batch [30/50] time 0.084 (0.133) data 0.000 (0.049) loss 1.7412 (1.8827) acc 46.8750 (45.5208) lr 4.4380e-06 eta 0:00:29
epoch [196/200] batch [35/50] time 0.084 (0.132) data 0.000 (0.048) loss 1.5020 (1.8980) acc 56.2500 (45.4464) lr 4.4380e-06 eta 0:00:28
epoch [196/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.042) loss 1.7627 (1.9157) acc 43.7500 (44.9219) lr 4.4380e-06 eta 0:00:26
epoch [196/200] batch [45/50] time 0.083 (0.121) data 0.000 (0.038) loss 1.8096 (1.9047) acc 40.6250 (45.2778) lr 4.4380e-06 eta 0:00:24
epoch [196/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.034) loss 2.2305 (1.9141) acc 40.6250 (44.7500) lr 3.0827e-06 eta 0:00:23
epoch [197/200] batch [5/50] time 0.086 (0.309) data 0.000 (0.224) loss 2.1367 (1.9520) acc 37.5000 (42.5000) lr 3.0827e-06 eta 0:01:00
epoch [197/200] batch [10/50] time 0.085 (0.197) data 0.000 (0.112) loss 1.6465 (1.8783) acc 59.3750 (45.3125) lr 3.0827e-06 eta 0:00:37
epoch [197/200] batch [15/50] time 0.086 (0.160) data 0.000 (0.075) loss 1.6709 (1.8797) acc 56.2500 (46.8750) lr 3.0827e-06 eta 0:00:29
epoch [197/200] batch [20/50] time 0.086 (0.142) data 0.000 (0.056) loss 2.0645 (1.9077) acc 40.6250 (45.6250) lr 3.0827e-06 eta 0:00:25
epoch [197/200] batch [25/50] time 0.167 (0.134) data 0.083 (0.048) loss 2.2480 (1.9471) acc 40.6250 (44.5000) lr 3.0827e-06 eta 0:00:23
epoch [197/200] batch [30/50] time 0.085 (0.126) data 0.000 (0.040) loss 1.1641 (1.9084) acc 62.5000 (45.2083) lr 3.0827e-06 eta 0:00:21
epoch [197/200] batch [35/50] time 0.086 (0.124) data 0.001 (0.038) loss 2.2461 (1.9176) acc 37.5000 (45.0893) lr 3.0827e-06 eta 0:00:20
epoch [197/200] batch [40/50] time 0.084 (0.119) data 0.000 (0.034) loss 1.8223 (1.9006) acc 43.7500 (45.2344) lr 3.0827e-06 eta 0:00:19
epoch [197/200] batch [45/50] time 0.083 (0.119) data 0.000 (0.034) loss 2.1699 (1.8965) acc 34.3750 (45.2083) lr 3.0827e-06 eta 0:00:18
epoch [197/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.030) loss 1.9121 (1.8890) acc 46.8750 (45.3750) lr 1.9733e-06 eta 0:00:17
epoch [198/200] batch [5/50] time 0.085 (0.303) data 0.000 (0.218) loss 2.0469 (1.9143) acc 46.8750 (43.7500) lr 1.9733e-06 eta 0:00:43
epoch [198/200] batch [10/50] time 0.123 (0.198) data 0.038 (0.113) loss 1.7246 (1.9528) acc 53.1250 (45.6250) lr 1.9733e-06 eta 0:00:27
epoch [198/200] batch [15/50] time 0.086 (0.161) data 0.001 (0.075) loss 1.7295 (1.9406) acc 46.8750 (43.9583) lr 1.9733e-06 eta 0:00:21
epoch [198/200] batch [20/50] time 0.085 (0.142) data 0.000 (0.057) loss 1.8828 (1.9219) acc 43.7500 (44.8438) lr 1.9733e-06 eta 0:00:18
epoch [198/200] batch [25/50] time 0.086 (0.131) data 0.001 (0.045) loss 1.9463 (1.9011) acc 34.3750 (45.0000) lr 1.9733e-06 eta 0:00:16
epoch [198/200] batch [30/50] time 0.119 (0.125) data 0.034 (0.040) loss 1.8984 (1.8997) acc 43.7500 (45.0000) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [35/50] time 0.086 (0.120) data 0.001 (0.034) loss 1.6621 (1.9021) acc 50.0000 (44.9107) lr 1.9733e-06 eta 0:00:13
epoch [198/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.035) loss 1.9287 (1.8794) acc 40.6250 (45.3906) lr 1.9733e-06 eta 0:00:13
epoch [198/200] batch [45/50] time 0.083 (0.116) data 0.000 (0.031) loss 1.4785 (1.8628) acc 50.0000 (45.4861) lr 1.9733e-06 eta 0:00:12
epoch [198/200] batch [50/50] time 0.084 (0.116) data 0.000 (0.032) loss 1.8994 (1.8694) acc 40.6250 (44.9375) lr 1.1101e-06 eta 0:00:11
epoch [199/200] batch [5/50] time 0.085 (0.327) data 0.000 (0.242) loss 1.8232 (1.8623) acc 56.2500 (44.3750) lr 1.1101e-06 eta 0:00:31
epoch [199/200] batch [10/50] time 0.089 (0.213) data 0.000 (0.128) loss 1.6885 (1.8661) acc 43.7500 (45.0000) lr 1.1101e-06 eta 0:00:19
epoch [199/200] batch [15/50] time 0.085 (0.171) data 0.000 (0.085) loss 1.8936 (1.8088) acc 40.6250 (46.8750) lr 1.1101e-06 eta 0:00:14
epoch [199/200] batch [20/50] time 0.085 (0.158) data 0.000 (0.073) loss 1.9385 (1.8034) acc 43.7500 (48.5938) lr 1.1101e-06 eta 0:00:12
epoch [199/200] batch [25/50] time 0.201 (0.148) data 0.116 (0.063) loss 2.4219 (1.8508) acc 40.6250 (47.3750) lr 1.1101e-06 eta 0:00:11
epoch [199/200] batch [30/50] time 0.084 (0.137) data 0.000 (0.053) loss 1.5996 (1.8535) acc 53.1250 (47.3958) lr 1.1101e-06 eta 0:00:09
epoch [199/200] batch [35/50] time 0.085 (0.133) data 0.000 (0.048) loss 1.7129 (1.8568) acc 50.0000 (46.6964) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.042) loss 1.4863 (1.8443) acc 62.5000 (46.4062) lr 1.1101e-06 eta 0:00:07
epoch [199/200] batch [45/50] time 0.082 (0.126) data 0.000 (0.042) loss 1.8066 (1.8352) acc 31.2500 (45.9722) lr 1.1101e-06 eta 0:00:06
epoch [199/200] batch [50/50] time 0.082 (0.122) data 0.000 (0.038) loss 1.8125 (1.8247) acc 34.3750 (45.8750) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [5/50] time 0.085 (0.275) data 0.000 (0.190) loss 2.6152 (2.0506) acc 28.1250 (40.6250) lr 4.9344e-07 eta 0:00:12
epoch [200/200] batch [10/50] time 0.087 (0.194) data 0.001 (0.108) loss 1.6396 (1.9953) acc 56.2500 (43.4375) lr 4.9344e-07 eta 0:00:07
epoch [200/200] batch [15/50] time 0.085 (0.157) data 0.000 (0.072) loss 2.3711 (1.9997) acc 28.1250 (44.5833) lr 4.9344e-07 eta 0:00:05
epoch [200/200] batch [20/50] time 0.086 (0.139) data 0.000 (0.054) loss 2.2734 (1.9889) acc 46.8750 (44.8438) lr 4.9344e-07 eta 0:00:04
epoch [200/200] batch [25/50] time 0.085 (0.129) data 0.000 (0.043) loss 2.3750 (2.0036) acc 37.5000 (45.2500) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [30/50] time 0.086 (0.125) data 0.000 (0.040) loss 1.9346 (1.9993) acc 43.7500 (44.8958) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [35/50] time 0.086 (0.119) data 0.000 (0.034) loss 1.6885 (1.9657) acc 43.7500 (44.9107) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.035) loss 1.6768 (1.9326) acc 56.2500 (45.7812) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [45/50] time 0.083 (0.119) data 0.000 (0.034) loss 1.9424 (1.9283) acc 31.2500 (45.0000) lr 4.9344e-07 eta 0:00:00
epoch [200/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 1.8828 (1.9342) acc 50.0000 (45.2500) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/34 [00:00<?, ?it/s]  3%|▎         | 1/34 [00:03<01:59,  3.63s/it]  6%|▌         | 2/34 [00:03<00:50,  1.57s/it]  9%|▉         | 3/34 [00:03<00:28,  1.10it/s] 12%|█▏        | 4/34 [00:04<00:17,  1.67it/s] 15%|█▍        | 5/34 [00:04<00:12,  2.34it/s] 18%|█▊        | 6/34 [00:04<00:09,  3.10it/s] 21%|██        | 7/34 [00:04<00:06,  3.89it/s] 24%|██▎       | 8/34 [00:04<00:05,  4.67it/s] 26%|██▋       | 9/34 [00:06<00:19,  1.25it/s] 29%|██▉       | 10/34 [00:06<00:14,  1.70it/s] 32%|███▏      | 11/34 [00:06<00:10,  2.24it/s] 35%|███▌      | 12/34 [00:06<00:07,  2.88it/s] 38%|███▊      | 13/34 [00:07<00:05,  3.58it/s] 41%|████      | 14/34 [00:07<00:04,  4.32it/s] 44%|████▍     | 15/34 [00:07<00:03,  5.04it/s] 47%|████▋     | 16/34 [00:07<00:03,  5.70it/s] 50%|█████     | 17/34 [00:09<00:10,  1.64it/s] 53%|█████▎    | 18/34 [00:09<00:07,  2.17it/s] 56%|█████▌    | 19/34 [00:09<00:05,  2.73it/s] 59%|█████▉    | 20/34 [00:09<00:04,  3.42it/s] 62%|██████▏   | 21/34 [00:09<00:03,  4.06it/s] 65%|██████▍   | 22/34 [00:09<00:03,  3.51it/s] 68%|██████▊   | 23/34 [00:10<00:02,  4.24it/s] 71%|███████   | 24/34 [00:10<00:02,  4.96it/s] 74%|███████▎  | 25/34 [00:11<00:04,  1.83it/s] 76%|███████▋  | 26/34 [00:12<00:04,  1.86it/s] 79%|███████▉  | 27/34 [00:12<00:02,  2.43it/s] 82%|████████▏ | 28/34 [00:12<00:01,  3.09it/s] 85%|████████▌ | 29/34 [00:12<00:01,  3.80it/s] 88%|████████▊ | 30/34 [00:13<00:01,  2.73it/s] 91%|█████████ | 31/34 [00:13<00:00,  3.42it/s] 94%|█████████▍| 32/34 [00:13<00:00,  4.15it/s] 97%|█████████▋| 33/34 [00:13<00:00,  3.44it/s]100%|██████████| 34/34 [00:13<00:00,  2.46it/s]
=> result
* total: 3,333
* correct: 1,336
* accuracy: 40.1%
* error: 59.9%
* macro_f1: 39.2%
Elapsed: 0:20:24
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_train.sh fgvc_aircraft 2 0 vit_b16_ctxv1 16 CoOp
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_2.pkl
1600 3333 3333
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  100
# train_x  1,600
# val      3,333
# test     3,333
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/200] batch [5/50] time 0.083 (0.677) data 0.000 (0.242) loss 3.1758 (3.0105) acc 18.7500 (21.2500) lr 1.0000e-05 eta 1:52:49
epoch [1/200] batch [10/50] time 0.083 (0.380) data 0.000 (0.121) loss 3.5859 (3.0977) acc 12.5000 (20.9375) lr 1.0000e-05 eta 1:03:19
epoch [1/200] batch [15/50] time 0.084 (0.281) data 0.000 (0.081) loss 3.0078 (3.0017) acc 25.0000 (23.1250) lr 1.0000e-05 eta 0:46:50
epoch [1/200] batch [20/50] time 0.084 (0.232) data 0.000 (0.061) loss 3.1953 (3.0086) acc 15.6250 (21.7188) lr 1.0000e-05 eta 0:38:37
epoch [1/200] batch [25/50] time 0.083 (0.203) data 0.000 (0.049) loss 3.0195 (2.9711) acc 21.8750 (21.8750) lr 1.0000e-05 eta 0:33:40
epoch [1/200] batch [30/50] time 0.085 (0.183) data 0.000 (0.041) loss 2.9043 (2.9776) acc 25.0000 (21.3542) lr 1.0000e-05 eta 0:30:22
epoch [1/200] batch [35/50] time 0.084 (0.169) data 0.001 (0.035) loss 2.4336 (2.9593) acc 25.0000 (20.9821) lr 1.0000e-05 eta 0:28:01
epoch [1/200] batch [40/50] time 0.083 (0.158) data 0.000 (0.031) loss 3.4238 (2.9940) acc 18.7500 (20.3906) lr 1.0000e-05 eta 0:26:15
epoch [1/200] batch [45/50] time 0.082 (0.150) data 0.000 (0.027) loss 3.0449 (3.0200) acc 15.6250 (19.7917) lr 1.0000e-05 eta 0:24:51
epoch [1/200] batch [50/50] time 0.082 (0.143) data 0.000 (0.024) loss 2.7383 (2.9801) acc 25.0000 (20.6250) lr 2.0000e-03 eta 0:23:44
epoch [2/200] batch [5/50] time 0.083 (0.298) data 0.000 (0.215) loss 3.1523 (2.9984) acc 12.5000 (24.3750) lr 2.0000e-03 eta 0:49:25
epoch [2/200] batch [10/50] time 0.150 (0.199) data 0.066 (0.115) loss 2.8203 (2.8566) acc 34.3750 (25.0000) lr 2.0000e-03 eta 0:32:55
epoch [2/200] batch [15/50] time 0.085 (0.160) data 0.000 (0.077) loss 2.6562 (2.7730) acc 25.0000 (26.2500) lr 2.0000e-03 eta 0:26:34
epoch [2/200] batch [20/50] time 0.085 (0.147) data 0.000 (0.064) loss 2.5664 (2.7791) acc 28.1250 (25.6250) lr 2.0000e-03 eta 0:24:20
epoch [2/200] batch [25/50] time 0.109 (0.136) data 0.026 (0.052) loss 2.0117 (2.7480) acc 25.0000 (25.5000) lr 2.0000e-03 eta 0:22:26
epoch [2/200] batch [30/50] time 0.084 (0.127) data 0.001 (0.043) loss 2.9609 (2.7454) acc 21.8750 (24.8958) lr 2.0000e-03 eta 0:21:00
epoch [2/200] batch [35/50] time 0.084 (0.121) data 0.000 (0.037) loss 2.4766 (2.7213) acc 28.1250 (24.9107) lr 2.0000e-03 eta 0:20:00
epoch [2/200] batch [40/50] time 0.084 (0.117) data 0.000 (0.033) loss 2.8887 (2.7233) acc 12.5000 (24.6875) lr 2.0000e-03 eta 0:19:14
epoch [2/200] batch [45/50] time 0.084 (0.113) data 0.000 (0.029) loss 2.4141 (2.7135) acc 28.1250 (25.0000) lr 2.0000e-03 eta 0:18:38
epoch [2/200] batch [50/50] time 0.082 (0.110) data 0.000 (0.026) loss 2.5664 (2.6862) acc 31.2500 (25.5000) lr 1.9999e-03 eta 0:18:07
epoch [3/200] batch [5/50] time 0.103 (0.250) data 0.018 (0.166) loss 2.8340 (2.6043) acc 31.2500 (28.1250) lr 1.9999e-03 eta 0:41:17
epoch [3/200] batch [10/50] time 0.206 (0.180) data 0.122 (0.095) loss 2.6582 (2.5812) acc 21.8750 (27.1875) lr 1.9999e-03 eta 0:29:38
epoch [3/200] batch [15/50] time 0.084 (0.148) data 0.000 (0.064) loss 2.6270 (2.5749) acc 21.8750 (26.2500) lr 1.9999e-03 eta 0:24:21
epoch [3/200] batch [20/50] time 0.083 (0.132) data 0.000 (0.048) loss 3.0996 (2.5711) acc 12.5000 (25.9375) lr 1.9999e-03 eta 0:21:42
epoch [3/200] batch [25/50] time 0.084 (0.122) data 0.000 (0.038) loss 2.4180 (2.5554) acc 28.1250 (25.6250) lr 1.9999e-03 eta 0:20:06
epoch [3/200] batch [30/50] time 0.083 (0.118) data 0.000 (0.034) loss 2.5996 (2.5778) acc 25.0000 (25.3125) lr 1.9999e-03 eta 0:19:23
epoch [3/200] batch [35/50] time 0.084 (0.115) data 0.001 (0.032) loss 2.3379 (2.5741) acc 21.8750 (25.5357) lr 1.9999e-03 eta 0:18:57
epoch [3/200] batch [40/50] time 0.084 (0.111) data 0.000 (0.028) loss 2.2656 (2.5630) acc 25.0000 (25.1562) lr 1.9999e-03 eta 0:18:17
epoch [3/200] batch [45/50] time 0.084 (0.108) data 0.000 (0.025) loss 2.3770 (2.5678) acc 37.5000 (25.2083) lr 1.9999e-03 eta 0:17:46
epoch [3/200] batch [50/50] time 0.082 (0.106) data 0.000 (0.022) loss 2.7363 (2.5621) acc 21.8750 (25.5625) lr 1.9995e-03 eta 0:17:21
epoch [4/200] batch [5/50] time 0.086 (0.313) data 0.000 (0.228) loss 2.4473 (2.4820) acc 25.0000 (26.2500) lr 1.9995e-03 eta 0:51:19
epoch [4/200] batch [10/50] time 0.127 (0.203) data 0.044 (0.119) loss 2.2129 (2.4221) acc 28.1250 (27.1875) lr 1.9995e-03 eta 0:33:21
epoch [4/200] batch [15/50] time 0.085 (0.164) data 0.000 (0.079) loss 2.5000 (2.5014) acc 31.2500 (25.8333) lr 1.9995e-03 eta 0:26:50
epoch [4/200] batch [20/50] time 0.084 (0.150) data 0.000 (0.065) loss 2.5410 (2.5066) acc 15.6250 (25.6250) lr 1.9995e-03 eta 0:24:31
epoch [4/200] batch [25/50] time 0.084 (0.137) data 0.000 (0.052) loss 2.7285 (2.5170) acc 18.7500 (26.2500) lr 1.9995e-03 eta 0:22:21
epoch [4/200] batch [30/50] time 0.085 (0.131) data 0.000 (0.047) loss 2.5938 (2.4932) acc 31.2500 (27.0833) lr 1.9995e-03 eta 0:21:29
epoch [4/200] batch [35/50] time 0.084 (0.128) data 0.001 (0.044) loss 2.5938 (2.5135) acc 25.0000 (27.1429) lr 1.9995e-03 eta 0:20:56
epoch [4/200] batch [40/50] time 0.083 (0.122) data 0.000 (0.038) loss 2.5488 (2.5024) acc 25.0000 (27.1094) lr 1.9995e-03 eta 0:20:01
epoch [4/200] batch [45/50] time 0.084 (0.118) data 0.000 (0.034) loss 2.7656 (2.5076) acc 21.8750 (27.0833) lr 1.9995e-03 eta 0:19:21
epoch [4/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 2.6797 (2.5064) acc 18.7500 (26.9375) lr 1.9989e-03 eta 0:18:46
epoch [5/200] batch [5/50] time 0.083 (0.266) data 0.000 (0.182) loss 2.4609 (2.6547) acc 18.7500 (20.0000) lr 1.9989e-03 eta 0:43:21
epoch [5/200] batch [10/50] time 0.083 (0.175) data 0.000 (0.091) loss 2.6777 (2.6131) acc 18.7500 (21.8750) lr 1.9989e-03 eta 0:28:30
epoch [5/200] batch [15/50] time 0.084 (0.148) data 0.000 (0.064) loss 2.3086 (2.5263) acc 34.3750 (24.7917) lr 1.9989e-03 eta 0:24:08
epoch [5/200] batch [20/50] time 0.083 (0.132) data 0.000 (0.048) loss 2.0938 (2.4886) acc 40.6250 (25.7812) lr 1.9989e-03 eta 0:21:30
epoch [5/200] batch [25/50] time 0.084 (0.127) data 0.000 (0.043) loss 1.9678 (2.4400) acc 37.5000 (26.7500) lr 1.9989e-03 eta 0:20:41
epoch [5/200] batch [30/50] time 0.086 (0.124) data 0.000 (0.040) loss 2.4023 (2.4845) acc 28.1250 (26.5625) lr 1.9989e-03 eta 0:20:12
epoch [5/200] batch [35/50] time 0.085 (0.118) data 0.000 (0.035) loss 2.6445 (2.4812) acc 28.1250 (26.9643) lr 1.9989e-03 eta 0:19:15
epoch [5/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.036) loss 2.4238 (2.4818) acc 37.5000 (27.5000) lr 1.9989e-03 eta 0:19:27
epoch [5/200] batch [45/50] time 0.083 (0.116) data 0.000 (0.032) loss 2.8867 (2.5037) acc 25.0000 (27.7083) lr 1.9989e-03 eta 0:18:47
epoch [5/200] batch [50/50] time 0.083 (0.112) data 0.000 (0.029) loss 2.4688 (2.4999) acc 31.2500 (28.5625) lr 1.9980e-03 eta 0:18:16
epoch [6/200] batch [5/50] time 0.085 (0.297) data 0.000 (0.212) loss 2.5566 (2.5801) acc 31.2500 (24.3750) lr 1.9980e-03 eta 0:48:13
epoch [6/200] batch [10/50] time 0.085 (0.191) data 0.000 (0.106) loss 2.6895 (2.5580) acc 18.7500 (24.6875) lr 1.9980e-03 eta 0:31:01
epoch [6/200] batch [15/50] time 0.085 (0.156) data 0.000 (0.071) loss 2.7969 (2.5145) acc 18.7500 (26.0417) lr 1.9980e-03 eta 0:25:18
epoch [6/200] batch [20/50] time 0.087 (0.144) data 0.000 (0.059) loss 2.4160 (2.4928) acc 9.3750 (25.9375) lr 1.9980e-03 eta 0:23:18
epoch [6/200] batch [25/50] time 0.085 (0.132) data 0.000 (0.047) loss 2.5449 (2.4896) acc 25.0000 (26.2500) lr 1.9980e-03 eta 0:21:24
epoch [6/200] batch [30/50] time 0.084 (0.127) data 0.000 (0.042) loss 2.5977 (2.4821) acc 21.8750 (26.4583) lr 1.9980e-03 eta 0:20:34
epoch [6/200] batch [35/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.5176 (2.4533) acc 21.8750 (27.5000) lr 1.9980e-03 eta 0:20:26
epoch [6/200] batch [40/50] time 0.083 (0.121) data 0.000 (0.036) loss 2.3242 (2.4626) acc 46.8750 (28.3594) lr 1.9980e-03 eta 0:19:33
epoch [6/200] batch [45/50] time 0.083 (0.117) data 0.000 (0.032) loss 2.4180 (2.4612) acc 34.3750 (28.5417) lr 1.9980e-03 eta 0:18:52
epoch [6/200] batch [50/50] time 0.084 (0.113) data 0.000 (0.029) loss 2.4688 (2.4543) acc 31.2500 (28.5000) lr 1.9969e-03 eta 0:18:19
epoch [7/200] batch [5/50] time 0.084 (0.289) data 0.000 (0.205) loss 2.3750 (2.4664) acc 25.0000 (28.7500) lr 1.9969e-03 eta 0:46:40
epoch [7/200] batch [10/50] time 0.199 (0.198) data 0.115 (0.114) loss 2.9297 (2.5402) acc 21.8750 (27.8125) lr 1.9969e-03 eta 0:31:57
epoch [7/200] batch [15/50] time 0.084 (0.163) data 0.000 (0.079) loss 2.4512 (2.4069) acc 21.8750 (30.2083) lr 1.9969e-03 eta 0:26:17
epoch [7/200] batch [20/50] time 0.084 (0.147) data 0.000 (0.063) loss 2.2148 (2.3585) acc 31.2500 (31.4062) lr 1.9969e-03 eta 0:23:39
epoch [7/200] batch [25/50] time 0.084 (0.136) data 0.000 (0.052) loss 2.5938 (2.3600) acc 28.1250 (30.8750) lr 1.9969e-03 eta 0:21:55
epoch [7/200] batch [30/50] time 0.084 (0.135) data 0.000 (0.051) loss 2.5430 (2.3915) acc 28.1250 (30.1042) lr 1.9969e-03 eta 0:21:43
epoch [7/200] batch [35/50] time 0.085 (0.128) data 0.000 (0.044) loss 2.4043 (2.3923) acc 28.1250 (30.4464) lr 1.9969e-03 eta 0:20:34
epoch [7/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.041) loss 2.1445 (2.3849) acc 37.5000 (30.7812) lr 1.9969e-03 eta 0:20:08
epoch [7/200] batch [45/50] time 0.139 (0.124) data 0.056 (0.040) loss 2.3164 (2.3854) acc 25.0000 (30.6250) lr 1.9969e-03 eta 0:19:52
epoch [7/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.036) loss 2.5254 (2.3869) acc 31.2500 (30.3125) lr 1.9956e-03 eta 0:19:13
epoch [8/200] batch [5/50] time 0.085 (0.272) data 0.000 (0.187) loss 1.8242 (2.1305) acc 50.0000 (39.3750) lr 1.9956e-03 eta 0:43:41
epoch [8/200] batch [10/50] time 0.084 (0.178) data 0.000 (0.094) loss 2.1230 (2.2367) acc 31.2500 (31.8750) lr 1.9956e-03 eta 0:28:36
epoch [8/200] batch [15/50] time 0.084 (0.147) data 0.000 (0.063) loss 2.4121 (2.2605) acc 31.2500 (32.5000) lr 1.9956e-03 eta 0:23:35
epoch [8/200] batch [20/50] time 0.084 (0.131) data 0.000 (0.047) loss 2.2910 (2.2980) acc 31.2500 (31.0938) lr 1.9956e-03 eta 0:21:04
epoch [8/200] batch [25/50] time 0.084 (0.123) data 0.000 (0.039) loss 2.4297 (2.3265) acc 28.1250 (31.0000) lr 1.9956e-03 eta 0:19:43
epoch [8/200] batch [30/50] time 0.250 (0.122) data 0.167 (0.038) loss 2.3086 (2.3262) acc 31.2500 (30.6250) lr 1.9956e-03 eta 0:19:34
epoch [8/200] batch [35/50] time 0.084 (0.117) data 0.001 (0.033) loss 2.5625 (2.3531) acc 28.1250 (30.0893) lr 1.9956e-03 eta 0:18:42
epoch [8/200] batch [40/50] time 0.082 (0.118) data 0.000 (0.034) loss 2.6094 (2.3471) acc 28.1250 (30.2344) lr 1.9956e-03 eta 0:18:52
epoch [8/200] batch [45/50] time 0.083 (0.114) data 0.000 (0.030) loss 2.1270 (2.3354) acc 34.3750 (30.2083) lr 1.9956e-03 eta 0:18:14
epoch [8/200] batch [50/50] time 0.082 (0.115) data 0.000 (0.032) loss 1.8525 (2.3354) acc 53.1250 (30.1875) lr 1.9940e-03 eta 0:18:26
epoch [9/200] batch [5/50] time 0.085 (0.285) data 0.001 (0.199) loss 2.3672 (2.3672) acc 34.3750 (30.6250) lr 1.9940e-03 eta 0:45:32
epoch [9/200] batch [10/50] time 0.084 (0.185) data 0.000 (0.101) loss 2.2148 (2.3283) acc 31.2500 (33.4375) lr 1.9940e-03 eta 0:29:38
epoch [9/200] batch [15/50] time 0.085 (0.152) data 0.000 (0.067) loss 2.3613 (2.3121) acc 25.0000 (33.7500) lr 1.9940e-03 eta 0:24:16
epoch [9/200] batch [20/50] time 0.086 (0.139) data 0.000 (0.054) loss 2.3184 (2.3102) acc 34.3750 (34.0625) lr 1.9940e-03 eta 0:22:13
epoch [9/200] batch [25/50] time 0.130 (0.133) data 0.046 (0.049) loss 2.7441 (2.3462) acc 25.0000 (32.5000) lr 1.9940e-03 eta 0:21:15
epoch [9/200] batch [30/50] time 0.214 (0.130) data 0.129 (0.045) loss 2.0645 (2.3680) acc 40.6250 (31.8750) lr 1.9940e-03 eta 0:20:39
epoch [9/200] batch [35/50] time 0.084 (0.123) data 0.001 (0.039) loss 2.4219 (2.3599) acc 18.7500 (31.0714) lr 1.9940e-03 eta 0:19:38
epoch [9/200] batch [40/50] time 0.083 (0.123) data 0.000 (0.038) loss 2.1484 (2.3410) acc 34.3750 (31.0156) lr 1.9940e-03 eta 0:19:34
epoch [9/200] batch [45/50] time 0.083 (0.118) data 0.000 (0.034) loss 2.3027 (2.3350) acc 28.1250 (30.5556) lr 1.9940e-03 eta 0:18:51
epoch [9/200] batch [50/50] time 0.082 (0.118) data 0.000 (0.034) loss 2.5020 (2.3461) acc 15.6250 (30.5625) lr 1.9921e-03 eta 0:18:51
epoch [10/200] batch [5/50] time 0.085 (0.270) data 0.001 (0.185) loss 2.1855 (2.3605) acc 37.5000 (31.2500) lr 1.9921e-03 eta 0:42:55
epoch [10/200] batch [10/50] time 0.085 (0.179) data 0.000 (0.094) loss 2.1973 (2.2085) acc 34.3750 (34.3750) lr 1.9921e-03 eta 0:28:26
epoch [10/200] batch [15/50] time 0.089 (0.148) data 0.000 (0.063) loss 2.4453 (2.2180) acc 34.3750 (33.9583) lr 1.9921e-03 eta 0:23:33
epoch [10/200] batch [20/50] time 0.085 (0.132) data 0.000 (0.047) loss 2.1152 (2.2771) acc 34.3750 (33.9062) lr 1.9921e-03 eta 0:21:02
epoch [10/200] batch [25/50] time 0.084 (0.128) data 0.000 (0.043) loss 1.9150 (2.2846) acc 31.2500 (33.3750) lr 1.9921e-03 eta 0:20:17
epoch [10/200] batch [30/50] time 0.085 (0.126) data 0.000 (0.041) loss 2.4805 (2.2786) acc 21.8750 (33.3333) lr 1.9921e-03 eta 0:19:59
epoch [10/200] batch [35/50] time 0.085 (0.120) data 0.001 (0.035) loss 2.1348 (2.2999) acc 28.1250 (32.4107) lr 1.9921e-03 eta 0:19:03
epoch [10/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.035) loss 2.0840 (2.2889) acc 34.3750 (32.5000) lr 1.9921e-03 eta 0:19:04
epoch [10/200] batch [45/50] time 0.275 (0.121) data 0.192 (0.036) loss 2.0234 (2.2915) acc 40.6250 (32.5000) lr 1.9921e-03 eta 0:19:05
epoch [10/200] batch [50/50] time 0.082 (0.117) data 0.000 (0.032) loss 2.5723 (2.3026) acc 34.3750 (32.0625) lr 1.9900e-03 eta 0:18:28
epoch [11/200] batch [5/50] time 0.084 (0.307) data 0.000 (0.222) loss 2.3223 (2.1916) acc 31.2500 (32.5000) lr 1.9900e-03 eta 0:48:34
epoch [11/200] batch [10/50] time 0.084 (0.205) data 0.000 (0.121) loss 2.5156 (2.2343) acc 28.1250 (32.5000) lr 1.9900e-03 eta 0:32:28
epoch [11/200] batch [15/50] time 0.084 (0.165) data 0.000 (0.081) loss 2.0352 (2.2635) acc 37.5000 (32.5000) lr 1.9900e-03 eta 0:26:04
epoch [11/200] batch [20/50] time 0.085 (0.153) data 0.000 (0.069) loss 2.6445 (2.2870) acc 15.6250 (30.7812) lr 1.9900e-03 eta 0:24:14
epoch [11/200] batch [25/50] time 0.106 (0.140) data 0.023 (0.056) loss 2.0605 (2.2974) acc 28.1250 (30.2500) lr 1.9900e-03 eta 0:22:11
epoch [11/200] batch [30/50] time 0.085 (0.131) data 0.000 (0.047) loss 2.6523 (2.2965) acc 28.1250 (31.2500) lr 1.9900e-03 eta 0:20:42
epoch [11/200] batch [35/50] time 0.086 (0.125) data 0.001 (0.040) loss 2.0547 (2.2843) acc 34.3750 (31.0714) lr 1.9900e-03 eta 0:19:40
epoch [11/200] batch [40/50] time 0.083 (0.120) data 0.000 (0.035) loss 2.2676 (2.2881) acc 34.3750 (31.1719) lr 1.9900e-03 eta 0:18:51
epoch [11/200] batch [45/50] time 0.083 (0.116) data 0.000 (0.032) loss 2.7559 (2.2963) acc 18.7500 (31.4583) lr 1.9900e-03 eta 0:18:12
epoch [11/200] batch [50/50] time 0.084 (0.112) data 0.000 (0.028) loss 2.2051 (2.2918) acc 34.3750 (31.4375) lr 1.9877e-03 eta 0:17:41
epoch [12/200] batch [5/50] time 0.084 (0.324) data 0.000 (0.240) loss 2.4863 (2.3809) acc 21.8750 (27.5000) lr 1.9877e-03 eta 0:51:01
epoch [12/200] batch [10/50] time 0.083 (0.204) data 0.000 (0.120) loss 1.9072 (2.2430) acc 53.1250 (30.9375) lr 1.9877e-03 eta 0:32:05
epoch [12/200] batch [15/50] time 0.085 (0.164) data 0.000 (0.080) loss 1.9062 (2.2530) acc 31.2500 (30.4167) lr 1.9877e-03 eta 0:25:45
epoch [12/200] batch [20/50] time 0.084 (0.145) data 0.000 (0.061) loss 2.3164 (2.2625) acc 31.2500 (30.9375) lr 1.9877e-03 eta 0:22:47
epoch [12/200] batch [25/50] time 0.085 (0.133) data 0.000 (0.049) loss 2.1152 (2.2527) acc 31.2500 (30.8750) lr 1.9877e-03 eta 0:20:52
epoch [12/200] batch [30/50] time 0.084 (0.125) data 0.000 (0.041) loss 2.4277 (2.2524) acc 31.2500 (31.6667) lr 1.9877e-03 eta 0:19:35
epoch [12/200] batch [35/50] time 0.084 (0.122) data 0.001 (0.038) loss 2.5801 (2.2693) acc 21.8750 (30.8929) lr 1.9877e-03 eta 0:19:05
epoch [12/200] batch [40/50] time 0.083 (0.117) data 0.000 (0.033) loss 2.4199 (2.2585) acc 25.0000 (31.0938) lr 1.9877e-03 eta 0:18:20
epoch [12/200] batch [45/50] time 0.085 (0.114) data 0.000 (0.030) loss 2.0723 (2.2667) acc 53.1250 (31.1806) lr 1.9877e-03 eta 0:17:52
epoch [12/200] batch [50/50] time 0.083 (0.111) data 0.000 (0.027) loss 2.1797 (2.2464) acc 46.8750 (31.8750) lr 1.9851e-03 eta 0:17:22
epoch [13/200] batch [5/50] time 0.085 (0.361) data 0.000 (0.276) loss 2.1348 (2.0900) acc 43.7500 (36.2500) lr 1.9851e-03 eta 0:56:31
epoch [13/200] batch [10/50] time 0.085 (0.238) data 0.000 (0.153) loss 2.0645 (2.1931) acc 37.5000 (32.1875) lr 1.9851e-03 eta 0:37:13
epoch [13/200] batch [15/50] time 0.085 (0.187) data 0.000 (0.102) loss 2.4668 (2.2006) acc 31.2500 (33.5417) lr 1.9851e-03 eta 0:29:14
epoch [13/200] batch [20/50] time 0.085 (0.171) data 0.000 (0.086) loss 2.7500 (2.2435) acc 25.0000 (32.6562) lr 1.9851e-03 eta 0:26:44
epoch [13/200] batch [25/50] time 0.227 (0.160) data 0.143 (0.075) loss 1.8809 (2.2528) acc 50.0000 (33.2500) lr 1.9851e-03 eta 0:24:55
epoch [13/200] batch [30/50] time 0.086 (0.147) data 0.000 (0.062) loss 1.9873 (2.2490) acc 34.3750 (32.3958) lr 1.9851e-03 eta 0:23:00
epoch [13/200] batch [35/50] time 0.085 (0.141) data 0.001 (0.056) loss 2.1465 (2.2549) acc 40.6250 (32.5893) lr 1.9851e-03 eta 0:22:03
epoch [13/200] batch [40/50] time 0.084 (0.134) data 0.000 (0.049) loss 2.3984 (2.2613) acc 31.2500 (32.5000) lr 1.9851e-03 eta 0:20:55
epoch [13/200] batch [45/50] time 0.083 (0.133) data 0.000 (0.048) loss 2.5684 (2.2693) acc 31.2500 (32.3611) lr 1.9851e-03 eta 0:20:41
epoch [13/200] batch [50/50] time 0.083 (0.128) data 0.000 (0.043) loss 2.2617 (2.2883) acc 25.0000 (32.0625) lr 1.9823e-03 eta 0:19:54
epoch [14/200] batch [5/50] time 0.086 (0.266) data 0.000 (0.181) loss 2.0957 (2.2285) acc 37.5000 (31.2500) lr 1.9823e-03 eta 0:41:28
epoch [14/200] batch [10/50] time 0.086 (0.176) data 0.000 (0.091) loss 2.3535 (2.2086) acc 31.2500 (33.4375) lr 1.9823e-03 eta 0:27:25
epoch [14/200] batch [15/50] time 0.085 (0.147) data 0.000 (0.061) loss 2.7070 (2.2225) acc 25.0000 (32.7083) lr 1.9823e-03 eta 0:22:50
epoch [14/200] batch [20/50] time 0.212 (0.138) data 0.127 (0.052) loss 1.8545 (2.2382) acc 43.7500 (32.9688) lr 1.9823e-03 eta 0:21:25
epoch [14/200] batch [25/50] time 0.087 (0.128) data 0.000 (0.042) loss 2.0664 (2.2749) acc 21.8750 (31.7500) lr 1.9823e-03 eta 0:19:50
epoch [14/200] batch [30/50] time 0.085 (0.130) data 0.000 (0.044) loss 2.1934 (2.2691) acc 40.6250 (32.8125) lr 1.9823e-03 eta 0:20:07
epoch [14/200] batch [35/50] time 0.084 (0.123) data 0.001 (0.038) loss 2.2734 (2.2629) acc 37.5000 (33.0357) lr 1.9823e-03 eta 0:19:07
epoch [14/200] batch [40/50] time 0.083 (0.123) data 0.000 (0.038) loss 2.4648 (2.2696) acc 25.0000 (32.6562) lr 1.9823e-03 eta 0:19:02
epoch [14/200] batch [45/50] time 0.084 (0.121) data 0.000 (0.036) loss 1.9814 (2.2593) acc 37.5000 (32.7778) lr 1.9823e-03 eta 0:18:46
epoch [14/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.033) loss 2.2598 (2.2785) acc 25.0000 (32.4375) lr 1.9792e-03 eta 0:18:10
epoch [15/200] batch [5/50] time 0.083 (0.324) data 0.000 (0.240) loss 1.9668 (2.1969) acc 37.5000 (34.3750) lr 1.9792e-03 eta 0:50:13
epoch [15/200] batch [10/50] time 0.084 (0.211) data 0.000 (0.127) loss 2.1836 (2.1671) acc 43.7500 (35.9375) lr 1.9792e-03 eta 0:32:38
epoch [15/200] batch [15/50] time 0.085 (0.169) data 0.000 (0.085) loss 2.5254 (2.2184) acc 21.8750 (32.5000) lr 1.9792e-03 eta 0:26:05
epoch [15/200] batch [20/50] time 0.084 (0.149) data 0.000 (0.065) loss 1.7217 (2.2577) acc 65.6250 (32.8125) lr 1.9792e-03 eta 0:22:59
epoch [15/200] batch [25/50] time 0.089 (0.136) data 0.002 (0.052) loss 2.0547 (2.2453) acc 37.5000 (32.6250) lr 1.9792e-03 eta 0:21:01
epoch [15/200] batch [30/50] time 0.086 (0.128) data 0.001 (0.043) loss 2.2598 (2.2467) acc 34.3750 (32.7083) lr 1.9792e-03 eta 0:19:42
epoch [15/200] batch [35/50] time 0.085 (0.121) data 0.001 (0.037) loss 2.3477 (2.2208) acc 34.3750 (33.3929) lr 1.9792e-03 eta 0:18:45
epoch [15/200] batch [40/50] time 0.083 (0.117) data 0.000 (0.033) loss 2.6582 (2.2492) acc 31.2500 (33.1250) lr 1.9792e-03 eta 0:18:03
epoch [15/200] batch [45/50] time 0.083 (0.113) data 0.000 (0.029) loss 2.1035 (2.2557) acc 46.8750 (33.3333) lr 1.9792e-03 eta 0:17:29
epoch [15/200] batch [50/50] time 0.084 (0.110) data 0.000 (0.026) loss 2.2500 (2.2639) acc 37.5000 (33.1875) lr 1.9759e-03 eta 0:17:01
epoch [16/200] batch [5/50] time 0.085 (0.280) data 0.000 (0.194) loss 2.0703 (1.9902) acc 43.7500 (40.0000) lr 1.9759e-03 eta 0:43:10
epoch [16/200] batch [10/50] time 0.086 (0.183) data 0.000 (0.097) loss 2.4453 (2.1717) acc 34.3750 (35.9375) lr 1.9759e-03 eta 0:28:12
epoch [16/200] batch [15/50] time 0.085 (0.150) data 0.000 (0.065) loss 2.6016 (2.2169) acc 18.7500 (34.3750) lr 1.9759e-03 eta 0:23:08
epoch [16/200] batch [20/50] time 0.084 (0.134) data 0.000 (0.049) loss 2.1992 (2.2046) acc 43.7500 (34.2188) lr 1.9759e-03 eta 0:20:36
epoch [16/200] batch [25/50] time 0.128 (0.126) data 0.043 (0.041) loss 2.3281 (2.2394) acc 28.1250 (33.1250) lr 1.9759e-03 eta 0:19:21
epoch [16/200] batch [30/50] time 0.089 (0.119) data 0.000 (0.034) loss 2.5391 (2.2152) acc 25.0000 (34.4792) lr 1.9759e-03 eta 0:18:20
epoch [16/200] batch [35/50] time 0.085 (0.114) data 0.000 (0.029) loss 2.5625 (2.2070) acc 31.2500 (34.3750) lr 1.9759e-03 eta 0:17:34
epoch [16/200] batch [40/50] time 0.085 (0.113) data 0.000 (0.028) loss 2.1152 (2.2034) acc 40.6250 (33.9844) lr 1.9759e-03 eta 0:17:23
epoch [16/200] batch [45/50] time 0.083 (0.110) data 0.000 (0.025) loss 2.2480 (2.2082) acc 40.6250 (33.6806) lr 1.9759e-03 eta 0:16:53
epoch [16/200] batch [50/50] time 0.083 (0.111) data 0.000 (0.027) loss 2.4102 (2.2180) acc 28.1250 (33.6250) lr 1.9724e-03 eta 0:17:04
epoch [17/200] batch [5/50] time 0.086 (0.303) data 0.000 (0.217) loss 2.1289 (2.1559) acc 28.1250 (35.6250) lr 1.9724e-03 eta 0:46:21
epoch [17/200] batch [10/50] time 0.264 (0.212) data 0.180 (0.127) loss 2.1094 (2.1596) acc 34.3750 (35.6250) lr 1.9724e-03 eta 0:32:25
epoch [17/200] batch [15/50] time 0.084 (0.170) data 0.000 (0.085) loss 2.3711 (2.2720) acc 43.7500 (33.7500) lr 1.9724e-03 eta 0:25:57
epoch [17/200] batch [20/50] time 0.085 (0.154) data 0.000 (0.069) loss 2.1309 (2.2532) acc 37.5000 (34.2188) lr 1.9724e-03 eta 0:23:35
epoch [17/200] batch [25/50] time 0.084 (0.140) data 0.000 (0.056) loss 2.2305 (2.2482) acc 31.2500 (33.6250) lr 1.9724e-03 eta 0:21:27
epoch [17/200] batch [30/50] time 0.084 (0.133) data 0.000 (0.049) loss 2.4512 (2.2566) acc 31.2500 (33.0208) lr 1.9724e-03 eta 0:20:21
epoch [17/200] batch [35/50] time 0.353 (0.134) data 0.270 (0.049) loss 2.4355 (2.2636) acc 25.0000 (33.1250) lr 1.9724e-03 eta 0:20:27
epoch [17/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.043) loss 2.2441 (2.2467) acc 40.6250 (33.5156) lr 1.9724e-03 eta 0:19:29
epoch [17/200] batch [45/50] time 0.084 (0.125) data 0.000 (0.041) loss 2.1816 (2.2520) acc 37.5000 (33.4028) lr 1.9724e-03 eta 0:19:07
epoch [17/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 2.8086 (2.2564) acc 25.0000 (33.3125) lr 1.9686e-03 eta 0:18:28
epoch [18/200] batch [5/50] time 0.085 (0.299) data 0.001 (0.214) loss 2.0859 (2.3734) acc 37.5000 (33.1250) lr 1.9686e-03 eta 0:45:31
epoch [18/200] batch [10/50] time 0.085 (0.198) data 0.000 (0.114) loss 2.5508 (2.3521) acc 28.1250 (33.4375) lr 1.9686e-03 eta 0:30:14
epoch [18/200] batch [15/50] time 0.084 (0.161) data 0.000 (0.076) loss 2.4688 (2.2929) acc 28.1250 (33.5417) lr 1.9686e-03 eta 0:24:27
epoch [18/200] batch [20/50] time 0.083 (0.146) data 0.000 (0.061) loss 2.0000 (2.2746) acc 37.5000 (32.8125) lr 1.9686e-03 eta 0:22:08
epoch [18/200] batch [25/50] time 0.085 (0.133) data 0.000 (0.049) loss 2.3047 (2.2593) acc 25.0000 (32.7500) lr 1.9686e-03 eta 0:20:15
epoch [18/200] batch [30/50] time 0.172 (0.128) data 0.088 (0.044) loss 2.6621 (2.2604) acc 15.6250 (33.3333) lr 1.9686e-03 eta 0:19:27
epoch [18/200] batch [35/50] time 0.085 (0.122) data 0.001 (0.038) loss 2.1953 (2.2386) acc 40.6250 (33.3929) lr 1.9686e-03 eta 0:18:29
epoch [18/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.039) loss 2.0527 (2.2242) acc 43.7500 (33.6719) lr 1.9686e-03 eta 0:18:39
epoch [18/200] batch [45/50] time 0.083 (0.118) data 0.000 (0.035) loss 2.3926 (2.2305) acc 25.0000 (33.4722) lr 1.9686e-03 eta 0:17:58
epoch [18/200] batch [50/50] time 0.084 (0.118) data 0.000 (0.035) loss 2.1152 (2.2335) acc 46.8750 (33.5625) lr 1.9646e-03 eta 0:17:57
epoch [19/200] batch [5/50] time 0.087 (0.286) data 0.000 (0.201) loss 2.4707 (2.2121) acc 28.1250 (36.2500) lr 1.9646e-03 eta 0:43:21
epoch [19/200] batch [10/50] time 0.085 (0.186) data 0.000 (0.101) loss 1.7031 (2.1241) acc 43.7500 (38.1250) lr 1.9646e-03 eta 0:28:06
epoch [19/200] batch [15/50] time 0.086 (0.156) data 0.001 (0.070) loss 2.3047 (2.2058) acc 28.1250 (33.9583) lr 1.9646e-03 eta 0:23:36
epoch [19/200] batch [20/50] time 0.086 (0.138) data 0.001 (0.053) loss 2.0605 (2.2147) acc 37.5000 (34.2188) lr 1.9646e-03 eta 0:20:57
epoch [19/200] batch [25/50] time 0.086 (0.128) data 0.000 (0.042) loss 2.1660 (2.2407) acc 40.6250 (32.7500) lr 1.9646e-03 eta 0:19:22
epoch [19/200] batch [30/50] time 0.087 (0.121) data 0.001 (0.035) loss 2.6094 (2.2866) acc 25.0000 (31.8750) lr 1.9646e-03 eta 0:18:18
epoch [19/200] batch [35/50] time 0.086 (0.116) data 0.001 (0.031) loss 2.1875 (2.2764) acc 53.1250 (32.8571) lr 1.9646e-03 eta 0:17:33
epoch [19/200] batch [40/50] time 0.086 (0.113) data 0.000 (0.027) loss 2.2832 (2.2553) acc 31.2500 (33.4375) lr 1.9646e-03 eta 0:17:02
epoch [19/200] batch [45/50] time 0.167 (0.112) data 0.084 (0.026) loss 2.0039 (2.2412) acc 34.3750 (33.8194) lr 1.9646e-03 eta 0:16:50
epoch [19/200] batch [50/50] time 0.083 (0.109) data 0.000 (0.023) loss 2.1211 (2.2296) acc 31.2500 (33.8750) lr 1.9603e-03 eta 0:16:24
epoch [20/200] batch [5/50] time 0.086 (0.281) data 0.000 (0.196) loss 2.6387 (2.3375) acc 18.7500 (31.8750) lr 1.9603e-03 eta 0:42:21
epoch [20/200] batch [10/50] time 0.086 (0.184) data 0.001 (0.099) loss 1.8486 (2.2437) acc 37.5000 (32.8125) lr 1.9603e-03 eta 0:27:46
epoch [20/200] batch [15/50] time 0.086 (0.154) data 0.000 (0.069) loss 2.3438 (2.2174) acc 34.3750 (34.3750) lr 1.9603e-03 eta 0:23:15
epoch [20/200] batch [20/50] time 0.193 (0.148) data 0.109 (0.063) loss 1.8564 (2.1775) acc 28.1250 (33.9062) lr 1.9603e-03 eta 0:22:16
epoch [20/200] batch [25/50] time 0.086 (0.136) data 0.001 (0.050) loss 1.9473 (2.1677) acc 25.0000 (33.3750) lr 1.9603e-03 eta 0:20:23
epoch [20/200] batch [30/50] time 0.088 (0.132) data 0.000 (0.047) loss 2.9102 (2.2053) acc 28.1250 (33.4375) lr 1.9603e-03 eta 0:19:49
epoch [20/200] batch [35/50] time 0.086 (0.125) data 0.001 (0.040) loss 2.2109 (2.1905) acc 28.1250 (33.8393) lr 1.9603e-03 eta 0:18:49
epoch [20/200] batch [40/50] time 0.084 (0.124) data 0.000 (0.039) loss 1.9971 (2.1828) acc 37.5000 (34.1406) lr 1.9603e-03 eta 0:18:40
epoch [20/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.038) loss 2.6133 (2.1860) acc 18.7500 (33.9583) lr 1.9603e-03 eta 0:18:25
epoch [20/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.034) loss 2.2930 (2.1770) acc 40.6250 (34.1875) lr 1.9558e-03 eta 0:17:48
epoch [21/200] batch [5/50] time 0.086 (0.268) data 0.000 (0.183) loss 2.1309 (2.0699) acc 31.2500 (36.2500) lr 1.9558e-03 eta 0:40:12
epoch [21/200] batch [10/50] time 0.085 (0.178) data 0.000 (0.093) loss 2.5781 (2.2502) acc 34.3750 (34.0625) lr 1.9558e-03 eta 0:26:38
epoch [21/200] batch [15/50] time 0.085 (0.149) data 0.000 (0.064) loss 2.8125 (2.2626) acc 18.7500 (32.5000) lr 1.9558e-03 eta 0:22:20
epoch [21/200] batch [20/50] time 0.249 (0.141) data 0.166 (0.057) loss 2.3379 (2.2657) acc 15.6250 (32.9688) lr 1.9558e-03 eta 0:21:08
epoch [21/200] batch [25/50] time 0.085 (0.130) data 0.001 (0.045) loss 2.1426 (2.2188) acc 31.2500 (34.3750) lr 1.9558e-03 eta 0:19:27
epoch [21/200] batch [30/50] time 0.084 (0.131) data 0.000 (0.046) loss 2.2188 (2.1971) acc 34.3750 (34.6875) lr 1.9558e-03 eta 0:19:30
epoch [21/200] batch [35/50] time 0.084 (0.124) data 0.000 (0.039) loss 2.4805 (2.1894) acc 25.0000 (34.9107) lr 1.9558e-03 eta 0:18:31
epoch [21/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.039) loss 2.2168 (2.1908) acc 43.7500 (34.6875) lr 1.9558e-03 eta 0:18:25
epoch [21/200] batch [45/50] time 0.083 (0.121) data 0.000 (0.037) loss 2.4199 (2.1927) acc 31.2500 (34.4444) lr 1.9558e-03 eta 0:18:03
epoch [21/200] batch [50/50] time 0.082 (0.117) data 0.000 (0.033) loss 1.7686 (2.1846) acc 46.8750 (34.5000) lr 1.9511e-03 eta 0:17:28
epoch [22/200] batch [5/50] time 0.086 (0.268) data 0.000 (0.182) loss 2.0352 (2.1334) acc 34.3750 (36.8750) lr 1.9511e-03 eta 0:39:54
epoch [22/200] batch [10/50] time 0.085 (0.177) data 0.000 (0.091) loss 2.0469 (2.1683) acc 34.3750 (34.6875) lr 1.9511e-03 eta 0:26:19
epoch [22/200] batch [15/50] time 0.085 (0.146) data 0.000 (0.061) loss 2.0742 (2.1669) acc 37.5000 (35.0000) lr 1.9511e-03 eta 0:21:47
epoch [22/200] batch [20/50] time 0.085 (0.131) data 0.000 (0.046) loss 2.3457 (2.1798) acc 18.7500 (34.5312) lr 1.9511e-03 eta 0:19:31
epoch [22/200] batch [25/50] time 0.085 (0.124) data 0.000 (0.038) loss 2.2188 (2.1755) acc 31.2500 (34.6250) lr 1.9511e-03 eta 0:18:24
epoch [22/200] batch [30/50] time 0.333 (0.126) data 0.249 (0.040) loss 1.7480 (2.1268) acc 53.1250 (36.4583) lr 1.9511e-03 eta 0:18:40
epoch [22/200] batch [35/50] time 0.084 (0.120) data 0.000 (0.035) loss 2.5508 (2.1424) acc 31.2500 (36.6071) lr 1.9511e-03 eta 0:17:47
epoch [22/200] batch [40/50] time 0.083 (0.122) data 0.000 (0.037) loss 2.6016 (2.1612) acc 34.3750 (36.1719) lr 1.9511e-03 eta 0:18:03
epoch [22/200] batch [45/50] time 0.083 (0.117) data 0.000 (0.033) loss 2.8516 (2.1558) acc 25.0000 (36.8750) lr 1.9511e-03 eta 0:17:24
epoch [22/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.031) loss 2.2734 (2.1663) acc 40.6250 (36.7500) lr 1.9461e-03 eta 0:17:09
epoch [23/200] batch [5/50] time 0.084 (0.273) data 0.000 (0.188) loss 1.7490 (2.0549) acc 43.7500 (40.0000) lr 1.9461e-03 eta 0:40:27
epoch [23/200] batch [10/50] time 0.086 (0.179) data 0.000 (0.094) loss 2.0645 (2.1028) acc 43.7500 (39.0625) lr 1.9461e-03 eta 0:26:31
epoch [23/200] batch [15/50] time 0.084 (0.148) data 0.000 (0.063) loss 2.0156 (2.1416) acc 34.3750 (36.8750) lr 1.9461e-03 eta 0:21:51
epoch [23/200] batch [20/50] time 0.085 (0.132) data 0.000 (0.047) loss 2.2910 (2.1411) acc 28.1250 (35.6250) lr 1.9461e-03 eta 0:19:30
epoch [23/200] batch [25/50] time 0.087 (0.122) data 0.000 (0.038) loss 2.0352 (2.1667) acc 31.2500 (34.6250) lr 1.9461e-03 eta 0:18:06
epoch [23/200] batch [30/50] time 0.085 (0.117) data 0.000 (0.033) loss 2.2285 (2.1737) acc 43.7500 (34.3750) lr 1.9461e-03 eta 0:17:20
epoch [23/200] batch [35/50] time 0.084 (0.113) data 0.001 (0.028) loss 2.2969 (2.1841) acc 21.8750 (33.9286) lr 1.9461e-03 eta 0:16:38
epoch [23/200] batch [40/50] time 0.084 (0.110) data 0.000 (0.026) loss 2.1953 (2.1821) acc 21.8750 (33.2812) lr 1.9461e-03 eta 0:16:18
epoch [23/200] batch [45/50] time 0.175 (0.109) data 0.093 (0.025) loss 2.2344 (2.1809) acc 40.6250 (33.7500) lr 1.9461e-03 eta 0:16:09
epoch [23/200] batch [50/50] time 0.082 (0.107) data 0.000 (0.023) loss 2.4648 (2.1842) acc 53.1250 (34.1250) lr 1.9409e-03 eta 0:15:44
epoch [24/200] batch [5/50] time 0.085 (0.308) data 0.000 (0.223) loss 2.1699 (2.0303) acc 34.3750 (40.6250) lr 1.9409e-03 eta 0:45:26
epoch [24/200] batch [10/50] time 0.085 (0.206) data 0.000 (0.121) loss 2.1504 (2.0823) acc 40.6250 (39.6875) lr 1.9409e-03 eta 0:30:24
epoch [24/200] batch [15/50] time 0.085 (0.166) data 0.000 (0.081) loss 2.2676 (2.1597) acc 31.2500 (37.2917) lr 1.9409e-03 eta 0:24:26
epoch [24/200] batch [20/50] time 0.086 (0.154) data 0.000 (0.069) loss 2.3965 (2.1714) acc 31.2500 (35.9375) lr 1.9409e-03 eta 0:22:36
epoch [24/200] batch [25/50] time 0.086 (0.140) data 0.001 (0.055) loss 2.2891 (2.1671) acc 25.0000 (35.1250) lr 1.9409e-03 eta 0:20:35
epoch [24/200] batch [30/50] time 0.083 (0.132) data 0.000 (0.048) loss 2.3926 (2.1566) acc 25.0000 (35.5208) lr 1.9409e-03 eta 0:19:27
epoch [24/200] batch [35/50] time 0.083 (0.129) data 0.000 (0.045) loss 2.0762 (2.1524) acc 37.5000 (35.8036) lr 1.9409e-03 eta 0:18:58
epoch [24/200] batch [40/50] time 0.084 (0.124) data 0.000 (0.039) loss 1.9541 (2.1417) acc 34.3750 (35.7031) lr 1.9409e-03 eta 0:18:08
epoch [24/200] batch [45/50] time 0.149 (0.121) data 0.066 (0.036) loss 2.4746 (2.1498) acc 18.7500 (35.5556) lr 1.9409e-03 eta 0:17:42
epoch [24/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.033) loss 2.6133 (2.1707) acc 28.1250 (35.1875) lr 1.9354e-03 eta 0:17:09
epoch [25/200] batch [5/50] time 0.085 (0.309) data 0.000 (0.225) loss 2.1016 (2.1379) acc 56.2500 (39.3750) lr 1.9354e-03 eta 0:45:18
epoch [25/200] batch [10/50] time 0.084 (0.197) data 0.000 (0.112) loss 2.4883 (2.1265) acc 34.3750 (40.6250) lr 1.9354e-03 eta 0:28:48
epoch [25/200] batch [15/50] time 0.085 (0.159) data 0.000 (0.075) loss 2.1602 (2.1201) acc 37.5000 (39.5833) lr 1.9354e-03 eta 0:23:19
epoch [25/200] batch [20/50] time 0.086 (0.143) data 0.000 (0.059) loss 2.1133 (2.1245) acc 34.3750 (38.2812) lr 1.9354e-03 eta 0:20:56
epoch [25/200] batch [25/50] time 0.085 (0.131) data 0.000 (0.047) loss 2.2227 (2.1139) acc 40.6250 (39.0000) lr 1.9354e-03 eta 0:19:12
epoch [25/200] batch [30/50] time 0.083 (0.128) data 0.000 (0.044) loss 2.0273 (2.1304) acc 34.3750 (37.8125) lr 1.9354e-03 eta 0:18:42
epoch [25/200] batch [35/50] time 0.227 (0.126) data 0.138 (0.042) loss 2.5293 (2.1458) acc 28.1250 (37.2321) lr 1.9354e-03 eta 0:18:22
epoch [25/200] batch [40/50] time 0.084 (0.121) data 0.000 (0.037) loss 1.9492 (2.1369) acc 34.3750 (37.0312) lr 1.9354e-03 eta 0:17:37
epoch [25/200] batch [45/50] time 0.083 (0.118) data 0.000 (0.034) loss 1.8428 (2.1532) acc 43.7500 (36.3194) lr 1.9354e-03 eta 0:17:16
epoch [25/200] batch [50/50] time 0.084 (0.115) data 0.000 (0.031) loss 2.4824 (2.1566) acc 31.2500 (36.2500) lr 1.9298e-03 eta 0:16:45
epoch [26/200] batch [5/50] time 0.085 (0.284) data 0.001 (0.199) loss 1.8193 (2.1309) acc 43.7500 (35.6250) lr 1.9298e-03 eta 0:41:26
epoch [26/200] batch [10/50] time 0.099 (0.186) data 0.014 (0.101) loss 1.9658 (2.0968) acc 37.5000 (35.9375) lr 1.9298e-03 eta 0:27:09
epoch [26/200] batch [15/50] time 0.086 (0.162) data 0.000 (0.077) loss 2.2500 (2.1040) acc 31.2500 (35.6250) lr 1.9298e-03 eta 0:23:38
epoch [26/200] batch [20/50] time 0.084 (0.146) data 0.000 (0.060) loss 2.4062 (2.1412) acc 40.6250 (36.0938) lr 1.9298e-03 eta 0:21:11
epoch [26/200] batch [25/50] time 0.085 (0.134) data 0.000 (0.048) loss 2.2695 (2.1443) acc 40.6250 (37.0000) lr 1.9298e-03 eta 0:19:25
epoch [26/200] batch [30/50] time 0.088 (0.133) data 0.001 (0.048) loss 2.1699 (2.1263) acc 40.6250 (37.7083) lr 1.9298e-03 eta 0:19:18
epoch [26/200] batch [35/50] time 0.085 (0.126) data 0.000 (0.041) loss 2.5137 (2.1404) acc 28.1250 (37.2321) lr 1.9298e-03 eta 0:18:19
epoch [26/200] batch [40/50] time 0.085 (0.121) data 0.000 (0.036) loss 2.3066 (2.1632) acc 40.6250 (36.7188) lr 1.9298e-03 eta 0:17:34
epoch [26/200] batch [45/50] time 0.084 (0.117) data 0.000 (0.032) loss 2.2715 (2.1598) acc 37.5000 (37.1528) lr 1.9298e-03 eta 0:16:57
epoch [26/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.029) loss 1.9844 (2.1615) acc 50.0000 (37.0625) lr 1.9239e-03 eta 0:16:28
epoch [27/200] batch [5/50] time 0.086 (0.323) data 0.000 (0.238) loss 1.8652 (2.2516) acc 43.7500 (35.6250) lr 1.9239e-03 eta 0:46:49
epoch [27/200] batch [10/50] time 0.087 (0.204) data 0.000 (0.119) loss 2.1152 (2.1988) acc 34.3750 (35.3125) lr 1.9239e-03 eta 0:29:35
epoch [27/200] batch [15/50] time 0.095 (0.165) data 0.010 (0.080) loss 1.9648 (2.1556) acc 46.8750 (37.0833) lr 1.9239e-03 eta 0:23:56
epoch [27/200] batch [20/50] time 0.084 (0.145) data 0.000 (0.060) loss 2.1270 (2.1413) acc 31.2500 (36.4062) lr 1.9239e-03 eta 0:20:59
epoch [27/200] batch [25/50] time 0.084 (0.140) data 0.000 (0.056) loss 2.3516 (2.1715) acc 40.6250 (36.0000) lr 1.9239e-03 eta 0:20:18
epoch [27/200] batch [30/50] time 0.183 (0.134) data 0.100 (0.050) loss 2.3242 (2.1608) acc 31.2500 (36.5625) lr 1.9239e-03 eta 0:19:25
epoch [27/200] batch [35/50] time 0.084 (0.129) data 0.000 (0.045) loss 2.2715 (2.1613) acc 31.2500 (36.1607) lr 1.9239e-03 eta 0:18:38
epoch [27/200] batch [40/50] time 0.084 (0.130) data 0.000 (0.046) loss 2.2773 (2.1575) acc 37.5000 (36.2500) lr 1.9239e-03 eta 0:18:45
epoch [27/200] batch [45/50] time 0.084 (0.125) data 0.000 (0.041) loss 2.2891 (2.1663) acc 37.5000 (35.4861) lr 1.9239e-03 eta 0:18:00
epoch [27/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.036) loss 2.9004 (2.1784) acc 28.1250 (35.1875) lr 1.9178e-03 eta 0:17:23
epoch [28/200] batch [5/50] time 0.084 (0.315) data 0.000 (0.230) loss 1.6045 (2.0557) acc 40.6250 (36.8750) lr 1.9178e-03 eta 0:45:23
epoch [28/200] batch [10/50] time 0.087 (0.207) data 0.000 (0.122) loss 2.3613 (2.1424) acc 43.7500 (38.7500) lr 1.9178e-03 eta 0:29:44
epoch [28/200] batch [15/50] time 0.086 (0.166) data 0.000 (0.081) loss 2.0879 (2.0806) acc 31.2500 (39.5833) lr 1.9178e-03 eta 0:23:54
epoch [28/200] batch [20/50] time 0.085 (0.156) data 0.000 (0.071) loss 1.9736 (2.0507) acc 34.3750 (39.3750) lr 1.9178e-03 eta 0:22:29
epoch [28/200] batch [25/50] time 0.225 (0.148) data 0.141 (0.063) loss 2.0215 (2.0888) acc 43.7500 (38.0000) lr 1.9178e-03 eta 0:21:13
epoch [28/200] batch [30/50] time 0.085 (0.137) data 0.000 (0.052) loss 1.9873 (2.0829) acc 43.7500 (37.8125) lr 1.9178e-03 eta 0:19:42
epoch [28/200] batch [35/50] time 0.085 (0.134) data 0.001 (0.049) loss 2.2793 (2.0945) acc 34.3750 (37.6786) lr 1.9178e-03 eta 0:19:15
epoch [28/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.043) loss 2.6074 (2.0986) acc 28.1250 (37.7344) lr 1.9178e-03 eta 0:18:20
epoch [28/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.042) loss 1.9512 (2.0922) acc 40.6250 (37.5000) lr 1.9178e-03 eta 0:18:03
epoch [28/200] batch [50/50] time 0.083 (0.126) data 0.000 (0.042) loss 2.2012 (2.0950) acc 34.3750 (37.3750) lr 1.9114e-03 eta 0:18:01
epoch [29/200] batch [5/50] time 0.086 (0.275) data 0.000 (0.188) loss 2.1562 (2.0812) acc 34.3750 (31.8750) lr 1.9114e-03 eta 0:39:21
epoch [29/200] batch [10/50] time 0.117 (0.183) data 0.033 (0.098) loss 2.1953 (2.1575) acc 40.6250 (33.4375) lr 1.9114e-03 eta 0:26:13
epoch [29/200] batch [15/50] time 0.086 (0.151) data 0.000 (0.065) loss 2.5195 (2.1167) acc 28.1250 (35.4167) lr 1.9114e-03 eta 0:21:32
epoch [29/200] batch [20/50] time 0.087 (0.145) data 0.000 (0.060) loss 1.9883 (2.1161) acc 43.7500 (36.5625) lr 1.9114e-03 eta 0:20:47
epoch [29/200] batch [25/50] time 0.086 (0.133) data 0.000 (0.048) loss 2.2637 (2.1170) acc 37.5000 (36.3750) lr 1.9114e-03 eta 0:19:04
epoch [29/200] batch [30/50] time 0.086 (0.131) data 0.000 (0.046) loss 1.9951 (2.1496) acc 40.6250 (36.0417) lr 1.9114e-03 eta 0:18:42
epoch [29/200] batch [35/50] time 0.085 (0.131) data 0.000 (0.045) loss 2.0352 (2.1625) acc 31.2500 (35.0000) lr 1.9114e-03 eta 0:18:38
epoch [29/200] batch [40/50] time 0.084 (0.125) data 0.000 (0.040) loss 1.9941 (2.1697) acc 40.6250 (35.1562) lr 1.9114e-03 eta 0:17:48
epoch [29/200] batch [45/50] time 0.084 (0.120) data 0.000 (0.035) loss 1.8682 (2.1657) acc 43.7500 (35.4861) lr 1.9114e-03 eta 0:17:08
epoch [29/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.032) loss 1.9814 (2.1596) acc 43.7500 (35.1250) lr 1.9048e-03 eta 0:16:36
epoch [30/200] batch [5/50] time 0.085 (0.303) data 0.000 (0.217) loss 2.0352 (2.1119) acc 46.8750 (44.3750) lr 1.9048e-03 eta 0:43:07
epoch [30/200] batch [10/50] time 0.085 (0.209) data 0.000 (0.124) loss 2.0039 (2.1651) acc 37.5000 (41.2500) lr 1.9048e-03 eta 0:29:43
epoch [30/200] batch [15/50] time 0.086 (0.168) data 0.000 (0.083) loss 2.2383 (2.1770) acc 46.8750 (41.0417) lr 1.9048e-03 eta 0:23:51
epoch [30/200] batch [20/50] time 0.085 (0.154) data 0.000 (0.069) loss 2.0527 (2.1647) acc 28.1250 (39.5312) lr 1.9048e-03 eta 0:21:52
epoch [30/200] batch [25/50] time 0.274 (0.148) data 0.190 (0.063) loss 2.3223 (2.1774) acc 31.2500 (38.7500) lr 1.9048e-03 eta 0:20:58
epoch [30/200] batch [30/50] time 0.085 (0.137) data 0.000 (0.052) loss 2.1426 (2.1975) acc 28.1250 (36.5625) lr 1.9048e-03 eta 0:19:29
epoch [30/200] batch [35/50] time 0.085 (0.134) data 0.000 (0.049) loss 1.9824 (2.1915) acc 43.7500 (36.4286) lr 1.9048e-03 eta 0:18:58
epoch [30/200] batch [40/50] time 0.084 (0.128) data 0.000 (0.043) loss 1.6836 (2.1633) acc 40.6250 (36.7188) lr 1.9048e-03 eta 0:18:05
epoch [30/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.042) loss 1.9629 (2.1442) acc 50.0000 (37.2917) lr 1.9048e-03 eta 0:17:53
epoch [30/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.038) loss 2.1367 (2.1343) acc 31.2500 (37.4375) lr 1.8980e-03 eta 0:17:15
epoch [31/200] batch [5/50] time 0.086 (0.328) data 0.000 (0.242) loss 2.3613 (2.0938) acc 25.0000 (30.6250) lr 1.8980e-03 eta 0:46:30
epoch [31/200] batch [10/50] time 0.086 (0.207) data 0.001 (0.121) loss 2.4805 (2.1502) acc 31.2500 (32.8125) lr 1.8980e-03 eta 0:29:20
epoch [31/200] batch [15/50] time 0.085 (0.167) data 0.000 (0.081) loss 1.8545 (2.1563) acc 50.0000 (32.7083) lr 1.8980e-03 eta 0:23:34
epoch [31/200] batch [20/50] time 0.086 (0.150) data 0.000 (0.065) loss 2.2559 (2.1431) acc 37.5000 (34.2188) lr 1.8980e-03 eta 0:21:13
epoch [31/200] batch [25/50] time 0.086 (0.137) data 0.000 (0.052) loss 1.7490 (2.1195) acc 43.7500 (35.1250) lr 1.8980e-03 eta 0:19:24
epoch [31/200] batch [30/50] time 0.087 (0.129) data 0.000 (0.043) loss 2.1699 (2.1209) acc 34.3750 (34.8958) lr 1.8980e-03 eta 0:18:11
epoch [31/200] batch [35/50] time 0.086 (0.123) data 0.001 (0.037) loss 2.1406 (2.1088) acc 40.6250 (35.9821) lr 1.8980e-03 eta 0:17:19
epoch [31/200] batch [40/50] time 0.085 (0.118) data 0.000 (0.032) loss 2.1074 (2.1181) acc 31.2500 (35.4688) lr 1.8980e-03 eta 0:16:38
epoch [31/200] batch [45/50] time 0.083 (0.114) data 0.000 (0.029) loss 1.8584 (2.1072) acc 53.1250 (35.7639) lr 1.8980e-03 eta 0:16:06
epoch [31/200] batch [50/50] time 0.084 (0.111) data 0.000 (0.026) loss 2.6758 (2.1249) acc 18.7500 (35.5000) lr 1.8910e-03 eta 0:15:40
epoch [32/200] batch [5/50] time 0.085 (0.305) data 0.000 (0.219) loss 1.9209 (2.1035) acc 43.7500 (38.7500) lr 1.8910e-03 eta 0:42:52
epoch [32/200] batch [10/50] time 0.133 (0.200) data 0.048 (0.115) loss 2.4492 (2.1303) acc 31.2500 (36.8750) lr 1.8910e-03 eta 0:28:12
epoch [32/200] batch [15/50] time 0.083 (0.162) data 0.000 (0.077) loss 1.8330 (2.0610) acc 43.7500 (40.0000) lr 1.8910e-03 eta 0:22:45
epoch [32/200] batch [20/50] time 0.084 (0.150) data 0.000 (0.065) loss 2.6875 (2.1316) acc 15.6250 (37.6562) lr 1.8910e-03 eta 0:21:08
epoch [32/200] batch [25/50] time 0.084 (0.137) data 0.000 (0.052) loss 2.3613 (2.1369) acc 37.5000 (38.2500) lr 1.8910e-03 eta 0:19:15
epoch [32/200] batch [30/50] time 0.085 (0.137) data 0.000 (0.052) loss 2.0098 (2.1358) acc 43.7500 (38.2292) lr 1.8910e-03 eta 0:19:12
epoch [32/200] batch [35/50] time 0.084 (0.132) data 0.000 (0.047) loss 2.1289 (2.1354) acc 31.2500 (37.7679) lr 1.8910e-03 eta 0:18:28
epoch [32/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.041) loss 1.8301 (2.1331) acc 34.3750 (37.4219) lr 1.8910e-03 eta 0:17:37
epoch [32/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.9141 (2.1352) acc 43.7500 (37.4306) lr 1.8910e-03 eta 0:17:29
epoch [32/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 2.0703 (2.1425) acc 28.1250 (37.0625) lr 1.8838e-03 eta 0:16:53
epoch [33/200] batch [5/50] time 0.085 (0.369) data 0.001 (0.284) loss 2.4141 (2.2725) acc 34.3750 (37.5000) lr 1.8838e-03 eta 0:51:34
epoch [33/200] batch [10/50] time 0.086 (0.227) data 0.000 (0.142) loss 1.8516 (2.2630) acc 46.8750 (35.6250) lr 1.8838e-03 eta 0:31:44
epoch [33/200] batch [15/50] time 0.086 (0.183) data 0.000 (0.098) loss 1.6143 (2.1108) acc 53.1250 (39.1667) lr 1.8838e-03 eta 0:25:35
epoch [33/200] batch [20/50] time 0.086 (0.166) data 0.000 (0.081) loss 2.2051 (2.1719) acc 31.2500 (37.8125) lr 1.8838e-03 eta 0:23:10
epoch [33/200] batch [25/50] time 0.086 (0.150) data 0.000 (0.065) loss 1.9053 (2.1805) acc 34.3750 (36.8750) lr 1.8838e-03 eta 0:20:54
epoch [33/200] batch [30/50] time 0.085 (0.145) data 0.000 (0.060) loss 1.8818 (2.1417) acc 40.6250 (37.0833) lr 1.8838e-03 eta 0:20:09
epoch [33/200] batch [35/50] time 0.244 (0.141) data 0.160 (0.056) loss 1.9863 (2.1317) acc 34.3750 (36.8750) lr 1.8838e-03 eta 0:19:35
epoch [33/200] batch [40/50] time 0.083 (0.133) data 0.000 (0.049) loss 1.8125 (2.1128) acc 43.7500 (37.2656) lr 1.8838e-03 eta 0:18:35
epoch [33/200] batch [45/50] time 0.083 (0.131) data 0.000 (0.047) loss 2.2129 (2.1364) acc 34.3750 (36.6667) lr 1.8838e-03 eta 0:18:16
epoch [33/200] batch [50/50] time 0.082 (0.126) data 0.000 (0.042) loss 2.4395 (2.1445) acc 34.3750 (36.5000) lr 1.8763e-03 eta 0:17:35
epoch [34/200] batch [5/50] time 0.085 (0.310) data 0.000 (0.225) loss 1.8887 (2.1617) acc 46.8750 (39.3750) lr 1.8763e-03 eta 0:43:07
epoch [34/200] batch [10/50] time 0.085 (0.210) data 0.000 (0.125) loss 1.8857 (2.0510) acc 53.1250 (42.1875) lr 1.8763e-03 eta 0:29:11
epoch [34/200] batch [15/50] time 0.085 (0.169) data 0.001 (0.084) loss 1.4990 (2.0309) acc 53.1250 (42.9167) lr 1.8763e-03 eta 0:23:24
epoch [34/200] batch [20/50] time 0.085 (0.159) data 0.000 (0.074) loss 1.9150 (2.0147) acc 50.0000 (42.6562) lr 1.8763e-03 eta 0:22:02
epoch [34/200] batch [25/50] time 0.084 (0.144) data 0.000 (0.059) loss 2.4160 (2.0547) acc 25.0000 (41.6250) lr 1.8763e-03 eta 0:19:56
epoch [34/200] batch [30/50] time 0.084 (0.134) data 0.000 (0.050) loss 2.6055 (2.0593) acc 28.1250 (41.3542) lr 1.8763e-03 eta 0:18:38
epoch [34/200] batch [35/50] time 0.087 (0.127) data 0.000 (0.043) loss 2.3730 (2.0859) acc 31.2500 (39.9107) lr 1.8763e-03 eta 0:17:38
epoch [34/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.041) loss 2.1699 (2.0887) acc 31.2500 (38.9062) lr 1.8763e-03 eta 0:17:17
epoch [34/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.1387 (2.1081) acc 37.5000 (38.7500) lr 1.8763e-03 eta 0:17:05
epoch [34/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.1504 (2.1131) acc 21.8750 (38.1875) lr 1.8686e-03 eta 0:16:31
epoch [35/200] batch [5/50] time 0.084 (0.310) data 0.000 (0.226) loss 2.3223 (2.1121) acc 43.7500 (40.6250) lr 1.8686e-03 eta 0:42:54
epoch [35/200] batch [10/50] time 0.084 (0.200) data 0.000 (0.116) loss 2.1543 (2.1486) acc 53.1250 (40.0000) lr 1.8686e-03 eta 0:27:40
epoch [35/200] batch [15/50] time 0.088 (0.162) data 0.000 (0.077) loss 1.8799 (2.0980) acc 40.6250 (39.1667) lr 1.8686e-03 eta 0:22:21
epoch [35/200] batch [20/50] time 0.131 (0.145) data 0.047 (0.060) loss 2.1113 (2.0565) acc 40.6250 (40.0000) lr 1.8686e-03 eta 0:19:59
epoch [35/200] batch [25/50] time 0.084 (0.133) data 0.000 (0.048) loss 2.1328 (2.0597) acc 31.2500 (39.5000) lr 1.8686e-03 eta 0:18:20
epoch [35/200] batch [30/50] time 0.084 (0.129) data 0.000 (0.045) loss 2.3457 (2.0935) acc 28.1250 (37.9167) lr 1.8686e-03 eta 0:17:49
epoch [35/200] batch [35/50] time 0.084 (0.123) data 0.000 (0.038) loss 2.2148 (2.0886) acc 34.3750 (38.1250) lr 1.8686e-03 eta 0:16:55
epoch [35/200] batch [40/50] time 0.084 (0.122) data 0.000 (0.037) loss 2.1992 (2.1020) acc 21.8750 (37.6562) lr 1.8686e-03 eta 0:16:45
epoch [35/200] batch [45/50] time 0.084 (0.120) data 0.000 (0.036) loss 2.0684 (2.0951) acc 34.3750 (37.7778) lr 1.8686e-03 eta 0:16:33
epoch [35/200] batch [50/50] time 0.082 (0.117) data 0.000 (0.033) loss 2.1543 (2.1166) acc 34.3750 (37.2500) lr 1.8607e-03 eta 0:16:02
epoch [36/200] batch [5/50] time 0.086 (0.285) data 0.000 (0.200) loss 2.3359 (2.0768) acc 21.8750 (36.8750) lr 1.8607e-03 eta 0:39:12
epoch [36/200] batch [10/50] time 0.222 (0.199) data 0.140 (0.114) loss 2.4785 (2.0732) acc 31.2500 (39.3750) lr 1.8607e-03 eta 0:27:23
epoch [36/200] batch [15/50] time 0.086 (0.161) data 0.000 (0.076) loss 1.6562 (2.0832) acc 53.1250 (39.7917) lr 1.8607e-03 eta 0:22:07
epoch [36/200] batch [20/50] time 0.084 (0.144) data 0.000 (0.059) loss 1.9170 (2.0847) acc 37.5000 (38.5938) lr 1.8607e-03 eta 0:19:48
epoch [36/200] batch [25/50] time 0.085 (0.137) data 0.000 (0.052) loss 1.9639 (2.0908) acc 46.8750 (39.0000) lr 1.8607e-03 eta 0:18:46
epoch [36/200] batch [30/50] time 0.086 (0.128) data 0.000 (0.043) loss 2.4863 (2.1241) acc 31.2500 (38.7500) lr 1.8607e-03 eta 0:17:35
epoch [36/200] batch [35/50] time 0.084 (0.128) data 0.000 (0.043) loss 1.8838 (2.1100) acc 37.5000 (38.6607) lr 1.8607e-03 eta 0:17:29
epoch [36/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.042) loss 2.2148 (2.0967) acc 40.6250 (39.1406) lr 1.8607e-03 eta 0:17:20
epoch [36/200] batch [45/50] time 0.083 (0.122) data 0.000 (0.037) loss 2.0488 (2.0921) acc 40.6250 (38.8889) lr 1.8607e-03 eta 0:16:40
epoch [36/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 1.9150 (2.0951) acc 46.8750 (38.8750) lr 1.8526e-03 eta 0:16:16
epoch [37/200] batch [5/50] time 0.087 (0.286) data 0.001 (0.200) loss 1.7568 (1.9832) acc 46.8750 (37.5000) lr 1.8526e-03 eta 0:39:01
epoch [37/200] batch [10/50] time 0.083 (0.195) data 0.000 (0.110) loss 2.0469 (2.0350) acc 31.2500 (38.7500) lr 1.8526e-03 eta 0:26:35
epoch [37/200] batch [15/50] time 0.084 (0.158) data 0.000 (0.074) loss 2.1602 (1.9827) acc 21.8750 (38.3333) lr 1.8526e-03 eta 0:21:31
epoch [37/200] batch [20/50] time 0.147 (0.146) data 0.063 (0.062) loss 2.0488 (2.0081) acc 28.1250 (37.9688) lr 1.8526e-03 eta 0:19:51
epoch [37/200] batch [25/50] time 0.083 (0.133) data 0.000 (0.050) loss 2.0410 (2.0252) acc 28.1250 (37.7500) lr 1.8526e-03 eta 0:18:08
epoch [37/200] batch [30/50] time 0.084 (0.133) data 0.000 (0.049) loss 2.4043 (2.0468) acc 21.8750 (37.2917) lr 1.8526e-03 eta 0:18:03
epoch [37/200] batch [35/50] time 0.087 (0.126) data 0.000 (0.042) loss 1.9248 (2.0662) acc 43.7500 (36.7857) lr 1.8526e-03 eta 0:17:06
epoch [37/200] batch [40/50] time 0.083 (0.120) data 0.000 (0.037) loss 2.2930 (2.0665) acc 28.1250 (36.8750) lr 1.8526e-03 eta 0:16:22
epoch [37/200] batch [45/50] time 0.083 (0.118) data 0.000 (0.035) loss 2.1699 (2.0594) acc 31.2500 (37.5000) lr 1.8526e-03 eta 0:16:05
epoch [37/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 2.2070 (2.0820) acc 25.0000 (36.3750) lr 1.8443e-03 eta 0:15:35
epoch [38/200] batch [5/50] time 0.084 (0.309) data 0.000 (0.225) loss 1.9932 (2.1818) acc 37.5000 (32.5000) lr 1.8443e-03 eta 0:41:58
epoch [38/200] batch [10/50] time 0.085 (0.197) data 0.000 (0.113) loss 1.9873 (2.0753) acc 37.5000 (36.2500) lr 1.8443e-03 eta 0:26:42
epoch [38/200] batch [15/50] time 0.084 (0.162) data 0.000 (0.078) loss 2.2617 (2.0329) acc 43.7500 (39.3750) lr 1.8443e-03 eta 0:22:00
epoch [38/200] batch [20/50] time 0.084 (0.154) data 0.000 (0.070) loss 2.0137 (2.0716) acc 40.6250 (38.5938) lr 1.8443e-03 eta 0:20:49
epoch [38/200] batch [25/50] time 0.084 (0.140) data 0.000 (0.056) loss 2.3789 (2.0540) acc 28.1250 (38.5000) lr 1.8443e-03 eta 0:18:56
epoch [38/200] batch [30/50] time 0.084 (0.135) data 0.000 (0.051) loss 2.3105 (2.0659) acc 31.2500 (37.9167) lr 1.8443e-03 eta 0:18:16
epoch [38/200] batch [35/50] time 0.252 (0.133) data 0.169 (0.049) loss 1.7480 (2.0827) acc 40.6250 (37.5000) lr 1.8443e-03 eta 0:17:55
epoch [38/200] batch [40/50] time 0.084 (0.127) data 0.000 (0.043) loss 1.8779 (2.1026) acc 40.6250 (37.1875) lr 1.8443e-03 eta 0:17:06
epoch [38/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.0625 (2.0886) acc 43.7500 (37.2917) lr 1.8443e-03 eta 0:16:43
epoch [38/200] batch [50/50] time 0.084 (0.120) data 0.000 (0.036) loss 2.3848 (2.1145) acc 34.3750 (36.4375) lr 1.8358e-03 eta 0:16:10
epoch [39/200] batch [5/50] time 0.084 (0.353) data 0.000 (0.269) loss 2.1543 (2.0992) acc 46.8750 (42.5000) lr 1.8358e-03 eta 0:47:39
epoch [39/200] batch [10/50] time 0.085 (0.238) data 0.000 (0.154) loss 2.7285 (2.1475) acc 18.7500 (39.0625) lr 1.8358e-03 eta 0:32:09
epoch [39/200] batch [15/50] time 0.083 (0.187) data 0.000 (0.103) loss 2.1777 (2.1173) acc 34.3750 (39.1667) lr 1.8358e-03 eta 0:25:10
epoch [39/200] batch [20/50] time 0.085 (0.169) data 0.000 (0.085) loss 2.5156 (2.1400) acc 25.0000 (37.8125) lr 1.8358e-03 eta 0:22:45
epoch [39/200] batch [25/50] time 0.179 (0.156) data 0.096 (0.072) loss 2.3887 (2.1181) acc 25.0000 (37.7500) lr 1.8358e-03 eta 0:20:58
epoch [39/200] batch [30/50] time 0.085 (0.144) data 0.000 (0.060) loss 2.2344 (2.1383) acc 37.5000 (36.8750) lr 1.8358e-03 eta 0:19:22
epoch [39/200] batch [35/50] time 0.085 (0.140) data 0.001 (0.056) loss 1.8867 (2.1323) acc 28.1250 (36.1607) lr 1.8358e-03 eta 0:18:45
epoch [39/200] batch [40/50] time 0.084 (0.133) data 0.000 (0.049) loss 1.8154 (2.1243) acc 43.7500 (36.0938) lr 1.8358e-03 eta 0:17:48
epoch [39/200] batch [45/50] time 0.084 (0.128) data 0.000 (0.044) loss 2.0977 (2.1204) acc 34.3750 (36.3889) lr 1.8358e-03 eta 0:17:08
epoch [39/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.0195 (2.1068) acc 31.2500 (36.6250) lr 1.8271e-03 eta 0:16:32
epoch [40/200] batch [5/50] time 0.083 (0.304) data 0.000 (0.220) loss 1.8496 (2.2234) acc 56.2500 (39.3750) lr 1.8271e-03 eta 0:40:43
epoch [40/200] batch [10/50] time 0.278 (0.213) data 0.194 (0.129) loss 2.1016 (2.2331) acc 46.8750 (38.4375) lr 1.8271e-03 eta 0:28:32
epoch [40/200] batch [15/50] time 0.084 (0.170) data 0.000 (0.086) loss 2.9336 (2.2747) acc 12.5000 (35.4167) lr 1.8271e-03 eta 0:22:45
epoch [40/200] batch [20/50] time 0.083 (0.160) data 0.000 (0.076) loss 2.0117 (2.2065) acc 28.1250 (36.4062) lr 1.8271e-03 eta 0:21:23
epoch [40/200] batch [25/50] time 0.084 (0.145) data 0.000 (0.061) loss 1.7393 (2.1684) acc 53.1250 (36.8750) lr 1.8271e-03 eta 0:19:20
epoch [40/200] batch [30/50] time 0.085 (0.135) data 0.000 (0.051) loss 1.5293 (2.1473) acc 59.3750 (37.0833) lr 1.8271e-03 eta 0:17:59
epoch [40/200] batch [35/50] time 0.265 (0.133) data 0.183 (0.049) loss 2.2422 (2.1353) acc 37.5000 (37.5893) lr 1.8271e-03 eta 0:17:42
epoch [40/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.043) loss 2.0098 (2.1215) acc 40.6250 (37.1094) lr 1.8271e-03 eta 0:16:52
epoch [40/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.8799 (2.1095) acc 43.7500 (37.5694) lr 1.8271e-03 eta 0:16:39
epoch [40/200] batch [50/50] time 0.084 (0.121) data 0.000 (0.037) loss 2.0156 (2.1205) acc 53.1250 (37.4375) lr 1.8181e-03 eta 0:16:05
epoch [41/200] batch [5/50] time 0.084 (0.273) data 0.000 (0.189) loss 1.9385 (2.0889) acc 37.5000 (38.7500) lr 1.8181e-03 eta 0:36:25
epoch [41/200] batch [10/50] time 0.084 (0.197) data 0.000 (0.113) loss 1.5488 (2.0096) acc 53.1250 (42.5000) lr 1.8181e-03 eta 0:26:17
epoch [41/200] batch [15/50] time 0.085 (0.160) data 0.000 (0.075) loss 1.8330 (2.0074) acc 53.1250 (42.5000) lr 1.8181e-03 eta 0:21:16
epoch [41/200] batch [20/50] time 0.086 (0.143) data 0.000 (0.059) loss 1.8242 (1.9968) acc 40.6250 (43.1250) lr 1.8181e-03 eta 0:19:03
epoch [41/200] batch [25/50] time 0.085 (0.132) data 0.000 (0.047) loss 2.1484 (1.9783) acc 37.5000 (43.5000) lr 1.8181e-03 eta 0:17:30
epoch [41/200] batch [30/50] time 0.086 (0.124) data 0.000 (0.039) loss 1.8555 (2.0000) acc 31.2500 (42.3958) lr 1.8181e-03 eta 0:16:29
epoch [41/200] batch [35/50] time 0.085 (0.119) data 0.000 (0.034) loss 1.9434 (2.0090) acc 46.8750 (42.2321) lr 1.8181e-03 eta 0:15:44
epoch [41/200] batch [40/50] time 0.083 (0.114) data 0.000 (0.030) loss 2.1328 (2.0227) acc 34.3750 (41.4844) lr 1.8181e-03 eta 0:15:08
epoch [41/200] batch [45/50] time 0.083 (0.111) data 0.000 (0.026) loss 2.3125 (2.0452) acc 21.8750 (40.6250) lr 1.8181e-03 eta 0:14:40
epoch [41/200] batch [50/50] time 0.085 (0.108) data 0.000 (0.024) loss 1.9297 (2.0599) acc 34.3750 (40.5000) lr 1.8090e-03 eta 0:14:18
epoch [42/200] batch [5/50] time 0.085 (0.335) data 0.000 (0.250) loss 2.4336 (2.0221) acc 31.2500 (42.5000) lr 1.8090e-03 eta 0:44:21
epoch [42/200] batch [10/50] time 0.084 (0.210) data 0.000 (0.125) loss 2.1035 (2.1180) acc 31.2500 (38.1250) lr 1.8090e-03 eta 0:27:44
epoch [42/200] batch [15/50] time 0.084 (0.168) data 0.001 (0.083) loss 2.2500 (2.1169) acc 31.2500 (37.0833) lr 1.8090e-03 eta 0:22:12
epoch [42/200] batch [20/50] time 0.083 (0.152) data 0.000 (0.068) loss 1.9258 (2.0552) acc 31.2500 (38.1250) lr 1.8090e-03 eta 0:20:04
epoch [42/200] batch [25/50] time 0.084 (0.138) data 0.000 (0.054) loss 2.0703 (2.0859) acc 31.2500 (36.7500) lr 1.8090e-03 eta 0:18:16
epoch [42/200] batch [30/50] time 0.085 (0.136) data 0.000 (0.051) loss 2.3340 (2.1116) acc 31.2500 (36.0417) lr 1.8090e-03 eta 0:17:53
epoch [42/200] batch [35/50] time 0.085 (0.133) data 0.001 (0.049) loss 2.5059 (2.1329) acc 21.8750 (36.2500) lr 1.8090e-03 eta 0:17:36
epoch [42/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.043) loss 2.0801 (2.1262) acc 40.6250 (37.0312) lr 1.8090e-03 eta 0:16:46
epoch [42/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.042) loss 2.5000 (2.1116) acc 25.0000 (37.0139) lr 1.8090e-03 eta 0:16:35
epoch [42/200] batch [50/50] time 0.084 (0.122) data 0.000 (0.038) loss 2.3164 (2.1150) acc 28.1250 (37.0625) lr 1.7997e-03 eta 0:16:01
epoch [43/200] batch [5/50] time 0.085 (0.302) data 0.000 (0.218) loss 2.3496 (2.2348) acc 25.0000 (33.1250) lr 1.7997e-03 eta 0:39:48
epoch [43/200] batch [10/50] time 0.084 (0.211) data 0.000 (0.126) loss 1.9277 (2.1416) acc 43.7500 (36.2500) lr 1.7997e-03 eta 0:27:41
epoch [43/200] batch [15/50] time 0.084 (0.168) data 0.000 (0.084) loss 2.2695 (2.0926) acc 37.5000 (38.7500) lr 1.7997e-03 eta 0:22:07
epoch [43/200] batch [20/50] time 0.084 (0.152) data 0.000 (0.068) loss 1.9121 (2.0830) acc 53.1250 (39.2188) lr 1.7997e-03 eta 0:20:00
epoch [43/200] batch [25/50] time 0.085 (0.139) data 0.000 (0.055) loss 2.1055 (2.1046) acc 43.7500 (38.3750) lr 1.7997e-03 eta 0:18:12
epoch [43/200] batch [30/50] time 0.084 (0.133) data 0.000 (0.050) loss 1.9316 (2.1049) acc 34.3750 (38.1250) lr 1.7997e-03 eta 0:17:30
epoch [43/200] batch [35/50] time 0.084 (0.132) data 0.000 (0.048) loss 2.0371 (2.1076) acc 40.6250 (38.6607) lr 1.7997e-03 eta 0:17:16
epoch [43/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.042) loss 2.4434 (2.1242) acc 37.5000 (37.9688) lr 1.7997e-03 eta 0:16:28
epoch [43/200] batch [45/50] time 0.084 (0.124) data 0.000 (0.040) loss 1.9561 (2.1155) acc 46.8750 (38.2639) lr 1.7997e-03 eta 0:16:11
epoch [43/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.9902 (2.1190) acc 40.6250 (37.7500) lr 1.7902e-03 eta 0:15:38
epoch [44/200] batch [5/50] time 0.085 (0.305) data 0.000 (0.220) loss 1.8672 (2.0986) acc 46.8750 (40.6250) lr 1.7902e-03 eta 0:39:52
epoch [44/200] batch [10/50] time 0.086 (0.202) data 0.000 (0.117) loss 2.0898 (2.0695) acc 40.6250 (40.9375) lr 1.7902e-03 eta 0:26:20
epoch [44/200] batch [15/50] time 0.086 (0.163) data 0.000 (0.078) loss 2.0312 (2.0692) acc 34.3750 (40.6250) lr 1.7902e-03 eta 0:21:14
epoch [44/200] batch [20/50] time 0.085 (0.143) data 0.000 (0.059) loss 1.7998 (2.0571) acc 34.3750 (40.0000) lr 1.7902e-03 eta 0:18:42
epoch [44/200] batch [25/50] time 0.084 (0.132) data 0.000 (0.047) loss 1.7988 (2.0295) acc 31.2500 (40.0000) lr 1.7902e-03 eta 0:17:09
epoch [44/200] batch [30/50] time 0.084 (0.127) data 0.000 (0.043) loss 2.0938 (2.0429) acc 34.3750 (38.9583) lr 1.7902e-03 eta 0:16:34
epoch [44/200] batch [35/50] time 0.254 (0.126) data 0.171 (0.042) loss 2.0059 (2.0583) acc 43.7500 (38.9286) lr 1.7902e-03 eta 0:16:24
epoch [44/200] batch [40/50] time 0.084 (0.121) data 0.000 (0.036) loss 2.4785 (2.0886) acc 28.1250 (39.1406) lr 1.7902e-03 eta 0:15:43
epoch [44/200] batch [45/50] time 0.084 (0.120) data 0.000 (0.035) loss 2.0625 (2.0846) acc 53.1250 (39.7222) lr 1.7902e-03 eta 0:15:34
epoch [44/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.032) loss 1.6436 (2.0818) acc 43.7500 (39.8125) lr 1.7804e-03 eta 0:15:05
epoch [45/200] batch [5/50] time 0.085 (0.258) data 0.000 (0.173) loss 2.4570 (2.1811) acc 43.7500 (33.1250) lr 1.7804e-03 eta 0:33:32
epoch [45/200] batch [10/50] time 0.084 (0.172) data 0.000 (0.087) loss 2.1992 (2.1048) acc 31.2500 (34.6875) lr 1.7804e-03 eta 0:22:17
epoch [45/200] batch [15/50] time 0.086 (0.143) data 0.000 (0.058) loss 1.7119 (2.0828) acc 53.1250 (36.8750) lr 1.7804e-03 eta 0:18:31
epoch [45/200] batch [20/50] time 0.084 (0.128) data 0.000 (0.043) loss 2.1035 (2.1219) acc 40.6250 (35.7812) lr 1.7804e-03 eta 0:16:36
epoch [45/200] batch [25/50] time 0.085 (0.124) data 0.000 (0.039) loss 2.3281 (2.1278) acc 37.5000 (36.0000) lr 1.7804e-03 eta 0:16:02
epoch [45/200] batch [30/50] time 0.251 (0.123) data 0.167 (0.038) loss 2.3477 (2.1514) acc 34.3750 (35.5208) lr 1.7804e-03 eta 0:15:54
epoch [45/200] batch [35/50] time 0.085 (0.117) data 0.001 (0.033) loss 1.8721 (2.1238) acc 43.7500 (37.0536) lr 1.7804e-03 eta 0:15:11
epoch [45/200] batch [40/50] time 0.084 (0.118) data 0.000 (0.033) loss 1.4746 (2.0888) acc 50.0000 (37.6562) lr 1.7804e-03 eta 0:15:12
epoch [45/200] batch [45/50] time 0.084 (0.114) data 0.000 (0.030) loss 1.5977 (2.0919) acc 50.0000 (37.5694) lr 1.7804e-03 eta 0:14:42
epoch [45/200] batch [50/50] time 0.083 (0.111) data 0.000 (0.027) loss 2.4668 (2.0961) acc 40.6250 (38.1875) lr 1.7705e-03 eta 0:14:19
epoch [46/200] batch [5/50] time 0.086 (0.314) data 0.000 (0.229) loss 2.1074 (2.0939) acc 34.3750 (37.5000) lr 1.7705e-03 eta 0:40:34
epoch [46/200] batch [10/50] time 0.182 (0.209) data 0.097 (0.124) loss 1.8594 (1.9854) acc 43.7500 (39.0625) lr 1.7705e-03 eta 0:26:58
epoch [46/200] batch [15/50] time 0.084 (0.168) data 0.000 (0.083) loss 2.0762 (2.0074) acc 34.3750 (39.1667) lr 1.7705e-03 eta 0:21:37
epoch [46/200] batch [20/50] time 0.084 (0.152) data 0.000 (0.067) loss 2.4219 (2.0737) acc 34.3750 (37.8125) lr 1.7705e-03 eta 0:19:31
epoch [46/200] batch [25/50] time 0.085 (0.138) data 0.000 (0.054) loss 1.6895 (2.0515) acc 46.8750 (38.3750) lr 1.7705e-03 eta 0:17:47
epoch [46/200] batch [30/50] time 0.084 (0.135) data 0.000 (0.051) loss 1.6104 (2.0429) acc 53.1250 (39.2708) lr 1.7705e-03 eta 0:17:22
epoch [46/200] batch [35/50] time 0.084 (0.134) data 0.000 (0.050) loss 1.9854 (2.0271) acc 37.5000 (39.1964) lr 1.7705e-03 eta 0:17:13
epoch [46/200] batch [40/50] time 0.082 (0.128) data 0.000 (0.043) loss 2.2246 (2.0321) acc 34.3750 (38.9844) lr 1.7705e-03 eta 0:16:23
epoch [46/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.9863 (2.0468) acc 40.6250 (38.8889) lr 1.7705e-03 eta 0:16:03
epoch [46/200] batch [50/50] time 0.082 (0.121) data 0.000 (0.037) loss 2.2598 (2.0396) acc 43.7500 (39.3125) lr 1.7604e-03 eta 0:15:29
epoch [47/200] batch [5/50] time 0.086 (0.294) data 0.000 (0.209) loss 2.0879 (1.9467) acc 53.1250 (41.8750) lr 1.7604e-03 eta 0:37:42
epoch [47/200] batch [10/50] time 0.301 (0.211) data 0.214 (0.126) loss 3.0352 (2.0903) acc 21.8750 (39.6875) lr 1.7604e-03 eta 0:27:05
epoch [47/200] batch [15/50] time 0.085 (0.169) data 0.000 (0.084) loss 1.8828 (2.1272) acc 28.1250 (37.9167) lr 1.7604e-03 eta 0:21:40
epoch [47/200] batch [20/50] time 0.085 (0.150) data 0.000 (0.065) loss 2.1387 (2.1604) acc 34.3750 (37.0312) lr 1.7604e-03 eta 0:19:09
epoch [47/200] batch [25/50] time 0.086 (0.137) data 0.001 (0.052) loss 2.0820 (2.1507) acc 31.2500 (36.8750) lr 1.7604e-03 eta 0:17:28
epoch [47/200] batch [30/50] time 0.086 (0.132) data 0.000 (0.047) loss 1.9795 (2.1455) acc 31.2500 (36.5625) lr 1.7604e-03 eta 0:16:54
epoch [47/200] batch [35/50] time 0.085 (0.127) data 0.001 (0.042) loss 2.1367 (2.1628) acc 34.3750 (35.8036) lr 1.7604e-03 eta 0:16:15
epoch [47/200] batch [40/50] time 0.085 (0.122) data 0.000 (0.037) loss 2.0078 (2.1288) acc 37.5000 (36.4844) lr 1.7604e-03 eta 0:15:33
epoch [47/200] batch [45/50] time 0.083 (0.118) data 0.000 (0.033) loss 1.7646 (2.1052) acc 46.8750 (37.0139) lr 1.7604e-03 eta 0:15:00
epoch [47/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.030) loss 2.3086 (2.1019) acc 37.5000 (36.8750) lr 1.7501e-03 eta 0:14:33
epoch [48/200] batch [5/50] time 0.085 (0.274) data 0.000 (0.189) loss 2.4590 (2.1395) acc 21.8750 (36.8750) lr 1.7501e-03 eta 0:34:53
epoch [48/200] batch [10/50] time 0.084 (0.180) data 0.000 (0.095) loss 2.4824 (2.0709) acc 34.3750 (38.7500) lr 1.7501e-03 eta 0:22:51
epoch [48/200] batch [15/50] time 0.086 (0.148) data 0.000 (0.063) loss 2.0586 (2.1041) acc 46.8750 (39.1667) lr 1.7501e-03 eta 0:18:51
epoch [48/200] batch [20/50] time 0.085 (0.132) data 0.000 (0.047) loss 2.0039 (2.0916) acc 53.1250 (40.0000) lr 1.7501e-03 eta 0:16:50
epoch [48/200] batch [25/50] time 0.211 (0.128) data 0.126 (0.043) loss 1.9893 (2.0829) acc 40.6250 (40.7500) lr 1.7501e-03 eta 0:16:17
epoch [48/200] batch [30/50] time 0.105 (0.122) data 0.021 (0.037) loss 2.0957 (2.0980) acc 43.7500 (40.3125) lr 1.7501e-03 eta 0:15:27
epoch [48/200] batch [35/50] time 0.084 (0.117) data 0.001 (0.032) loss 2.8164 (2.0955) acc 34.3750 (40.3571) lr 1.7501e-03 eta 0:14:48
epoch [48/200] batch [40/50] time 0.084 (0.115) data 0.000 (0.030) loss 2.2539 (2.1133) acc 25.0000 (39.7656) lr 1.7501e-03 eta 0:14:36
epoch [48/200] batch [45/50] time 0.083 (0.112) data 0.000 (0.027) loss 1.9854 (2.1082) acc 40.6250 (39.3750) lr 1.7501e-03 eta 0:14:09
epoch [48/200] batch [50/50] time 0.084 (0.111) data 0.000 (0.027) loss 2.3164 (2.1153) acc 31.2500 (38.9375) lr 1.7396e-03 eta 0:14:06
epoch [49/200] batch [5/50] time 0.085 (0.292) data 0.000 (0.207) loss 1.5273 (2.1555) acc 50.0000 (35.0000) lr 1.7396e-03 eta 0:36:54
epoch [49/200] batch [10/50] time 0.179 (0.198) data 0.095 (0.114) loss 1.9590 (2.0897) acc 31.2500 (36.8750) lr 1.7396e-03 eta 0:25:05
epoch [49/200] batch [15/50] time 0.086 (0.161) data 0.000 (0.076) loss 1.8936 (2.0258) acc 43.7500 (38.7500) lr 1.7396e-03 eta 0:20:19
epoch [49/200] batch [20/50] time 0.085 (0.148) data 0.000 (0.063) loss 1.8496 (1.9868) acc 43.7500 (38.9062) lr 1.7396e-03 eta 0:18:39
epoch [49/200] batch [25/50] time 0.085 (0.135) data 0.001 (0.050) loss 2.5293 (2.0328) acc 25.0000 (37.8750) lr 1.7396e-03 eta 0:17:03
epoch [49/200] batch [30/50] time 0.086 (0.133) data 0.001 (0.048) loss 2.0547 (2.0314) acc 40.6250 (38.2292) lr 1.7396e-03 eta 0:16:43
epoch [49/200] batch [35/50] time 0.085 (0.133) data 0.001 (0.048) loss 1.9990 (2.0431) acc 40.6250 (38.0357) lr 1.7396e-03 eta 0:16:45
epoch [49/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.042) loss 1.9590 (2.0636) acc 50.0000 (38.1250) lr 1.7396e-03 eta 0:15:58
epoch [49/200] batch [45/50] time 0.084 (0.124) data 0.000 (0.039) loss 2.1035 (2.0729) acc 46.8750 (38.1250) lr 1.7396e-03 eta 0:15:35
epoch [49/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.035) loss 2.4082 (2.0754) acc 25.0000 (38.2500) lr 1.7290e-03 eta 0:15:04
epoch [50/200] batch [5/50] time 0.086 (0.313) data 0.000 (0.227) loss 1.5723 (2.0264) acc 50.0000 (46.8750) lr 1.7290e-03 eta 0:39:19
epoch [50/200] batch [10/50] time 0.086 (0.199) data 0.000 (0.114) loss 2.3320 (2.0039) acc 31.2500 (44.0625) lr 1.7290e-03 eta 0:25:02
epoch [50/200] batch [15/50] time 0.085 (0.165) data 0.000 (0.080) loss 2.1016 (1.9711) acc 28.1250 (42.9167) lr 1.7290e-03 eta 0:20:44
epoch [50/200] batch [20/50] time 0.084 (0.152) data 0.000 (0.066) loss 2.1172 (2.0050) acc 34.3750 (41.5625) lr 1.7290e-03 eta 0:19:00
epoch [50/200] batch [25/50] time 0.084 (0.138) data 0.000 (0.053) loss 2.4336 (2.0466) acc 21.8750 (39.7500) lr 1.7290e-03 eta 0:17:18
epoch [50/200] batch [30/50] time 0.085 (0.137) data 0.000 (0.052) loss 2.0430 (2.0605) acc 37.5000 (38.6458) lr 1.7290e-03 eta 0:17:09
epoch [50/200] batch [35/50] time 0.213 (0.133) data 0.129 (0.048) loss 2.2559 (2.0758) acc 40.6250 (38.8393) lr 1.7290e-03 eta 0:16:41
epoch [50/200] batch [40/50] time 0.084 (0.127) data 0.000 (0.042) loss 2.3301 (2.0986) acc 28.1250 (38.7500) lr 1.7290e-03 eta 0:15:54
epoch [50/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.039) loss 2.1113 (2.0962) acc 37.5000 (38.6111) lr 1.7290e-03 eta 0:15:29
epoch [50/200] batch [50/50] time 0.082 (0.120) data 0.000 (0.035) loss 2.1816 (2.0985) acc 28.1250 (38.2500) lr 1.7181e-03 eta 0:14:58
epoch [51/200] batch [5/50] time 0.083 (0.291) data 0.000 (0.208) loss 1.8887 (2.0686) acc 34.3750 (40.0000) lr 1.7181e-03 eta 0:36:22
epoch [51/200] batch [10/50] time 0.084 (0.202) data 0.000 (0.119) loss 2.1504 (2.0530) acc 28.1250 (38.1250) lr 1.7181e-03 eta 0:25:16
epoch [51/200] batch [15/50] time 0.085 (0.163) data 0.000 (0.080) loss 2.1855 (2.0178) acc 34.3750 (38.5417) lr 1.7181e-03 eta 0:20:20
epoch [51/200] batch [20/50] time 0.104 (0.149) data 0.021 (0.066) loss 2.0547 (2.0637) acc 40.6250 (37.5000) lr 1.7181e-03 eta 0:18:36
epoch [51/200] batch [25/50] time 0.084 (0.136) data 0.000 (0.053) loss 2.0176 (2.0662) acc 37.5000 (38.2500) lr 1.7181e-03 eta 0:16:59
epoch [51/200] batch [30/50] time 0.085 (0.134) data 0.000 (0.050) loss 2.2012 (2.0633) acc 40.6250 (38.2292) lr 1.7181e-03 eta 0:16:38
epoch [51/200] batch [35/50] time 0.085 (0.127) data 0.000 (0.043) loss 1.9102 (2.0540) acc 50.0000 (38.5714) lr 1.7181e-03 eta 0:15:45
epoch [51/200] batch [40/50] time 0.083 (0.121) data 0.000 (0.038) loss 1.8574 (2.0438) acc 37.5000 (38.6719) lr 1.7181e-03 eta 0:15:04
epoch [51/200] batch [45/50] time 0.085 (0.117) data 0.000 (0.033) loss 2.1680 (2.0460) acc 31.2500 (38.6806) lr 1.7181e-03 eta 0:14:32
epoch [51/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.030) loss 2.3184 (2.0548) acc 40.6250 (38.5000) lr 1.7071e-03 eta 0:14:05
epoch [52/200] batch [5/50] time 0.085 (0.317) data 0.000 (0.232) loss 1.9102 (2.1586) acc 40.6250 (39.3750) lr 1.7071e-03 eta 0:39:20
epoch [52/200] batch [10/50] time 0.084 (0.201) data 0.000 (0.116) loss 2.6230 (2.1494) acc 15.6250 (38.4375) lr 1.7071e-03 eta 0:24:52
epoch [52/200] batch [15/50] time 0.083 (0.165) data 0.000 (0.081) loss 2.0703 (2.1948) acc 37.5000 (35.0000) lr 1.7071e-03 eta 0:20:26
epoch [52/200] batch [20/50] time 0.085 (0.150) data 0.000 (0.066) loss 2.0664 (2.1652) acc 37.5000 (36.0938) lr 1.7071e-03 eta 0:18:36
epoch [52/200] batch [25/50] time 0.084 (0.137) data 0.000 (0.053) loss 2.2754 (2.1582) acc 34.3750 (35.5000) lr 1.7071e-03 eta 0:16:57
epoch [52/200] batch [30/50] time 0.084 (0.135) data 0.000 (0.051) loss 1.9385 (2.1481) acc 43.7500 (36.2500) lr 1.7071e-03 eta 0:16:39
epoch [52/200] batch [35/50] time 0.230 (0.132) data 0.146 (0.048) loss 1.9561 (2.1276) acc 40.6250 (37.2321) lr 1.7071e-03 eta 0:16:15
epoch [52/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.042) loss 1.6074 (2.1019) acc 56.2500 (37.6562) lr 1.7071e-03 eta 0:15:31
epoch [52/200] batch [45/50] time 0.084 (0.123) data 0.000 (0.039) loss 1.8525 (2.0905) acc 50.0000 (38.0556) lr 1.7071e-03 eta 0:15:07
epoch [52/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.2285 (2.0877) acc 37.5000 (38.0625) lr 1.6959e-03 eta 0:14:37
epoch [53/200] batch [5/50] time 0.085 (0.267) data 0.001 (0.183) loss 2.0156 (2.0709) acc 37.5000 (39.3750) lr 1.6959e-03 eta 0:32:55
epoch [53/200] batch [10/50] time 0.084 (0.176) data 0.000 (0.092) loss 1.7266 (1.9534) acc 40.6250 (41.5625) lr 1.6959e-03 eta 0:21:39
epoch [53/200] batch [15/50] time 0.085 (0.151) data 0.000 (0.066) loss 1.9111 (1.9677) acc 46.8750 (42.0833) lr 1.6959e-03 eta 0:18:32
epoch [53/200] batch [20/50] time 0.084 (0.134) data 0.000 (0.050) loss 1.8789 (1.9961) acc 40.6250 (42.6562) lr 1.6959e-03 eta 0:16:28
epoch [53/200] batch [25/50] time 0.085 (0.126) data 0.001 (0.042) loss 2.4238 (2.0160) acc 31.2500 (42.1250) lr 1.6959e-03 eta 0:15:26
epoch [53/200] batch [30/50] time 0.084 (0.119) data 0.000 (0.035) loss 2.1875 (2.0219) acc 34.3750 (41.7708) lr 1.6959e-03 eta 0:14:35
epoch [53/200] batch [35/50] time 0.084 (0.118) data 0.000 (0.034) loss 2.0410 (2.0323) acc 43.7500 (41.0714) lr 1.6959e-03 eta 0:14:26
epoch [53/200] batch [40/50] time 0.308 (0.119) data 0.225 (0.035) loss 1.8965 (2.0520) acc 40.6250 (40.4688) lr 1.6959e-03 eta 0:14:36
epoch [53/200] batch [45/50] time 0.083 (0.115) data 0.000 (0.031) loss 2.0117 (2.0555) acc 37.5000 (40.4861) lr 1.6959e-03 eta 0:14:06
epoch [53/200] batch [50/50] time 0.083 (0.113) data 0.000 (0.030) loss 2.4648 (2.0705) acc 21.8750 (39.1875) lr 1.6845e-03 eta 0:13:53
epoch [54/200] batch [5/50] time 0.086 (0.320) data 0.000 (0.234) loss 2.4473 (2.0205) acc 34.3750 (38.7500) lr 1.6845e-03 eta 0:39:11
epoch [54/200] batch [10/50] time 0.083 (0.202) data 0.000 (0.117) loss 2.7168 (2.0603) acc 28.1250 (38.7500) lr 1.6845e-03 eta 0:24:43
epoch [54/200] batch [15/50] time 0.083 (0.167) data 0.000 (0.082) loss 1.8174 (2.0763) acc 50.0000 (40.0000) lr 1.6845e-03 eta 0:20:21
epoch [54/200] batch [20/50] time 0.084 (0.155) data 0.000 (0.071) loss 1.5938 (2.0756) acc 46.8750 (39.3750) lr 1.6845e-03 eta 0:18:59
epoch [54/200] batch [25/50] time 0.084 (0.141) data 0.000 (0.057) loss 1.6484 (2.0478) acc 56.2500 (40.0000) lr 1.6845e-03 eta 0:17:13
epoch [54/200] batch [30/50] time 0.086 (0.136) data 0.000 (0.052) loss 2.1055 (2.0535) acc 37.5000 (40.0000) lr 1.6845e-03 eta 0:16:36
epoch [54/200] batch [35/50] time 0.251 (0.134) data 0.167 (0.049) loss 1.6895 (2.0389) acc 56.2500 (40.7143) lr 1.6845e-03 eta 0:16:17
epoch [54/200] batch [40/50] time 0.084 (0.128) data 0.000 (0.043) loss 1.9883 (2.0389) acc 43.7500 (41.5625) lr 1.6845e-03 eta 0:15:32
epoch [54/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.8008 (2.0347) acc 40.6250 (41.5278) lr 1.6845e-03 eta 0:15:10
epoch [54/200] batch [50/50] time 0.084 (0.121) data 0.000 (0.037) loss 2.7852 (2.0562) acc 28.1250 (41.0625) lr 1.6730e-03 eta 0:14:39
epoch [55/200] batch [5/50] time 0.128 (0.291) data 0.043 (0.206) loss 2.0020 (2.0119) acc 37.5000 (41.2500) lr 1.6730e-03 eta 0:35:22
epoch [55/200] batch [10/50] time 0.087 (0.188) data 0.000 (0.103) loss 2.3516 (1.8980) acc 43.7500 (46.2500) lr 1.6730e-03 eta 0:22:53
epoch [55/200] batch [15/50] time 0.085 (0.157) data 0.000 (0.072) loss 1.9150 (1.9612) acc 37.5000 (42.9167) lr 1.6730e-03 eta 0:19:02
epoch [55/200] batch [20/50] time 0.306 (0.150) data 0.222 (0.065) loss 2.1523 (1.9608) acc 37.5000 (41.7188) lr 1.6730e-03 eta 0:18:13
epoch [55/200] batch [25/50] time 0.084 (0.137) data 0.000 (0.052) loss 2.5469 (2.0102) acc 25.0000 (41.1250) lr 1.6730e-03 eta 0:16:37
epoch [55/200] batch [30/50] time 0.085 (0.133) data 0.000 (0.048) loss 2.1406 (2.0211) acc 40.6250 (41.2500) lr 1.6730e-03 eta 0:16:04
epoch [55/200] batch [35/50] time 0.084 (0.126) data 0.000 (0.041) loss 1.8770 (2.0024) acc 43.7500 (41.5179) lr 1.6730e-03 eta 0:15:14
epoch [55/200] batch [40/50] time 0.084 (0.125) data 0.000 (0.041) loss 1.8564 (2.0124) acc 46.8750 (41.3281) lr 1.6730e-03 eta 0:15:10
epoch [55/200] batch [45/50] time 0.084 (0.125) data 0.000 (0.040) loss 1.8008 (2.0254) acc 43.7500 (40.6944) lr 1.6730e-03 eta 0:15:04
epoch [55/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.9775 (2.0376) acc 37.5000 (40.0625) lr 1.6613e-03 eta 0:14:33
epoch [56/200] batch [5/50] time 0.088 (0.294) data 0.000 (0.209) loss 1.9961 (1.9822) acc 40.6250 (43.7500) lr 1.6613e-03 eta 0:35:30
epoch [56/200] batch [10/50] time 0.085 (0.190) data 0.000 (0.105) loss 1.9082 (1.9399) acc 34.3750 (43.7500) lr 1.6613e-03 eta 0:22:53
epoch [56/200] batch [15/50] time 0.085 (0.160) data 0.000 (0.076) loss 2.2266 (2.0577) acc 40.6250 (40.8333) lr 1.6613e-03 eta 0:19:20
epoch [56/200] batch [20/50] time 0.084 (0.151) data 0.000 (0.066) loss 1.7607 (1.9946) acc 46.8750 (41.2500) lr 1.6613e-03 eta 0:18:11
epoch [56/200] batch [25/50] time 0.085 (0.138) data 0.000 (0.053) loss 2.0410 (2.0032) acc 40.6250 (41.2500) lr 1.6613e-03 eta 0:16:34
epoch [56/200] batch [30/50] time 0.085 (0.135) data 0.000 (0.051) loss 2.4434 (1.9986) acc 21.8750 (40.8333) lr 1.6613e-03 eta 0:16:18
epoch [56/200] batch [35/50] time 0.285 (0.134) data 0.201 (0.049) loss 1.7305 (2.0086) acc 56.2500 (41.1607) lr 1.6613e-03 eta 0:16:06
epoch [56/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.043) loss 2.2598 (2.0055) acc 34.3750 (40.9375) lr 1.6613e-03 eta 0:15:20
epoch [56/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.5391 (2.0342) acc 34.3750 (40.4861) lr 1.6613e-03 eta 0:14:49
epoch [56/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.4512 (2.0531) acc 43.7500 (40.0000) lr 1.6494e-03 eta 0:14:19
epoch [57/200] batch [5/50] time 0.088 (0.295) data 0.000 (0.209) loss 1.8125 (2.0402) acc 46.8750 (40.0000) lr 1.6494e-03 eta 0:35:25
epoch [57/200] batch [10/50] time 0.128 (0.194) data 0.042 (0.109) loss 1.8271 (2.0403) acc 43.7500 (41.2500) lr 1.6494e-03 eta 0:23:18
epoch [57/200] batch [15/50] time 0.084 (0.158) data 0.000 (0.073) loss 2.2441 (2.0993) acc 21.8750 (38.3333) lr 1.6494e-03 eta 0:18:55
epoch [57/200] batch [20/50] time 0.085 (0.147) data 0.000 (0.062) loss 2.1035 (2.1267) acc 37.5000 (37.9688) lr 1.6494e-03 eta 0:17:34
epoch [57/200] batch [25/50] time 0.084 (0.134) data 0.001 (0.050) loss 2.3301 (2.1012) acc 34.3750 (39.1250) lr 1.6494e-03 eta 0:16:04
epoch [57/200] batch [30/50] time 0.085 (0.135) data 0.000 (0.050) loss 2.5664 (2.1076) acc 21.8750 (39.2708) lr 1.6494e-03 eta 0:16:07
epoch [57/200] batch [35/50] time 0.330 (0.135) data 0.246 (0.050) loss 2.3945 (2.0876) acc 34.3750 (39.1964) lr 1.6494e-03 eta 0:16:05
epoch [57/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.044) loss 1.8740 (2.0633) acc 43.7500 (39.9219) lr 1.6494e-03 eta 0:15:18
epoch [57/200] batch [45/50] time 0.084 (0.126) data 0.000 (0.042) loss 2.4551 (2.0699) acc 21.8750 (40.0694) lr 1.6494e-03 eta 0:15:00
epoch [57/200] batch [50/50] time 0.085 (0.122) data 0.000 (0.038) loss 1.8672 (2.0695) acc 46.8750 (40.3750) lr 1.6374e-03 eta 0:14:29
epoch [58/200] batch [5/50] time 0.086 (0.295) data 0.000 (0.209) loss 2.0273 (2.0824) acc 40.6250 (34.3750) lr 1.6374e-03 eta 0:35:08
epoch [58/200] batch [10/50] time 0.145 (0.196) data 0.059 (0.111) loss 1.8037 (2.1036) acc 43.7500 (36.2500) lr 1.6374e-03 eta 0:23:21
epoch [58/200] batch [15/50] time 0.085 (0.168) data 0.000 (0.083) loss 1.5830 (2.0499) acc 56.2500 (37.2917) lr 1.6374e-03 eta 0:19:58
epoch [58/200] batch [20/50] time 0.086 (0.156) data 0.000 (0.070) loss 2.1230 (2.0753) acc 28.1250 (37.0312) lr 1.6374e-03 eta 0:18:31
epoch [58/200] batch [25/50] time 0.086 (0.142) data 0.001 (0.056) loss 2.0254 (2.0891) acc 40.6250 (36.6250) lr 1.6374e-03 eta 0:16:51
epoch [58/200] batch [30/50] time 0.084 (0.140) data 0.000 (0.055) loss 2.0293 (2.0833) acc 28.1250 (36.2500) lr 1.6374e-03 eta 0:16:39
epoch [58/200] batch [35/50] time 0.243 (0.137) data 0.160 (0.052) loss 2.1680 (2.0808) acc 37.5000 (36.6964) lr 1.6374e-03 eta 0:16:13
epoch [58/200] batch [40/50] time 0.085 (0.130) data 0.000 (0.045) loss 2.3262 (2.0573) acc 43.7500 (37.8125) lr 1.6374e-03 eta 0:15:25
epoch [58/200] batch [45/50] time 0.084 (0.127) data 0.000 (0.043) loss 2.0898 (2.0483) acc 37.5000 (38.2639) lr 1.6374e-03 eta 0:15:04
epoch [58/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.038) loss 2.3105 (2.0579) acc 28.1250 (38.1250) lr 1.6252e-03 eta 0:14:32
epoch [59/200] batch [5/50] time 0.085 (0.275) data 0.000 (0.191) loss 2.0840 (2.0691) acc 46.8750 (37.5000) lr 1.6252e-03 eta 0:32:32
epoch [59/200] batch [10/50] time 0.085 (0.180) data 0.000 (0.095) loss 2.0723 (1.9964) acc 31.2500 (38.1250) lr 1.6252e-03 eta 0:21:16
epoch [59/200] batch [15/50] time 0.085 (0.151) data 0.000 (0.066) loss 2.1602 (2.0234) acc 37.5000 (38.5417) lr 1.6252e-03 eta 0:17:48
epoch [59/200] batch [20/50] time 0.086 (0.141) data 0.000 (0.057) loss 2.2109 (2.0315) acc 40.6250 (39.3750) lr 1.6252e-03 eta 0:16:40
epoch [59/200] batch [25/50] time 0.085 (0.130) data 0.000 (0.045) loss 2.2383 (2.0724) acc 37.5000 (37.8750) lr 1.6252e-03 eta 0:15:19
epoch [59/200] batch [30/50] time 0.084 (0.126) data 0.000 (0.042) loss 1.9189 (2.0709) acc 34.3750 (37.9167) lr 1.6252e-03 eta 0:14:51
epoch [59/200] batch [35/50] time 0.239 (0.125) data 0.155 (0.040) loss 2.2812 (2.0626) acc 40.6250 (38.5714) lr 1.6252e-03 eta 0:14:40
epoch [59/200] batch [40/50] time 0.083 (0.120) data 0.000 (0.035) loss 2.2012 (2.0659) acc 25.0000 (38.4375) lr 1.6252e-03 eta 0:14:03
epoch [59/200] batch [45/50] time 0.083 (0.119) data 0.000 (0.035) loss 1.8643 (2.0536) acc 59.3750 (38.9583) lr 1.6252e-03 eta 0:14:00
epoch [59/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 2.0449 (2.0594) acc 34.3750 (39.2500) lr 1.6129e-03 eta 0:13:34
epoch [60/200] batch [5/50] time 0.241 (0.298) data 0.156 (0.213) loss 2.7285 (2.1361) acc 25.0000 (35.6250) lr 1.6129e-03 eta 0:35:01
epoch [60/200] batch [10/50] time 0.086 (0.211) data 0.000 (0.126) loss 1.9824 (2.0234) acc 40.6250 (38.7500) lr 1.6129e-03 eta 0:24:42
epoch [60/200] batch [15/50] time 0.089 (0.176) data 0.000 (0.091) loss 1.9424 (2.0397) acc 37.5000 (39.1667) lr 1.6129e-03 eta 0:20:36
epoch [60/200] batch [20/50] time 0.084 (0.154) data 0.000 (0.069) loss 2.0176 (2.0320) acc 43.7500 (40.0000) lr 1.6129e-03 eta 0:18:02
epoch [60/200] batch [25/50] time 0.085 (0.147) data 0.000 (0.062) loss 1.5781 (1.9973) acc 56.2500 (41.0000) lr 1.6129e-03 eta 0:17:11
epoch [60/200] batch [30/50] time 0.085 (0.137) data 0.000 (0.052) loss 2.0898 (1.9903) acc 31.2500 (41.2500) lr 1.6129e-03 eta 0:15:58
epoch [60/200] batch [35/50] time 0.085 (0.129) data 0.001 (0.045) loss 2.0039 (2.0039) acc 43.7500 (40.9821) lr 1.6129e-03 eta 0:15:06
epoch [60/200] batch [40/50] time 0.108 (0.124) data 0.023 (0.040) loss 2.0195 (2.0052) acc 37.5000 (40.9375) lr 1.6129e-03 eta 0:14:31
epoch [60/200] batch [45/50] time 0.083 (0.120) data 0.000 (0.035) loss 1.9365 (2.0043) acc 31.2500 (40.1389) lr 1.6129e-03 eta 0:13:59
epoch [60/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.032) loss 2.2109 (2.0124) acc 34.3750 (40.0625) lr 1.6004e-03 eta 0:13:32
epoch [61/200] batch [5/50] time 0.085 (0.282) data 0.000 (0.197) loss 1.7129 (1.9219) acc 50.0000 (45.6250) lr 1.6004e-03 eta 0:32:53
epoch [61/200] batch [10/50] time 0.086 (0.193) data 0.000 (0.108) loss 1.8252 (1.9217) acc 50.0000 (44.3750) lr 1.6004e-03 eta 0:22:28
epoch [61/200] batch [15/50] time 0.085 (0.157) data 0.000 (0.072) loss 2.0605 (1.9565) acc 40.6250 (42.7083) lr 1.6004e-03 eta 0:18:18
epoch [61/200] batch [20/50] time 0.084 (0.143) data 0.000 (0.058) loss 1.8584 (1.9775) acc 50.0000 (42.8125) lr 1.6004e-03 eta 0:16:38
epoch [61/200] batch [25/50] time 0.088 (0.131) data 0.002 (0.047) loss 2.9570 (2.0196) acc 9.3750 (41.2500) lr 1.6004e-03 eta 0:15:16
epoch [61/200] batch [30/50] time 0.085 (0.127) data 0.000 (0.042) loss 1.9189 (2.0295) acc 37.5000 (41.0417) lr 1.6004e-03 eta 0:14:45
epoch [61/200] batch [35/50] time 0.084 (0.121) data 0.000 (0.036) loss 2.5410 (2.0440) acc 25.0000 (40.7143) lr 1.6004e-03 eta 0:14:03
epoch [61/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.035) loss 1.7822 (2.0371) acc 37.5000 (40.6250) lr 1.6004e-03 eta 0:13:52
epoch [61/200] batch [45/50] time 0.082 (0.116) data 0.000 (0.031) loss 1.5518 (2.0259) acc 56.2500 (40.4861) lr 1.6004e-03 eta 0:13:24
epoch [61/200] batch [50/50] time 0.083 (0.112) data 0.000 (0.028) loss 1.9873 (2.0284) acc 28.1250 (40.1875) lr 1.5878e-03 eta 0:13:00
epoch [62/200] batch [5/50] time 0.085 (0.296) data 0.000 (0.210) loss 2.1621 (2.0822) acc 43.7500 (42.5000) lr 1.5878e-03 eta 0:34:12
epoch [62/200] batch [10/50] time 0.284 (0.210) data 0.200 (0.125) loss 1.9443 (2.0336) acc 46.8750 (41.5625) lr 1.5878e-03 eta 0:24:18
epoch [62/200] batch [15/50] time 0.084 (0.168) data 0.000 (0.084) loss 1.9658 (2.0403) acc 50.0000 (40.6250) lr 1.5878e-03 eta 0:19:27
epoch [62/200] batch [20/50] time 0.085 (0.156) data 0.000 (0.071) loss 1.7793 (1.9840) acc 43.7500 (42.6562) lr 1.5878e-03 eta 0:18:00
epoch [62/200] batch [25/50] time 0.085 (0.142) data 0.000 (0.057) loss 2.9082 (2.0494) acc 25.0000 (40.5000) lr 1.5878e-03 eta 0:16:20
epoch [62/200] batch [30/50] time 0.086 (0.132) data 0.000 (0.048) loss 1.6973 (2.0456) acc 43.7500 (39.5833) lr 1.5878e-03 eta 0:15:14
epoch [62/200] batch [35/50] time 0.084 (0.125) data 0.000 (0.041) loss 1.9619 (2.0299) acc 34.3750 (39.3750) lr 1.5878e-03 eta 0:14:26
epoch [62/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.036) loss 1.9258 (2.0399) acc 40.6250 (39.3750) lr 1.5878e-03 eta 0:13:50
epoch [62/200] batch [45/50] time 0.084 (0.116) data 0.000 (0.032) loss 2.3379 (2.0553) acc 37.5000 (39.4444) lr 1.5878e-03 eta 0:13:22
epoch [62/200] batch [50/50] time 0.083 (0.113) data 0.000 (0.029) loss 2.3926 (2.0545) acc 37.5000 (39.5625) lr 1.5750e-03 eta 0:12:58
epoch [63/200] batch [5/50] time 0.084 (0.300) data 0.000 (0.214) loss 1.9424 (2.1074) acc 40.6250 (33.7500) lr 1.5750e-03 eta 0:34:27
epoch [63/200] batch [10/50] time 0.147 (0.199) data 0.063 (0.114) loss 1.7070 (1.9437) acc 53.1250 (41.8750) lr 1.5750e-03 eta 0:22:48
epoch [63/200] batch [15/50] time 0.085 (0.161) data 0.000 (0.076) loss 2.2520 (1.9830) acc 28.1250 (40.2083) lr 1.5750e-03 eta 0:18:25
epoch [63/200] batch [20/50] time 0.084 (0.142) data 0.000 (0.057) loss 1.9395 (1.9949) acc 56.2500 (41.2500) lr 1.5750e-03 eta 0:16:14
epoch [63/200] batch [25/50] time 0.085 (0.130) data 0.000 (0.046) loss 2.2402 (2.0112) acc 12.5000 (40.6250) lr 1.5750e-03 eta 0:14:56
epoch [63/200] batch [30/50] time 0.085 (0.125) data 0.000 (0.040) loss 2.2500 (2.0196) acc 31.2500 (40.3125) lr 1.5750e-03 eta 0:14:19
epoch [63/200] batch [35/50] time 0.179 (0.122) data 0.096 (0.037) loss 1.7051 (2.0218) acc 37.5000 (39.6429) lr 1.5750e-03 eta 0:13:57
epoch [63/200] batch [40/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.9004 (2.0061) acc 25.0000 (39.5312) lr 1.5750e-03 eta 0:13:43
epoch [63/200] batch [45/50] time 0.085 (0.117) data 0.000 (0.033) loss 1.6377 (2.0279) acc 46.8750 (39.0972) lr 1.5750e-03 eta 0:13:25
epoch [63/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.030) loss 1.9541 (2.0270) acc 31.2500 (39.1250) lr 1.5621e-03 eta 0:13:01
epoch [64/200] batch [5/50] time 0.087 (0.292) data 0.000 (0.206) loss 1.7441 (1.9709) acc 40.6250 (43.1250) lr 1.5621e-03 eta 0:33:15
epoch [64/200] batch [10/50] time 0.185 (0.199) data 0.102 (0.113) loss 2.1055 (2.0667) acc 37.5000 (39.3750) lr 1.5621e-03 eta 0:22:38
epoch [64/200] batch [15/50] time 0.086 (0.161) data 0.001 (0.076) loss 2.2246 (2.0727) acc 28.1250 (38.7500) lr 1.5621e-03 eta 0:18:20
epoch [64/200] batch [20/50] time 0.086 (0.154) data 0.000 (0.069) loss 1.9678 (2.0849) acc 37.5000 (39.2188) lr 1.5621e-03 eta 0:17:34
epoch [64/200] batch [25/50] time 0.085 (0.141) data 0.000 (0.056) loss 1.6914 (2.0381) acc 46.8750 (39.8750) lr 1.5621e-03 eta 0:15:59
epoch [64/200] batch [30/50] time 0.086 (0.135) data 0.000 (0.050) loss 2.0078 (2.0348) acc 40.6250 (39.1667) lr 1.5621e-03 eta 0:15:18
epoch [64/200] batch [35/50] time 0.085 (0.133) data 0.001 (0.048) loss 1.8252 (2.0307) acc 50.0000 (39.9107) lr 1.5621e-03 eta 0:15:06
epoch [64/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.042) loss 2.2832 (2.0305) acc 31.2500 (40.4688) lr 1.5621e-03 eta 0:14:23
epoch [64/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.039) loss 2.0684 (2.0294) acc 31.2500 (40.4167) lr 1.5621e-03 eta 0:14:02
epoch [64/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.035) loss 1.8984 (2.0138) acc 46.8750 (40.8750) lr 1.5490e-03 eta 0:13:33
epoch [65/200] batch [5/50] time 0.085 (0.291) data 0.000 (0.206) loss 2.3027 (2.0223) acc 28.1250 (41.2500) lr 1.5490e-03 eta 0:32:57
epoch [65/200] batch [10/50] time 0.086 (0.188) data 0.000 (0.103) loss 1.8711 (2.0051) acc 37.5000 (43.1250) lr 1.5490e-03 eta 0:21:17
epoch [65/200] batch [15/50] time 0.084 (0.161) data 0.000 (0.076) loss 1.8350 (1.9730) acc 50.0000 (44.3750) lr 1.5490e-03 eta 0:18:13
epoch [65/200] batch [20/50] time 0.085 (0.150) data 0.000 (0.065) loss 1.9854 (1.9987) acc 50.0000 (43.4375) lr 1.5490e-03 eta 0:16:56
epoch [65/200] batch [25/50] time 0.084 (0.137) data 0.000 (0.052) loss 2.1309 (2.0282) acc 40.6250 (42.8750) lr 1.5490e-03 eta 0:15:27
epoch [65/200] batch [30/50] time 0.085 (0.133) data 0.000 (0.049) loss 1.6328 (1.9990) acc 50.0000 (43.3333) lr 1.5490e-03 eta 0:15:03
epoch [65/200] batch [35/50] time 0.233 (0.131) data 0.148 (0.046) loss 2.2207 (2.0104) acc 43.7500 (43.1250) lr 1.5490e-03 eta 0:14:44
epoch [65/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.9971 (2.0204) acc 34.3750 (42.1875) lr 1.5490e-03 eta 0:14:04
epoch [65/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.039) loss 1.7295 (2.0209) acc 46.8750 (41.5278) lr 1.5490e-03 eta 0:13:54
epoch [65/200] batch [50/50] time 0.082 (0.119) data 0.000 (0.035) loss 2.0859 (2.0319) acc 31.2500 (41.1875) lr 1.5358e-03 eta 0:13:25
epoch [66/200] batch [5/50] time 0.086 (0.305) data 0.000 (0.220) loss 2.0938 (2.1049) acc 34.3750 (41.2500) lr 1.5358e-03 eta 0:34:16
epoch [66/200] batch [10/50] time 0.200 (0.206) data 0.116 (0.121) loss 2.0000 (2.1063) acc 37.5000 (39.6875) lr 1.5358e-03 eta 0:23:09
epoch [66/200] batch [15/50] time 0.084 (0.166) data 0.000 (0.081) loss 1.6094 (2.0610) acc 50.0000 (40.0000) lr 1.5358e-03 eta 0:18:34
epoch [66/200] batch [20/50] time 0.084 (0.153) data 0.000 (0.068) loss 2.5254 (2.1186) acc 31.2500 (39.3750) lr 1.5358e-03 eta 0:17:08
epoch [66/200] batch [25/50] time 0.084 (0.139) data 0.000 (0.055) loss 2.0508 (2.1027) acc 43.7500 (40.1250) lr 1.5358e-03 eta 0:15:34
epoch [66/200] batch [30/50] time 0.084 (0.134) data 0.000 (0.050) loss 1.8779 (2.0980) acc 40.6250 (40.7292) lr 1.5358e-03 eta 0:15:00
epoch [66/200] batch [35/50] time 0.084 (0.131) data 0.000 (0.047) loss 1.8369 (2.0711) acc 46.8750 (41.3393) lr 1.5358e-03 eta 0:14:40
epoch [66/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.042) loss 1.4980 (2.0479) acc 56.2500 (41.7188) lr 1.5358e-03 eta 0:14:02
epoch [66/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.4160 (2.0442) acc 34.3750 (41.3889) lr 1.5358e-03 eta 0:13:50
epoch [66/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.7832 (2.0353) acc 46.8750 (40.8750) lr 1.5225e-03 eta 0:13:22
epoch [67/200] batch [5/50] time 0.085 (0.269) data 0.000 (0.184) loss 1.5391 (1.8602) acc 46.8750 (38.1250) lr 1.5225e-03 eta 0:29:57
epoch [67/200] batch [10/50] time 0.085 (0.177) data 0.000 (0.092) loss 2.0020 (1.9626) acc 37.5000 (38.7500) lr 1.5225e-03 eta 0:19:43
epoch [67/200] batch [15/50] time 0.086 (0.156) data 0.000 (0.072) loss 1.8516 (1.9637) acc 40.6250 (39.7917) lr 1.5225e-03 eta 0:17:24
epoch [67/200] batch [20/50] time 0.229 (0.146) data 0.144 (0.061) loss 1.5078 (1.9683) acc 50.0000 (40.6250) lr 1.5225e-03 eta 0:16:13
epoch [67/200] batch [25/50] time 0.085 (0.137) data 0.000 (0.053) loss 2.1699 (1.9884) acc 40.6250 (39.8750) lr 1.5225e-03 eta 0:15:17
epoch [67/200] batch [30/50] time 0.085 (0.132) data 0.000 (0.048) loss 1.7393 (1.9499) acc 53.1250 (41.5625) lr 1.5225e-03 eta 0:14:42
epoch [67/200] batch [35/50] time 0.085 (0.126) data 0.000 (0.041) loss 2.1953 (1.9778) acc 31.2500 (41.0714) lr 1.5225e-03 eta 0:13:57
epoch [67/200] batch [40/50] time 0.082 (0.125) data 0.000 (0.040) loss 2.5293 (1.9981) acc 34.3750 (41.6406) lr 1.5225e-03 eta 0:13:52
epoch [67/200] batch [45/50] time 0.161 (0.123) data 0.080 (0.039) loss 2.4805 (2.0391) acc 31.2500 (40.6250) lr 1.5225e-03 eta 0:13:37
epoch [67/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.4316 (2.0530) acc 31.2500 (40.1875) lr 1.5090e-03 eta 0:13:10
epoch [68/200] batch [5/50] time 0.085 (0.305) data 0.000 (0.220) loss 1.8301 (1.8779) acc 50.0000 (43.7500) lr 1.5090e-03 eta 0:33:47
epoch [68/200] batch [10/50] time 0.086 (0.195) data 0.000 (0.110) loss 2.2266 (1.9926) acc 21.8750 (38.4375) lr 1.5090e-03 eta 0:21:36
epoch [68/200] batch [15/50] time 0.085 (0.167) data 0.000 (0.082) loss 1.9570 (2.0384) acc 46.8750 (37.9167) lr 1.5090e-03 eta 0:18:24
epoch [68/200] batch [20/50] time 0.257 (0.155) data 0.172 (0.070) loss 1.7803 (2.0446) acc 56.2500 (38.1250) lr 1.5090e-03 eta 0:17:06
epoch [68/200] batch [25/50] time 0.084 (0.141) data 0.001 (0.056) loss 1.9697 (2.0219) acc 40.6250 (38.3750) lr 1.5090e-03 eta 0:15:32
epoch [68/200] batch [30/50] time 0.084 (0.135) data 0.000 (0.051) loss 1.8809 (2.0093) acc 40.6250 (39.8958) lr 1.5090e-03 eta 0:14:56
epoch [68/200] batch [35/50] time 0.085 (0.128) data 0.001 (0.043) loss 2.3867 (2.0125) acc 31.2500 (39.5536) lr 1.5090e-03 eta 0:14:07
epoch [68/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.042) loss 2.7363 (2.0318) acc 34.3750 (39.5312) lr 1.5090e-03 eta 0:13:53
epoch [68/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 2.1875 (2.0400) acc 34.3750 (39.0278) lr 1.5090e-03 eta 0:13:47
epoch [68/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.6631 (2.0429) acc 46.8750 (38.9375) lr 1.4955e-03 eta 0:13:19
epoch [69/200] batch [5/50] time 0.085 (0.320) data 0.000 (0.234) loss 2.2871 (1.9252) acc 46.8750 (42.5000) lr 1.4955e-03 eta 0:35:11
epoch [69/200] batch [10/50] time 0.084 (0.212) data 0.000 (0.127) loss 1.8262 (1.9288) acc 53.1250 (41.8750) lr 1.4955e-03 eta 0:23:17
epoch [69/200] batch [15/50] time 0.085 (0.169) data 0.000 (0.085) loss 2.0547 (1.9663) acc 43.7500 (42.2917) lr 1.4955e-03 eta 0:18:34
epoch [69/200] batch [20/50] time 0.085 (0.157) data 0.000 (0.073) loss 2.2402 (2.0297) acc 21.8750 (39.6875) lr 1.4955e-03 eta 0:17:13
epoch [69/200] batch [25/50] time 0.298 (0.151) data 0.215 (0.067) loss 1.7617 (2.0284) acc 34.3750 (40.0000) lr 1.4955e-03 eta 0:16:32
epoch [69/200] batch [30/50] time 0.084 (0.140) data 0.000 (0.056) loss 2.1602 (2.0448) acc 34.3750 (39.8958) lr 1.4955e-03 eta 0:15:19
epoch [69/200] batch [35/50] time 0.084 (0.137) data 0.000 (0.053) loss 1.8066 (2.0408) acc 43.7500 (40.0000) lr 1.4955e-03 eta 0:14:58
epoch [69/200] batch [40/50] time 0.083 (0.130) data 0.000 (0.046) loss 2.9961 (2.0642) acc 25.0000 (40.2344) lr 1.4955e-03 eta 0:14:14
epoch [69/200] batch [45/50] time 0.085 (0.128) data 0.000 (0.044) loss 1.6768 (2.0592) acc 50.0000 (40.1389) lr 1.4955e-03 eta 0:13:56
epoch [69/200] batch [50/50] time 0.084 (0.123) data 0.000 (0.039) loss 2.0020 (2.0532) acc 40.6250 (40.5000) lr 1.4818e-03 eta 0:13:27
epoch [70/200] batch [5/50] time 0.085 (0.313) data 0.000 (0.228) loss 1.8047 (2.0213) acc 40.6250 (41.2500) lr 1.4818e-03 eta 0:34:10
epoch [70/200] batch [10/50] time 0.085 (0.217) data 0.001 (0.133) loss 1.8711 (1.9204) acc 43.7500 (42.1875) lr 1.4818e-03 eta 0:23:37
epoch [70/200] batch [15/50] time 0.084 (0.173) data 0.000 (0.089) loss 2.0918 (2.0005) acc 43.7500 (42.5000) lr 1.4818e-03 eta 0:18:48
epoch [70/200] batch [20/50] time 0.084 (0.154) data 0.000 (0.069) loss 1.7354 (2.0058) acc 40.6250 (41.7188) lr 1.4818e-03 eta 0:16:42
epoch [70/200] batch [25/50] time 0.086 (0.140) data 0.000 (0.056) loss 1.9609 (2.0054) acc 37.5000 (40.1250) lr 1.4818e-03 eta 0:15:11
epoch [70/200] batch [30/50] time 0.084 (0.131) data 0.000 (0.046) loss 2.1426 (2.0076) acc 28.1250 (40.0000) lr 1.4818e-03 eta 0:14:11
epoch [70/200] batch [35/50] time 0.084 (0.124) data 0.000 (0.040) loss 2.0508 (2.0095) acc 28.1250 (39.7321) lr 1.4818e-03 eta 0:13:27
epoch [70/200] batch [40/50] time 0.084 (0.119) data 0.000 (0.035) loss 2.0859 (2.0168) acc 37.5000 (38.9844) lr 1.4818e-03 eta 0:12:54
epoch [70/200] batch [45/50] time 0.084 (0.115) data 0.000 (0.031) loss 2.3477 (2.0253) acc 37.5000 (39.1667) lr 1.4818e-03 eta 0:12:28
epoch [70/200] batch [50/50] time 0.083 (0.112) data 0.000 (0.028) loss 2.1660 (2.0150) acc 37.5000 (39.4375) lr 1.4679e-03 eta 0:12:07
epoch [71/200] batch [5/50] time 0.085 (0.309) data 0.000 (0.223) loss 2.0215 (2.0139) acc 46.8750 (42.5000) lr 1.4679e-03 eta 0:33:24
epoch [71/200] batch [10/50] time 0.085 (0.197) data 0.000 (0.112) loss 2.0273 (1.9205) acc 34.3750 (41.8750) lr 1.4679e-03 eta 0:21:20
epoch [71/200] batch [15/50] time 0.086 (0.169) data 0.001 (0.084) loss 2.1055 (1.9473) acc 28.1250 (39.7917) lr 1.4679e-03 eta 0:18:16
epoch [71/200] batch [20/50] time 0.086 (0.154) data 0.001 (0.069) loss 1.9014 (1.9540) acc 40.6250 (39.3750) lr 1.4679e-03 eta 0:16:38
epoch [71/200] batch [25/50] time 0.085 (0.140) data 0.000 (0.055) loss 1.5908 (1.9409) acc 59.3750 (40.5000) lr 1.4679e-03 eta 0:15:08
epoch [71/200] batch [30/50] time 0.084 (0.135) data 0.000 (0.050) loss 1.9287 (1.9655) acc 34.3750 (40.2083) lr 1.4679e-03 eta 0:14:32
epoch [71/200] batch [35/50] time 0.229 (0.132) data 0.144 (0.047) loss 2.0586 (1.9448) acc 34.3750 (40.9821) lr 1.4679e-03 eta 0:14:13
epoch [71/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.3867 (1.9534) acc 34.3750 (41.0938) lr 1.4679e-03 eta 0:13:34
epoch [71/200] batch [45/50] time 0.084 (0.125) data 0.000 (0.040) loss 2.2930 (1.9826) acc 31.2500 (40.1389) lr 1.4679e-03 eta 0:13:24
epoch [71/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 2.0430 (1.9972) acc 31.2500 (39.5000) lr 1.4540e-03 eta 0:12:56
epoch [72/200] batch [5/50] time 0.084 (0.293) data 0.000 (0.209) loss 1.9395 (2.1457) acc 43.7500 (36.2500) lr 1.4540e-03 eta 0:31:26
epoch [72/200] batch [10/50] time 0.085 (0.189) data 0.000 (0.105) loss 1.9570 (2.1031) acc 34.3750 (37.1875) lr 1.4540e-03 eta 0:20:16
epoch [72/200] batch [15/50] time 0.085 (0.154) data 0.000 (0.070) loss 2.1445 (2.0816) acc 43.7500 (39.3750) lr 1.4540e-03 eta 0:16:32
epoch [72/200] batch [20/50] time 0.113 (0.138) data 0.028 (0.054) loss 2.0977 (2.0726) acc 37.5000 (40.3125) lr 1.4540e-03 eta 0:14:48
epoch [72/200] batch [25/50] time 0.085 (0.129) data 0.000 (0.044) loss 2.0488 (2.0357) acc 40.6250 (40.2500) lr 1.4540e-03 eta 0:13:47
epoch [72/200] batch [30/50] time 0.248 (0.130) data 0.164 (0.045) loss 2.2051 (2.0174) acc 37.5000 (41.3542) lr 1.4540e-03 eta 0:13:51
epoch [72/200] batch [35/50] time 0.084 (0.123) data 0.000 (0.039) loss 1.8818 (1.9953) acc 37.5000 (41.5179) lr 1.4540e-03 eta 0:13:10
epoch [72/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.039) loss 1.8643 (1.9855) acc 50.0000 (42.1094) lr 1.4540e-03 eta 0:13:11
epoch [72/200] batch [45/50] time 0.084 (0.119) data 0.000 (0.035) loss 1.7988 (1.9952) acc 53.1250 (41.6667) lr 1.4540e-03 eta 0:12:42
epoch [72/200] batch [50/50] time 0.084 (0.116) data 0.000 (0.032) loss 2.2910 (2.0114) acc 28.1250 (41.4375) lr 1.4399e-03 eta 0:12:24
epoch [73/200] batch [5/50] time 0.085 (0.307) data 0.000 (0.223) loss 2.1367 (2.2039) acc 53.1250 (41.8750) lr 1.4399e-03 eta 0:32:41
epoch [73/200] batch [10/50] time 0.097 (0.201) data 0.014 (0.118) loss 1.9219 (2.1059) acc 46.8750 (42.1875) lr 1.4399e-03 eta 0:21:27
epoch [73/200] batch [15/50] time 0.084 (0.162) data 0.000 (0.079) loss 1.8779 (2.0304) acc 31.2500 (41.8750) lr 1.4399e-03 eta 0:17:16
epoch [73/200] batch [20/50] time 0.084 (0.153) data 0.000 (0.069) loss 1.7051 (2.0195) acc 46.8750 (42.6562) lr 1.4399e-03 eta 0:16:14
epoch [73/200] batch [25/50] time 0.086 (0.139) data 0.000 (0.055) loss 1.8252 (1.9923) acc 46.8750 (43.2500) lr 1.4399e-03 eta 0:14:47
epoch [73/200] batch [30/50] time 0.086 (0.138) data 0.000 (0.054) loss 1.6621 (2.0263) acc 59.3750 (42.1875) lr 1.4399e-03 eta 0:14:39
epoch [73/200] batch [35/50] time 0.086 (0.133) data 0.001 (0.049) loss 2.3965 (2.0382) acc 21.8750 (41.1607) lr 1.4399e-03 eta 0:14:08
epoch [73/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.043) loss 2.2500 (2.0392) acc 37.5000 (41.0938) lr 1.4399e-03 eta 0:13:29
epoch [73/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.042) loss 2.3184 (2.0227) acc 31.2500 (41.2500) lr 1.4399e-03 eta 0:13:22
epoch [73/200] batch [50/50] time 0.085 (0.122) data 0.000 (0.038) loss 2.4648 (2.0285) acc 25.0000 (40.6875) lr 1.4258e-03 eta 0:12:54
epoch [74/200] batch [5/50] time 0.085 (0.273) data 0.000 (0.187) loss 2.1230 (2.0240) acc 34.3750 (38.1250) lr 1.4258e-03 eta 0:28:50
epoch [74/200] batch [10/50] time 0.085 (0.179) data 0.001 (0.094) loss 2.3848 (1.9913) acc 37.5000 (42.1875) lr 1.4258e-03 eta 0:18:57
epoch [74/200] batch [15/50] time 0.085 (0.148) data 0.000 (0.063) loss 2.2207 (2.0408) acc 37.5000 (40.6250) lr 1.4258e-03 eta 0:15:39
epoch [74/200] batch [20/50] time 0.085 (0.132) data 0.000 (0.047) loss 1.9951 (2.0415) acc 34.3750 (40.7812) lr 1.4258e-03 eta 0:13:58
epoch [74/200] batch [25/50] time 0.085 (0.129) data 0.000 (0.044) loss 1.9141 (2.0362) acc 37.5000 (40.7500) lr 1.4258e-03 eta 0:13:34
epoch [74/200] batch [30/50] time 0.085 (0.126) data 0.000 (0.041) loss 1.9170 (2.0213) acc 31.2500 (41.1458) lr 1.4258e-03 eta 0:13:16
epoch [74/200] batch [35/50] time 0.084 (0.120) data 0.001 (0.035) loss 1.9902 (2.0269) acc 37.5000 (40.7143) lr 1.4258e-03 eta 0:12:38
epoch [74/200] batch [40/50] time 0.084 (0.117) data 0.000 (0.032) loss 2.1250 (2.0199) acc 34.3750 (40.6250) lr 1.4258e-03 eta 0:12:17
epoch [74/200] batch [45/50] time 0.274 (0.117) data 0.191 (0.033) loss 2.3848 (2.0405) acc 40.6250 (40.4861) lr 1.4258e-03 eta 0:12:19
epoch [74/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.030) loss 1.9502 (2.0266) acc 43.7500 (40.8750) lr 1.4115e-03 eta 0:11:57
epoch [75/200] batch [5/50] time 0.124 (0.326) data 0.040 (0.242) loss 1.9189 (1.8984) acc 43.7500 (41.2500) lr 1.4115e-03 eta 0:34:15
epoch [75/200] batch [10/50] time 0.085 (0.206) data 0.000 (0.121) loss 1.6201 (1.9024) acc 53.1250 (41.2500) lr 1.4115e-03 eta 0:21:33
epoch [75/200] batch [15/50] time 0.085 (0.166) data 0.000 (0.081) loss 2.1328 (1.9184) acc 43.7500 (42.0833) lr 1.4115e-03 eta 0:17:20
epoch [75/200] batch [20/50] time 0.085 (0.146) data 0.000 (0.061) loss 1.7041 (1.9210) acc 53.1250 (41.0938) lr 1.4115e-03 eta 0:15:14
epoch [75/200] batch [25/50] time 0.086 (0.140) data 0.000 (0.055) loss 2.1582 (1.9902) acc 37.5000 (39.5000) lr 1.4115e-03 eta 0:14:40
epoch [75/200] batch [30/50] time 0.155 (0.133) data 0.071 (0.049) loss 2.0840 (1.9998) acc 28.1250 (39.6875) lr 1.4115e-03 eta 0:13:56
epoch [75/200] batch [35/50] time 0.085 (0.127) data 0.000 (0.042) loss 2.0801 (2.0147) acc 43.7500 (40.0000) lr 1.4115e-03 eta 0:13:12
epoch [75/200] batch [40/50] time 0.082 (0.128) data 0.000 (0.043) loss 1.7441 (2.0197) acc 53.1250 (39.8438) lr 1.4115e-03 eta 0:13:21
epoch [75/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.039) loss 1.6904 (2.0199) acc 56.2500 (39.9306) lr 1.4115e-03 eta 0:12:49
epoch [75/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 1.7959 (2.0188) acc 53.1250 (40.2500) lr 1.3971e-03 eta 0:12:23
epoch [76/200] batch [5/50] time 0.085 (0.320) data 0.001 (0.235) loss 2.4766 (2.1246) acc 31.2500 (38.1250) lr 1.3971e-03 eta 0:33:15
epoch [76/200] batch [10/50] time 0.086 (0.213) data 0.000 (0.129) loss 2.2383 (2.0411) acc 34.3750 (40.3125) lr 1.3971e-03 eta 0:22:12
epoch [76/200] batch [15/50] time 0.085 (0.171) data 0.000 (0.086) loss 2.1406 (2.0480) acc 40.6250 (40.0000) lr 1.3971e-03 eta 0:17:45
epoch [76/200] batch [20/50] time 0.085 (0.155) data 0.000 (0.070) loss 2.0176 (2.0566) acc 43.7500 (40.6250) lr 1.3971e-03 eta 0:16:07
epoch [76/200] batch [25/50] time 0.113 (0.142) data 0.028 (0.057) loss 1.9531 (2.0333) acc 46.8750 (42.0000) lr 1.3971e-03 eta 0:14:45
epoch [76/200] batch [30/50] time 0.088 (0.133) data 0.000 (0.048) loss 2.1348 (2.0418) acc 28.1250 (40.7292) lr 1.3971e-03 eta 0:13:46
epoch [76/200] batch [35/50] time 0.085 (0.126) data 0.000 (0.041) loss 1.8027 (2.0326) acc 56.2500 (40.7143) lr 1.3971e-03 eta 0:13:04
epoch [76/200] batch [40/50] time 0.084 (0.121) data 0.000 (0.036) loss 2.1758 (2.0312) acc 31.2500 (40.2344) lr 1.3971e-03 eta 0:12:31
epoch [76/200] batch [45/50] time 0.084 (0.118) data 0.000 (0.033) loss 2.2363 (2.0358) acc 40.6250 (40.5556) lr 1.3971e-03 eta 0:12:12
epoch [76/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.030) loss 1.6377 (2.0109) acc 53.1250 (41.3125) lr 1.3827e-03 eta 0:11:50
epoch [77/200] batch [5/50] time 0.084 (0.376) data 0.000 (0.292) loss 2.3672 (2.1863) acc 21.8750 (30.0000) lr 1.3827e-03 eta 0:38:51
epoch [77/200] batch [10/50] time 0.085 (0.230) data 0.000 (0.146) loss 1.7109 (1.9253) acc 46.8750 (40.6250) lr 1.3827e-03 eta 0:23:46
epoch [77/200] batch [15/50] time 0.087 (0.182) data 0.000 (0.098) loss 2.0117 (1.9240) acc 43.7500 (40.2083) lr 1.3827e-03 eta 0:18:47
epoch [77/200] batch [20/50] time 0.085 (0.158) data 0.000 (0.073) loss 1.9404 (1.9368) acc 43.7500 (42.0312) lr 1.3827e-03 eta 0:16:15
epoch [77/200] batch [25/50] time 0.084 (0.143) data 0.000 (0.059) loss 1.5830 (1.9385) acc 50.0000 (41.3750) lr 1.3827e-03 eta 0:14:44
epoch [77/200] batch [30/50] time 0.087 (0.134) data 0.000 (0.049) loss 2.1875 (1.9405) acc 40.6250 (41.5625) lr 1.3827e-03 eta 0:13:43
epoch [77/200] batch [35/50] time 0.087 (0.132) data 0.001 (0.047) loss 1.7861 (1.9208) acc 43.7500 (42.3214) lr 1.3827e-03 eta 0:13:31
epoch [77/200] batch [40/50] time 0.084 (0.131) data 0.000 (0.047) loss 2.1562 (1.9295) acc 46.8750 (41.9531) lr 1.3827e-03 eta 0:13:28
epoch [77/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.042) loss 2.2148 (1.9419) acc 28.1250 (41.3889) lr 1.3827e-03 eta 0:12:55
epoch [77/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.038) loss 1.9014 (1.9557) acc 40.6250 (41.1875) lr 1.3681e-03 eta 0:12:28
epoch [78/200] batch [5/50] time 0.085 (0.265) data 0.000 (0.181) loss 1.7041 (1.8080) acc 56.2500 (45.0000) lr 1.3681e-03 eta 0:27:11
epoch [78/200] batch [10/50] time 0.084 (0.175) data 0.000 (0.091) loss 2.2480 (1.8664) acc 18.7500 (41.5625) lr 1.3681e-03 eta 0:17:55
epoch [78/200] batch [15/50] time 0.085 (0.146) data 0.000 (0.062) loss 2.8379 (2.0283) acc 21.8750 (38.9583) lr 1.3681e-03 eta 0:14:57
epoch [78/200] batch [20/50] time 0.085 (0.131) data 0.000 (0.046) loss 1.7930 (1.9998) acc 40.6250 (38.9062) lr 1.3681e-03 eta 0:13:22
epoch [78/200] batch [25/50] time 0.085 (0.127) data 0.000 (0.043) loss 2.3984 (2.0319) acc 31.2500 (39.2500) lr 1.3681e-03 eta 0:13:00
epoch [78/200] batch [30/50] time 0.084 (0.126) data 0.000 (0.042) loss 1.5469 (1.9908) acc 50.0000 (39.6875) lr 1.3681e-03 eta 0:12:51
epoch [78/200] batch [35/50] time 0.084 (0.120) data 0.000 (0.036) loss 1.6152 (1.9855) acc 43.7500 (40.4464) lr 1.3681e-03 eta 0:12:13
epoch [78/200] batch [40/50] time 0.082 (0.119) data 0.000 (0.035) loss 2.0273 (1.9931) acc 40.6250 (40.2344) lr 1.3681e-03 eta 0:12:09
epoch [78/200] batch [45/50] time 0.185 (0.118) data 0.103 (0.034) loss 1.8164 (1.9873) acc 46.8750 (40.3472) lr 1.3681e-03 eta 0:11:57
epoch [78/200] batch [50/50] time 0.082 (0.115) data 0.000 (0.031) loss 1.9551 (1.9961) acc 40.6250 (40.0000) lr 1.3535e-03 eta 0:11:40
epoch [79/200] batch [5/50] time 0.085 (0.283) data 0.000 (0.197) loss 1.6113 (1.8207) acc 53.1250 (43.7500) lr 1.3535e-03 eta 0:28:44
epoch [79/200] batch [10/50] time 0.085 (0.184) data 0.000 (0.099) loss 1.8262 (1.8419) acc 43.7500 (46.8750) lr 1.3535e-03 eta 0:18:40
epoch [79/200] batch [15/50] time 0.085 (0.161) data 0.000 (0.076) loss 1.8760 (1.9535) acc 43.7500 (43.5417) lr 1.3535e-03 eta 0:16:20
epoch [79/200] batch [20/50] time 0.086 (0.142) data 0.000 (0.057) loss 2.1094 (1.9684) acc 40.6250 (43.2812) lr 1.3535e-03 eta 0:14:25
epoch [79/200] batch [25/50] time 0.084 (0.141) data 0.000 (0.056) loss 2.2402 (2.0020) acc 34.3750 (42.3750) lr 1.3535e-03 eta 0:14:14
epoch [79/200] batch [30/50] time 0.084 (0.136) data 0.000 (0.050) loss 2.0078 (1.9910) acc 37.5000 (41.8750) lr 1.3535e-03 eta 0:13:44
epoch [79/200] batch [35/50] time 0.085 (0.129) data 0.001 (0.043) loss 2.0898 (1.9855) acc 34.3750 (41.7857) lr 1.3535e-03 eta 0:12:59
epoch [79/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.040) loss 1.9717 (1.9731) acc 28.1250 (41.5625) lr 1.3535e-03 eta 0:12:41
epoch [79/200] batch [45/50] time 0.223 (0.124) data 0.141 (0.039) loss 2.1660 (1.9917) acc 37.5000 (41.2500) lr 1.3535e-03 eta 0:12:31
epoch [79/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.035) loss 2.2910 (1.9975) acc 34.3750 (41.3750) lr 1.3387e-03 eta 0:12:05
epoch [80/200] batch [5/50] time 0.084 (0.309) data 0.000 (0.225) loss 2.4648 (2.1361) acc 28.1250 (33.1250) lr 1.3387e-03 eta 0:31:08
epoch [80/200] batch [10/50] time 0.084 (0.203) data 0.001 (0.119) loss 2.0977 (2.1019) acc 34.3750 (35.3125) lr 1.3387e-03 eta 0:20:23
epoch [80/200] batch [15/50] time 0.085 (0.163) data 0.000 (0.079) loss 2.1016 (2.0590) acc 37.5000 (36.8750) lr 1.3387e-03 eta 0:16:25
epoch [80/200] batch [20/50] time 0.085 (0.144) data 0.000 (0.060) loss 1.5352 (2.0038) acc 62.5000 (39.3750) lr 1.3387e-03 eta 0:14:25
epoch [80/200] batch [25/50] time 0.084 (0.132) data 0.000 (0.048) loss 2.1426 (2.0034) acc 28.1250 (39.3750) lr 1.3387e-03 eta 0:13:14
epoch [80/200] batch [30/50] time 0.088 (0.124) data 0.000 (0.040) loss 1.5459 (1.9782) acc 59.3750 (40.2083) lr 1.3387e-03 eta 0:12:27
epoch [80/200] batch [35/50] time 0.086 (0.118) data 0.001 (0.034) loss 2.2988 (1.9784) acc 37.5000 (40.2679) lr 1.3387e-03 eta 0:11:52
epoch [80/200] batch [40/50] time 0.083 (0.116) data 0.000 (0.032) loss 1.7451 (1.9586) acc 46.8750 (40.8594) lr 1.3387e-03 eta 0:11:38
epoch [80/200] batch [45/50] time 0.084 (0.113) data 0.000 (0.028) loss 1.7646 (1.9676) acc 59.3750 (40.8333) lr 1.3387e-03 eta 0:11:15
epoch [80/200] batch [50/50] time 0.084 (0.113) data 0.000 (0.029) loss 2.0371 (1.9586) acc 43.7500 (41.3750) lr 1.3239e-03 eta 0:11:16
epoch [81/200] batch [5/50] time 0.084 (0.277) data 0.000 (0.193) loss 1.6924 (1.9695) acc 50.0000 (45.0000) lr 1.3239e-03 eta 0:27:43
epoch [81/200] batch [10/50] time 0.084 (0.193) data 0.000 (0.109) loss 2.0957 (1.9865) acc 34.3750 (42.8125) lr 1.3239e-03 eta 0:19:15
epoch [81/200] batch [15/50] time 0.084 (0.157) data 0.000 (0.073) loss 2.2188 (2.0169) acc 34.3750 (41.8750) lr 1.3239e-03 eta 0:15:38
epoch [81/200] batch [20/50] time 0.085 (0.142) data 0.000 (0.057) loss 2.1113 (2.0557) acc 37.5000 (39.8438) lr 1.3239e-03 eta 0:14:06
epoch [81/200] batch [25/50] time 0.085 (0.130) data 0.000 (0.046) loss 2.1387 (2.0161) acc 37.5000 (41.2500) lr 1.3239e-03 eta 0:12:57
epoch [81/200] batch [30/50] time 0.084 (0.127) data 0.000 (0.043) loss 2.5039 (2.0281) acc 28.1250 (41.1458) lr 1.3239e-03 eta 0:12:40
epoch [81/200] batch [35/50] time 0.084 (0.121) data 0.001 (0.037) loss 2.0820 (1.9982) acc 46.8750 (41.6071) lr 1.3239e-03 eta 0:12:03
epoch [81/200] batch [40/50] time 0.083 (0.120) data 0.000 (0.036) loss 2.2305 (2.0035) acc 34.3750 (41.3281) lr 1.3239e-03 eta 0:11:57
epoch [81/200] batch [45/50] time 0.084 (0.118) data 0.000 (0.034) loss 2.0898 (1.9878) acc 31.2500 (41.2500) lr 1.3239e-03 eta 0:11:44
epoch [81/200] batch [50/50] time 0.082 (0.115) data 0.000 (0.031) loss 2.2422 (1.9959) acc 31.2500 (40.6875) lr 1.3090e-03 eta 0:11:22
epoch [82/200] batch [5/50] time 0.084 (0.329) data 0.000 (0.244) loss 2.1797 (2.1020) acc 37.5000 (30.6250) lr 1.3090e-03 eta 0:32:33
epoch [82/200] batch [10/50] time 0.245 (0.222) data 0.162 (0.138) loss 1.6328 (1.9549) acc 59.3750 (42.1875) lr 1.3090e-03 eta 0:22:01
epoch [82/200] batch [15/50] time 0.084 (0.176) data 0.000 (0.092) loss 1.7490 (1.9563) acc 59.3750 (43.7500) lr 1.3090e-03 eta 0:17:26
epoch [82/200] batch [20/50] time 0.086 (0.162) data 0.000 (0.078) loss 2.1875 (2.0059) acc 34.3750 (42.0312) lr 1.3090e-03 eta 0:16:00
epoch [82/200] batch [25/50] time 0.085 (0.147) data 0.000 (0.062) loss 2.1660 (1.9874) acc 40.6250 (42.0000) lr 1.3090e-03 eta 0:14:28
epoch [82/200] batch [30/50] time 0.084 (0.143) data 0.000 (0.059) loss 1.9365 (1.9964) acc 40.6250 (40.6250) lr 1.3090e-03 eta 0:14:07
epoch [82/200] batch [35/50] time 0.085 (0.140) data 0.001 (0.056) loss 1.8760 (2.0121) acc 40.6250 (40.5357) lr 1.3090e-03 eta 0:13:46
epoch [82/200] batch [40/50] time 0.084 (0.133) data 0.000 (0.049) loss 2.0156 (2.0272) acc 43.7500 (39.8438) lr 1.3090e-03 eta 0:13:04
epoch [82/200] batch [45/50] time 0.084 (0.128) data 0.000 (0.044) loss 2.2246 (2.0451) acc 43.7500 (39.8611) lr 1.3090e-03 eta 0:12:38
epoch [82/200] batch [50/50] time 0.083 (0.124) data 0.000 (0.040) loss 1.6494 (2.0319) acc 56.2500 (40.0625) lr 1.2940e-03 eta 0:12:11
epoch [83/200] batch [5/50] time 0.084 (0.280) data 0.000 (0.195) loss 1.9365 (1.8664) acc 50.0000 (46.2500) lr 1.2940e-03 eta 0:27:29
epoch [83/200] batch [10/50] time 0.085 (0.186) data 0.000 (0.101) loss 1.7148 (1.9410) acc 50.0000 (44.0625) lr 1.2940e-03 eta 0:18:16
epoch [83/200] batch [15/50] time 0.085 (0.156) data 0.001 (0.071) loss 2.2305 (1.9805) acc 31.2500 (42.5000) lr 1.2940e-03 eta 0:15:15
epoch [83/200] batch [20/50] time 0.086 (0.145) data 0.000 (0.060) loss 1.6377 (1.9509) acc 46.8750 (42.1875) lr 1.2940e-03 eta 0:14:10
epoch [83/200] batch [25/50] time 0.301 (0.141) data 0.216 (0.057) loss 2.1777 (1.9604) acc 37.5000 (42.5000) lr 1.2940e-03 eta 0:13:50
epoch [83/200] batch [30/50] time 0.084 (0.132) data 0.000 (0.047) loss 1.9648 (1.9635) acc 43.7500 (41.8750) lr 1.2940e-03 eta 0:12:53
epoch [83/200] batch [35/50] time 0.084 (0.130) data 0.000 (0.046) loss 1.6104 (1.9498) acc 65.6250 (43.1250) lr 1.2940e-03 eta 0:12:42
epoch [83/200] batch [40/50] time 0.084 (0.124) data 0.000 (0.040) loss 2.2832 (1.9703) acc 34.3750 (42.9688) lr 1.2940e-03 eta 0:12:08
epoch [83/200] batch [45/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.9180 (1.9396) acc 46.8750 (43.6806) lr 1.2940e-03 eta 0:11:41
epoch [83/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.032) loss 2.0742 (1.9602) acc 31.2500 (42.8125) lr 1.2790e-03 eta 0:11:19
epoch [84/200] batch [5/50] time 0.083 (0.302) data 0.000 (0.218) loss 1.6934 (1.9773) acc 65.6250 (45.0000) lr 1.2790e-03 eta 0:29:25
epoch [84/200] batch [10/50] time 0.205 (0.205) data 0.122 (0.122) loss 1.9688 (1.9690) acc 43.7500 (46.2500) lr 1.2790e-03 eta 0:19:59
epoch [84/200] batch [15/50] time 0.084 (0.165) data 0.000 (0.081) loss 1.9414 (1.9260) acc 46.8750 (45.8333) lr 1.2790e-03 eta 0:16:02
epoch [84/200] batch [20/50] time 0.085 (0.153) data 0.000 (0.069) loss 2.1133 (1.9252) acc 37.5000 (45.1562) lr 1.2790e-03 eta 0:14:51
epoch [84/200] batch [25/50] time 0.085 (0.139) data 0.000 (0.055) loss 1.7334 (1.9571) acc 50.0000 (43.5000) lr 1.2790e-03 eta 0:13:31
epoch [84/200] batch [30/50] time 0.085 (0.137) data 0.000 (0.052) loss 2.2422 (1.9615) acc 37.5000 (43.7500) lr 1.2790e-03 eta 0:13:15
epoch [84/200] batch [35/50] time 0.085 (0.129) data 0.000 (0.045) loss 2.1211 (1.9613) acc 34.3750 (44.1071) lr 1.2790e-03 eta 0:12:31
epoch [84/200] batch [40/50] time 0.085 (0.124) data 0.000 (0.039) loss 2.3027 (2.0083) acc 34.3750 (42.3438) lr 1.2790e-03 eta 0:11:58
epoch [84/200] batch [45/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.4561 (2.0027) acc 56.2500 (42.2222) lr 1.2790e-03 eta 0:11:39
epoch [84/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.033) loss 1.8701 (2.0161) acc 43.7500 (41.5000) lr 1.2639e-03 eta 0:11:17
epoch [85/200] batch [5/50] time 0.086 (0.307) data 0.000 (0.221) loss 1.6064 (1.7602) acc 53.1250 (40.6250) lr 1.2639e-03 eta 0:29:36
epoch [85/200] batch [10/50] time 0.086 (0.200) data 0.000 (0.115) loss 1.8818 (1.9786) acc 40.6250 (38.1250) lr 1.2639e-03 eta 0:19:18
epoch [85/200] batch [15/50] time 0.087 (0.163) data 0.000 (0.077) loss 1.8848 (1.9778) acc 46.8750 (40.6250) lr 1.2639e-03 eta 0:15:40
epoch [85/200] batch [20/50] time 0.085 (0.153) data 0.001 (0.067) loss 2.3086 (2.0203) acc 34.3750 (40.1562) lr 1.2639e-03 eta 0:14:41
epoch [85/200] batch [25/50] time 0.086 (0.139) data 0.000 (0.054) loss 2.2246 (1.9947) acc 37.5000 (41.0000) lr 1.2639e-03 eta 0:13:23
epoch [85/200] batch [30/50] time 0.085 (0.134) data 0.000 (0.049) loss 2.1465 (1.9995) acc 46.8750 (41.5625) lr 1.2639e-03 eta 0:12:51
epoch [85/200] batch [35/50] time 0.119 (0.131) data 0.035 (0.046) loss 1.8633 (1.9907) acc 53.1250 (42.0536) lr 1.2639e-03 eta 0:12:34
epoch [85/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.3516 (2.0054) acc 25.0000 (41.0156) lr 1.2639e-03 eta 0:11:59
epoch [85/200] batch [45/50] time 0.086 (0.126) data 0.000 (0.041) loss 1.9297 (2.0088) acc 43.7500 (40.6944) lr 1.2639e-03 eta 0:12:04
epoch [85/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.037) loss 1.9111 (2.0187) acc 53.1250 (40.3125) lr 1.2487e-03 eta 0:11:39
epoch [86/200] batch [5/50] time 0.085 (0.298) data 0.000 (0.213) loss 2.2031 (1.9043) acc 46.8750 (45.0000) lr 1.2487e-03 eta 0:28:33
epoch [86/200] batch [10/50] time 0.179 (0.208) data 0.095 (0.124) loss 2.5410 (2.0054) acc 25.0000 (41.5625) lr 1.2487e-03 eta 0:19:55
epoch [86/200] batch [15/50] time 0.084 (0.167) data 0.000 (0.083) loss 1.8447 (1.9698) acc 37.5000 (42.2917) lr 1.2487e-03 eta 0:15:56
epoch [86/200] batch [20/50] time 0.085 (0.150) data 0.001 (0.066) loss 1.6875 (1.9910) acc 53.1250 (41.0938) lr 1.2487e-03 eta 0:14:18
epoch [86/200] batch [25/50] time 0.086 (0.137) data 0.000 (0.053) loss 1.8652 (1.9961) acc 40.6250 (40.0000) lr 1.2487e-03 eta 0:13:03
epoch [86/200] batch [30/50] time 0.086 (0.128) data 0.000 (0.044) loss 1.7480 (1.9883) acc 43.7500 (40.6250) lr 1.2487e-03 eta 0:12:13
epoch [86/200] batch [35/50] time 0.084 (0.122) data 0.000 (0.038) loss 2.4785 (2.0099) acc 34.3750 (40.5357) lr 1.2487e-03 eta 0:11:36
epoch [86/200] batch [40/50] time 0.084 (0.119) data 0.000 (0.035) loss 1.9717 (1.9975) acc 40.6250 (40.3906) lr 1.2487e-03 eta 0:11:19
epoch [86/200] batch [45/50] time 0.084 (0.115) data 0.000 (0.031) loss 2.1484 (2.0162) acc 31.2500 (39.7917) lr 1.2487e-03 eta 0:10:56
epoch [86/200] batch [50/50] time 0.084 (0.112) data 0.000 (0.028) loss 2.0391 (2.0220) acc 40.6250 (39.6250) lr 1.2334e-03 eta 0:10:37
epoch [87/200] batch [5/50] time 0.084 (0.308) data 0.001 (0.223) loss 2.1738 (2.0016) acc 46.8750 (45.6250) lr 1.2334e-03 eta 0:29:13
epoch [87/200] batch [10/50] time 0.084 (0.196) data 0.000 (0.112) loss 1.7529 (1.9729) acc 53.1250 (45.0000) lr 1.2334e-03 eta 0:18:35
epoch [87/200] batch [15/50] time 0.086 (0.159) data 0.000 (0.075) loss 1.5820 (1.9841) acc 50.0000 (44.1667) lr 1.2334e-03 eta 0:15:05
epoch [87/200] batch [20/50] time 0.085 (0.141) data 0.000 (0.056) loss 1.8408 (1.9573) acc 40.6250 (44.0625) lr 1.2334e-03 eta 0:13:19
epoch [87/200] batch [25/50] time 0.085 (0.130) data 0.000 (0.045) loss 2.1289 (1.9729) acc 37.5000 (43.5000) lr 1.2334e-03 eta 0:12:15
epoch [87/200] batch [30/50] time 0.084 (0.122) data 0.000 (0.038) loss 2.6211 (1.9712) acc 25.0000 (43.1250) lr 1.2334e-03 eta 0:11:31
epoch [87/200] batch [35/50] time 0.084 (0.117) data 0.000 (0.032) loss 1.7148 (1.9503) acc 53.1250 (43.5714) lr 1.2334e-03 eta 0:11:00
epoch [87/200] batch [40/50] time 0.083 (0.117) data 0.000 (0.032) loss 2.3809 (1.9690) acc 43.7500 (43.0469) lr 1.2334e-03 eta 0:10:59
epoch [87/200] batch [45/50] time 0.084 (0.115) data 0.000 (0.031) loss 1.8379 (1.9542) acc 46.8750 (42.9861) lr 1.2334e-03 eta 0:10:53
epoch [87/200] batch [50/50] time 0.083 (0.112) data 0.000 (0.028) loss 2.2754 (1.9676) acc 37.5000 (42.3125) lr 1.2181e-03 eta 0:10:34
epoch [88/200] batch [5/50] time 0.086 (0.330) data 0.000 (0.245) loss 1.4395 (1.8090) acc 65.6250 (45.6250) lr 1.2181e-03 eta 0:31:03
epoch [88/200] batch [10/50] time 0.085 (0.208) data 0.000 (0.123) loss 1.9727 (1.9445) acc 43.7500 (42.8125) lr 1.2181e-03 eta 0:19:30
epoch [88/200] batch [15/50] time 0.084 (0.178) data 0.000 (0.094) loss 2.1836 (1.9589) acc 37.5000 (40.6250) lr 1.2181e-03 eta 0:16:44
epoch [88/200] batch [20/50] time 0.085 (0.159) data 0.000 (0.075) loss 1.9248 (1.9719) acc 40.6250 (41.4062) lr 1.2181e-03 eta 0:14:57
epoch [88/200] batch [25/50] time 0.084 (0.144) data 0.000 (0.060) loss 2.0977 (2.0115) acc 46.8750 (41.2500) lr 1.2181e-03 eta 0:13:32
epoch [88/200] batch [30/50] time 0.085 (0.141) data 0.000 (0.057) loss 1.8213 (2.0393) acc 46.8750 (40.9375) lr 1.2181e-03 eta 0:13:15
epoch [88/200] batch [35/50] time 0.290 (0.139) data 0.207 (0.055) loss 2.0508 (2.0331) acc 31.2500 (40.9821) lr 1.2181e-03 eta 0:13:01
epoch [88/200] batch [40/50] time 0.083 (0.132) data 0.000 (0.048) loss 1.9180 (2.0165) acc 43.7500 (41.4844) lr 1.2181e-03 eta 0:12:22
epoch [88/200] batch [45/50] time 0.083 (0.129) data 0.000 (0.045) loss 2.2422 (2.0269) acc 34.3750 (40.9722) lr 1.2181e-03 eta 0:12:04
epoch [88/200] batch [50/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.7510 (2.0331) acc 56.2500 (41.1250) lr 1.2028e-03 eta 0:11:37
epoch [89/200] batch [5/50] time 0.084 (0.266) data 0.000 (0.182) loss 2.1133 (1.9354) acc 43.7500 (40.0000) lr 1.2028e-03 eta 0:24:48
epoch [89/200] batch [10/50] time 0.083 (0.190) data 0.000 (0.106) loss 2.2520 (1.9463) acc 40.6250 (42.1875) lr 1.2028e-03 eta 0:17:43
epoch [89/200] batch [15/50] time 0.084 (0.155) data 0.000 (0.071) loss 1.8984 (1.9443) acc 37.5000 (42.2917) lr 1.2028e-03 eta 0:14:23
epoch [89/200] batch [20/50] time 0.084 (0.137) data 0.000 (0.053) loss 2.3184 (1.9756) acc 28.1250 (41.7188) lr 1.2028e-03 eta 0:12:44
epoch [89/200] batch [25/50] time 0.084 (0.126) data 0.000 (0.043) loss 2.1484 (1.9661) acc 21.8750 (41.0000) lr 1.2028e-03 eta 0:11:44
epoch [89/200] batch [30/50] time 0.085 (0.122) data 0.000 (0.039) loss 2.1348 (1.9652) acc 50.0000 (41.5625) lr 1.2028e-03 eta 0:11:21
epoch [89/200] batch [35/50] time 0.084 (0.117) data 0.000 (0.033) loss 2.0684 (1.9642) acc 34.3750 (41.7857) lr 1.2028e-03 eta 0:10:50
epoch [89/200] batch [40/50] time 0.083 (0.114) data 0.000 (0.031) loss 1.7305 (1.9556) acc 50.0000 (42.0312) lr 1.2028e-03 eta 0:10:35
epoch [89/200] batch [45/50] time 0.241 (0.115) data 0.158 (0.031) loss 1.8330 (1.9515) acc 53.1250 (42.4306) lr 1.2028e-03 eta 0:10:36
epoch [89/200] batch [50/50] time 0.083 (0.111) data 0.000 (0.028) loss 2.1641 (1.9476) acc 40.6250 (42.6875) lr 1.1874e-03 eta 0:10:18
epoch [90/200] batch [5/50] time 0.085 (0.274) data 0.000 (0.189) loss 1.7969 (1.9332) acc 50.0000 (45.6250) lr 1.1874e-03 eta 0:25:22
epoch [90/200] batch [10/50] time 0.146 (0.194) data 0.060 (0.108) loss 1.6279 (1.9023) acc 46.8750 (44.6875) lr 1.1874e-03 eta 0:17:53
epoch [90/200] batch [15/50] time 0.086 (0.158) data 0.000 (0.072) loss 2.1074 (1.9887) acc 53.1250 (43.5417) lr 1.1874e-03 eta 0:14:33
epoch [90/200] batch [20/50] time 0.086 (0.146) data 0.000 (0.060) loss 1.8906 (1.9715) acc 40.6250 (42.8125) lr 1.1874e-03 eta 0:13:24
epoch [90/200] batch [25/50] time 0.084 (0.133) data 0.000 (0.048) loss 1.5840 (1.9714) acc 53.1250 (43.3750) lr 1.1874e-03 eta 0:12:16
epoch [90/200] batch [30/50] time 0.085 (0.128) data 0.000 (0.043) loss 2.0723 (1.9440) acc 31.2500 (43.0208) lr 1.1874e-03 eta 0:11:45
epoch [90/200] batch [35/50] time 0.084 (0.124) data 0.000 (0.039) loss 1.9268 (1.9460) acc 28.1250 (42.1429) lr 1.1874e-03 eta 0:11:25
epoch [90/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.035) loss 2.0684 (1.9522) acc 34.3750 (41.7969) lr 1.1874e-03 eta 0:10:59
epoch [90/200] batch [45/50] time 0.269 (0.120) data 0.185 (0.035) loss 1.6318 (1.9419) acc 50.0000 (42.0833) lr 1.1874e-03 eta 0:10:59
epoch [90/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.032) loss 1.9775 (1.9476) acc 40.6250 (41.8125) lr 1.1719e-03 eta 0:10:38
epoch [91/200] batch [5/50] time 0.240 (0.292) data 0.156 (0.207) loss 1.6875 (2.0014) acc 53.1250 (39.3750) lr 1.1719e-03 eta 0:26:42
epoch [91/200] batch [10/50] time 0.086 (0.188) data 0.000 (0.104) loss 2.0547 (2.0146) acc 46.8750 (41.5625) lr 1.1719e-03 eta 0:17:13
epoch [91/200] batch [15/50] time 0.085 (0.164) data 0.000 (0.079) loss 1.7695 (1.9648) acc 40.6250 (42.7083) lr 1.1719e-03 eta 0:14:57
epoch [91/200] batch [20/50] time 0.084 (0.144) data 0.000 (0.059) loss 2.0547 (2.0088) acc 37.5000 (41.7188) lr 1.1719e-03 eta 0:13:08
epoch [91/200] batch [25/50] time 0.084 (0.139) data 0.000 (0.054) loss 2.0508 (2.0046) acc 37.5000 (41.0000) lr 1.1719e-03 eta 0:12:38
epoch [91/200] batch [30/50] time 0.102 (0.134) data 0.018 (0.050) loss 2.8242 (2.0230) acc 25.0000 (40.0000) lr 1.1719e-03 eta 0:12:12
epoch [91/200] batch [35/50] time 0.084 (0.127) data 0.000 (0.043) loss 2.3145 (2.0068) acc 34.3750 (40.8929) lr 1.1719e-03 eta 0:11:33
epoch [91/200] batch [40/50] time 0.084 (0.128) data 0.000 (0.044) loss 1.6797 (2.0006) acc 56.2500 (41.0938) lr 1.1719e-03 eta 0:11:41
epoch [91/200] batch [45/50] time 0.084 (0.123) data 0.000 (0.039) loss 2.1641 (2.0051) acc 34.3750 (40.5556) lr 1.1719e-03 eta 0:11:13
epoch [91/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 1.7021 (1.9910) acc 53.1250 (40.8125) lr 1.1564e-03 eta 0:10:50
epoch [92/200] batch [5/50] time 0.085 (0.280) data 0.000 (0.195) loss 2.1406 (1.8490) acc 31.2500 (45.6250) lr 1.1564e-03 eta 0:25:23
epoch [92/200] batch [10/50] time 0.085 (0.182) data 0.000 (0.098) loss 1.7646 (1.8612) acc 53.1250 (47.5000) lr 1.1564e-03 eta 0:16:30
epoch [92/200] batch [15/50] time 0.084 (0.156) data 0.000 (0.071) loss 1.7812 (1.9120) acc 56.2500 (46.4583) lr 1.1564e-03 eta 0:14:05
epoch [92/200] batch [20/50] time 0.207 (0.144) data 0.124 (0.060) loss 1.8496 (1.9192) acc 50.0000 (44.3750) lr 1.1564e-03 eta 0:13:01
epoch [92/200] batch [25/50] time 0.084 (0.132) data 0.000 (0.048) loss 1.8555 (1.9433) acc 40.6250 (43.6250) lr 1.1564e-03 eta 0:11:55
epoch [92/200] batch [30/50] time 0.085 (0.127) data 0.000 (0.043) loss 2.0078 (1.9810) acc 34.3750 (42.0833) lr 1.1564e-03 eta 0:11:29
epoch [92/200] batch [35/50] time 0.085 (0.121) data 0.000 (0.037) loss 1.9199 (1.9879) acc 40.6250 (42.2321) lr 1.1564e-03 eta 0:10:56
epoch [92/200] batch [40/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.7832 (1.9869) acc 50.0000 (42.6562) lr 1.1564e-03 eta 0:10:56
epoch [92/200] batch [45/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.7451 (1.9779) acc 46.8750 (42.9861) lr 1.1564e-03 eta 0:10:48
epoch [92/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.032) loss 1.9609 (1.9655) acc 46.8750 (43.3125) lr 1.1409e-03 eta 0:10:27
epoch [93/200] batch [5/50] time 0.085 (0.276) data 0.000 (0.191) loss 2.2422 (2.2086) acc 43.7500 (38.1250) lr 1.1409e-03 eta 0:24:48
epoch [93/200] batch [10/50] time 0.086 (0.189) data 0.000 (0.104) loss 1.9277 (2.0665) acc 40.6250 (40.9375) lr 1.1409e-03 eta 0:17:00
epoch [93/200] batch [15/50] time 0.085 (0.159) data 0.000 (0.074) loss 1.6943 (2.0234) acc 43.7500 (40.2083) lr 1.1409e-03 eta 0:14:16
epoch [93/200] batch [20/50] time 0.231 (0.148) data 0.148 (0.063) loss 2.2520 (2.0184) acc 37.5000 (40.1562) lr 1.1409e-03 eta 0:13:16
epoch [93/200] batch [25/50] time 0.084 (0.135) data 0.000 (0.051) loss 1.7441 (1.9963) acc 46.8750 (41.2500) lr 1.1409e-03 eta 0:12:08
epoch [93/200] batch [30/50] time 0.084 (0.131) data 0.000 (0.046) loss 2.1250 (2.0318) acc 28.1250 (40.0000) lr 1.1409e-03 eta 0:11:43
epoch [93/200] batch [35/50] time 0.084 (0.124) data 0.000 (0.040) loss 1.8672 (2.0143) acc 46.8750 (40.6250) lr 1.1409e-03 eta 0:11:06
epoch [93/200] batch [40/50] time 0.084 (0.124) data 0.000 (0.040) loss 1.6973 (2.0354) acc 50.0000 (40.7031) lr 1.1409e-03 eta 0:11:06
epoch [93/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.040) loss 1.8379 (2.0394) acc 46.8750 (40.4861) lr 1.1409e-03 eta 0:11:08
epoch [93/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.036) loss 1.4580 (2.0169) acc 56.2500 (40.8750) lr 1.1253e-03 eta 0:10:45
epoch [94/200] batch [5/50] time 0.084 (0.347) data 0.000 (0.263) loss 2.1992 (1.8963) acc 40.6250 (42.5000) lr 1.1253e-03 eta 0:30:55
epoch [94/200] batch [10/50] time 0.085 (0.216) data 0.000 (0.132) loss 1.8154 (1.9039) acc 50.0000 (43.7500) lr 1.1253e-03 eta 0:19:12
epoch [94/200] batch [15/50] time 0.084 (0.178) data 0.000 (0.093) loss 1.8223 (1.9391) acc 40.6250 (42.2917) lr 1.1253e-03 eta 0:15:47
epoch [94/200] batch [20/50] time 0.272 (0.164) data 0.188 (0.080) loss 1.8379 (1.9667) acc 43.7500 (43.2812) lr 1.1253e-03 eta 0:14:33
epoch [94/200] batch [25/50] time 0.086 (0.148) data 0.000 (0.064) loss 2.3125 (1.9849) acc 31.2500 (42.8750) lr 1.1253e-03 eta 0:13:09
epoch [94/200] batch [30/50] time 0.086 (0.142) data 0.000 (0.058) loss 2.0449 (1.9972) acc 40.6250 (42.6042) lr 1.1253e-03 eta 0:12:36
epoch [94/200] batch [35/50] time 0.086 (0.134) data 0.001 (0.049) loss 2.3281 (2.0284) acc 31.2500 (41.9643) lr 1.1253e-03 eta 0:11:52
epoch [94/200] batch [40/50] time 0.083 (0.130) data 0.000 (0.046) loss 1.8027 (2.0056) acc 37.5000 (42.3438) lr 1.1253e-03 eta 0:11:32
epoch [94/200] batch [45/50] time 0.083 (0.129) data 0.000 (0.045) loss 2.1055 (2.0172) acc 43.7500 (41.9444) lr 1.1253e-03 eta 0:11:24
epoch [94/200] batch [50/50] time 0.083 (0.124) data 0.000 (0.040) loss 1.5811 (2.0031) acc 46.8750 (42.0625) lr 1.1097e-03 eta 0:10:59
epoch [95/200] batch [5/50] time 0.085 (0.298) data 0.001 (0.213) loss 1.6475 (1.9064) acc 50.0000 (44.3750) lr 1.1097e-03 eta 0:26:18
epoch [95/200] batch [10/50] time 0.085 (0.215) data 0.000 (0.130) loss 2.0059 (1.9949) acc 31.2500 (41.5625) lr 1.1097e-03 eta 0:18:58
epoch [95/200] batch [15/50] time 0.084 (0.172) data 0.000 (0.087) loss 2.0098 (1.9524) acc 46.8750 (42.0833) lr 1.1097e-03 eta 0:15:07
epoch [95/200] batch [20/50] time 0.085 (0.150) data 0.000 (0.065) loss 1.6758 (1.9088) acc 59.3750 (43.2812) lr 1.1097e-03 eta 0:13:11
epoch [95/200] batch [25/50] time 0.086 (0.137) data 0.000 (0.052) loss 1.7461 (1.9375) acc 53.1250 (42.7500) lr 1.1097e-03 eta 0:12:02
epoch [95/200] batch [30/50] time 0.085 (0.128) data 0.000 (0.044) loss 2.4258 (1.9912) acc 25.0000 (40.8333) lr 1.1097e-03 eta 0:11:16
epoch [95/200] batch [35/50] time 0.189 (0.125) data 0.106 (0.040) loss 1.4727 (1.9849) acc 53.1250 (40.2679) lr 1.1097e-03 eta 0:10:58
epoch [95/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.035) loss 1.5400 (1.9759) acc 59.3750 (40.6250) lr 1.1097e-03 eta 0:10:30
epoch [95/200] batch [45/50] time 0.083 (0.118) data 0.000 (0.034) loss 1.8848 (1.9850) acc 37.5000 (40.9028) lr 1.1097e-03 eta 0:10:18
epoch [95/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.030) loss 1.6592 (1.9702) acc 46.8750 (40.8125) lr 1.0941e-03 eta 0:09:59
epoch [96/200] batch [5/50] time 0.086 (0.325) data 0.000 (0.239) loss 2.2070 (2.0846) acc 34.3750 (36.2500) lr 1.0941e-03 eta 0:28:24
epoch [96/200] batch [10/50] time 0.085 (0.210) data 0.000 (0.124) loss 1.8555 (1.9935) acc 50.0000 (41.8750) lr 1.0941e-03 eta 0:18:18
epoch [96/200] batch [15/50] time 0.086 (0.168) data 0.001 (0.083) loss 1.9658 (1.9805) acc 43.7500 (43.7500) lr 1.0941e-03 eta 0:14:41
epoch [96/200] batch [20/50] time 0.085 (0.154) data 0.000 (0.069) loss 2.1836 (2.0403) acc 40.6250 (41.7188) lr 1.0941e-03 eta 0:13:24
epoch [96/200] batch [25/50] time 0.187 (0.144) data 0.102 (0.059) loss 2.3652 (2.0422) acc 25.0000 (40.5000) lr 1.0941e-03 eta 0:12:33
epoch [96/200] batch [30/50] time 0.085 (0.134) data 0.000 (0.049) loss 2.2070 (2.0332) acc 37.5000 (40.8333) lr 1.0941e-03 eta 0:11:41
epoch [96/200] batch [35/50] time 0.085 (0.127) data 0.001 (0.042) loss 1.7705 (2.0293) acc 56.2500 (41.2500) lr 1.0941e-03 eta 0:11:04
epoch [96/200] batch [40/50] time 0.084 (0.122) data 0.000 (0.037) loss 2.3672 (2.0155) acc 28.1250 (41.1719) lr 1.0941e-03 eta 0:10:35
epoch [96/200] batch [45/50] time 0.085 (0.118) data 0.000 (0.033) loss 1.8643 (2.0072) acc 43.7500 (41.3889) lr 1.0941e-03 eta 0:10:13
epoch [96/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.030) loss 1.8604 (2.0135) acc 56.2500 (41.5000) lr 1.0785e-03 eta 0:09:54
epoch [97/200] batch [5/50] time 0.085 (0.292) data 0.000 (0.207) loss 1.9990 (2.1023) acc 34.3750 (35.6250) lr 1.0785e-03 eta 0:25:16
epoch [97/200] batch [10/50] time 0.084 (0.198) data 0.000 (0.114) loss 1.9238 (1.9963) acc 34.3750 (38.7500) lr 1.0785e-03 eta 0:17:09
epoch [97/200] batch [15/50] time 0.085 (0.162) data 0.000 (0.078) loss 1.8467 (1.9815) acc 46.8750 (40.6250) lr 1.0785e-03 eta 0:14:00
epoch [97/200] batch [20/50] time 0.084 (0.149) data 0.000 (0.065) loss 1.5400 (1.9063) acc 53.1250 (42.5000) lr 1.0785e-03 eta 0:12:53
epoch [97/200] batch [25/50] time 0.084 (0.136) data 0.000 (0.052) loss 2.0254 (1.9438) acc 46.8750 (41.7500) lr 1.0785e-03 eta 0:11:45
epoch [97/200] batch [30/50] time 0.084 (0.134) data 0.000 (0.050) loss 2.2559 (1.9604) acc 31.2500 (42.0833) lr 1.0785e-03 eta 0:11:34
epoch [97/200] batch [35/50] time 0.250 (0.132) data 0.167 (0.048) loss 1.8770 (1.9479) acc 43.7500 (41.9643) lr 1.0785e-03 eta 0:11:21
epoch [97/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.042) loss 2.4629 (1.9567) acc 18.7500 (41.7969) lr 1.0785e-03 eta 0:10:50
epoch [97/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.0312 (1.9633) acc 50.0000 (41.7361) lr 1.0785e-03 eta 0:10:38
epoch [97/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.7939 (1.9741) acc 50.0000 (41.3750) lr 1.0628e-03 eta 0:10:16
epoch [98/200] batch [5/50] time 0.085 (0.303) data 0.000 (0.218) loss 1.4023 (1.8641) acc 56.2500 (48.7500) lr 1.0628e-03 eta 0:25:59
epoch [98/200] batch [10/50] time 0.087 (0.195) data 0.001 (0.109) loss 2.0977 (1.9216) acc 40.6250 (43.7500) lr 1.0628e-03 eta 0:16:39
epoch [98/200] batch [15/50] time 0.086 (0.158) data 0.001 (0.073) loss 2.0801 (1.9118) acc 34.3750 (43.9583) lr 1.0628e-03 eta 0:13:33
epoch [98/200] batch [20/50] time 0.086 (0.140) data 0.000 (0.055) loss 2.3301 (1.9708) acc 43.7500 (42.5000) lr 1.0628e-03 eta 0:11:59
epoch [98/200] batch [25/50] time 0.086 (0.130) data 0.000 (0.045) loss 2.1562 (1.9340) acc 40.6250 (44.0000) lr 1.0628e-03 eta 0:11:06
epoch [98/200] batch [30/50] time 0.086 (0.123) data 0.000 (0.037) loss 2.0391 (1.9690) acc 43.7500 (43.1250) lr 1.0628e-03 eta 0:10:27
epoch [98/200] batch [35/50] time 0.086 (0.121) data 0.001 (0.036) loss 2.0918 (1.9734) acc 37.5000 (42.4107) lr 1.0628e-03 eta 0:10:19
epoch [98/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.035) loss 1.8730 (1.9999) acc 40.6250 (41.1719) lr 1.0628e-03 eta 0:10:12
epoch [98/200] batch [45/50] time 0.084 (0.116) data 0.001 (0.031) loss 2.1191 (1.9852) acc 28.1250 (41.3889) lr 1.0628e-03 eta 0:09:51
epoch [98/200] batch [50/50] time 0.084 (0.114) data 0.000 (0.029) loss 2.0801 (1.9734) acc 34.3750 (41.1250) lr 1.0471e-03 eta 0:09:39
epoch [99/200] batch [5/50] time 0.085 (0.264) data 0.000 (0.179) loss 2.2383 (1.9809) acc 37.5000 (36.8750) lr 1.0471e-03 eta 0:22:26
epoch [99/200] batch [10/50] time 0.086 (0.175) data 0.000 (0.090) loss 1.9326 (1.8958) acc 50.0000 (42.1875) lr 1.0471e-03 eta 0:14:50
epoch [99/200] batch [15/50] time 0.085 (0.145) data 0.000 (0.060) loss 2.0195 (1.9459) acc 46.8750 (41.2500) lr 1.0471e-03 eta 0:12:19
epoch [99/200] batch [20/50] time 0.084 (0.130) data 0.000 (0.045) loss 2.0352 (1.9322) acc 34.3750 (41.0938) lr 1.0471e-03 eta 0:11:01
epoch [99/200] batch [25/50] time 0.086 (0.123) data 0.000 (0.038) loss 1.8115 (1.9694) acc 53.1250 (40.7500) lr 1.0471e-03 eta 0:10:24
epoch [99/200] batch [30/50] time 0.084 (0.117) data 0.000 (0.032) loss 1.5977 (1.9633) acc 46.8750 (40.9375) lr 1.0471e-03 eta 0:09:52
epoch [99/200] batch [35/50] time 0.084 (0.112) data 0.000 (0.027) loss 1.9404 (1.9547) acc 40.6250 (41.3393) lr 1.0471e-03 eta 0:09:28
epoch [99/200] batch [40/50] time 0.085 (0.109) data 0.000 (0.024) loss 2.5977 (1.9610) acc 25.0000 (41.4062) lr 1.0471e-03 eta 0:09:10
epoch [99/200] batch [45/50] time 0.085 (0.109) data 0.000 (0.024) loss 1.6514 (1.9689) acc 46.8750 (41.7361) lr 1.0471e-03 eta 0:09:08
epoch [99/200] batch [50/50] time 0.083 (0.107) data 0.000 (0.022) loss 2.1777 (1.9780) acc 37.5000 (41.4375) lr 1.0314e-03 eta 0:09:00
epoch [100/200] batch [5/50] time 0.085 (0.267) data 0.000 (0.182) loss 1.8594 (2.0525) acc 43.7500 (36.8750) lr 1.0314e-03 eta 0:22:24
epoch [100/200] batch [10/50] time 0.085 (0.176) data 0.000 (0.091) loss 2.2461 (1.9399) acc 40.6250 (40.0000) lr 1.0314e-03 eta 0:14:46
epoch [100/200] batch [15/50] time 0.085 (0.146) data 0.000 (0.061) loss 1.9111 (1.9572) acc 37.5000 (40.0000) lr 1.0314e-03 eta 0:12:13
epoch [100/200] batch [20/50] time 0.085 (0.131) data 0.000 (0.047) loss 1.8916 (2.0203) acc 37.5000 (39.3750) lr 1.0314e-03 eta 0:11:00
epoch [100/200] batch [25/50] time 0.272 (0.130) data 0.188 (0.045) loss 1.7852 (1.9860) acc 56.2500 (41.1250) lr 1.0314e-03 eta 0:10:51
epoch [100/200] batch [30/50] time 0.085 (0.122) data 0.000 (0.037) loss 2.2051 (1.9880) acc 40.6250 (40.7292) lr 1.0314e-03 eta 0:10:13
epoch [100/200] batch [35/50] time 0.085 (0.117) data 0.001 (0.032) loss 2.1074 (1.9811) acc 43.7500 (41.4286) lr 1.0314e-03 eta 0:09:45
epoch [100/200] batch [40/50] time 0.083 (0.113) data 0.000 (0.028) loss 1.6113 (1.9942) acc 53.1250 (41.1719) lr 1.0314e-03 eta 0:09:25
epoch [100/200] batch [45/50] time 0.083 (0.110) data 0.000 (0.025) loss 2.2734 (1.9922) acc 31.2500 (40.9722) lr 1.0314e-03 eta 0:09:08
epoch [100/200] batch [50/50] time 0.083 (0.107) data 0.000 (0.023) loss 2.3828 (1.9812) acc 37.5000 (41.6250) lr 1.0157e-03 eta 0:08:54
epoch [101/200] batch [5/50] time 0.084 (0.310) data 0.000 (0.226) loss 1.9551 (1.9568) acc 37.5000 (37.5000) lr 1.0157e-03 eta 0:25:48
epoch [101/200] batch [10/50] time 0.177 (0.206) data 0.094 (0.123) loss 1.8926 (2.0468) acc 43.7500 (36.8750) lr 1.0157e-03 eta 0:17:08
epoch [101/200] batch [15/50] time 0.084 (0.165) data 0.000 (0.082) loss 2.1172 (2.0537) acc 28.1250 (39.5833) lr 1.0157e-03 eta 0:13:44
epoch [101/200] batch [20/50] time 0.083 (0.151) data 0.000 (0.068) loss 2.0059 (2.0309) acc 28.1250 (38.9062) lr 1.0157e-03 eta 0:12:32
epoch [101/200] batch [25/50] time 0.085 (0.138) data 0.000 (0.054) loss 1.4590 (2.0100) acc 62.5000 (40.5000) lr 1.0157e-03 eta 0:11:24
epoch [101/200] batch [30/50] time 0.084 (0.135) data 0.000 (0.052) loss 1.7549 (2.0024) acc 59.3750 (41.6667) lr 1.0157e-03 eta 0:11:12
epoch [101/200] batch [35/50] time 0.084 (0.133) data 0.000 (0.050) loss 1.7480 (1.9945) acc 56.2500 (42.4107) lr 1.0157e-03 eta 0:11:00
epoch [101/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.043) loss 1.6436 (1.9796) acc 50.0000 (42.3438) lr 1.0157e-03 eta 0:10:29
epoch [101/200] batch [45/50] time 0.084 (0.123) data 0.000 (0.040) loss 2.3203 (1.9902) acc 34.3750 (42.5000) lr 1.0157e-03 eta 0:10:11
epoch [101/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.036) loss 1.6807 (1.9907) acc 43.7500 (42.1875) lr 1.0000e-03 eta 0:09:50
epoch [102/200] batch [5/50] time 0.087 (0.302) data 0.000 (0.216) loss 2.0215 (1.8443) acc 46.8750 (44.3750) lr 1.0000e-03 eta 0:24:52
epoch [102/200] batch [10/50] time 0.252 (0.211) data 0.166 (0.125) loss 2.4551 (1.9613) acc 50.0000 (44.3750) lr 1.0000e-03 eta 0:17:20
epoch [102/200] batch [15/50] time 0.085 (0.169) data 0.000 (0.083) loss 2.3340 (1.9818) acc 31.2500 (43.1250) lr 1.0000e-03 eta 0:13:53
epoch [102/200] batch [20/50] time 0.085 (0.153) data 0.000 (0.066) loss 2.1738 (1.9817) acc 43.7500 (43.5938) lr 1.0000e-03 eta 0:12:34
epoch [102/200] batch [25/50] time 0.085 (0.139) data 0.000 (0.053) loss 1.8105 (2.0011) acc 40.6250 (42.5000) lr 1.0000e-03 eta 0:11:26
epoch [102/200] batch [30/50] time 0.085 (0.131) data 0.000 (0.044) loss 2.0820 (2.0137) acc 31.2500 (41.7708) lr 1.0000e-03 eta 0:10:42
epoch [102/200] batch [35/50] time 0.085 (0.124) data 0.000 (0.038) loss 1.7979 (1.9911) acc 43.7500 (41.6071) lr 1.0000e-03 eta 0:10:10
epoch [102/200] batch [40/50] time 0.084 (0.119) data 0.000 (0.033) loss 1.9502 (1.9903) acc 43.7500 (41.6406) lr 1.0000e-03 eta 0:09:45
epoch [102/200] batch [45/50] time 0.083 (0.115) data 0.000 (0.030) loss 1.8027 (1.9842) acc 46.8750 (41.5972) lr 1.0000e-03 eta 0:09:25
epoch [102/200] batch [50/50] time 0.083 (0.112) data 0.000 (0.027) loss 1.3301 (1.9809) acc 65.6250 (41.8750) lr 9.8429e-04 eta 0:09:08
epoch [103/200] batch [5/50] time 0.084 (0.292) data 0.000 (0.208) loss 2.3828 (2.1154) acc 18.7500 (40.0000) lr 9.8429e-04 eta 0:23:49
epoch [103/200] batch [10/50] time 0.083 (0.205) data 0.000 (0.121) loss 1.8564 (1.9883) acc 40.6250 (40.9375) lr 9.8429e-04 eta 0:16:41
epoch [103/200] batch [15/50] time 0.084 (0.166) data 0.001 (0.082) loss 2.0566 (1.9749) acc 28.1250 (40.4167) lr 9.8429e-04 eta 0:13:31
epoch [103/200] batch [20/50] time 0.084 (0.155) data 0.000 (0.071) loss 1.6689 (1.9120) acc 56.2500 (44.0625) lr 9.8429e-04 eta 0:12:34
epoch [103/200] batch [25/50] time 0.122 (0.142) data 0.037 (0.058) loss 2.0312 (1.9591) acc 34.3750 (42.1250) lr 9.8429e-04 eta 0:11:33
epoch [103/200] batch [30/50] time 0.086 (0.139) data 0.000 (0.055) loss 1.9180 (1.9593) acc 43.7500 (42.3958) lr 9.8429e-04 eta 0:11:15
epoch [103/200] batch [35/50] time 0.258 (0.136) data 0.174 (0.052) loss 2.1875 (1.9691) acc 31.2500 (41.6964) lr 9.8429e-04 eta 0:11:01
epoch [103/200] batch [40/50] time 0.083 (0.129) data 0.000 (0.045) loss 1.9883 (1.9552) acc 46.8750 (42.2656) lr 9.8429e-04 eta 0:10:28
epoch [103/200] batch [45/50] time 0.083 (0.128) data 0.000 (0.044) loss 1.7725 (1.9510) acc 53.1250 (42.4306) lr 9.8429e-04 eta 0:10:21
epoch [103/200] batch [50/50] time 0.084 (0.124) data 0.000 (0.040) loss 2.5332 (1.9669) acc 37.5000 (42.1250) lr 9.6859e-04 eta 0:09:59
epoch [104/200] batch [5/50] time 0.084 (0.293) data 0.000 (0.208) loss 2.0996 (1.9500) acc 43.7500 (43.7500) lr 9.6859e-04 eta 0:23:39
epoch [104/200] batch [10/50] time 0.086 (0.189) data 0.001 (0.104) loss 2.0195 (1.9579) acc 34.3750 (42.8125) lr 9.6859e-04 eta 0:15:15
epoch [104/200] batch [15/50] time 0.085 (0.155) data 0.000 (0.070) loss 1.6787 (1.9590) acc 46.8750 (44.3750) lr 9.6859e-04 eta 0:12:28
epoch [104/200] batch [20/50] time 0.086 (0.137) data 0.000 (0.052) loss 2.6992 (2.0349) acc 37.5000 (42.0312) lr 9.6859e-04 eta 0:11:04
epoch [104/200] batch [25/50] time 0.087 (0.127) data 0.001 (0.042) loss 1.6963 (2.0296) acc 43.7500 (41.7500) lr 9.6859e-04 eta 0:10:13
epoch [104/200] batch [30/50] time 0.085 (0.120) data 0.000 (0.035) loss 1.9521 (2.0191) acc 46.8750 (41.4583) lr 9.6859e-04 eta 0:09:39
epoch [104/200] batch [35/50] time 0.085 (0.115) data 0.001 (0.030) loss 1.8086 (2.0085) acc 50.0000 (41.6964) lr 9.6859e-04 eta 0:09:14
epoch [104/200] batch [40/50] time 0.084 (0.111) data 0.000 (0.026) loss 2.0391 (2.0056) acc 40.6250 (41.6406) lr 9.6859e-04 eta 0:08:55
epoch [104/200] batch [45/50] time 0.083 (0.108) data 0.000 (0.023) loss 2.0762 (2.0021) acc 37.5000 (41.8056) lr 9.6859e-04 eta 0:08:40
epoch [104/200] batch [50/50] time 0.085 (0.106) data 0.000 (0.021) loss 1.6582 (1.9898) acc 34.3750 (42.1875) lr 9.5289e-04 eta 0:08:28
epoch [105/200] batch [5/50] time 0.084 (0.316) data 0.000 (0.233) loss 2.0039 (1.8508) acc 34.3750 (45.6250) lr 9.5289e-04 eta 0:25:17
epoch [105/200] batch [10/50] time 0.260 (0.218) data 0.177 (0.134) loss 1.9434 (1.8854) acc 43.7500 (46.5625) lr 9.5289e-04 eta 0:17:23
epoch [105/200] batch [15/50] time 0.083 (0.173) data 0.000 (0.090) loss 2.0254 (1.8758) acc 40.6250 (44.1667) lr 9.5289e-04 eta 0:13:47
epoch [105/200] batch [20/50] time 0.084 (0.152) data 0.000 (0.068) loss 2.3535 (1.8859) acc 34.3750 (44.2188) lr 9.5289e-04 eta 0:12:05
epoch [105/200] batch [25/50] time 0.211 (0.143) data 0.129 (0.060) loss 1.9795 (1.9217) acc 46.8750 (44.2500) lr 9.5289e-04 eta 0:11:24
epoch [105/200] batch [30/50] time 0.085 (0.133) data 0.000 (0.050) loss 1.6328 (1.9249) acc 59.3750 (43.8542) lr 9.5289e-04 eta 0:10:36
epoch [105/200] batch [35/50] time 0.086 (0.128) data 0.001 (0.044) loss 1.1953 (1.9009) acc 68.7500 (44.2857) lr 9.5289e-04 eta 0:10:08
epoch [105/200] batch [40/50] time 0.083 (0.122) data 0.000 (0.039) loss 2.2344 (1.9456) acc 31.2500 (43.2031) lr 9.5289e-04 eta 0:09:41
epoch [105/200] batch [45/50] time 0.083 (0.118) data 0.001 (0.034) loss 1.6553 (1.9432) acc 56.2500 (43.3333) lr 9.5289e-04 eta 0:09:20
epoch [105/200] batch [50/50] time 0.085 (0.114) data 0.000 (0.031) loss 1.8711 (1.9523) acc 43.7500 (42.8750) lr 9.3721e-04 eta 0:09:03
epoch [106/200] batch [5/50] time 0.085 (0.322) data 0.000 (0.237) loss 2.3984 (1.9598) acc 25.0000 (41.2500) lr 9.3721e-04 eta 0:25:26
epoch [106/200] batch [10/50] time 0.096 (0.225) data 0.005 (0.139) loss 1.5801 (1.9012) acc 37.5000 (41.5625) lr 9.3721e-04 eta 0:17:48
epoch [106/200] batch [15/50] time 0.086 (0.180) data 0.000 (0.093) loss 1.8203 (1.9544) acc 50.0000 (42.7083) lr 9.3721e-04 eta 0:14:10
epoch [106/200] batch [20/50] time 0.086 (0.156) data 0.000 (0.070) loss 1.7354 (1.9538) acc 40.6250 (42.5000) lr 9.3721e-04 eta 0:12:19
epoch [106/200] batch [25/50] time 0.086 (0.142) data 0.001 (0.056) loss 2.0391 (1.9462) acc 31.2500 (41.8750) lr 9.3721e-04 eta 0:11:11
epoch [106/200] batch [30/50] time 0.086 (0.133) data 0.000 (0.047) loss 1.6143 (1.9410) acc 59.3750 (42.0833) lr 9.3721e-04 eta 0:10:26
epoch [106/200] batch [35/50] time 0.086 (0.126) data 0.001 (0.040) loss 1.9219 (1.9375) acc 50.0000 (42.4107) lr 9.3721e-04 eta 0:09:55
epoch [106/200] batch [40/50] time 0.084 (0.121) data 0.000 (0.035) loss 2.0820 (1.9652) acc 43.7500 (41.4844) lr 9.3721e-04 eta 0:09:30
epoch [106/200] batch [45/50] time 0.083 (0.119) data 0.000 (0.033) loss 2.4941 (1.9627) acc 31.2500 (41.8056) lr 9.3721e-04 eta 0:09:18
epoch [106/200] batch [50/50] time 0.085 (0.115) data 0.000 (0.030) loss 2.0840 (1.9514) acc 37.5000 (41.6875) lr 9.2154e-04 eta 0:09:01
epoch [107/200] batch [5/50] time 0.083 (0.309) data 0.000 (0.225) loss 1.7832 (1.9434) acc 50.0000 (45.6250) lr 9.2154e-04 eta 0:24:10
epoch [107/200] batch [10/50] time 0.085 (0.197) data 0.000 (0.112) loss 2.4609 (2.0038) acc 34.3750 (42.5000) lr 9.2154e-04 eta 0:15:21
epoch [107/200] batch [15/50] time 0.084 (0.159) data 0.000 (0.075) loss 1.9121 (1.9706) acc 40.6250 (43.1250) lr 9.2154e-04 eta 0:12:25
epoch [107/200] batch [20/50] time 0.085 (0.141) data 0.001 (0.057) loss 1.9863 (1.9084) acc 50.0000 (45.0000) lr 9.2154e-04 eta 0:11:01
epoch [107/200] batch [25/50] time 0.084 (0.130) data 0.000 (0.046) loss 1.8262 (1.9097) acc 50.0000 (44.8750) lr 9.2154e-04 eta 0:10:07
epoch [107/200] batch [30/50] time 0.085 (0.125) data 0.000 (0.041) loss 1.8164 (1.9304) acc 53.1250 (44.1667) lr 9.2154e-04 eta 0:09:45
epoch [107/200] batch [35/50] time 0.084 (0.120) data 0.000 (0.035) loss 2.1914 (1.9462) acc 40.6250 (43.8393) lr 9.2154e-04 eta 0:09:17
epoch [107/200] batch [40/50] time 0.084 (0.121) data 0.000 (0.037) loss 2.1992 (1.9706) acc 46.8750 (42.9688) lr 9.2154e-04 eta 0:09:21
epoch [107/200] batch [45/50] time 0.084 (0.117) data 0.000 (0.033) loss 1.6719 (1.9678) acc 46.8750 (42.9167) lr 9.2154e-04 eta 0:09:04
epoch [107/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.030) loss 2.2539 (1.9702) acc 37.5000 (43.2500) lr 9.0589e-04 eta 0:08:48
epoch [108/200] batch [5/50] time 0.084 (0.312) data 0.000 (0.228) loss 1.7832 (1.8906) acc 43.7500 (41.2500) lr 9.0589e-04 eta 0:24:08
epoch [108/200] batch [10/50] time 0.084 (0.205) data 0.000 (0.122) loss 1.4346 (1.9300) acc 59.3750 (43.1250) lr 9.0589e-04 eta 0:15:52
epoch [108/200] batch [15/50] time 0.084 (0.165) data 0.000 (0.081) loss 1.6533 (1.8516) acc 46.8750 (44.7917) lr 9.0589e-04 eta 0:12:44
epoch [108/200] batch [20/50] time 0.083 (0.150) data 0.000 (0.067) loss 2.2109 (1.8775) acc 28.1250 (43.9062) lr 9.0589e-04 eta 0:11:36
epoch [108/200] batch [25/50] time 0.266 (0.145) data 0.182 (0.061) loss 2.2871 (1.9773) acc 31.2500 (40.7500) lr 9.0589e-04 eta 0:11:08
epoch [108/200] batch [30/50] time 0.086 (0.135) data 0.000 (0.051) loss 1.7422 (1.9610) acc 46.8750 (41.2500) lr 9.0589e-04 eta 0:10:22
epoch [108/200] batch [35/50] time 0.084 (0.131) data 0.001 (0.047) loss 2.0000 (1.9542) acc 43.7500 (41.2500) lr 9.0589e-04 eta 0:10:03
epoch [108/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.041) loss 2.0371 (1.9546) acc 40.6250 (41.7188) lr 9.0589e-04 eta 0:09:35
epoch [108/200] batch [45/50] time 0.084 (0.124) data 0.000 (0.040) loss 1.7832 (1.9566) acc 46.8750 (41.4583) lr 9.0589e-04 eta 0:09:32
epoch [108/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.8516 (1.9280) acc 43.7500 (42.4375) lr 8.9027e-04 eta 0:09:12
epoch [109/200] batch [5/50] time 0.084 (0.323) data 0.000 (0.239) loss 2.2207 (2.1574) acc 25.0000 (36.2500) lr 8.9027e-04 eta 0:24:46
epoch [109/200] batch [10/50] time 0.085 (0.220) data 0.000 (0.136) loss 2.1211 (2.1664) acc 34.3750 (36.2500) lr 8.9027e-04 eta 0:16:48
epoch [109/200] batch [15/50] time 0.085 (0.175) data 0.000 (0.091) loss 2.3359 (2.0847) acc 28.1250 (37.9167) lr 8.9027e-04 eta 0:13:20
epoch [109/200] batch [20/50] time 0.085 (0.160) data 0.000 (0.076) loss 1.9053 (2.0284) acc 40.6250 (39.6875) lr 8.9027e-04 eta 0:12:12
epoch [109/200] batch [25/50] time 0.282 (0.153) data 0.199 (0.069) loss 1.6035 (2.0011) acc 46.8750 (40.7500) lr 8.9027e-04 eta 0:11:39
epoch [109/200] batch [30/50] time 0.084 (0.142) data 0.000 (0.057) loss 2.2480 (2.0075) acc 40.6250 (40.8333) lr 8.9027e-04 eta 0:10:46
epoch [109/200] batch [35/50] time 0.084 (0.137) data 0.001 (0.053) loss 2.0469 (2.0109) acc 40.6250 (40.3571) lr 8.9027e-04 eta 0:10:26
epoch [109/200] batch [40/50] time 0.084 (0.131) data 0.000 (0.046) loss 2.0273 (2.0154) acc 46.8750 (40.4688) lr 8.9027e-04 eta 0:09:55
epoch [109/200] batch [45/50] time 0.083 (0.128) data 0.000 (0.044) loss 1.9883 (2.0225) acc 40.6250 (39.7222) lr 8.9027e-04 eta 0:09:41
epoch [109/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.0977 (2.0185) acc 37.5000 (40.1875) lr 8.7467e-04 eta 0:09:21
epoch [110/200] batch [5/50] time 0.086 (0.320) data 0.000 (0.234) loss 1.9893 (2.0734) acc 37.5000 (38.1250) lr 8.7467e-04 eta 0:24:13
epoch [110/200] batch [10/50] time 0.085 (0.217) data 0.000 (0.132) loss 2.0078 (1.9661) acc 43.7500 (43.1250) lr 8.7467e-04 eta 0:16:23
epoch [110/200] batch [15/50] time 0.086 (0.173) data 0.000 (0.088) loss 1.5957 (2.0027) acc 62.5000 (43.1250) lr 8.7467e-04 eta 0:13:04
epoch [110/200] batch [20/50] time 0.085 (0.160) data 0.000 (0.075) loss 1.5645 (1.9776) acc 40.6250 (42.3438) lr 8.7467e-04 eta 0:12:05
epoch [110/200] batch [25/50] time 0.306 (0.154) data 0.223 (0.069) loss 2.0918 (1.9398) acc 40.6250 (43.2500) lr 8.7467e-04 eta 0:11:36
epoch [110/200] batch [30/50] time 0.086 (0.142) data 0.000 (0.058) loss 1.5225 (1.9284) acc 40.6250 (42.9167) lr 8.7467e-04 eta 0:10:43
epoch [110/200] batch [35/50] time 0.085 (0.138) data 0.000 (0.053) loss 2.1953 (1.9522) acc 37.5000 (42.6786) lr 8.7467e-04 eta 0:10:23
epoch [110/200] batch [40/50] time 0.084 (0.131) data 0.000 (0.047) loss 1.9072 (1.9706) acc 46.8750 (42.1094) lr 8.7467e-04 eta 0:09:52
epoch [110/200] batch [45/50] time 0.083 (0.130) data 0.000 (0.045) loss 2.2109 (1.9678) acc 40.6250 (42.2917) lr 8.7467e-04 eta 0:09:45
epoch [110/200] batch [50/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.9434 (1.9654) acc 46.8750 (42.3125) lr 8.5910e-04 eta 0:09:23
epoch [111/200] batch [5/50] time 0.084 (0.313) data 0.000 (0.228) loss 2.0566 (1.8975) acc 40.6250 (43.1250) lr 8.5910e-04 eta 0:23:26
epoch [111/200] batch [10/50] time 0.084 (0.210) data 0.000 (0.125) loss 2.1797 (1.8762) acc 34.3750 (42.8125) lr 8.5910e-04 eta 0:15:41
epoch [111/200] batch [15/50] time 0.085 (0.168) data 0.000 (0.084) loss 1.8887 (1.8671) acc 25.0000 (41.2500) lr 8.5910e-04 eta 0:12:33
epoch [111/200] batch [20/50] time 0.084 (0.147) data 0.000 (0.063) loss 2.0781 (1.9291) acc 50.0000 (40.4688) lr 8.5910e-04 eta 0:10:59
epoch [111/200] batch [25/50] time 0.111 (0.136) data 0.028 (0.052) loss 2.0566 (1.9662) acc 46.8750 (40.2500) lr 8.5910e-04 eta 0:10:08
epoch [111/200] batch [30/50] time 0.086 (0.128) data 0.000 (0.043) loss 1.7148 (1.9832) acc 62.5000 (41.0417) lr 8.5910e-04 eta 0:09:29
epoch [111/200] batch [35/50] time 0.085 (0.122) data 0.000 (0.037) loss 1.7012 (1.9946) acc 56.2500 (40.5357) lr 8.5910e-04 eta 0:09:02
epoch [111/200] batch [40/50] time 0.083 (0.117) data 0.000 (0.032) loss 1.5723 (1.9844) acc 59.3750 (41.1719) lr 8.5910e-04 eta 0:08:41
epoch [111/200] batch [45/50] time 0.083 (0.113) data 0.000 (0.029) loss 2.5430 (1.9865) acc 28.1250 (41.3889) lr 8.5910e-04 eta 0:08:24
epoch [111/200] batch [50/50] time 0.083 (0.110) data 0.000 (0.026) loss 2.0684 (1.9905) acc 46.8750 (41.6875) lr 8.4357e-04 eta 0:08:10
epoch [112/200] batch [5/50] time 0.086 (0.316) data 0.000 (0.230) loss 1.7910 (1.8756) acc 50.0000 (41.2500) lr 8.4357e-04 eta 0:23:24
epoch [112/200] batch [10/50] time 0.117 (0.204) data 0.032 (0.119) loss 2.2070 (1.8691) acc 40.6250 (45.0000) lr 8.4357e-04 eta 0:15:05
epoch [112/200] batch [15/50] time 0.085 (0.165) data 0.000 (0.079) loss 1.9795 (1.8894) acc 37.5000 (44.5833) lr 8.4357e-04 eta 0:12:10
epoch [112/200] batch [20/50] time 0.086 (0.152) data 0.000 (0.067) loss 1.8320 (1.8810) acc 46.8750 (45.4688) lr 8.4357e-04 eta 0:11:13
epoch [112/200] batch [25/50] time 0.083 (0.138) data 0.000 (0.053) loss 1.9502 (1.8907) acc 50.0000 (45.1250) lr 8.4357e-04 eta 0:10:12
epoch [112/200] batch [30/50] time 0.084 (0.137) data 0.000 (0.052) loss 2.2812 (1.9121) acc 40.6250 (44.3750) lr 8.4357e-04 eta 0:10:04
epoch [112/200] batch [35/50] time 0.084 (0.133) data 0.000 (0.048) loss 2.3477 (1.9349) acc 34.3750 (43.6607) lr 8.4357e-04 eta 0:09:45
epoch [112/200] batch [40/50] time 0.084 (0.127) data 0.000 (0.042) loss 1.8311 (1.9332) acc 46.8750 (42.8125) lr 8.4357e-04 eta 0:09:18
epoch [112/200] batch [45/50] time 0.084 (0.124) data 0.000 (0.039) loss 1.9180 (1.9372) acc 43.7500 (42.9861) lr 8.4357e-04 eta 0:09:04
epoch [112/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.035) loss 1.8301 (1.9281) acc 43.7500 (43.1875) lr 8.2807e-04 eta 0:08:46
epoch [113/200] batch [5/50] time 0.085 (0.315) data 0.001 (0.231) loss 2.1406 (2.0668) acc 40.6250 (41.8750) lr 8.2807e-04 eta 0:23:02
epoch [113/200] batch [10/50] time 0.085 (0.199) data 0.000 (0.116) loss 2.2754 (2.0305) acc 40.6250 (39.3750) lr 8.2807e-04 eta 0:14:35
epoch [113/200] batch [15/50] time 0.084 (0.161) data 0.000 (0.077) loss 1.9424 (1.9303) acc 43.7500 (43.3333) lr 8.2807e-04 eta 0:11:47
epoch [113/200] batch [20/50] time 0.084 (0.142) data 0.000 (0.058) loss 1.7080 (1.9380) acc 50.0000 (42.9688) lr 8.2807e-04 eta 0:10:22
epoch [113/200] batch [25/50] time 0.085 (0.131) data 0.000 (0.046) loss 1.7803 (1.9270) acc 37.5000 (42.3750) lr 8.2807e-04 eta 0:09:31
epoch [113/200] batch [30/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.2578 (1.9493) acc 37.5000 (41.4583) lr 8.2807e-04 eta 0:09:08
epoch [113/200] batch [35/50] time 0.084 (0.125) data 0.001 (0.041) loss 1.4619 (1.9695) acc 65.6250 (42.1429) lr 8.2807e-04 eta 0:09:05
epoch [113/200] batch [40/50] time 0.083 (0.120) data 0.000 (0.036) loss 2.0723 (1.9690) acc 43.7500 (42.9688) lr 8.2807e-04 eta 0:08:42
epoch [113/200] batch [45/50] time 0.083 (0.118) data 0.000 (0.035) loss 1.9766 (1.9733) acc 43.7500 (42.6389) lr 8.2807e-04 eta 0:08:35
epoch [113/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 1.9893 (1.9754) acc 43.7500 (42.5000) lr 8.1262e-04 eta 0:08:19
epoch [114/200] batch [5/50] time 0.086 (0.320) data 0.000 (0.234) loss 2.1211 (1.7762) acc 31.2500 (45.6250) lr 8.1262e-04 eta 0:23:09
epoch [114/200] batch [10/50] time 0.084 (0.209) data 0.000 (0.124) loss 1.7705 (1.8644) acc 43.7500 (43.7500) lr 8.1262e-04 eta 0:15:06
epoch [114/200] batch [15/50] time 0.083 (0.167) data 0.000 (0.083) loss 2.0469 (1.8115) acc 46.8750 (44.3750) lr 8.1262e-04 eta 0:12:04
epoch [114/200] batch [20/50] time 0.085 (0.153) data 0.000 (0.069) loss 1.8311 (1.8639) acc 62.5000 (44.3750) lr 8.1262e-04 eta 0:11:02
epoch [114/200] batch [25/50] time 0.232 (0.145) data 0.149 (0.061) loss 2.0195 (1.9125) acc 37.5000 (43.0000) lr 8.1262e-04 eta 0:10:27
epoch [114/200] batch [30/50] time 0.085 (0.135) data 0.000 (0.051) loss 1.4580 (1.8905) acc 59.3750 (43.7500) lr 8.1262e-04 eta 0:09:43
epoch [114/200] batch [35/50] time 0.085 (0.132) data 0.001 (0.048) loss 2.0879 (1.9112) acc 43.7500 (43.7500) lr 8.1262e-04 eta 0:09:30
epoch [114/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.042) loss 1.6406 (1.9062) acc 59.3750 (44.2969) lr 8.1262e-04 eta 0:09:03
epoch [114/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 2.0117 (1.9101) acc 43.7500 (43.8194) lr 8.1262e-04 eta 0:08:58
epoch [114/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 2.0918 (1.9137) acc 34.3750 (43.6250) lr 7.9721e-04 eta 0:08:39
epoch [115/200] batch [5/50] time 0.088 (0.296) data 0.000 (0.210) loss 1.7510 (1.8941) acc 50.0000 (43.7500) lr 7.9721e-04 eta 0:21:10
epoch [115/200] batch [10/50] time 0.086 (0.191) data 0.000 (0.105) loss 1.7314 (1.7903) acc 37.5000 (45.9375) lr 7.9721e-04 eta 0:13:39
epoch [115/200] batch [15/50] time 0.086 (0.156) data 0.000 (0.070) loss 1.6797 (1.8275) acc 50.0000 (45.6250) lr 7.9721e-04 eta 0:11:08
epoch [115/200] batch [20/50] time 0.224 (0.148) data 0.142 (0.061) loss 1.7207 (1.8744) acc 50.0000 (45.0000) lr 7.9721e-04 eta 0:10:31
epoch [115/200] batch [25/50] time 0.084 (0.135) data 0.000 (0.049) loss 2.1289 (1.9109) acc 43.7500 (44.2500) lr 7.9721e-04 eta 0:09:37
epoch [115/200] batch [30/50] time 0.085 (0.134) data 0.000 (0.048) loss 1.9844 (1.8941) acc 46.8750 (45.1042) lr 7.9721e-04 eta 0:09:30
epoch [115/200] batch [35/50] time 0.085 (0.127) data 0.000 (0.041) loss 1.9443 (1.9127) acc 46.8750 (43.9286) lr 7.9721e-04 eta 0:08:59
epoch [115/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.039) loss 1.9736 (1.9073) acc 37.5000 (43.9844) lr 7.9721e-04 eta 0:08:50
epoch [115/200] batch [45/50] time 0.084 (0.123) data 0.000 (0.038) loss 1.9404 (1.9405) acc 43.7500 (42.6389) lr 7.9721e-04 eta 0:08:42
epoch [115/200] batch [50/50] time 0.085 (0.119) data 0.000 (0.034) loss 2.3340 (1.9524) acc 37.5000 (42.3125) lr 7.8186e-04 eta 0:08:25
epoch [116/200] batch [5/50] time 0.085 (0.268) data 0.000 (0.184) loss 2.4668 (1.8551) acc 46.8750 (51.8750) lr 7.8186e-04 eta 0:18:58
epoch [116/200] batch [10/50] time 0.121 (0.180) data 0.037 (0.096) loss 2.0781 (1.8836) acc 40.6250 (49.3750) lr 7.8186e-04 eta 0:12:42
epoch [116/200] batch [15/50] time 0.084 (0.153) data 0.000 (0.069) loss 2.2402 (1.9236) acc 40.6250 (46.4583) lr 7.8186e-04 eta 0:10:46
epoch [116/200] batch [20/50] time 0.287 (0.146) data 0.205 (0.062) loss 1.9824 (1.9794) acc 37.5000 (44.6875) lr 7.8186e-04 eta 0:10:16
epoch [116/200] batch [25/50] time 0.085 (0.134) data 0.000 (0.049) loss 2.1895 (2.0189) acc 37.5000 (42.8750) lr 7.8186e-04 eta 0:09:24
epoch [116/200] batch [30/50] time 0.084 (0.132) data 0.000 (0.047) loss 1.8408 (2.0184) acc 40.6250 (42.2917) lr 7.8186e-04 eta 0:09:15
epoch [116/200] batch [35/50] time 0.086 (0.125) data 0.001 (0.041) loss 1.6533 (2.0038) acc 46.8750 (42.7679) lr 7.8186e-04 eta 0:08:46
epoch [116/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.9990 (1.9939) acc 46.8750 (42.8125) lr 7.8186e-04 eta 0:08:46
epoch [116/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 1.9014 (1.9853) acc 46.8750 (43.1250) lr 7.8186e-04 eta 0:08:39
epoch [116/200] batch [50/50] time 0.084 (0.120) data 0.000 (0.036) loss 1.6299 (1.9609) acc 59.3750 (43.1250) lr 7.6655e-04 eta 0:08:22
epoch [117/200] batch [5/50] time 0.085 (0.294) data 0.000 (0.209) loss 1.8096 (1.8662) acc 37.5000 (41.2500) lr 7.6655e-04 eta 0:20:32
epoch [117/200] batch [10/50] time 0.084 (0.189) data 0.000 (0.105) loss 2.0371 (1.9765) acc 34.3750 (41.2500) lr 7.6655e-04 eta 0:13:12
epoch [117/200] batch [15/50] time 0.084 (0.165) data 0.000 (0.081) loss 1.8311 (1.9330) acc 37.5000 (41.2500) lr 7.6655e-04 eta 0:11:32
epoch [117/200] batch [20/50] time 0.084 (0.147) data 0.000 (0.062) loss 1.8877 (1.9193) acc 37.5000 (41.2500) lr 7.6655e-04 eta 0:10:14
epoch [117/200] batch [25/50] time 0.084 (0.134) data 0.000 (0.050) loss 1.9355 (1.9393) acc 37.5000 (41.0000) lr 7.6655e-04 eta 0:09:21
epoch [117/200] batch [30/50] time 0.086 (0.126) data 0.000 (0.042) loss 2.2930 (1.9400) acc 37.5000 (41.4583) lr 7.6655e-04 eta 0:08:46
epoch [117/200] batch [35/50] time 0.084 (0.120) data 0.001 (0.036) loss 1.9404 (1.9332) acc 37.5000 (41.8750) lr 7.6655e-04 eta 0:08:20
epoch [117/200] batch [40/50] time 0.083 (0.116) data 0.000 (0.031) loss 2.5781 (1.9772) acc 31.2500 (41.0938) lr 7.6655e-04 eta 0:08:01
epoch [117/200] batch [45/50] time 0.084 (0.112) data 0.000 (0.028) loss 2.1934 (1.9739) acc 40.6250 (41.3194) lr 7.6655e-04 eta 0:07:46
epoch [117/200] batch [50/50] time 0.083 (0.109) data 0.000 (0.025) loss 1.7119 (1.9785) acc 43.7500 (41.0000) lr 7.5131e-04 eta 0:07:33
epoch [118/200] batch [5/50] time 0.086 (0.332) data 0.000 (0.246) loss 1.9932 (1.8779) acc 53.1250 (47.5000) lr 7.5131e-04 eta 0:22:56
epoch [118/200] batch [10/50] time 0.086 (0.219) data 0.000 (0.133) loss 2.2109 (1.9021) acc 50.0000 (44.3750) lr 7.5131e-04 eta 0:15:06
epoch [118/200] batch [15/50] time 0.086 (0.174) data 0.000 (0.089) loss 2.3027 (1.9543) acc 46.8750 (45.4167) lr 7.5131e-04 eta 0:12:01
epoch [118/200] batch [20/50] time 0.086 (0.158) data 0.000 (0.073) loss 2.2441 (2.0059) acc 28.1250 (42.8125) lr 7.5131e-04 eta 0:10:54
epoch [118/200] batch [25/50] time 0.196 (0.148) data 0.111 (0.063) loss 1.7871 (2.0102) acc 50.0000 (42.6250) lr 7.5131e-04 eta 0:10:11
epoch [118/200] batch [30/50] time 0.086 (0.138) data 0.000 (0.052) loss 1.8369 (1.9915) acc 43.7500 (43.0208) lr 7.5131e-04 eta 0:09:28
epoch [118/200] batch [35/50] time 0.085 (0.134) data 0.000 (0.049) loss 1.9004 (1.9945) acc 40.6250 (42.9464) lr 7.5131e-04 eta 0:09:13
epoch [118/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.043) loss 1.7979 (1.9658) acc 40.6250 (43.5156) lr 7.5131e-04 eta 0:08:46
epoch [118/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.041) loss 1.7490 (1.9670) acc 43.7500 (42.9167) lr 7.5131e-04 eta 0:08:35
epoch [118/200] batch [50/50] time 0.084 (0.121) data 0.000 (0.036) loss 2.1797 (1.9736) acc 43.7500 (42.6875) lr 7.3613e-04 eta 0:08:17
epoch [119/200] batch [5/50] time 0.085 (0.326) data 0.000 (0.241) loss 1.9199 (1.8893) acc 43.7500 (41.2500) lr 7.3613e-04 eta 0:22:15
epoch [119/200] batch [10/50] time 0.086 (0.212) data 0.000 (0.127) loss 2.1191 (1.9427) acc 50.0000 (41.2500) lr 7.3613e-04 eta 0:14:28
epoch [119/200] batch [15/50] time 0.087 (0.170) data 0.001 (0.085) loss 2.3203 (1.9177) acc 34.3750 (42.5000) lr 7.3613e-04 eta 0:11:34
epoch [119/200] batch [20/50] time 0.085 (0.149) data 0.000 (0.064) loss 1.7383 (1.8719) acc 43.7500 (44.0625) lr 7.3613e-04 eta 0:10:09
epoch [119/200] batch [25/50] time 0.245 (0.143) data 0.160 (0.058) loss 1.9414 (1.9134) acc 37.5000 (42.2500) lr 7.3613e-04 eta 0:09:43
epoch [119/200] batch [30/50] time 0.085 (0.133) data 0.000 (0.048) loss 2.3613 (1.9329) acc 34.3750 (42.2917) lr 7.3613e-04 eta 0:09:03
epoch [119/200] batch [35/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.0098 (1.9549) acc 31.2500 (41.6964) lr 7.3613e-04 eta 0:08:34
epoch [119/200] batch [40/50] time 0.084 (0.121) data 0.000 (0.036) loss 1.8154 (1.9679) acc 37.5000 (40.9375) lr 7.3613e-04 eta 0:08:12
epoch [119/200] batch [45/50] time 0.085 (0.117) data 0.000 (0.032) loss 1.5703 (1.9749) acc 43.7500 (40.7639) lr 7.3613e-04 eta 0:07:54
epoch [119/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.030) loss 1.8281 (1.9714) acc 53.1250 (41.1250) lr 7.2101e-04 eta 0:07:43
epoch [120/200] batch [5/50] time 0.085 (0.313) data 0.000 (0.229) loss 1.7061 (1.8693) acc 40.6250 (42.5000) lr 7.2101e-04 eta 0:21:04
epoch [120/200] batch [10/50] time 0.085 (0.216) data 0.000 (0.132) loss 2.0312 (1.9359) acc 40.6250 (42.8125) lr 7.2101e-04 eta 0:14:32
epoch [120/200] batch [15/50] time 0.084 (0.172) data 0.000 (0.088) loss 1.8223 (1.9212) acc 34.3750 (43.1250) lr 7.2101e-04 eta 0:11:34
epoch [120/200] batch [20/50] time 0.084 (0.150) data 0.000 (0.066) loss 1.9004 (1.9219) acc 46.8750 (44.3750) lr 7.2101e-04 eta 0:10:05
epoch [120/200] batch [25/50] time 0.084 (0.137) data 0.000 (0.053) loss 1.8359 (1.9448) acc 50.0000 (43.3750) lr 7.2101e-04 eta 0:09:11
epoch [120/200] batch [30/50] time 0.084 (0.131) data 0.000 (0.047) loss 2.2051 (1.9350) acc 37.5000 (43.3333) lr 7.2101e-04 eta 0:08:45
epoch [120/200] batch [35/50] time 0.084 (0.129) data 0.001 (0.045) loss 1.7080 (1.9383) acc 46.8750 (42.9464) lr 7.2101e-04 eta 0:08:38
epoch [120/200] batch [40/50] time 0.083 (0.123) data 0.000 (0.040) loss 1.8086 (1.9176) acc 46.8750 (42.8125) lr 7.2101e-04 eta 0:08:14
epoch [120/200] batch [45/50] time 0.084 (0.122) data 0.000 (0.038) loss 1.6445 (1.9258) acc 50.0000 (42.5694) lr 7.2101e-04 eta 0:08:08
epoch [120/200] batch [50/50] time 0.084 (0.118) data 0.000 (0.034) loss 1.7979 (1.9359) acc 53.1250 (42.7500) lr 7.0596e-04 eta 0:07:52
epoch [121/200] batch [5/50] time 0.085 (0.291) data 0.000 (0.206) loss 1.8984 (1.9789) acc 37.5000 (40.6250) lr 7.0596e-04 eta 0:19:22
epoch [121/200] batch [10/50] time 0.085 (0.188) data 0.001 (0.103) loss 2.0312 (2.0281) acc 46.8750 (42.5000) lr 7.0596e-04 eta 0:12:31
epoch [121/200] batch [15/50] time 0.085 (0.154) data 0.000 (0.069) loss 2.6367 (1.9947) acc 25.0000 (43.5417) lr 7.0596e-04 eta 0:10:14
epoch [121/200] batch [20/50] time 0.086 (0.141) data 0.000 (0.056) loss 2.2441 (1.9651) acc 34.3750 (45.0000) lr 7.0596e-04 eta 0:09:21
epoch [121/200] batch [25/50] time 0.086 (0.130) data 0.001 (0.045) loss 1.8652 (1.9356) acc 46.8750 (45.1250) lr 7.0596e-04 eta 0:08:37
epoch [121/200] batch [30/50] time 0.086 (0.127) data 0.000 (0.041) loss 2.0625 (1.9468) acc 31.2500 (44.2708) lr 7.0596e-04 eta 0:08:22
epoch [121/200] batch [35/50] time 0.086 (0.124) data 0.001 (0.039) loss 1.5840 (1.9294) acc 56.2500 (44.0179) lr 7.0596e-04 eta 0:08:11
epoch [121/200] batch [40/50] time 0.085 (0.122) data 0.000 (0.037) loss 1.9521 (1.9443) acc 56.2500 (44.0625) lr 7.0596e-04 eta 0:08:04
epoch [121/200] batch [45/50] time 0.084 (0.118) data 0.000 (0.033) loss 1.6885 (1.9295) acc 37.5000 (44.2361) lr 7.0596e-04 eta 0:07:46
epoch [121/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.032) loss 1.8496 (1.9414) acc 43.7500 (43.3125) lr 6.9098e-04 eta 0:07:41
epoch [122/200] batch [5/50] time 0.085 (0.318) data 0.000 (0.233) loss 1.9971 (2.1047) acc 43.7500 (44.3750) lr 6.9098e-04 eta 0:20:52
epoch [122/200] batch [10/50] time 0.086 (0.230) data 0.000 (0.146) loss 2.1895 (1.9509) acc 37.5000 (45.9375) lr 6.9098e-04 eta 0:15:07
epoch [122/200] batch [15/50] time 0.084 (0.182) data 0.000 (0.097) loss 2.0703 (1.9810) acc 37.5000 (44.3750) lr 6.9098e-04 eta 0:11:54
epoch [122/200] batch [20/50] time 0.085 (0.164) data 0.000 (0.079) loss 2.1699 (2.0027) acc 37.5000 (44.3750) lr 6.9098e-04 eta 0:10:43
epoch [122/200] batch [25/50] time 0.203 (0.153) data 0.121 (0.068) loss 2.0059 (1.9689) acc 56.2500 (44.5000) lr 6.9098e-04 eta 0:09:59
epoch [122/200] batch [30/50] time 0.085 (0.141) data 0.000 (0.057) loss 1.7744 (1.9655) acc 46.8750 (44.7917) lr 6.9098e-04 eta 0:09:14
epoch [122/200] batch [35/50] time 0.083 (0.136) data 0.000 (0.052) loss 2.1445 (1.9421) acc 40.6250 (44.5536) lr 6.9098e-04 eta 0:08:53
epoch [122/200] batch [40/50] time 0.083 (0.130) data 0.000 (0.046) loss 2.0859 (1.9192) acc 46.8750 (44.8438) lr 6.9098e-04 eta 0:08:27
epoch [122/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.042) loss 2.1172 (1.9177) acc 37.5000 (45.0694) lr 6.9098e-04 eta 0:08:13
epoch [122/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.038) loss 2.0762 (1.9222) acc 46.8750 (45.1875) lr 6.7608e-04 eta 0:07:55
epoch [123/200] batch [5/50] time 0.083 (0.318) data 0.000 (0.234) loss 1.7559 (1.9068) acc 53.1250 (43.1250) lr 6.7608e-04 eta 0:20:36
epoch [123/200] batch [10/50] time 0.084 (0.207) data 0.000 (0.123) loss 1.9355 (1.9707) acc 46.8750 (41.5625) lr 6.7608e-04 eta 0:13:24
epoch [123/200] batch [15/50] time 0.084 (0.166) data 0.000 (0.082) loss 1.8379 (1.9329) acc 50.0000 (43.7500) lr 6.7608e-04 eta 0:10:44
epoch [123/200] batch [20/50] time 0.084 (0.152) data 0.000 (0.068) loss 2.0371 (1.9526) acc 40.6250 (42.6562) lr 6.7608e-04 eta 0:09:48
epoch [123/200] batch [25/50] time 0.087 (0.138) data 0.000 (0.054) loss 2.1816 (1.9371) acc 34.3750 (43.5000) lr 6.7608e-04 eta 0:08:55
epoch [123/200] batch [30/50] time 0.085 (0.135) data 0.000 (0.051) loss 2.3008 (1.9539) acc 34.3750 (42.5000) lr 6.7608e-04 eta 0:08:42
epoch [123/200] batch [35/50] time 0.207 (0.131) data 0.123 (0.047) loss 1.7471 (1.9267) acc 50.0000 (43.2143) lr 6.7608e-04 eta 0:08:27
epoch [123/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.8477 (1.9594) acc 50.0000 (42.6562) lr 6.7608e-04 eta 0:08:04
epoch [123/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 2.0254 (1.9493) acc 43.7500 (42.6389) lr 6.7608e-04 eta 0:08:00
epoch [123/200] batch [50/50] time 0.085 (0.121) data 0.000 (0.037) loss 1.8447 (1.9449) acc 46.8750 (42.5625) lr 6.6126e-04 eta 0:07:44
epoch [124/200] batch [5/50] time 0.085 (0.287) data 0.001 (0.201) loss 2.3594 (2.1021) acc 28.1250 (36.2500) lr 6.6126e-04 eta 0:18:22
epoch [124/200] batch [10/50] time 0.085 (0.193) data 0.000 (0.108) loss 2.0391 (1.9311) acc 40.6250 (41.2500) lr 6.6126e-04 eta 0:12:20
epoch [124/200] batch [15/50] time 0.084 (0.156) data 0.000 (0.072) loss 1.8271 (1.9743) acc 46.8750 (41.4583) lr 6.6126e-04 eta 0:10:00
epoch [124/200] batch [20/50] time 0.084 (0.145) data 0.000 (0.061) loss 1.6074 (1.9664) acc 43.7500 (41.4062) lr 6.6126e-04 eta 0:09:15
epoch [124/200] batch [25/50] time 0.084 (0.133) data 0.000 (0.049) loss 1.8760 (1.9665) acc 46.8750 (42.5000) lr 6.6126e-04 eta 0:08:28
epoch [124/200] batch [30/50] time 0.084 (0.128) data 0.000 (0.044) loss 2.1582 (1.9982) acc 40.6250 (42.0833) lr 6.6126e-04 eta 0:08:10
epoch [124/200] batch [35/50] time 0.199 (0.125) data 0.113 (0.041) loss 2.0801 (1.9976) acc 34.3750 (41.6071) lr 6.6126e-04 eta 0:07:58
epoch [124/200] batch [40/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.6904 (1.9871) acc 46.8750 (42.0312) lr 6.6126e-04 eta 0:07:37
epoch [124/200] batch [45/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.7070 (1.9696) acc 53.1250 (42.7778) lr 6.6126e-04 eta 0:07:40
epoch [124/200] batch [50/50] time 0.084 (0.117) data 0.000 (0.033) loss 1.9570 (1.9700) acc 40.6250 (42.9375) lr 6.4653e-04 eta 0:07:25
epoch [125/200] batch [5/50] time 0.084 (0.288) data 0.000 (0.203) loss 2.0820 (2.0266) acc 46.8750 (44.3750) lr 6.4653e-04 eta 0:18:12
epoch [125/200] batch [10/50] time 0.112 (0.189) data 0.028 (0.105) loss 2.2812 (1.9528) acc 21.8750 (41.5625) lr 6.4653e-04 eta 0:11:56
epoch [125/200] batch [15/50] time 0.085 (0.154) data 0.000 (0.070) loss 1.9326 (1.8973) acc 46.8750 (43.7500) lr 6.4653e-04 eta 0:09:43
epoch [125/200] batch [20/50] time 0.084 (0.147) data 0.000 (0.063) loss 1.7207 (1.9225) acc 43.7500 (43.5938) lr 6.4653e-04 eta 0:09:15
epoch [125/200] batch [25/50] time 0.085 (0.134) data 0.000 (0.050) loss 1.9277 (1.9571) acc 40.6250 (42.2500) lr 6.4653e-04 eta 0:08:27
epoch [125/200] batch [30/50] time 0.084 (0.131) data 0.000 (0.047) loss 1.9961 (1.9487) acc 53.1250 (43.3333) lr 6.4653e-04 eta 0:08:15
epoch [125/200] batch [35/50] time 0.086 (0.129) data 0.000 (0.045) loss 1.8672 (1.9398) acc 46.8750 (43.3929) lr 6.4653e-04 eta 0:08:05
epoch [125/200] batch [40/50] time 0.083 (0.123) data 0.000 (0.039) loss 1.7002 (1.9332) acc 56.2500 (44.0625) lr 6.4653e-04 eta 0:07:43
epoch [125/200] batch [45/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.9180 (1.9536) acc 56.2500 (43.8889) lr 6.4653e-04 eta 0:07:35
epoch [125/200] batch [50/50] time 0.084 (0.117) data 0.000 (0.033) loss 1.9121 (1.9746) acc 31.2500 (43.5625) lr 6.3188e-04 eta 0:07:20
epoch [126/200] batch [5/50] time 0.086 (0.304) data 0.001 (0.219) loss 2.0781 (1.9594) acc 34.3750 (42.5000) lr 6.3188e-04 eta 0:19:00
epoch [126/200] batch [10/50] time 0.096 (0.196) data 0.009 (0.111) loss 1.9727 (1.9749) acc 40.6250 (44.3750) lr 6.3188e-04 eta 0:12:13
epoch [126/200] batch [15/50] time 0.083 (0.159) data 0.000 (0.074) loss 1.9121 (1.9274) acc 37.5000 (43.7500) lr 6.3188e-04 eta 0:09:53
epoch [126/200] batch [20/50] time 0.084 (0.147) data 0.000 (0.062) loss 2.0820 (1.9409) acc 25.0000 (42.0312) lr 6.3188e-04 eta 0:09:08
epoch [126/200] batch [25/50] time 0.107 (0.135) data 0.024 (0.051) loss 1.8271 (1.9066) acc 46.8750 (42.8750) lr 6.3188e-04 eta 0:08:24
epoch [126/200] batch [30/50] time 0.085 (0.128) data 0.001 (0.043) loss 2.0293 (1.9065) acc 43.7500 (42.6042) lr 6.3188e-04 eta 0:07:55
epoch [126/200] batch [35/50] time 0.084 (0.124) data 0.000 (0.040) loss 1.8535 (1.9030) acc 46.8750 (43.0357) lr 6.3188e-04 eta 0:07:41
epoch [126/200] batch [40/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.0234 (1.9144) acc 25.0000 (42.4219) lr 6.3188e-04 eta 0:07:22
epoch [126/200] batch [45/50] time 0.201 (0.118) data 0.118 (0.034) loss 1.9033 (1.9141) acc 53.1250 (42.5694) lr 6.3188e-04 eta 0:07:16
epoch [126/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.030) loss 1.7646 (1.9101) acc 53.1250 (42.7500) lr 6.1732e-04 eta 0:07:03
epoch [127/200] batch [5/50] time 0.086 (0.254) data 0.000 (0.169) loss 2.1934 (2.0617) acc 37.5000 (43.7500) lr 6.1732e-04 eta 0:15:39
epoch [127/200] batch [10/50] time 0.087 (0.174) data 0.000 (0.089) loss 2.3320 (2.0540) acc 31.2500 (41.5625) lr 6.1732e-04 eta 0:10:41
epoch [127/200] batch [15/50] time 0.085 (0.145) data 0.000 (0.059) loss 1.8691 (2.0669) acc 43.7500 (40.6250) lr 6.1732e-04 eta 0:08:52
epoch [127/200] batch [20/50] time 0.085 (0.132) data 0.000 (0.047) loss 1.9268 (1.9979) acc 37.5000 (41.4062) lr 6.1732e-04 eta 0:08:06
epoch [127/200] batch [25/50] time 0.161 (0.126) data 0.077 (0.041) loss 2.2949 (1.9809) acc 21.8750 (40.6250) lr 6.1732e-04 eta 0:07:42
epoch [127/200] batch [30/50] time 0.084 (0.123) data 0.000 (0.037) loss 1.6475 (1.9463) acc 50.0000 (41.9792) lr 6.1732e-04 eta 0:07:30
epoch [127/200] batch [35/50] time 0.084 (0.124) data 0.001 (0.039) loss 2.0723 (1.9534) acc 34.3750 (41.6071) lr 6.1732e-04 eta 0:07:33
epoch [127/200] batch [40/50] time 0.083 (0.119) data 0.000 (0.034) loss 2.3809 (1.9647) acc 28.1250 (41.0156) lr 6.1732e-04 eta 0:07:14
epoch [127/200] batch [45/50] time 0.084 (0.118) data 0.000 (0.034) loss 1.8867 (1.9677) acc 34.3750 (40.6944) lr 6.1732e-04 eta 0:07:11
epoch [127/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.030) loss 1.6729 (1.9733) acc 53.1250 (40.3125) lr 6.0285e-04 eta 0:06:58
epoch [128/200] batch [5/50] time 0.085 (0.290) data 0.000 (0.206) loss 1.5713 (1.7881) acc 50.0000 (48.7500) lr 6.0285e-04 eta 0:17:36
epoch [128/200] batch [10/50] time 0.085 (0.187) data 0.000 (0.103) loss 1.7559 (1.8411) acc 53.1250 (47.5000) lr 6.0285e-04 eta 0:11:21
epoch [128/200] batch [15/50] time 0.084 (0.153) data 0.000 (0.069) loss 1.9883 (1.9000) acc 46.8750 (46.8750) lr 6.0285e-04 eta 0:09:15
epoch [128/200] batch [20/50] time 0.085 (0.136) data 0.000 (0.052) loss 2.4922 (1.9625) acc 40.6250 (45.6250) lr 6.0285e-04 eta 0:08:12
epoch [128/200] batch [25/50] time 0.084 (0.131) data 0.000 (0.047) loss 1.5635 (1.9460) acc 59.3750 (45.3750) lr 6.0285e-04 eta 0:07:54
epoch [128/200] batch [30/50] time 0.084 (0.125) data 0.000 (0.041) loss 1.9746 (1.9599) acc 37.5000 (43.6458) lr 6.0285e-04 eta 0:07:34
epoch [128/200] batch [35/50] time 0.084 (0.121) data 0.000 (0.037) loss 2.3242 (1.9869) acc 37.5000 (43.1250) lr 6.0285e-04 eta 0:07:15
epoch [128/200] batch [40/50] time 0.084 (0.116) data 0.000 (0.032) loss 1.4541 (1.9546) acc 43.7500 (43.5156) lr 6.0285e-04 eta 0:06:58
epoch [128/200] batch [45/50] time 0.083 (0.116) data 0.000 (0.033) loss 1.6113 (1.9332) acc 43.7500 (43.4722) lr 6.0285e-04 eta 0:06:59
epoch [128/200] batch [50/50] time 0.084 (0.113) data 0.000 (0.029) loss 1.9570 (1.9449) acc 46.8750 (43.0625) lr 5.8849e-04 eta 0:06:47
epoch [129/200] batch [5/50] time 0.086 (0.315) data 0.000 (0.229) loss 1.8887 (1.8750) acc 46.8750 (45.6250) lr 5.8849e-04 eta 0:18:50
epoch [129/200] batch [10/50] time 0.086 (0.206) data 0.001 (0.120) loss 1.7246 (1.8755) acc 43.7500 (42.8125) lr 5.8849e-04 eta 0:12:20
epoch [129/200] batch [15/50] time 0.086 (0.166) data 0.000 (0.080) loss 1.5400 (1.8663) acc 65.6250 (46.2500) lr 5.8849e-04 eta 0:09:55
epoch [129/200] batch [20/50] time 0.086 (0.150) data 0.000 (0.064) loss 1.7715 (1.8908) acc 40.6250 (44.3750) lr 5.8849e-04 eta 0:08:55
epoch [129/200] batch [25/50] time 0.087 (0.137) data 0.001 (0.051) loss 2.0723 (1.8819) acc 28.1250 (43.8750) lr 5.8849e-04 eta 0:08:09
epoch [129/200] batch [30/50] time 0.086 (0.132) data 0.000 (0.047) loss 1.2969 (1.8650) acc 65.6250 (44.3750) lr 5.8849e-04 eta 0:07:52
epoch [129/200] batch [35/50] time 0.187 (0.129) data 0.103 (0.043) loss 1.9229 (1.8570) acc 37.5000 (44.1071) lr 5.8849e-04 eta 0:07:39
epoch [129/200] batch [40/50] time 0.083 (0.123) data 0.000 (0.038) loss 1.8350 (1.8791) acc 50.0000 (44.1406) lr 5.8849e-04 eta 0:07:18
epoch [129/200] batch [45/50] time 0.083 (0.121) data 0.000 (0.036) loss 1.7002 (1.8934) acc 46.8750 (43.8194) lr 5.8849e-04 eta 0:07:10
epoch [129/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.033) loss 1.9902 (1.9299) acc 53.1250 (43.1875) lr 5.7422e-04 eta 0:06:56
epoch [130/200] batch [5/50] time 0.083 (0.338) data 0.000 (0.255) loss 1.7070 (1.8750) acc 43.7500 (43.7500) lr 5.7422e-04 eta 0:19:58
epoch [130/200] batch [10/50] time 0.177 (0.220) data 0.094 (0.137) loss 2.1914 (1.9141) acc 46.8750 (44.6875) lr 5.7422e-04 eta 0:13:00
epoch [130/200] batch [15/50] time 0.084 (0.175) data 0.000 (0.091) loss 1.9453 (1.9284) acc 43.7500 (44.1667) lr 5.7422e-04 eta 0:10:19
epoch [130/200] batch [20/50] time 0.084 (0.162) data 0.000 (0.078) loss 2.2402 (1.9569) acc 37.5000 (43.7500) lr 5.7422e-04 eta 0:09:32
epoch [130/200] batch [25/50] time 0.085 (0.147) data 0.000 (0.063) loss 2.2715 (1.9675) acc 40.6250 (42.7500) lr 5.7422e-04 eta 0:08:37
epoch [130/200] batch [30/50] time 0.085 (0.141) data 0.000 (0.057) loss 1.4395 (1.9353) acc 53.1250 (43.5417) lr 5.7422e-04 eta 0:08:17
epoch [130/200] batch [35/50] time 0.083 (0.138) data 0.000 (0.054) loss 1.4893 (1.9066) acc 53.1250 (44.0179) lr 5.7422e-04 eta 0:08:04
epoch [130/200] batch [40/50] time 0.084 (0.131) data 0.000 (0.047) loss 1.9404 (1.9083) acc 40.6250 (43.5938) lr 5.7422e-04 eta 0:07:39
epoch [130/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.042) loss 2.0078 (1.9149) acc 46.8750 (43.9583) lr 5.7422e-04 eta 0:07:22
epoch [130/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.038) loss 2.3320 (1.9177) acc 43.7500 (44.3750) lr 5.6006e-04 eta 0:07:06
epoch [131/200] batch [5/50] time 0.084 (0.300) data 0.000 (0.216) loss 1.7998 (1.9221) acc 40.6250 (41.8750) lr 5.6006e-04 eta 0:17:30
epoch [131/200] batch [10/50] time 0.084 (0.193) data 0.000 (0.108) loss 1.2881 (1.9017) acc 59.3750 (42.8125) lr 5.6006e-04 eta 0:11:11
epoch [131/200] batch [15/50] time 0.084 (0.161) data 0.000 (0.077) loss 1.7314 (1.9278) acc 34.3750 (41.0417) lr 5.6006e-04 eta 0:09:22
epoch [131/200] batch [20/50] time 0.225 (0.151) data 0.141 (0.067) loss 2.4668 (1.9678) acc 34.3750 (42.3438) lr 5.6006e-04 eta 0:08:46
epoch [131/200] batch [25/50] time 0.085 (0.138) data 0.000 (0.054) loss 1.7266 (1.9271) acc 46.8750 (42.2500) lr 5.6006e-04 eta 0:07:59
epoch [131/200] batch [30/50] time 0.085 (0.135) data 0.000 (0.051) loss 1.5264 (1.9029) acc 53.1250 (42.7083) lr 5.6006e-04 eta 0:07:48
epoch [131/200] batch [35/50] time 0.085 (0.128) data 0.000 (0.044) loss 1.9092 (1.8994) acc 43.7500 (43.3036) lr 5.6006e-04 eta 0:07:23
epoch [131/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.044) loss 2.1016 (1.9002) acc 46.8750 (43.3594) lr 5.6006e-04 eta 0:07:24
epoch [131/200] batch [45/50] time 0.085 (0.125) data 0.000 (0.041) loss 2.0020 (1.9055) acc 34.3750 (43.1944) lr 5.6006e-04 eta 0:07:11
epoch [131/200] batch [50/50] time 0.084 (0.121) data 0.000 (0.037) loss 1.8701 (1.9170) acc 50.0000 (42.8125) lr 5.4601e-04 eta 0:06:56
epoch [132/200] batch [5/50] time 0.084 (0.298) data 0.000 (0.213) loss 1.7646 (1.8574) acc 50.0000 (45.6250) lr 5.4601e-04 eta 0:17:06
epoch [132/200] batch [10/50] time 0.146 (0.197) data 0.063 (0.113) loss 2.3809 (1.9014) acc 37.5000 (44.0625) lr 5.4601e-04 eta 0:11:19
epoch [132/200] batch [15/50] time 0.084 (0.160) data 0.001 (0.076) loss 2.3242 (1.9346) acc 25.0000 (42.7083) lr 5.4601e-04 eta 0:09:08
epoch [132/200] batch [20/50] time 0.086 (0.141) data 0.000 (0.057) loss 2.3711 (1.9510) acc 31.2500 (41.7188) lr 5.4601e-04 eta 0:08:02
epoch [132/200] batch [25/50] time 0.085 (0.130) data 0.000 (0.046) loss 1.7539 (1.9530) acc 53.1250 (42.7500) lr 5.4601e-04 eta 0:07:26
epoch [132/200] batch [30/50] time 0.084 (0.124) data 0.000 (0.040) loss 2.0410 (1.9778) acc 40.6250 (41.6667) lr 5.4601e-04 eta 0:07:03
epoch [132/200] batch [35/50] time 0.085 (0.118) data 0.000 (0.034) loss 2.0996 (1.9615) acc 43.7500 (41.8750) lr 5.4601e-04 eta 0:06:43
epoch [132/200] batch [40/50] time 0.083 (0.114) data 0.000 (0.030) loss 1.7207 (1.9340) acc 50.0000 (42.7344) lr 5.4601e-04 eta 0:06:28
epoch [132/200] batch [45/50] time 0.083 (0.110) data 0.000 (0.027) loss 1.9277 (1.9256) acc 37.5000 (42.8472) lr 5.4601e-04 eta 0:06:16
epoch [132/200] batch [50/50] time 0.085 (0.108) data 0.000 (0.024) loss 1.5518 (1.9165) acc 46.8750 (43.1250) lr 5.3207e-04 eta 0:06:06
epoch [133/200] batch [5/50] time 0.084 (0.272) data 0.000 (0.188) loss 2.2090 (2.1158) acc 34.3750 (38.1250) lr 5.3207e-04 eta 0:15:22
epoch [133/200] batch [10/50] time 0.084 (0.178) data 0.000 (0.094) loss 1.7471 (2.0057) acc 40.6250 (41.8750) lr 5.3207e-04 eta 0:10:04
epoch [133/200] batch [15/50] time 0.086 (0.147) data 0.001 (0.063) loss 2.0918 (2.0022) acc 31.2500 (40.4167) lr 5.3207e-04 eta 0:08:18
epoch [133/200] batch [20/50] time 0.084 (0.132) data 0.000 (0.047) loss 1.6221 (1.9792) acc 56.2500 (41.8750) lr 5.3207e-04 eta 0:07:24
epoch [133/200] batch [25/50] time 0.084 (0.124) data 0.000 (0.040) loss 1.6689 (1.9891) acc 56.2500 (42.7500) lr 5.3207e-04 eta 0:06:57
epoch [133/200] batch [30/50] time 0.192 (0.121) data 0.107 (0.037) loss 1.9756 (2.0061) acc 37.5000 (42.0833) lr 5.3207e-04 eta 0:06:47
epoch [133/200] batch [35/50] time 0.084 (0.118) data 0.000 (0.033) loss 1.7803 (1.9842) acc 46.8750 (42.3214) lr 5.3207e-04 eta 0:06:35
epoch [133/200] batch [40/50] time 0.085 (0.116) data 0.000 (0.032) loss 2.2500 (1.9800) acc 37.5000 (41.9531) lr 5.3207e-04 eta 0:06:29
epoch [133/200] batch [45/50] time 0.083 (0.112) data 0.000 (0.028) loss 1.7920 (1.9627) acc 37.5000 (42.3611) lr 5.3207e-04 eta 0:06:16
epoch [133/200] batch [50/50] time 0.085 (0.110) data 0.000 (0.026) loss 2.1250 (1.9571) acc 25.0000 (42.5000) lr 5.1825e-04 eta 0:06:09
epoch [134/200] batch [5/50] time 0.086 (0.281) data 0.000 (0.196) loss 1.6455 (1.8791) acc 53.1250 (46.8750) lr 5.1825e-04 eta 0:15:39
epoch [134/200] batch [10/50] time 0.085 (0.183) data 0.000 (0.098) loss 1.7529 (1.9061) acc 37.5000 (43.4375) lr 5.1825e-04 eta 0:10:12
epoch [134/200] batch [15/50] time 0.085 (0.152) data 0.000 (0.067) loss 1.9180 (1.8971) acc 46.8750 (43.3333) lr 5.1825e-04 eta 0:08:27
epoch [134/200] batch [20/50] time 0.085 (0.140) data 0.000 (0.055) loss 1.8496 (1.9380) acc 37.5000 (42.3438) lr 5.1825e-04 eta 0:07:45
epoch [134/200] batch [25/50] time 0.085 (0.129) data 0.000 (0.044) loss 1.9209 (1.9102) acc 53.1250 (43.6250) lr 5.1825e-04 eta 0:07:07
epoch [134/200] batch [30/50] time 0.085 (0.121) data 0.000 (0.037) loss 1.7910 (1.8978) acc 43.7500 (43.1250) lr 5.1825e-04 eta 0:06:43
epoch [134/200] batch [35/50] time 0.084 (0.116) data 0.001 (0.031) loss 1.5820 (1.8799) acc 53.1250 (43.9286) lr 5.1825e-04 eta 0:06:25
epoch [134/200] batch [40/50] time 0.084 (0.112) data 0.000 (0.028) loss 2.3789 (1.8929) acc 34.3750 (44.2188) lr 5.1825e-04 eta 0:06:11
epoch [134/200] batch [45/50] time 0.082 (0.109) data 0.000 (0.025) loss 1.9463 (1.8842) acc 56.2500 (44.3750) lr 5.1825e-04 eta 0:06:00
epoch [134/200] batch [50/50] time 0.083 (0.106) data 0.000 (0.022) loss 1.4473 (1.8967) acc 59.3750 (44.1875) lr 5.0454e-04 eta 0:05:51
epoch [135/200] batch [5/50] time 0.085 (0.312) data 0.000 (0.228) loss 2.3223 (2.0068) acc 40.6250 (48.7500) lr 5.0454e-04 eta 0:17:08
epoch [135/200] batch [10/50] time 0.085 (0.212) data 0.000 (0.128) loss 1.9336 (1.9884) acc 53.1250 (45.3125) lr 5.0454e-04 eta 0:11:38
epoch [135/200] batch [15/50] time 0.084 (0.170) data 0.000 (0.086) loss 1.7676 (1.9735) acc 50.0000 (45.0000) lr 5.0454e-04 eta 0:09:17
epoch [135/200] batch [20/50] time 0.084 (0.148) data 0.000 (0.064) loss 1.6816 (1.9229) acc 50.0000 (45.9375) lr 5.0454e-04 eta 0:08:06
epoch [135/200] batch [25/50] time 0.084 (0.135) data 0.000 (0.052) loss 1.8311 (1.9520) acc 46.8750 (43.8750) lr 5.0454e-04 eta 0:07:23
epoch [135/200] batch [30/50] time 0.085 (0.127) data 0.000 (0.043) loss 1.4951 (1.9196) acc 50.0000 (43.9583) lr 5.0454e-04 eta 0:06:54
epoch [135/200] batch [35/50] time 0.083 (0.125) data 0.000 (0.042) loss 2.2402 (1.9460) acc 43.7500 (43.6607) lr 5.0454e-04 eta 0:06:49
epoch [135/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.036) loss 1.7158 (1.9534) acc 53.1250 (44.0625) lr 5.0454e-04 eta 0:06:31
epoch [135/200] batch [45/50] time 0.083 (0.116) data 0.000 (0.032) loss 2.1211 (1.9664) acc 43.7500 (44.0972) lr 5.0454e-04 eta 0:06:17
epoch [135/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 1.5840 (1.9678) acc 62.5000 (44.0000) lr 4.9096e-04 eta 0:06:12
epoch [136/200] batch [5/50] time 0.085 (0.262) data 0.000 (0.177) loss 1.7529 (2.0344) acc 50.0000 (43.1250) lr 4.9096e-04 eta 0:14:10
epoch [136/200] batch [10/50] time 0.181 (0.183) data 0.098 (0.099) loss 1.7129 (1.9597) acc 50.0000 (45.3125) lr 4.9096e-04 eta 0:09:53
epoch [136/200] batch [15/50] time 0.085 (0.156) data 0.000 (0.071) loss 2.3828 (2.0049) acc 21.8750 (42.0833) lr 4.9096e-04 eta 0:08:24
epoch [136/200] batch [20/50] time 0.200 (0.147) data 0.118 (0.062) loss 1.8984 (1.9831) acc 40.6250 (43.1250) lr 4.9096e-04 eta 0:07:53
epoch [136/200] batch [25/50] time 0.085 (0.134) data 0.000 (0.050) loss 1.6318 (1.9589) acc 50.0000 (44.3750) lr 4.9096e-04 eta 0:07:12
epoch [136/200] batch [30/50] time 0.084 (0.126) data 0.000 (0.042) loss 2.2324 (1.9289) acc 43.7500 (44.5833) lr 4.9096e-04 eta 0:06:45
epoch [136/200] batch [35/50] time 0.084 (0.120) data 0.001 (0.036) loss 1.4580 (1.9000) acc 56.2500 (45.1786) lr 4.9096e-04 eta 0:06:25
epoch [136/200] batch [40/50] time 0.084 (0.115) data 0.000 (0.031) loss 1.8389 (1.8883) acc 40.6250 (45.0000) lr 4.9096e-04 eta 0:06:10
epoch [136/200] batch [45/50] time 0.082 (0.112) data 0.000 (0.028) loss 1.4551 (1.8984) acc 59.3750 (44.9306) lr 4.9096e-04 eta 0:05:58
epoch [136/200] batch [50/50] time 0.083 (0.109) data 0.000 (0.025) loss 2.2305 (1.8946) acc 37.5000 (44.8125) lr 4.7750e-04 eta 0:05:48
epoch [137/200] batch [5/50] time 0.086 (0.310) data 0.000 (0.224) loss 1.7910 (1.7846) acc 40.6250 (45.0000) lr 4.7750e-04 eta 0:16:29
epoch [137/200] batch [10/50] time 0.085 (0.213) data 0.000 (0.128) loss 2.4512 (1.8563) acc 28.1250 (47.1875) lr 4.7750e-04 eta 0:11:20
epoch [137/200] batch [15/50] time 0.085 (0.171) data 0.000 (0.086) loss 1.5928 (1.8882) acc 40.6250 (45.0000) lr 4.7750e-04 eta 0:09:03
epoch [137/200] batch [20/50] time 0.084 (0.154) data 0.000 (0.069) loss 2.2520 (1.9215) acc 46.8750 (44.8438) lr 4.7750e-04 eta 0:08:10
epoch [137/200] batch [25/50] time 0.086 (0.140) data 0.001 (0.056) loss 1.8223 (1.9189) acc 43.7500 (44.3750) lr 4.7750e-04 eta 0:07:25
epoch [137/200] batch [30/50] time 0.085 (0.131) data 0.000 (0.046) loss 2.0918 (1.9008) acc 46.8750 (45.2083) lr 4.7750e-04 eta 0:06:55
epoch [137/200] batch [35/50] time 0.085 (0.125) data 0.001 (0.040) loss 2.1191 (1.9105) acc 43.7500 (45.0893) lr 4.7750e-04 eta 0:06:34
epoch [137/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.035) loss 2.5918 (1.9251) acc 25.0000 (44.6094) lr 4.7750e-04 eta 0:06:18
epoch [137/200] batch [45/50] time 0.084 (0.116) data 0.000 (0.031) loss 1.9482 (1.9164) acc 34.3750 (44.5833) lr 4.7750e-04 eta 0:06:05
epoch [137/200] batch [50/50] time 0.083 (0.112) data 0.000 (0.028) loss 1.9814 (1.8985) acc 28.1250 (44.3125) lr 4.6417e-04 eta 0:05:54
epoch [138/200] batch [5/50] time 0.085 (0.320) data 0.000 (0.235) loss 1.8398 (1.7582) acc 53.1250 (54.3750) lr 4.6417e-04 eta 0:16:46
epoch [138/200] batch [10/50] time 0.100 (0.211) data 0.000 (0.125) loss 1.9561 (1.9603) acc 31.2500 (44.6875) lr 4.6417e-04 eta 0:11:03
epoch [138/200] batch [15/50] time 0.085 (0.173) data 0.000 (0.084) loss 1.5674 (1.8637) acc 37.5000 (44.1667) lr 4.6417e-04 eta 0:09:02
epoch [138/200] batch [20/50] time 0.085 (0.152) data 0.000 (0.064) loss 1.9961 (1.9263) acc 34.3750 (42.0312) lr 4.6417e-04 eta 0:07:56
epoch [138/200] batch [25/50] time 0.192 (0.143) data 0.109 (0.056) loss 2.5625 (1.9609) acc 21.8750 (41.5000) lr 4.6417e-04 eta 0:07:27
epoch [138/200] batch [30/50] time 0.085 (0.133) data 0.000 (0.047) loss 1.9092 (1.9417) acc 43.7500 (42.0833) lr 4.6417e-04 eta 0:06:56
epoch [138/200] batch [35/50] time 0.084 (0.130) data 0.001 (0.043) loss 1.8936 (1.9217) acc 37.5000 (42.3214) lr 4.6417e-04 eta 0:06:43
epoch [138/200] batch [40/50] time 0.084 (0.124) data 0.000 (0.038) loss 1.7549 (1.9236) acc 50.0000 (42.1875) lr 4.6417e-04 eta 0:06:26
epoch [138/200] batch [45/50] time 0.083 (0.121) data 0.000 (0.035) loss 1.6123 (1.9149) acc 56.2500 (43.0556) lr 4.6417e-04 eta 0:06:14
epoch [138/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.031) loss 2.0430 (1.9310) acc 46.8750 (43.0000) lr 4.5098e-04 eta 0:06:02
epoch [139/200] batch [5/50] time 0.086 (0.294) data 0.000 (0.209) loss 1.8916 (1.8734) acc 50.0000 (46.8750) lr 4.5098e-04 eta 0:15:10
epoch [139/200] batch [10/50] time 0.109 (0.192) data 0.024 (0.107) loss 1.6309 (1.8961) acc 50.0000 (43.4375) lr 4.5098e-04 eta 0:09:54
epoch [139/200] batch [15/50] time 0.085 (0.157) data 0.000 (0.071) loss 1.7188 (1.8482) acc 43.7500 (43.9583) lr 4.5098e-04 eta 0:08:02
epoch [139/200] batch [20/50] time 0.085 (0.147) data 0.000 (0.062) loss 1.5830 (1.8181) acc 43.7500 (45.1562) lr 4.5098e-04 eta 0:07:32
epoch [139/200] batch [25/50] time 0.199 (0.139) data 0.114 (0.054) loss 1.9043 (1.8127) acc 43.7500 (45.1250) lr 4.5098e-04 eta 0:07:07
epoch [139/200] batch [30/50] time 0.085 (0.130) data 0.001 (0.045) loss 1.9336 (1.8396) acc 56.2500 (45.4167) lr 4.5098e-04 eta 0:06:39
epoch [139/200] batch [35/50] time 0.085 (0.124) data 0.000 (0.039) loss 2.2637 (1.8487) acc 34.3750 (45.2679) lr 4.5098e-04 eta 0:06:19
epoch [139/200] batch [40/50] time 0.085 (0.121) data 0.000 (0.036) loss 1.6982 (1.8561) acc 56.2500 (45.4688) lr 4.5098e-04 eta 0:06:10
epoch [139/200] batch [45/50] time 0.083 (0.122) data 0.000 (0.037) loss 2.1836 (1.8543) acc 46.8750 (45.6250) lr 4.5098e-04 eta 0:06:11
epoch [139/200] batch [50/50] time 0.082 (0.118) data 0.000 (0.033) loss 1.9805 (1.8863) acc 40.6250 (45.1250) lr 4.3792e-04 eta 0:05:59
epoch [140/200] batch [5/50] time 0.085 (0.296) data 0.000 (0.210) loss 2.0176 (1.9209) acc 28.1250 (41.2500) lr 4.3792e-04 eta 0:15:00
epoch [140/200] batch [10/50] time 0.084 (0.197) data 0.000 (0.112) loss 1.8115 (1.9296) acc 53.1250 (44.6875) lr 4.3792e-04 eta 0:09:57
epoch [140/200] batch [15/50] time 0.084 (0.162) data 0.000 (0.077) loss 1.7461 (1.9136) acc 43.7500 (45.6250) lr 4.3792e-04 eta 0:08:11
epoch [140/200] batch [20/50] time 0.184 (0.149) data 0.102 (0.065) loss 1.8906 (1.8755) acc 40.6250 (46.0938) lr 4.3792e-04 eta 0:07:32
epoch [140/200] batch [25/50] time 0.084 (0.136) data 0.000 (0.052) loss 2.1230 (1.8926) acc 34.3750 (44.8750) lr 4.3792e-04 eta 0:06:52
epoch [140/200] batch [30/50] time 0.084 (0.138) data 0.000 (0.054) loss 1.8242 (1.8782) acc 53.1250 (45.0000) lr 4.3792e-04 eta 0:06:57
epoch [140/200] batch [35/50] time 0.086 (0.131) data 0.001 (0.046) loss 1.6963 (1.8625) acc 46.8750 (45.3571) lr 4.3792e-04 eta 0:06:33
epoch [140/200] batch [40/50] time 0.084 (0.129) data 0.000 (0.045) loss 1.9629 (1.8551) acc 56.2500 (45.6250) lr 4.3792e-04 eta 0:06:29
epoch [140/200] batch [45/50] time 0.084 (0.124) data 0.000 (0.040) loss 1.6465 (1.8360) acc 56.2500 (46.2500) lr 4.3792e-04 eta 0:06:13
epoch [140/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.6162 (1.8402) acc 56.2500 (46.0625) lr 4.2499e-04 eta 0:06:00
epoch [141/200] batch [5/50] time 0.084 (0.323) data 0.000 (0.239) loss 2.1836 (2.0492) acc 34.3750 (41.8750) lr 4.2499e-04 eta 0:16:08
epoch [141/200] batch [10/50] time 0.085 (0.211) data 0.000 (0.127) loss 1.6406 (1.9529) acc 46.8750 (44.0625) lr 4.2499e-04 eta 0:10:29
epoch [141/200] batch [15/50] time 0.084 (0.169) data 0.000 (0.085) loss 1.8633 (1.9305) acc 46.8750 (44.5833) lr 4.2499e-04 eta 0:08:23
epoch [141/200] batch [20/50] time 0.084 (0.149) data 0.000 (0.065) loss 2.0176 (1.9131) acc 37.5000 (45.4688) lr 4.2499e-04 eta 0:07:23
epoch [141/200] batch [25/50] time 0.231 (0.142) data 0.148 (0.058) loss 1.7900 (1.8875) acc 37.5000 (45.3750) lr 4.2499e-04 eta 0:07:01
epoch [141/200] batch [30/50] time 0.084 (0.132) data 0.000 (0.048) loss 1.7783 (1.8935) acc 40.6250 (45.1042) lr 4.2499e-04 eta 0:06:32
epoch [141/200] batch [35/50] time 0.084 (0.129) data 0.000 (0.045) loss 2.1270 (1.8796) acc 40.6250 (45.0893) lr 4.2499e-04 eta 0:06:22
epoch [141/200] batch [40/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.1211 (1.9083) acc 40.6250 (43.8281) lr 4.2499e-04 eta 0:06:05
epoch [141/200] batch [45/50] time 0.084 (0.119) data 0.000 (0.035) loss 1.7256 (1.9196) acc 43.7500 (43.6806) lr 4.2499e-04 eta 0:05:51
epoch [141/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.032) loss 1.8477 (1.9157) acc 43.7500 (43.7500) lr 4.1221e-04 eta 0:05:40
epoch [142/200] batch [5/50] time 0.085 (0.313) data 0.000 (0.228) loss 2.0605 (1.8523) acc 31.2500 (43.1250) lr 4.1221e-04 eta 0:15:22
epoch [142/200] batch [10/50] time 0.084 (0.199) data 0.000 (0.114) loss 2.0391 (1.9376) acc 31.2500 (41.8750) lr 4.1221e-04 eta 0:09:43
epoch [142/200] batch [15/50] time 0.084 (0.160) data 0.000 (0.076) loss 1.3877 (1.9245) acc 62.5000 (42.2917) lr 4.1221e-04 eta 0:07:50
epoch [142/200] batch [20/50] time 0.090 (0.142) data 0.006 (0.058) loss 1.8896 (1.8591) acc 37.5000 (44.8438) lr 4.1221e-04 eta 0:06:55
epoch [142/200] batch [25/50] time 0.084 (0.130) data 0.000 (0.046) loss 2.1660 (1.8968) acc 31.2500 (43.6250) lr 4.1221e-04 eta 0:06:20
epoch [142/200] batch [30/50] time 0.084 (0.128) data 0.000 (0.044) loss 2.0840 (1.8937) acc 34.3750 (42.9167) lr 4.1221e-04 eta 0:06:12
epoch [142/200] batch [35/50] time 0.084 (0.122) data 0.001 (0.038) loss 2.1055 (1.9064) acc 43.7500 (42.3214) lr 4.1221e-04 eta 0:05:54
epoch [142/200] batch [40/50] time 0.084 (0.121) data 0.000 (0.037) loss 1.7061 (1.9030) acc 50.0000 (42.5000) lr 4.1221e-04 eta 0:05:51
epoch [142/200] batch [45/50] time 0.082 (0.120) data 0.000 (0.036) loss 1.9121 (1.9206) acc 40.6250 (42.7778) lr 4.1221e-04 eta 0:05:49
epoch [142/200] batch [50/50] time 0.084 (0.117) data 0.000 (0.033) loss 2.0098 (1.9054) acc 40.6250 (43.3125) lr 3.9958e-04 eta 0:05:37
epoch [143/200] batch [5/50] time 0.084 (0.307) data 0.000 (0.223) loss 1.6387 (2.0758) acc 46.8750 (38.1250) lr 3.9958e-04 eta 0:14:48
epoch [143/200] batch [10/50] time 0.086 (0.196) data 0.000 (0.111) loss 1.8037 (2.0306) acc 46.8750 (40.3125) lr 3.9958e-04 eta 0:09:26
epoch [143/200] batch [15/50] time 0.085 (0.159) data 0.000 (0.074) loss 1.9111 (2.0264) acc 37.5000 (41.0417) lr 3.9958e-04 eta 0:07:39
epoch [143/200] batch [20/50] time 0.087 (0.141) data 0.001 (0.056) loss 1.6650 (2.0190) acc 46.8750 (40.0000) lr 3.9958e-04 eta 0:06:46
epoch [143/200] batch [25/50] time 0.085 (0.130) data 0.000 (0.045) loss 2.0215 (2.0189) acc 40.6250 (40.0000) lr 3.9958e-04 eta 0:06:13
epoch [143/200] batch [30/50] time 0.085 (0.123) data 0.000 (0.037) loss 1.4775 (1.9815) acc 50.0000 (41.4583) lr 3.9958e-04 eta 0:05:51
epoch [143/200] batch [35/50] time 0.085 (0.117) data 0.001 (0.032) loss 1.7139 (1.9635) acc 43.7500 (41.7857) lr 3.9958e-04 eta 0:05:35
epoch [143/200] batch [40/50] time 0.083 (0.113) data 0.000 (0.028) loss 1.7012 (1.9464) acc 59.3750 (42.5000) lr 3.9958e-04 eta 0:05:23
epoch [143/200] batch [45/50] time 0.084 (0.110) data 0.000 (0.025) loss 2.1523 (1.9502) acc 53.1250 (42.8472) lr 3.9958e-04 eta 0:05:13
epoch [143/200] batch [50/50] time 0.084 (0.107) data 0.000 (0.023) loss 2.0527 (1.9457) acc 43.7500 (43.1875) lr 3.8709e-04 eta 0:05:05
epoch [144/200] batch [5/50] time 0.086 (0.319) data 0.000 (0.233) loss 1.7471 (2.1379) acc 43.7500 (37.5000) lr 3.8709e-04 eta 0:15:08
epoch [144/200] batch [10/50] time 0.085 (0.203) data 0.000 (0.117) loss 2.0234 (1.9909) acc 43.7500 (39.6875) lr 3.8709e-04 eta 0:09:35
epoch [144/200] batch [15/50] time 0.085 (0.163) data 0.000 (0.078) loss 1.8867 (1.9533) acc 50.0000 (41.2500) lr 3.8709e-04 eta 0:07:43
epoch [144/200] batch [20/50] time 0.086 (0.146) data 0.000 (0.061) loss 1.8818 (1.9188) acc 43.7500 (42.1875) lr 3.8709e-04 eta 0:06:53
epoch [144/200] batch [25/50] time 0.086 (0.134) data 0.000 (0.049) loss 1.9951 (1.8839) acc 46.8750 (43.7500) lr 3.8709e-04 eta 0:06:18
epoch [144/200] batch [30/50] time 0.084 (0.133) data 0.000 (0.048) loss 2.3145 (1.8938) acc 34.3750 (43.4375) lr 3.8709e-04 eta 0:06:15
epoch [144/200] batch [35/50] time 0.084 (0.129) data 0.000 (0.044) loss 1.8545 (1.9048) acc 53.1250 (43.3036) lr 3.8709e-04 eta 0:06:02
epoch [144/200] batch [40/50] time 0.083 (0.123) data 0.000 (0.038) loss 1.7490 (1.9160) acc 50.0000 (43.2812) lr 3.8709e-04 eta 0:05:46
epoch [144/200] batch [45/50] time 0.084 (0.123) data 0.000 (0.039) loss 1.9863 (1.9265) acc 56.2500 (43.4028) lr 3.8709e-04 eta 0:05:45
epoch [144/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 1.6836 (1.9446) acc 53.1250 (43.3125) lr 3.7476e-04 eta 0:05:33
epoch [145/200] batch [5/50] time 0.086 (0.299) data 0.000 (0.213) loss 1.6650 (1.9215) acc 59.3750 (45.0000) lr 3.7476e-04 eta 0:13:55
epoch [145/200] batch [10/50] time 0.086 (0.200) data 0.000 (0.115) loss 2.0723 (1.9920) acc 40.6250 (40.9375) lr 3.7476e-04 eta 0:09:18
epoch [145/200] batch [15/50] time 0.086 (0.162) data 0.000 (0.077) loss 1.8477 (1.9102) acc 46.8750 (43.3333) lr 3.7476e-04 eta 0:07:31
epoch [145/200] batch [20/50] time 0.086 (0.156) data 0.000 (0.071) loss 1.7373 (1.8853) acc 46.8750 (43.5938) lr 3.7476e-04 eta 0:07:14
epoch [145/200] batch [25/50] time 0.101 (0.143) data 0.015 (0.057) loss 2.1172 (1.9059) acc 37.5000 (42.8750) lr 3.7476e-04 eta 0:06:36
epoch [145/200] batch [30/50] time 0.086 (0.133) data 0.000 (0.048) loss 1.8398 (1.9046) acc 46.8750 (42.6042) lr 3.7476e-04 eta 0:06:09
epoch [145/200] batch [35/50] time 0.085 (0.126) data 0.000 (0.041) loss 1.8301 (1.9242) acc 56.2500 (42.7679) lr 3.7476e-04 eta 0:05:49
epoch [145/200] batch [40/50] time 0.085 (0.121) data 0.000 (0.036) loss 1.7480 (1.9271) acc 40.6250 (42.2656) lr 3.7476e-04 eta 0:05:34
epoch [145/200] batch [45/50] time 0.084 (0.117) data 0.000 (0.032) loss 1.8770 (1.9201) acc 56.2500 (42.7778) lr 3.7476e-04 eta 0:05:22
epoch [145/200] batch [50/50] time 0.082 (0.114) data 0.000 (0.029) loss 2.2676 (1.9348) acc 31.2500 (42.7500) lr 3.6258e-04 eta 0:05:12
epoch [146/200] batch [5/50] time 0.086 (0.354) data 0.001 (0.269) loss 1.5664 (1.7971) acc 56.2500 (46.2500) lr 3.6258e-04 eta 0:16:12
epoch [146/200] batch [10/50] time 0.115 (0.223) data 0.028 (0.138) loss 1.7969 (1.8520) acc 50.0000 (46.5625) lr 3.6258e-04 eta 0:10:10
epoch [146/200] batch [15/50] time 0.086 (0.177) data 0.000 (0.092) loss 2.0254 (1.8602) acc 43.7500 (46.2500) lr 3.6258e-04 eta 0:08:04
epoch [146/200] batch [20/50] time 0.084 (0.154) data 0.000 (0.069) loss 1.6836 (1.8766) acc 46.8750 (45.1562) lr 3.6258e-04 eta 0:07:00
epoch [146/200] batch [25/50] time 0.085 (0.140) data 0.000 (0.055) loss 1.7012 (1.8914) acc 46.8750 (44.3750) lr 3.6258e-04 eta 0:06:22
epoch [146/200] batch [30/50] time 0.096 (0.134) data 0.011 (0.049) loss 1.8301 (1.9118) acc 40.6250 (44.0625) lr 3.6258e-04 eta 0:06:04
epoch [146/200] batch [35/50] time 0.086 (0.129) data 0.000 (0.043) loss 1.8535 (1.8980) acc 46.8750 (44.3750) lr 3.6258e-04 eta 0:05:48
epoch [146/200] batch [40/50] time 0.084 (0.125) data 0.000 (0.040) loss 1.6592 (1.8759) acc 40.6250 (44.7656) lr 3.6258e-04 eta 0:05:39
epoch [146/200] batch [45/50] time 0.084 (0.121) data 0.000 (0.036) loss 2.0508 (1.8823) acc 40.6250 (45.1389) lr 3.6258e-04 eta 0:05:26
epoch [146/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.033) loss 1.8877 (1.8875) acc 40.6250 (44.5625) lr 3.5055e-04 eta 0:05:18
epoch [147/200] batch [5/50] time 0.085 (0.311) data 0.001 (0.225) loss 1.8184 (1.8662) acc 46.8750 (41.2500) lr 3.5055e-04 eta 0:13:57
epoch [147/200] batch [10/50] time 0.161 (0.206) data 0.077 (0.120) loss 2.1406 (1.9333) acc 40.6250 (41.2500) lr 3.5055e-04 eta 0:09:14
epoch [147/200] batch [15/50] time 0.085 (0.166) data 0.000 (0.080) loss 2.1133 (1.9000) acc 34.3750 (41.8750) lr 3.5055e-04 eta 0:07:25
epoch [147/200] batch [20/50] time 0.086 (0.155) data 0.000 (0.070) loss 1.7168 (1.9237) acc 43.7500 (42.3438) lr 3.5055e-04 eta 0:06:55
epoch [147/200] batch [25/50] time 0.086 (0.141) data 0.000 (0.056) loss 1.6709 (1.9066) acc 37.5000 (42.8750) lr 3.5055e-04 eta 0:06:16
epoch [147/200] batch [30/50] time 0.084 (0.137) data 0.000 (0.052) loss 1.7451 (1.8814) acc 31.2500 (43.1250) lr 3.5055e-04 eta 0:06:06
epoch [147/200] batch [35/50] time 0.084 (0.133) data 0.000 (0.048) loss 1.7227 (1.8779) acc 43.7500 (43.8393) lr 3.5055e-04 eta 0:05:53
epoch [147/200] batch [40/50] time 0.084 (0.127) data 0.000 (0.042) loss 2.3984 (1.8927) acc 31.2500 (43.5938) lr 3.5055e-04 eta 0:05:36
epoch [147/200] batch [45/50] time 0.084 (0.125) data 0.000 (0.040) loss 2.0332 (1.8964) acc 28.1250 (43.1250) lr 3.5055e-04 eta 0:05:30
epoch [147/200] batch [50/50] time 0.085 (0.121) data 0.000 (0.036) loss 1.7305 (1.8960) acc 56.2500 (43.4375) lr 3.3869e-04 eta 0:05:19
epoch [148/200] batch [5/50] time 0.085 (0.295) data 0.000 (0.210) loss 1.9951 (1.7254) acc 40.6250 (50.0000) lr 3.3869e-04 eta 0:12:59
epoch [148/200] batch [10/50] time 0.085 (0.190) data 0.000 (0.105) loss 1.7930 (1.8113) acc 53.1250 (48.1250) lr 3.3869e-04 eta 0:08:20
epoch [148/200] batch [15/50] time 0.084 (0.165) data 0.000 (0.080) loss 1.6494 (1.8785) acc 53.1250 (45.6250) lr 3.3869e-04 eta 0:07:13
epoch [148/200] batch [20/50] time 0.251 (0.153) data 0.167 (0.069) loss 1.8281 (1.8722) acc 43.7500 (45.3125) lr 3.3869e-04 eta 0:06:42
epoch [148/200] batch [25/50] time 0.084 (0.139) data 0.000 (0.055) loss 2.3164 (1.8624) acc 43.7500 (45.1250) lr 3.3869e-04 eta 0:06:05
epoch [148/200] batch [30/50] time 0.085 (0.136) data 0.000 (0.052) loss 2.4805 (1.8669) acc 34.3750 (44.5833) lr 3.3869e-04 eta 0:05:55
epoch [148/200] batch [35/50] time 0.085 (0.129) data 0.000 (0.044) loss 1.9658 (1.8708) acc 43.7500 (44.9107) lr 3.3869e-04 eta 0:05:36
epoch [148/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.042) loss 2.2500 (1.8792) acc 34.3750 (44.4531) lr 3.3869e-04 eta 0:05:28
epoch [148/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.7500 (1.8952) acc 50.0000 (44.3750) lr 3.3869e-04 eta 0:05:24
epoch [148/200] batch [50/50] time 0.082 (0.120) data 0.000 (0.037) loss 1.7227 (1.8932) acc 59.3750 (44.6875) lr 3.2699e-04 eta 0:05:12
epoch [149/200] batch [5/50] time 0.087 (0.321) data 0.000 (0.235) loss 1.7236 (1.9881) acc 43.7500 (39.3750) lr 3.2699e-04 eta 0:13:53
epoch [149/200] batch [10/50] time 0.086 (0.208) data 0.000 (0.122) loss 1.8096 (1.8702) acc 50.0000 (43.1250) lr 3.2699e-04 eta 0:08:57
epoch [149/200] batch [15/50] time 0.085 (0.167) data 0.000 (0.082) loss 1.9590 (1.8435) acc 34.3750 (44.3750) lr 3.2699e-04 eta 0:07:11
epoch [149/200] batch [20/50] time 0.086 (0.151) data 0.000 (0.066) loss 1.7412 (1.8313) acc 40.6250 (45.4688) lr 3.2699e-04 eta 0:06:29
epoch [149/200] batch [25/50] time 0.125 (0.140) data 0.040 (0.055) loss 1.7979 (1.8663) acc 50.0000 (44.7500) lr 3.2699e-04 eta 0:05:59
epoch [149/200] batch [30/50] time 0.085 (0.131) data 0.000 (0.045) loss 1.9102 (1.8697) acc 46.8750 (44.0625) lr 3.2699e-04 eta 0:05:35
epoch [149/200] batch [35/50] time 0.243 (0.129) data 0.159 (0.044) loss 1.9180 (1.8623) acc 46.8750 (44.1071) lr 3.2699e-04 eta 0:05:29
epoch [149/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.038) loss 2.1895 (1.8669) acc 37.5000 (44.1406) lr 3.2699e-04 eta 0:05:15
epoch [149/200] batch [45/50] time 0.082 (0.119) data 0.000 (0.034) loss 1.8213 (1.9018) acc 40.6250 (43.1944) lr 3.2699e-04 eta 0:05:03
epoch [149/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 2.0020 (1.8983) acc 46.8750 (43.4375) lr 3.1545e-04 eta 0:04:53
epoch [150/200] batch [5/50] time 0.084 (0.325) data 0.000 (0.240) loss 1.9541 (2.0375) acc 34.3750 (36.2500) lr 3.1545e-04 eta 0:13:46
epoch [150/200] batch [10/50] time 0.085 (0.205) data 0.000 (0.120) loss 2.0078 (2.0132) acc 46.8750 (38.7500) lr 3.1545e-04 eta 0:08:41
epoch [150/200] batch [15/50] time 0.085 (0.170) data 0.001 (0.085) loss 1.4697 (1.9389) acc 46.8750 (42.2917) lr 3.1545e-04 eta 0:07:09
epoch [150/200] batch [20/50] time 0.224 (0.155) data 0.141 (0.071) loss 1.9160 (1.9323) acc 37.5000 (42.0312) lr 3.1545e-04 eta 0:06:33
epoch [150/200] batch [25/50] time 0.085 (0.141) data 0.000 (0.057) loss 1.3359 (1.9264) acc 59.3750 (43.0000) lr 3.1545e-04 eta 0:05:56
epoch [150/200] batch [30/50] time 0.085 (0.140) data 0.000 (0.055) loss 2.1738 (1.9291) acc 40.6250 (43.2292) lr 3.1545e-04 eta 0:05:51
epoch [150/200] batch [35/50] time 0.086 (0.132) data 0.001 (0.047) loss 2.2266 (1.9422) acc 40.6250 (43.3929) lr 3.1545e-04 eta 0:05:31
epoch [150/200] batch [40/50] time 0.085 (0.128) data 0.000 (0.043) loss 2.0078 (1.9447) acc 43.7500 (43.2031) lr 3.1545e-04 eta 0:05:20
epoch [150/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.2910 (1.9420) acc 25.0000 (43.4028) lr 3.1545e-04 eta 0:05:12
epoch [150/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.036) loss 2.0508 (1.9543) acc 37.5000 (42.5000) lr 3.0409e-04 eta 0:05:01
epoch [151/200] batch [5/50] time 0.085 (0.319) data 0.000 (0.234) loss 1.4336 (1.9012) acc 59.3750 (43.7500) lr 3.0409e-04 eta 0:13:16
epoch [151/200] batch [10/50] time 0.085 (0.217) data 0.000 (0.133) loss 2.0391 (1.8771) acc 43.7500 (45.6250) lr 3.0409e-04 eta 0:09:00
epoch [151/200] batch [15/50] time 0.084 (0.173) data 0.000 (0.089) loss 1.7480 (1.9508) acc 40.6250 (43.1250) lr 3.0409e-04 eta 0:07:09
epoch [151/200] batch [20/50] time 0.084 (0.160) data 0.000 (0.076) loss 1.6641 (1.8952) acc 40.6250 (44.0625) lr 3.0409e-04 eta 0:06:36
epoch [151/200] batch [25/50] time 0.267 (0.152) data 0.182 (0.068) loss 1.8926 (1.8770) acc 43.7500 (44.7500) lr 3.0409e-04 eta 0:06:15
epoch [151/200] batch [30/50] time 0.084 (0.141) data 0.000 (0.057) loss 1.5586 (1.8729) acc 46.8750 (44.2708) lr 3.0409e-04 eta 0:05:47
epoch [151/200] batch [35/50] time 0.084 (0.136) data 0.000 (0.052) loss 2.0273 (1.8760) acc 31.2500 (44.1964) lr 3.0409e-04 eta 0:05:34
epoch [151/200] batch [40/50] time 0.082 (0.129) data 0.000 (0.045) loss 1.9639 (1.8899) acc 46.8750 (43.9844) lr 3.0409e-04 eta 0:05:17
epoch [151/200] batch [45/50] time 0.082 (0.126) data 0.000 (0.043) loss 1.6963 (1.8546) acc 43.7500 (44.7222) lr 3.0409e-04 eta 0:05:09
epoch [151/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.038) loss 1.8428 (1.8648) acc 50.0000 (44.5000) lr 2.9289e-04 eta 0:04:58
epoch [152/200] batch [5/50] time 0.084 (0.314) data 0.000 (0.230) loss 1.6748 (1.8104) acc 46.8750 (49.3750) lr 2.9289e-04 eta 0:12:48
epoch [152/200] batch [10/50] time 0.085 (0.200) data 0.000 (0.115) loss 2.0020 (1.9321) acc 40.6250 (45.3125) lr 2.9289e-04 eta 0:08:06
epoch [152/200] batch [15/50] time 0.086 (0.161) data 0.000 (0.077) loss 1.7197 (1.9021) acc 50.0000 (46.0417) lr 2.9289e-04 eta 0:06:32
epoch [152/200] batch [20/50] time 0.085 (0.143) data 0.000 (0.059) loss 1.6719 (1.8927) acc 40.6250 (44.5312) lr 2.9289e-04 eta 0:05:47
epoch [152/200] batch [25/50] time 0.085 (0.131) data 0.000 (0.047) loss 2.2207 (1.8939) acc 46.8750 (45.3750) lr 2.9289e-04 eta 0:05:18
epoch [152/200] batch [30/50] time 0.084 (0.126) data 0.000 (0.041) loss 1.5020 (1.8950) acc 56.2500 (44.2708) lr 2.9289e-04 eta 0:05:04
epoch [152/200] batch [35/50] time 0.090 (0.120) data 0.000 (0.035) loss 1.7949 (1.9085) acc 46.8750 (43.6607) lr 2.9289e-04 eta 0:04:49
epoch [152/200] batch [40/50] time 0.084 (0.116) data 0.000 (0.031) loss 1.8750 (1.8995) acc 43.7500 (43.9062) lr 2.9289e-04 eta 0:04:38
epoch [152/200] batch [45/50] time 0.083 (0.112) data 0.000 (0.028) loss 1.3096 (1.9054) acc 59.3750 (43.7500) lr 2.9289e-04 eta 0:04:29
epoch [152/200] batch [50/50] time 0.083 (0.109) data 0.000 (0.025) loss 2.2656 (1.9157) acc 28.1250 (43.2500) lr 2.8187e-04 eta 0:04:22
epoch [153/200] batch [5/50] time 0.084 (0.280) data 0.000 (0.195) loss 1.4590 (1.6211) acc 68.7500 (53.1250) lr 2.8187e-04 eta 0:11:10
epoch [153/200] batch [10/50] time 0.163 (0.190) data 0.078 (0.105) loss 2.1836 (1.7797) acc 34.3750 (49.3750) lr 2.8187e-04 eta 0:07:35
epoch [153/200] batch [15/50] time 0.087 (0.159) data 0.000 (0.074) loss 1.8125 (1.8420) acc 50.0000 (47.5000) lr 2.8187e-04 eta 0:06:19
epoch [153/200] batch [20/50] time 0.086 (0.152) data 0.000 (0.067) loss 1.9453 (1.8958) acc 40.6250 (44.6875) lr 2.8187e-04 eta 0:06:01
epoch [153/200] batch [25/50] time 0.085 (0.139) data 0.000 (0.053) loss 1.8633 (1.9051) acc 50.0000 (44.1250) lr 2.8187e-04 eta 0:05:29
epoch [153/200] batch [30/50] time 0.086 (0.137) data 0.000 (0.052) loss 1.7285 (1.8937) acc 50.0000 (44.4792) lr 2.8187e-04 eta 0:05:24
epoch [153/200] batch [35/50] time 0.222 (0.134) data 0.138 (0.048) loss 1.7363 (1.8662) acc 40.6250 (44.8214) lr 2.8187e-04 eta 0:05:15
epoch [153/200] batch [40/50] time 0.084 (0.128) data 0.000 (0.042) loss 1.7793 (1.8590) acc 40.6250 (44.6875) lr 2.8187e-04 eta 0:05:00
epoch [153/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.041) loss 1.4170 (1.8520) acc 43.7500 (45.0694) lr 2.8187e-04 eta 0:04:56
epoch [153/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.037) loss 1.9639 (1.8477) acc 34.3750 (45.0625) lr 2.7103e-04 eta 0:04:45
epoch [154/200] batch [5/50] time 0.083 (0.318) data 0.000 (0.233) loss 1.8359 (1.8406) acc 50.0000 (46.8750) lr 2.7103e-04 eta 0:12:24
epoch [154/200] batch [10/50] time 0.084 (0.207) data 0.000 (0.122) loss 1.5156 (1.7929) acc 53.1250 (47.8125) lr 2.7103e-04 eta 0:08:03
epoch [154/200] batch [15/50] time 0.085 (0.166) data 0.000 (0.082) loss 1.5781 (1.7728) acc 50.0000 (48.5417) lr 2.7103e-04 eta 0:06:27
epoch [154/200] batch [20/50] time 0.085 (0.146) data 0.000 (0.061) loss 1.8525 (1.8065) acc 40.6250 (47.9688) lr 2.7103e-04 eta 0:05:39
epoch [154/200] batch [25/50] time 0.136 (0.136) data 0.052 (0.051) loss 2.2695 (1.8502) acc 50.0000 (47.6250) lr 2.7103e-04 eta 0:05:15
epoch [154/200] batch [30/50] time 0.086 (0.127) data 0.000 (0.043) loss 1.6533 (1.8757) acc 40.6250 (46.6667) lr 2.7103e-04 eta 0:04:55
epoch [154/200] batch [35/50] time 0.087 (0.123) data 0.000 (0.039) loss 1.9980 (1.9041) acc 34.3750 (45.3571) lr 2.7103e-04 eta 0:04:45
epoch [154/200] batch [40/50] time 0.085 (0.119) data 0.000 (0.034) loss 1.8096 (1.9044) acc 46.8750 (45.7031) lr 2.7103e-04 eta 0:04:33
epoch [154/200] batch [45/50] time 0.084 (0.116) data 0.000 (0.031) loss 1.7383 (1.9073) acc 53.1250 (45.4861) lr 2.7103e-04 eta 0:04:27
epoch [154/200] batch [50/50] time 0.083 (0.113) data 0.000 (0.028) loss 1.6191 (1.8888) acc 50.0000 (45.7500) lr 2.6037e-04 eta 0:04:19
epoch [155/200] batch [5/50] time 0.086 (0.300) data 0.000 (0.214) loss 1.6074 (1.8684) acc 50.0000 (49.3750) lr 2.6037e-04 eta 0:11:28
epoch [155/200] batch [10/50] time 0.085 (0.193) data 0.000 (0.107) loss 1.6865 (1.8735) acc 46.8750 (47.5000) lr 2.6037e-04 eta 0:07:21
epoch [155/200] batch [15/50] time 0.085 (0.157) data 0.000 (0.072) loss 2.0840 (1.9157) acc 40.6250 (45.6250) lr 2.6037e-04 eta 0:05:58
epoch [155/200] batch [20/50] time 0.137 (0.142) data 0.054 (0.057) loss 1.7393 (1.9411) acc 56.2500 (45.7812) lr 2.6037e-04 eta 0:05:22
epoch [155/200] batch [25/50] time 0.085 (0.130) data 0.000 (0.045) loss 1.6924 (1.9235) acc 40.6250 (45.7500) lr 2.6037e-04 eta 0:04:56
epoch [155/200] batch [30/50] time 0.085 (0.127) data 0.000 (0.043) loss 1.6621 (1.9034) acc 50.0000 (46.2500) lr 2.6037e-04 eta 0:04:49
epoch [155/200] batch [35/50] time 0.084 (0.121) data 0.000 (0.037) loss 1.9219 (1.9047) acc 37.5000 (45.7143) lr 2.6037e-04 eta 0:04:34
epoch [155/200] batch [40/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.6826 (1.9057) acc 46.8750 (45.5469) lr 2.6037e-04 eta 0:04:34
epoch [155/200] batch [45/50] time 0.084 (0.120) data 0.000 (0.036) loss 1.6279 (1.9131) acc 56.2500 (45.0000) lr 2.6037e-04 eta 0:04:31
epoch [155/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.032) loss 2.3516 (1.9142) acc 25.0000 (44.5625) lr 2.4989e-04 eta 0:04:22
epoch [156/200] batch [5/50] time 0.085 (0.332) data 0.000 (0.248) loss 1.8730 (1.9258) acc 40.6250 (45.6250) lr 2.4989e-04 eta 0:12:26
epoch [156/200] batch [10/50] time 0.084 (0.215) data 0.000 (0.124) loss 1.9639 (1.8646) acc 40.6250 (45.0000) lr 2.4989e-04 eta 0:08:01
epoch [156/200] batch [15/50] time 0.085 (0.175) data 0.000 (0.083) loss 1.7666 (1.8699) acc 46.8750 (44.5833) lr 2.4989e-04 eta 0:06:31
epoch [156/200] batch [20/50] time 0.086 (0.153) data 0.000 (0.062) loss 1.5166 (1.8049) acc 56.2500 (46.5625) lr 2.4989e-04 eta 0:05:40
epoch [156/200] batch [25/50] time 0.084 (0.139) data 0.000 (0.050) loss 2.4102 (1.8545) acc 31.2500 (44.3750) lr 2.4989e-04 eta 0:05:09
epoch [156/200] batch [30/50] time 0.086 (0.130) data 0.000 (0.042) loss 2.2520 (1.8788) acc 43.7500 (44.3750) lr 2.4989e-04 eta 0:04:48
epoch [156/200] batch [35/50] time 0.086 (0.124) data 0.001 (0.036) loss 1.9727 (1.8911) acc 53.1250 (44.2857) lr 2.4989e-04 eta 0:04:33
epoch [156/200] batch [40/50] time 0.086 (0.119) data 0.000 (0.031) loss 1.9814 (1.9181) acc 37.5000 (43.8281) lr 2.4989e-04 eta 0:04:22
epoch [156/200] batch [45/50] time 0.086 (0.115) data 0.001 (0.028) loss 1.8340 (1.9071) acc 46.8750 (44.1667) lr 2.4989e-04 eta 0:04:13
epoch [156/200] batch [50/50] time 0.084 (0.112) data 0.000 (0.025) loss 1.7715 (1.8822) acc 37.5000 (44.7500) lr 2.3959e-04 eta 0:04:06
epoch [157/200] batch [5/50] time 0.087 (0.299) data 0.000 (0.214) loss 1.8262 (1.9883) acc 53.1250 (44.3750) lr 2.3959e-04 eta 0:10:57
epoch [157/200] batch [10/50] time 0.086 (0.202) data 0.000 (0.117) loss 1.8564 (1.9866) acc 37.5000 (43.7500) lr 2.3959e-04 eta 0:07:23
epoch [157/200] batch [15/50] time 0.085 (0.163) data 0.000 (0.078) loss 1.7422 (1.9636) acc 46.8750 (43.5417) lr 2.3959e-04 eta 0:05:57
epoch [157/200] batch [20/50] time 0.086 (0.152) data 0.000 (0.066) loss 2.5000 (1.9860) acc 34.3750 (43.7500) lr 2.3959e-04 eta 0:05:30
epoch [157/200] batch [25/50] time 0.290 (0.147) data 0.205 (0.061) loss 1.7539 (1.9450) acc 46.8750 (44.7500) lr 2.3959e-04 eta 0:05:19
epoch [157/200] batch [30/50] time 0.084 (0.136) data 0.000 (0.051) loss 2.1777 (1.9567) acc 37.5000 (44.0625) lr 2.3959e-04 eta 0:04:56
epoch [157/200] batch [35/50] time 0.084 (0.134) data 0.000 (0.049) loss 1.6426 (1.9256) acc 43.7500 (44.7321) lr 2.3959e-04 eta 0:04:50
epoch [157/200] batch [40/50] time 0.084 (0.128) data 0.000 (0.043) loss 1.9971 (1.9506) acc 37.5000 (44.1406) lr 2.3959e-04 eta 0:04:36
epoch [157/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.041) loss 1.6309 (1.9462) acc 59.3750 (44.4444) lr 2.3959e-04 eta 0:04:30
epoch [157/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.7480 (1.9169) acc 50.0000 (45.5000) lr 2.2949e-04 eta 0:04:21
epoch [158/200] batch [5/50] time 0.085 (0.330) data 0.000 (0.246) loss 1.8535 (1.7705) acc 46.8750 (47.5000) lr 2.2949e-04 eta 0:11:47
epoch [158/200] batch [10/50] time 0.085 (0.225) data 0.000 (0.142) loss 1.9326 (1.9760) acc 46.8750 (45.0000) lr 2.2949e-04 eta 0:08:02
epoch [158/200] batch [15/50] time 0.086 (0.178) data 0.000 (0.095) loss 1.7500 (2.0620) acc 46.8750 (42.7083) lr 2.2949e-04 eta 0:06:20
epoch [158/200] batch [20/50] time 0.084 (0.155) data 0.000 (0.071) loss 1.8154 (2.0058) acc 31.2500 (42.8125) lr 2.2949e-04 eta 0:05:30
epoch [158/200] batch [25/50] time 0.085 (0.141) data 0.000 (0.057) loss 1.7695 (1.9761) acc 46.8750 (42.7500) lr 2.2949e-04 eta 0:04:59
epoch [158/200] batch [30/50] time 0.085 (0.132) data 0.000 (0.048) loss 1.8887 (1.9186) acc 50.0000 (44.2708) lr 2.2949e-04 eta 0:04:39
epoch [158/200] batch [35/50] time 0.245 (0.130) data 0.161 (0.046) loss 1.5254 (1.8994) acc 53.1250 (44.2857) lr 2.2949e-04 eta 0:04:34
epoch [158/200] batch [40/50] time 0.084 (0.124) data 0.000 (0.040) loss 2.0117 (1.9275) acc 53.1250 (43.9062) lr 2.2949e-04 eta 0:04:21
epoch [158/200] batch [45/50] time 0.082 (0.122) data 0.000 (0.038) loss 1.5146 (1.9197) acc 56.2500 (43.8889) lr 2.2949e-04 eta 0:04:17
epoch [158/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.034) loss 2.3613 (1.9192) acc 40.6250 (43.7500) lr 2.1957e-04 eta 0:04:08
epoch [159/200] batch [5/50] time 0.083 (0.301) data 0.000 (0.218) loss 1.9678 (1.8621) acc 50.0000 (48.7500) lr 2.1957e-04 eta 0:10:31
epoch [159/200] batch [10/50] time 0.084 (0.204) data 0.000 (0.121) loss 1.6406 (1.8520) acc 56.2500 (47.8125) lr 2.1957e-04 eta 0:07:06
epoch [159/200] batch [15/50] time 0.084 (0.164) data 0.000 (0.080) loss 1.9766 (1.8669) acc 40.6250 (47.0833) lr 2.1957e-04 eta 0:05:42
epoch [159/200] batch [20/50] time 0.084 (0.151) data 0.000 (0.068) loss 2.1699 (1.9003) acc 34.3750 (45.0000) lr 2.1957e-04 eta 0:05:14
epoch [159/200] batch [25/50] time 0.084 (0.138) data 0.000 (0.054) loss 1.5361 (1.8698) acc 62.5000 (45.5000) lr 2.1957e-04 eta 0:04:46
epoch [159/200] batch [30/50] time 0.084 (0.136) data 0.000 (0.052) loss 1.7871 (1.9056) acc 43.7500 (45.0000) lr 2.1957e-04 eta 0:04:40
epoch [159/200] batch [35/50] time 0.083 (0.128) data 0.000 (0.044) loss 1.9736 (1.9019) acc 34.3750 (44.2857) lr 2.1957e-04 eta 0:04:24
epoch [159/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.039) loss 2.6992 (1.9268) acc 31.2500 (43.3594) lr 2.1957e-04 eta 0:04:12
epoch [159/200] batch [45/50] time 0.083 (0.118) data 0.000 (0.035) loss 1.9824 (1.9121) acc 31.2500 (43.3333) lr 2.1957e-04 eta 0:04:03
epoch [159/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 1.7383 (1.9000) acc 40.6250 (43.6250) lr 2.0984e-04 eta 0:03:55
epoch [160/200] batch [5/50] time 0.084 (0.312) data 0.000 (0.228) loss 1.5693 (1.9227) acc 59.3750 (43.7500) lr 2.0984e-04 eta 0:10:38
epoch [160/200] batch [10/50] time 0.084 (0.199) data 0.000 (0.114) loss 2.0898 (1.9649) acc 43.7500 (43.4375) lr 2.0984e-04 eta 0:06:44
epoch [160/200] batch [15/50] time 0.086 (0.161) data 0.000 (0.076) loss 2.0977 (1.9438) acc 43.7500 (44.1667) lr 2.0984e-04 eta 0:05:26
epoch [160/200] batch [20/50] time 0.094 (0.142) data 0.009 (0.058) loss 2.1055 (1.9562) acc 37.5000 (44.5312) lr 2.0984e-04 eta 0:04:48
epoch [160/200] batch [25/50] time 0.086 (0.131) data 0.000 (0.046) loss 1.7998 (1.9392) acc 43.7500 (44.6250) lr 2.0984e-04 eta 0:04:24
epoch [160/200] batch [30/50] time 0.084 (0.127) data 0.000 (0.042) loss 1.6338 (1.9294) acc 53.1250 (44.6875) lr 2.0984e-04 eta 0:04:16
epoch [160/200] batch [35/50] time 0.084 (0.121) data 0.000 (0.036) loss 1.4746 (1.9017) acc 68.7500 (45.0000) lr 2.0984e-04 eta 0:04:03
epoch [160/200] batch [40/50] time 0.082 (0.121) data 0.000 (0.036) loss 2.1426 (1.9069) acc 31.2500 (44.9219) lr 2.0984e-04 eta 0:04:02
epoch [160/200] batch [45/50] time 0.084 (0.120) data 0.001 (0.036) loss 1.7812 (1.9094) acc 50.0000 (44.5833) lr 2.0984e-04 eta 0:04:01
epoch [160/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.033) loss 1.9814 (1.8962) acc 46.8750 (44.2500) lr 2.0032e-04 eta 0:03:53
epoch [161/200] batch [5/50] time 0.085 (0.335) data 0.000 (0.250) loss 1.6475 (1.8553) acc 46.8750 (43.1250) lr 2.0032e-04 eta 0:11:08
epoch [161/200] batch [10/50] time 0.084 (0.210) data 0.000 (0.125) loss 2.0547 (1.8659) acc 43.7500 (44.0625) lr 2.0032e-04 eta 0:06:57
epoch [161/200] batch [15/50] time 0.085 (0.175) data 0.000 (0.091) loss 1.5967 (1.8173) acc 50.0000 (45.2083) lr 2.0032e-04 eta 0:05:47
epoch [161/200] batch [20/50] time 0.085 (0.161) data 0.000 (0.076) loss 2.4688 (1.8741) acc 34.3750 (44.5312) lr 2.0032e-04 eta 0:05:18
epoch [161/200] batch [25/50] time 0.084 (0.145) data 0.000 (0.061) loss 1.4854 (1.8744) acc 62.5000 (44.8750) lr 2.0032e-04 eta 0:04:47
epoch [161/200] batch [30/50] time 0.084 (0.140) data 0.000 (0.056) loss 1.8555 (1.8767) acc 43.7500 (44.1667) lr 2.0032e-04 eta 0:04:36
epoch [161/200] batch [35/50] time 0.255 (0.137) data 0.170 (0.053) loss 1.7256 (1.8751) acc 50.0000 (44.7321) lr 2.0032e-04 eta 0:04:29
epoch [161/200] batch [40/50] time 0.083 (0.131) data 0.000 (0.046) loss 1.9219 (1.8907) acc 40.6250 (44.6875) lr 2.0032e-04 eta 0:04:15
epoch [161/200] batch [45/50] time 0.082 (0.130) data 0.000 (0.045) loss 2.2500 (1.8941) acc 31.2500 (44.7222) lr 2.0032e-04 eta 0:04:13
epoch [161/200] batch [50/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.8789 (1.8949) acc 34.3750 (44.2500) lr 1.9098e-04 eta 0:04:03
epoch [162/200] batch [5/50] time 0.085 (0.273) data 0.000 (0.189) loss 1.7109 (1.9879) acc 53.1250 (43.7500) lr 1.9098e-04 eta 0:08:51
epoch [162/200] batch [10/50] time 0.084 (0.179) data 0.000 (0.095) loss 2.0020 (1.8895) acc 43.7500 (45.3125) lr 1.9098e-04 eta 0:05:46
epoch [162/200] batch [15/50] time 0.085 (0.148) data 0.000 (0.063) loss 1.5947 (1.8995) acc 43.7500 (43.7500) lr 1.9098e-04 eta 0:04:45
epoch [162/200] batch [20/50] time 0.085 (0.132) data 0.000 (0.048) loss 1.8926 (1.9142) acc 43.7500 (44.0625) lr 1.9098e-04 eta 0:04:14
epoch [162/200] batch [25/50] time 0.084 (0.127) data 0.000 (0.043) loss 1.5928 (1.8661) acc 46.8750 (44.5000) lr 1.9098e-04 eta 0:04:04
epoch [162/200] batch [30/50] time 0.084 (0.124) data 0.000 (0.040) loss 1.5635 (1.8824) acc 50.0000 (44.3750) lr 1.9098e-04 eta 0:03:58
epoch [162/200] batch [35/50] time 0.084 (0.119) data 0.001 (0.035) loss 2.0293 (1.9142) acc 43.7500 (43.3929) lr 1.9098e-04 eta 0:03:47
epoch [162/200] batch [40/50] time 0.083 (0.118) data 0.000 (0.034) loss 1.9990 (1.9404) acc 46.8750 (42.7344) lr 1.9098e-04 eta 0:03:44
epoch [162/200] batch [45/50] time 0.228 (0.117) data 0.145 (0.033) loss 1.7695 (1.9176) acc 43.7500 (42.8472) lr 1.9098e-04 eta 0:03:42
epoch [162/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.030) loss 1.7520 (1.9131) acc 40.6250 (42.6250) lr 1.8185e-04 eta 0:03:35
epoch [163/200] batch [5/50] time 0.085 (0.276) data 0.000 (0.191) loss 1.4795 (1.7715) acc 46.8750 (44.3750) lr 1.8185e-04 eta 0:08:43
epoch [163/200] batch [10/50] time 0.086 (0.190) data 0.000 (0.106) loss 1.6113 (1.8376) acc 43.7500 (45.3125) lr 1.8185e-04 eta 0:05:59
epoch [163/200] batch [15/50] time 0.089 (0.156) data 0.001 (0.071) loss 1.5625 (1.8118) acc 53.1250 (45.2083) lr 1.8185e-04 eta 0:04:53
epoch [163/200] batch [20/50] time 0.085 (0.142) data 0.000 (0.057) loss 1.9033 (1.8847) acc 37.5000 (44.8438) lr 1.8185e-04 eta 0:04:27
epoch [163/200] batch [25/50] time 0.139 (0.133) data 0.055 (0.048) loss 1.6914 (1.8973) acc 53.1250 (44.5000) lr 1.8185e-04 eta 0:04:09
epoch [163/200] batch [30/50] time 0.085 (0.125) data 0.000 (0.040) loss 1.9775 (1.9138) acc 43.7500 (43.8542) lr 1.8185e-04 eta 0:03:53
epoch [163/200] batch [35/50] time 0.085 (0.124) data 0.000 (0.039) loss 2.0566 (1.9317) acc 50.0000 (43.3929) lr 1.8185e-04 eta 0:03:51
epoch [163/200] batch [40/50] time 0.084 (0.119) data 0.000 (0.034) loss 1.9033 (1.9109) acc 28.1250 (43.5156) lr 1.8185e-04 eta 0:03:41
epoch [163/200] batch [45/50] time 0.083 (0.117) data 0.000 (0.032) loss 1.3965 (1.8960) acc 56.2500 (44.2361) lr 1.8185e-04 eta 0:03:36
epoch [163/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.029) loss 1.7188 (1.8935) acc 53.1250 (44.1875) lr 1.7292e-04 eta 0:03:30
epoch [164/200] batch [5/50] time 0.085 (0.288) data 0.000 (0.202) loss 2.3945 (1.9324) acc 40.6250 (42.5000) lr 1.7292e-04 eta 0:08:50
epoch [164/200] batch [10/50] time 0.277 (0.206) data 0.194 (0.121) loss 1.8672 (1.9342) acc 46.8750 (43.4375) lr 1.7292e-04 eta 0:06:18
epoch [164/200] batch [15/50] time 0.084 (0.165) data 0.000 (0.081) loss 2.5508 (2.0275) acc 25.0000 (41.6667) lr 1.7292e-04 eta 0:05:03
epoch [164/200] batch [20/50] time 0.085 (0.150) data 0.000 (0.065) loss 2.0449 (1.9512) acc 43.7500 (44.5312) lr 1.7292e-04 eta 0:04:33
epoch [164/200] batch [25/50] time 0.085 (0.137) data 0.000 (0.052) loss 1.9531 (1.9148) acc 43.7500 (45.5000) lr 1.7292e-04 eta 0:04:09
epoch [164/200] batch [30/50] time 0.085 (0.131) data 0.000 (0.046) loss 1.8740 (1.9122) acc 40.6250 (45.1042) lr 1.7292e-04 eta 0:03:58
epoch [164/200] batch [35/50] time 0.164 (0.130) data 0.081 (0.045) loss 1.7559 (1.8997) acc 50.0000 (45.1786) lr 1.7292e-04 eta 0:03:55
epoch [164/200] batch [40/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.2324 (1.9198) acc 34.3750 (44.5312) lr 1.7292e-04 eta 0:03:44
epoch [164/200] batch [45/50] time 0.083 (0.122) data 0.000 (0.038) loss 1.8750 (1.9239) acc 34.3750 (43.8194) lr 1.7292e-04 eta 0:03:40
epoch [164/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.034) loss 1.8662 (1.8941) acc 53.1250 (44.6875) lr 1.6419e-04 eta 0:03:33
epoch [165/200] batch [5/50] time 0.085 (0.274) data 0.000 (0.190) loss 2.1465 (2.0471) acc 31.2500 (38.7500) lr 1.6419e-04 eta 0:08:12
epoch [165/200] batch [10/50] time 0.084 (0.179) data 0.000 (0.095) loss 1.9170 (1.9420) acc 34.3750 (42.1875) lr 1.6419e-04 eta 0:05:21
epoch [165/200] batch [15/50] time 0.085 (0.149) data 0.001 (0.064) loss 1.4053 (1.9025) acc 62.5000 (43.7500) lr 1.6419e-04 eta 0:04:25
epoch [165/200] batch [20/50] time 0.120 (0.134) data 0.036 (0.050) loss 1.6035 (1.8955) acc 50.0000 (43.9062) lr 1.6419e-04 eta 0:03:59
epoch [165/200] batch [25/50] time 0.084 (0.128) data 0.000 (0.044) loss 1.8721 (1.8892) acc 56.2500 (44.6250) lr 1.6419e-04 eta 0:03:47
epoch [165/200] batch [30/50] time 0.084 (0.126) data 0.000 (0.042) loss 1.7432 (1.8700) acc 43.7500 (44.7917) lr 1.6419e-04 eta 0:03:43
epoch [165/200] batch [35/50] time 0.084 (0.120) data 0.000 (0.036) loss 1.5186 (1.8540) acc 53.1250 (45.3571) lr 1.6419e-04 eta 0:03:32
epoch [165/200] batch [40/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.0215 (1.8590) acc 37.5000 (45.3906) lr 1.6419e-04 eta 0:03:29
epoch [165/200] batch [45/50] time 0.084 (0.118) data 0.000 (0.033) loss 2.2441 (1.8793) acc 37.5000 (45.3472) lr 1.6419e-04 eta 0:03:26
epoch [165/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.030) loss 1.6514 (1.8628) acc 50.0000 (45.4375) lr 1.5567e-04 eta 0:03:19
epoch [166/200] batch [5/50] time 0.085 (0.289) data 0.000 (0.204) loss 2.0410 (1.7336) acc 31.2500 (46.8750) lr 1.5567e-04 eta 0:08:23
epoch [166/200] batch [10/50] time 0.084 (0.186) data 0.000 (0.102) loss 1.4941 (1.8146) acc 56.2500 (45.9375) lr 1.5567e-04 eta 0:05:23
epoch [166/200] batch [15/50] time 0.084 (0.152) data 0.001 (0.068) loss 2.2402 (1.8598) acc 40.6250 (44.3750) lr 1.5567e-04 eta 0:04:24
epoch [166/200] batch [20/50] time 0.085 (0.139) data 0.000 (0.055) loss 1.7627 (1.8774) acc 40.6250 (43.9062) lr 1.5567e-04 eta 0:04:00
epoch [166/200] batch [25/50] time 0.084 (0.132) data 0.000 (0.048) loss 1.6123 (1.8954) acc 46.8750 (43.7500) lr 1.5567e-04 eta 0:03:47
epoch [166/200] batch [30/50] time 0.083 (0.128) data 0.000 (0.044) loss 1.7871 (1.8886) acc 37.5000 (44.2708) lr 1.5567e-04 eta 0:03:39
epoch [166/200] batch [35/50] time 0.251 (0.126) data 0.167 (0.042) loss 1.8828 (1.8844) acc 46.8750 (44.8214) lr 1.5567e-04 eta 0:03:36
epoch [166/200] batch [40/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.8584 (1.8719) acc 53.1250 (45.3125) lr 1.5567e-04 eta 0:03:26
epoch [166/200] batch [45/50] time 0.084 (0.120) data 0.000 (0.036) loss 1.7588 (1.8657) acc 53.1250 (45.8333) lr 1.5567e-04 eta 0:03:24
epoch [166/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.033) loss 1.7803 (1.8780) acc 56.2500 (45.5000) lr 1.4736e-04 eta 0:03:17
epoch [167/200] batch [5/50] time 0.086 (0.296) data 0.001 (0.211) loss 1.7178 (1.8309) acc 46.8750 (46.2500) lr 1.4736e-04 eta 0:08:22
epoch [167/200] batch [10/50] time 0.101 (0.192) data 0.018 (0.107) loss 1.6221 (1.7592) acc 46.8750 (47.8125) lr 1.4736e-04 eta 0:05:24
epoch [167/200] batch [15/50] time 0.083 (0.156) data 0.000 (0.072) loss 1.7959 (1.7914) acc 53.1250 (46.4583) lr 1.4736e-04 eta 0:04:22
epoch [167/200] batch [20/50] time 0.083 (0.147) data 0.000 (0.063) loss 1.6426 (1.8058) acc 40.6250 (46.4062) lr 1.4736e-04 eta 0:04:06
epoch [167/200] batch [25/50] time 0.084 (0.134) data 0.000 (0.050) loss 1.8262 (1.8370) acc 46.8750 (45.8750) lr 1.4736e-04 eta 0:03:44
epoch [167/200] batch [30/50] time 0.086 (0.129) data 0.000 (0.045) loss 1.5371 (1.8394) acc 59.3750 (46.2500) lr 1.4736e-04 eta 0:03:35
epoch [167/200] batch [35/50] time 0.212 (0.127) data 0.129 (0.043) loss 1.8496 (1.8566) acc 31.2500 (44.6429) lr 1.4736e-04 eta 0:03:31
epoch [167/200] batch [40/50] time 0.084 (0.122) data 0.000 (0.038) loss 1.8545 (1.8686) acc 46.8750 (44.6094) lr 1.4736e-04 eta 0:03:22
epoch [167/200] batch [45/50] time 0.084 (0.118) data 0.000 (0.034) loss 1.5576 (1.8700) acc 50.0000 (44.2361) lr 1.4736e-04 eta 0:03:15
epoch [167/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 1.8301 (1.8708) acc 34.3750 (44.4375) lr 1.3926e-04 eta 0:03:09
epoch [168/200] batch [5/50] time 0.084 (0.316) data 0.001 (0.232) loss 1.4541 (1.7799) acc 59.3750 (45.0000) lr 1.3926e-04 eta 0:08:39
epoch [168/200] batch [10/50] time 0.118 (0.204) data 0.035 (0.120) loss 2.0391 (1.8473) acc 43.7500 (45.3125) lr 1.3926e-04 eta 0:05:33
epoch [168/200] batch [15/50] time 0.085 (0.164) data 0.000 (0.080) loss 1.6016 (1.8380) acc 46.8750 (47.0833) lr 1.3926e-04 eta 0:04:28
epoch [168/200] batch [20/50] time 0.085 (0.151) data 0.000 (0.067) loss 1.7031 (1.8694) acc 53.1250 (45.6250) lr 1.3926e-04 eta 0:04:06
epoch [168/200] batch [25/50] time 0.086 (0.138) data 0.000 (0.054) loss 1.9736 (1.8731) acc 40.6250 (45.1250) lr 1.3926e-04 eta 0:03:44
epoch [168/200] batch [30/50] time 0.086 (0.136) data 0.000 (0.052) loss 2.3730 (1.8974) acc 43.7500 (45.0000) lr 1.3926e-04 eta 0:03:40
epoch [168/200] batch [35/50] time 0.085 (0.133) data 0.001 (0.048) loss 1.9385 (1.9034) acc 40.6250 (45.0000) lr 1.3926e-04 eta 0:03:34
epoch [168/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.042) loss 1.9785 (1.8961) acc 28.1250 (44.4531) lr 1.3926e-04 eta 0:03:23
epoch [168/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.1426 (1.8998) acc 28.1250 (43.8889) lr 1.3926e-04 eta 0:03:17
epoch [168/200] batch [50/50] time 0.082 (0.119) data 0.000 (0.035) loss 2.3555 (1.9129) acc 43.7500 (44.0000) lr 1.3137e-04 eta 0:03:10
epoch [169/200] batch [5/50] time 0.086 (0.321) data 0.000 (0.235) loss 2.1602 (1.9051) acc 34.3750 (43.7500) lr 1.3137e-04 eta 0:08:31
epoch [169/200] batch [10/50] time 0.086 (0.203) data 0.000 (0.118) loss 1.9814 (1.8650) acc 40.6250 (44.0625) lr 1.3137e-04 eta 0:05:22
epoch [169/200] batch [15/50] time 0.084 (0.163) data 0.000 (0.079) loss 2.1738 (1.9087) acc 34.3750 (43.7500) lr 1.3137e-04 eta 0:04:18
epoch [169/200] batch [20/50] time 0.136 (0.146) data 0.052 (0.062) loss 1.7998 (1.8810) acc 43.7500 (43.9062) lr 1.3137e-04 eta 0:03:51
epoch [169/200] batch [25/50] time 0.084 (0.134) data 0.000 (0.049) loss 1.6318 (1.9129) acc 53.1250 (43.3750) lr 1.3137e-04 eta 0:03:30
epoch [169/200] batch [30/50] time 0.084 (0.130) data 0.000 (0.046) loss 1.4062 (1.8730) acc 68.7500 (44.7917) lr 1.3137e-04 eta 0:03:24
epoch [169/200] batch [35/50] time 0.084 (0.124) data 0.000 (0.039) loss 1.8125 (1.8672) acc 46.8750 (45.1786) lr 1.3137e-04 eta 0:03:13
epoch [169/200] batch [40/50] time 0.084 (0.122) data 0.000 (0.037) loss 2.2520 (1.8921) acc 40.6250 (45.0781) lr 1.3137e-04 eta 0:03:09
epoch [169/200] batch [45/50] time 0.084 (0.119) data 0.000 (0.035) loss 2.0352 (1.8976) acc 37.5000 (44.8611) lr 1.3137e-04 eta 0:03:04
epoch [169/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 1.9053 (1.9026) acc 46.8750 (44.9375) lr 1.2369e-04 eta 0:02:58
epoch [170/200] batch [5/50] time 0.084 (0.290) data 0.001 (0.205) loss 1.8242 (1.8877) acc 50.0000 (43.7500) lr 1.2369e-04 eta 0:07:27
epoch [170/200] batch [10/50] time 0.084 (0.187) data 0.000 (0.103) loss 1.9336 (2.0139) acc 50.0000 (41.2500) lr 1.2369e-04 eta 0:04:47
epoch [170/200] batch [15/50] time 0.085 (0.153) data 0.000 (0.069) loss 1.9395 (1.9484) acc 50.0000 (44.1667) lr 1.2369e-04 eta 0:03:54
epoch [170/200] batch [20/50] time 0.085 (0.136) data 0.000 (0.052) loss 2.2344 (1.9638) acc 40.6250 (44.2188) lr 1.2369e-04 eta 0:03:27
epoch [170/200] batch [25/50] time 0.084 (0.128) data 0.000 (0.044) loss 1.4336 (1.9398) acc 59.3750 (45.5000) lr 1.2369e-04 eta 0:03:14
epoch [170/200] batch [30/50] time 0.321 (0.128) data 0.238 (0.044) loss 1.7100 (1.9065) acc 56.2500 (46.4583) lr 1.2369e-04 eta 0:03:15
epoch [170/200] batch [35/50] time 0.083 (0.122) data 0.000 (0.038) loss 1.8086 (1.9323) acc 53.1250 (46.1607) lr 1.2369e-04 eta 0:03:05
epoch [170/200] batch [40/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.5332 (1.9128) acc 59.3750 (46.4062) lr 1.2369e-04 eta 0:03:02
epoch [170/200] batch [45/50] time 0.083 (0.117) data 0.000 (0.033) loss 1.8203 (1.8949) acc 50.0000 (46.8056) lr 1.2369e-04 eta 0:02:56
epoch [170/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 1.8145 (1.8721) acc 40.6250 (47.1250) lr 1.1623e-04 eta 0:02:52
epoch [171/200] batch [5/50] time 0.083 (0.293) data 0.000 (0.209) loss 1.6992 (2.0137) acc 50.0000 (42.5000) lr 1.1623e-04 eta 0:07:18
epoch [171/200] batch [10/50] time 0.261 (0.206) data 0.178 (0.123) loss 1.5596 (1.9899) acc 50.0000 (42.8125) lr 1.1623e-04 eta 0:05:07
epoch [171/200] batch [15/50] time 0.084 (0.166) data 0.000 (0.082) loss 2.1172 (1.9538) acc 53.1250 (43.5417) lr 1.1623e-04 eta 0:04:05
epoch [171/200] batch [20/50] time 0.084 (0.146) data 0.000 (0.062) loss 1.6211 (1.9243) acc 50.0000 (44.2188) lr 1.1623e-04 eta 0:03:35
epoch [171/200] batch [25/50] time 0.136 (0.136) data 0.052 (0.052) loss 1.8174 (1.9143) acc 50.0000 (44.1250) lr 1.1623e-04 eta 0:03:20
epoch [171/200] batch [30/50] time 0.084 (0.130) data 0.000 (0.046) loss 1.8525 (1.9180) acc 37.5000 (44.3750) lr 1.1623e-04 eta 0:03:11
epoch [171/200] batch [35/50] time 0.085 (0.125) data 0.001 (0.041) loss 1.8994 (1.9213) acc 43.7500 (44.6429) lr 1.1623e-04 eta 0:03:03
epoch [171/200] batch [40/50] time 0.082 (0.120) data 0.000 (0.036) loss 2.0020 (1.9109) acc 40.6250 (44.6094) lr 1.1623e-04 eta 0:02:54
epoch [171/200] batch [45/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.6094 (1.9316) acc 28.1250 (44.0972) lr 1.1623e-04 eta 0:02:53
epoch [171/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.032) loss 1.7217 (1.9324) acc 50.0000 (43.9375) lr 1.0899e-04 eta 0:02:47
epoch [172/200] batch [5/50] time 0.086 (0.330) data 0.000 (0.246) loss 1.6006 (1.6658) acc 56.2500 (51.2500) lr 1.0899e-04 eta 0:07:57
epoch [172/200] batch [10/50] time 0.083 (0.222) data 0.000 (0.137) loss 1.8936 (1.8363) acc 37.5000 (46.2500) lr 1.0899e-04 eta 0:05:19
epoch [172/200] batch [15/50] time 0.084 (0.176) data 0.000 (0.092) loss 1.4707 (1.8394) acc 59.3750 (47.0833) lr 1.0899e-04 eta 0:04:12
epoch [172/200] batch [20/50] time 0.084 (0.160) data 0.000 (0.076) loss 1.8408 (1.8481) acc 31.2500 (45.4688) lr 1.0899e-04 eta 0:03:48
epoch [172/200] batch [25/50] time 0.206 (0.149) data 0.121 (0.065) loss 2.1152 (1.8763) acc 50.0000 (45.8750) lr 1.0899e-04 eta 0:03:32
epoch [172/200] batch [30/50] time 0.084 (0.139) data 0.000 (0.054) loss 1.6133 (1.8701) acc 53.1250 (46.6667) lr 1.0899e-04 eta 0:03:16
epoch [172/200] batch [35/50] time 0.084 (0.136) data 0.000 (0.052) loss 2.1973 (1.8650) acc 34.3750 (46.4286) lr 1.0899e-04 eta 0:03:12
epoch [172/200] batch [40/50] time 0.083 (0.130) data 0.000 (0.046) loss 1.8193 (1.8746) acc 56.2500 (45.7812) lr 1.0899e-04 eta 0:03:02
epoch [172/200] batch [45/50] time 0.084 (0.127) data 0.000 (0.044) loss 1.5889 (1.8809) acc 56.2500 (45.8333) lr 1.0899e-04 eta 0:02:58
epoch [172/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.039) loss 1.7871 (1.8744) acc 43.7500 (46.0625) lr 1.0197e-04 eta 0:02:52
epoch [173/200] batch [5/50] time 0.085 (0.304) data 0.000 (0.219) loss 1.8916 (1.8277) acc 43.7500 (43.7500) lr 1.0197e-04 eta 0:07:03
epoch [173/200] batch [10/50] time 0.111 (0.202) data 0.027 (0.118) loss 2.1523 (1.8931) acc 28.1250 (42.1875) lr 1.0197e-04 eta 0:04:41
epoch [173/200] batch [15/50] time 0.085 (0.167) data 0.000 (0.083) loss 1.7686 (1.8996) acc 46.8750 (44.1667) lr 1.0197e-04 eta 0:03:51
epoch [173/200] batch [20/50] time 0.084 (0.154) data 0.000 (0.070) loss 2.1074 (1.9407) acc 34.3750 (43.9062) lr 1.0197e-04 eta 0:03:32
epoch [173/200] batch [25/50] time 0.085 (0.140) data 0.000 (0.056) loss 1.4824 (1.9019) acc 56.2500 (43.7500) lr 1.0197e-04 eta 0:03:12
epoch [173/200] batch [30/50] time 0.084 (0.137) data 0.000 (0.053) loss 1.9814 (1.8840) acc 46.8750 (44.8958) lr 1.0197e-04 eta 0:03:07
epoch [173/200] batch [35/50] time 0.108 (0.132) data 0.023 (0.048) loss 2.7402 (1.8721) acc 28.1250 (45.4464) lr 1.0197e-04 eta 0:03:00
epoch [173/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.042) loss 2.1387 (1.8546) acc 28.1250 (45.3906) lr 1.0197e-04 eta 0:02:51
epoch [173/200] batch [45/50] time 0.085 (0.124) data 0.000 (0.040) loss 1.9971 (1.8566) acc 43.7500 (45.5556) lr 1.0197e-04 eta 0:02:47
epoch [173/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 2.0137 (1.8636) acc 37.5000 (45.5625) lr 9.5173e-05 eta 0:02:41
epoch [174/200] batch [5/50] time 0.086 (0.320) data 0.001 (0.234) loss 1.9297 (1.8645) acc 46.8750 (47.5000) lr 9.5173e-05 eta 0:07:10
epoch [174/200] batch [10/50] time 0.085 (0.215) data 0.000 (0.130) loss 1.7344 (1.8390) acc 53.1250 (49.6875) lr 9.5173e-05 eta 0:04:47
epoch [174/200] batch [15/50] time 0.084 (0.171) data 0.000 (0.087) loss 2.1250 (1.9136) acc 34.3750 (45.8333) lr 9.5173e-05 eta 0:03:48
epoch [174/200] batch [20/50] time 0.085 (0.159) data 0.000 (0.074) loss 1.6592 (1.9258) acc 40.6250 (45.1562) lr 9.5173e-05 eta 0:03:31
epoch [174/200] batch [25/50] time 0.218 (0.149) data 0.135 (0.065) loss 1.8174 (1.9319) acc 53.1250 (45.0000) lr 9.5173e-05 eta 0:03:17
epoch [174/200] batch [30/50] time 0.084 (0.139) data 0.000 (0.054) loss 2.1387 (1.9334) acc 40.6250 (44.8958) lr 9.5173e-05 eta 0:03:03
epoch [174/200] batch [35/50] time 0.084 (0.136) data 0.000 (0.052) loss 1.4922 (1.9336) acc 56.2500 (44.5536) lr 9.5173e-05 eta 0:02:58
epoch [174/200] batch [40/50] time 0.083 (0.129) data 0.000 (0.045) loss 1.3340 (1.9170) acc 53.1250 (44.5312) lr 9.5173e-05 eta 0:02:49
epoch [174/200] batch [45/50] time 0.082 (0.129) data 0.000 (0.045) loss 1.7412 (1.9300) acc 46.8750 (44.3750) lr 9.5173e-05 eta 0:02:48
epoch [174/200] batch [50/50] time 0.083 (0.127) data 0.000 (0.043) loss 2.0137 (1.9248) acc 50.0000 (44.2500) lr 8.8597e-05 eta 0:02:44
epoch [175/200] batch [5/50] time 0.086 (0.311) data 0.001 (0.225) loss 2.1016 (1.8729) acc 21.8750 (40.0000) lr 8.8597e-05 eta 0:06:42
epoch [175/200] batch [10/50] time 0.085 (0.213) data 0.000 (0.128) loss 2.1250 (1.8834) acc 40.6250 (43.7500) lr 8.8597e-05 eta 0:04:35
epoch [175/200] batch [15/50] time 0.085 (0.171) data 0.000 (0.086) loss 1.7500 (1.8882) acc 50.0000 (44.5833) lr 8.8597e-05 eta 0:03:39
epoch [175/200] batch [20/50] time 0.084 (0.158) data 0.000 (0.073) loss 1.8271 (1.8536) acc 34.3750 (45.6250) lr 8.8597e-05 eta 0:03:22
epoch [175/200] batch [25/50] time 0.201 (0.148) data 0.116 (0.063) loss 2.1562 (1.8965) acc 37.5000 (44.2500) lr 8.8597e-05 eta 0:03:08
epoch [175/200] batch [30/50] time 0.086 (0.140) data 0.000 (0.056) loss 1.9834 (1.9078) acc 31.2500 (44.0625) lr 8.8597e-05 eta 0:02:57
epoch [175/200] batch [35/50] time 0.086 (0.135) data 0.001 (0.051) loss 1.9922 (1.8924) acc 46.8750 (44.6429) lr 8.8597e-05 eta 0:02:51
epoch [175/200] batch [40/50] time 0.083 (0.129) data 0.000 (0.044) loss 1.6240 (1.8963) acc 56.2500 (44.3750) lr 8.8597e-05 eta 0:02:42
epoch [175/200] batch [45/50] time 0.084 (0.126) data 0.000 (0.041) loss 1.7148 (1.8942) acc 53.1250 (44.0278) lr 8.8597e-05 eta 0:02:38
epoch [175/200] batch [50/50] time 0.084 (0.122) data 0.000 (0.037) loss 1.6631 (1.8833) acc 59.3750 (44.5000) lr 8.2245e-05 eta 0:02:32
epoch [176/200] batch [5/50] time 0.085 (0.320) data 0.000 (0.234) loss 1.8330 (1.8467) acc 46.8750 (46.2500) lr 8.2245e-05 eta 0:06:37
epoch [176/200] batch [10/50] time 0.085 (0.214) data 0.000 (0.128) loss 1.9385 (1.8250) acc 53.1250 (48.1250) lr 8.2245e-05 eta 0:04:24
epoch [176/200] batch [15/50] time 0.086 (0.171) data 0.001 (0.086) loss 1.8877 (1.8419) acc 53.1250 (47.0833) lr 8.2245e-05 eta 0:03:30
epoch [176/200] batch [20/50] time 0.085 (0.158) data 0.000 (0.073) loss 1.7559 (1.8449) acc 56.2500 (47.5000) lr 8.2245e-05 eta 0:03:14
epoch [176/200] batch [25/50] time 0.226 (0.149) data 0.143 (0.064) loss 1.8457 (1.8268) acc 40.6250 (47.1250) lr 8.2245e-05 eta 0:03:02
epoch [176/200] batch [30/50] time 0.085 (0.139) data 0.000 (0.054) loss 1.7168 (1.8332) acc 43.7500 (46.1458) lr 8.2245e-05 eta 0:02:48
epoch [176/200] batch [35/50] time 0.084 (0.135) data 0.001 (0.050) loss 1.9609 (1.8281) acc 37.5000 (45.9821) lr 8.2245e-05 eta 0:02:43
epoch [176/200] batch [40/50] time 0.084 (0.128) data 0.000 (0.044) loss 1.7100 (1.8236) acc 46.8750 (46.2500) lr 8.2245e-05 eta 0:02:35
epoch [176/200] batch [45/50] time 0.084 (0.126) data 0.000 (0.042) loss 1.7598 (1.8289) acc 43.7500 (45.8333) lr 8.2245e-05 eta 0:02:31
epoch [176/200] batch [50/50] time 0.084 (0.122) data 0.000 (0.038) loss 2.2441 (1.8517) acc 43.7500 (45.9375) lr 7.6120e-05 eta 0:02:26
epoch [177/200] batch [5/50] time 0.085 (0.285) data 0.000 (0.199) loss 2.4922 (1.9246) acc 37.5000 (46.8750) lr 7.6120e-05 eta 0:05:40
epoch [177/200] batch [10/50] time 0.085 (0.185) data 0.000 (0.100) loss 1.7061 (1.8386) acc 53.1250 (47.5000) lr 7.6120e-05 eta 0:03:40
epoch [177/200] batch [15/50] time 0.086 (0.152) data 0.002 (0.067) loss 1.9434 (1.8585) acc 34.3750 (45.0000) lr 7.6120e-05 eta 0:03:00
epoch [177/200] batch [20/50] time 0.086 (0.135) data 0.000 (0.050) loss 1.4355 (1.8392) acc 46.8750 (45.1562) lr 7.6120e-05 eta 0:02:39
epoch [177/200] batch [25/50] time 0.085 (0.130) data 0.000 (0.045) loss 1.7471 (1.8466) acc 46.8750 (44.3750) lr 7.6120e-05 eta 0:02:32
epoch [177/200] batch [30/50] time 0.086 (0.123) data 0.001 (0.038) loss 1.6982 (1.8480) acc 46.8750 (44.0625) lr 7.6120e-05 eta 0:02:23
epoch [177/200] batch [35/50] time 0.086 (0.120) data 0.001 (0.035) loss 1.7871 (1.8497) acc 40.6250 (44.3750) lr 7.6120e-05 eta 0:02:20
epoch [177/200] batch [40/50] time 0.085 (0.120) data 0.000 (0.035) loss 1.9570 (1.8384) acc 34.3750 (44.8438) lr 7.6120e-05 eta 0:02:19
epoch [177/200] batch [45/50] time 0.083 (0.116) data 0.000 (0.032) loss 2.3418 (1.8488) acc 31.2500 (44.9306) lr 7.6120e-05 eta 0:02:14
epoch [177/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 1.9316 (1.8493) acc 46.8750 (45.2500) lr 7.0224e-05 eta 0:02:12
epoch [178/200] batch [5/50] time 0.086 (0.269) data 0.002 (0.184) loss 1.6729 (1.8760) acc 46.8750 (43.7500) lr 7.0224e-05 eta 0:05:08
epoch [178/200] batch [10/50] time 0.084 (0.177) data 0.000 (0.092) loss 1.8643 (1.7639) acc 34.3750 (44.6875) lr 7.0224e-05 eta 0:03:21
epoch [178/200] batch [15/50] time 0.084 (0.156) data 0.000 (0.072) loss 1.8701 (1.7540) acc 43.7500 (46.2500) lr 7.0224e-05 eta 0:02:56
epoch [178/200] batch [20/50] time 0.084 (0.147) data 0.000 (0.063) loss 1.8496 (1.7799) acc 34.3750 (46.0938) lr 7.0224e-05 eta 0:02:45
epoch [178/200] batch [25/50] time 0.084 (0.134) data 0.000 (0.050) loss 1.7822 (1.7752) acc 46.8750 (46.3750) lr 7.0224e-05 eta 0:02:30
epoch [178/200] batch [30/50] time 0.084 (0.132) data 0.000 (0.048) loss 2.1250 (1.7945) acc 43.7500 (46.2500) lr 7.0224e-05 eta 0:02:28
epoch [178/200] batch [35/50] time 0.136 (0.127) data 0.053 (0.043) loss 2.2012 (1.8043) acc 43.7500 (46.5179) lr 7.0224e-05 eta 0:02:21
epoch [178/200] batch [40/50] time 0.084 (0.122) data 0.000 (0.038) loss 1.9795 (1.8420) acc 46.8750 (45.4688) lr 7.0224e-05 eta 0:02:15
epoch [178/200] batch [45/50] time 0.083 (0.118) data 0.000 (0.034) loss 1.6191 (1.8325) acc 53.1250 (45.1389) lr 7.0224e-05 eta 0:02:10
epoch [178/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 1.7949 (1.8463) acc 46.8750 (45.1875) lr 6.4556e-05 eta 0:02:06
epoch [179/200] batch [5/50] time 0.084 (0.307) data 0.000 (0.223) loss 1.8438 (1.7807) acc 46.8750 (47.5000) lr 6.4556e-05 eta 0:05:36
epoch [179/200] batch [10/50] time 0.193 (0.207) data 0.111 (0.122) loss 1.6533 (1.7958) acc 53.1250 (47.1875) lr 6.4556e-05 eta 0:03:45
epoch [179/200] batch [15/50] time 0.084 (0.166) data 0.000 (0.082) loss 1.9814 (1.8746) acc 34.3750 (42.2917) lr 6.4556e-05 eta 0:02:59
epoch [179/200] batch [20/50] time 0.084 (0.151) data 0.000 (0.067) loss 2.0918 (1.9006) acc 31.2500 (42.9688) lr 6.4556e-05 eta 0:02:43
epoch [179/200] batch [25/50] time 0.084 (0.138) data 0.000 (0.054) loss 1.9766 (1.9229) acc 40.6250 (42.0000) lr 6.4556e-05 eta 0:02:28
epoch [179/200] batch [30/50] time 0.085 (0.135) data 0.000 (0.051) loss 1.8652 (1.9275) acc 46.8750 (41.4583) lr 6.4556e-05 eta 0:02:24
epoch [179/200] batch [35/50] time 0.084 (0.133) data 0.000 (0.048) loss 1.9561 (1.9083) acc 40.6250 (42.3214) lr 6.4556e-05 eta 0:02:21
epoch [179/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.042) loss 1.5342 (1.9029) acc 56.2500 (43.2812) lr 6.4556e-05 eta 0:02:14
epoch [179/200] batch [45/50] time 0.084 (0.125) data 0.000 (0.041) loss 1.7559 (1.8928) acc 50.0000 (43.5417) lr 6.4556e-05 eta 0:02:11
epoch [179/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 2.0391 (1.8906) acc 43.7500 (43.2500) lr 5.9119e-05 eta 0:02:06
epoch [180/200] batch [5/50] time 0.084 (0.321) data 0.000 (0.237) loss 1.8037 (1.9131) acc 50.0000 (45.6250) lr 5.9119e-05 eta 0:05:35
epoch [180/200] batch [10/50] time 0.088 (0.210) data 0.000 (0.126) loss 2.2031 (1.9966) acc 31.2500 (40.3125) lr 5.9119e-05 eta 0:03:38
epoch [180/200] batch [15/50] time 0.084 (0.168) data 0.000 (0.084) loss 1.8760 (1.9224) acc 43.7500 (43.9583) lr 5.9119e-05 eta 0:02:53
epoch [180/200] batch [20/50] time 0.084 (0.147) data 0.000 (0.063) loss 1.7314 (1.9270) acc 56.2500 (44.0625) lr 5.9119e-05 eta 0:02:31
epoch [180/200] batch [25/50] time 0.085 (0.135) data 0.000 (0.050) loss 1.8398 (1.8661) acc 43.7500 (45.7500) lr 5.9119e-05 eta 0:02:18
epoch [180/200] batch [30/50] time 0.085 (0.126) data 0.000 (0.042) loss 1.5703 (1.8457) acc 56.2500 (46.9792) lr 5.9119e-05 eta 0:02:08
epoch [180/200] batch [35/50] time 0.084 (0.120) data 0.000 (0.036) loss 1.7969 (1.8405) acc 40.6250 (46.6964) lr 5.9119e-05 eta 0:02:02
epoch [180/200] batch [40/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.3984 (1.8331) acc 37.5000 (47.1875) lr 5.9119e-05 eta 0:02:00
epoch [180/200] batch [45/50] time 0.210 (0.118) data 0.127 (0.034) loss 1.8682 (1.8471) acc 50.0000 (47.0833) lr 5.9119e-05 eta 0:01:58
epoch [180/200] batch [50/50] time 0.084 (0.115) data 0.000 (0.031) loss 2.0762 (1.8652) acc 28.1250 (46.1875) lr 5.3915e-05 eta 0:01:54
epoch [181/200] batch [5/50] time 0.084 (0.315) data 0.000 (0.230) loss 1.6943 (1.9211) acc 43.7500 (39.3750) lr 5.3915e-05 eta 0:05:13
epoch [181/200] batch [10/50] time 0.084 (0.200) data 0.000 (0.115) loss 1.9951 (1.9600) acc 43.7500 (42.1875) lr 5.3915e-05 eta 0:03:17
epoch [181/200] batch [15/50] time 0.084 (0.161) data 0.000 (0.077) loss 1.6670 (1.9608) acc 59.3750 (43.3333) lr 5.3915e-05 eta 0:02:39
epoch [181/200] batch [20/50] time 0.084 (0.148) data 0.000 (0.064) loss 1.5625 (1.9080) acc 56.2500 (44.2188) lr 5.3915e-05 eta 0:02:25
epoch [181/200] batch [25/50] time 0.085 (0.135) data 0.000 (0.051) loss 2.0840 (1.9111) acc 43.7500 (44.1250) lr 5.3915e-05 eta 0:02:12
epoch [181/200] batch [30/50] time 0.085 (0.127) data 0.000 (0.043) loss 1.6484 (1.8934) acc 56.2500 (45.5208) lr 5.3915e-05 eta 0:02:03
epoch [181/200] batch [35/50] time 0.084 (0.121) data 0.000 (0.037) loss 1.6777 (1.8766) acc 46.8750 (45.9821) lr 5.3915e-05 eta 0:01:56
epoch [181/200] batch [40/50] time 0.084 (0.116) data 0.000 (0.032) loss 1.9697 (1.8790) acc 37.5000 (45.7031) lr 5.3915e-05 eta 0:01:51
epoch [181/200] batch [45/50] time 0.083 (0.113) data 0.000 (0.029) loss 1.7637 (1.8690) acc 53.1250 (45.8333) lr 5.3915e-05 eta 0:01:47
epoch [181/200] batch [50/50] time 0.085 (0.113) data 0.000 (0.029) loss 1.7646 (1.8551) acc 53.1250 (46.5625) lr 4.8943e-05 eta 0:01:47
epoch [182/200] batch [5/50] time 0.216 (0.289) data 0.131 (0.203) loss 1.8086 (1.8248) acc 50.0000 (47.5000) lr 4.8943e-05 eta 0:04:33
epoch [182/200] batch [10/50] time 0.085 (0.187) data 0.000 (0.102) loss 2.2441 (1.8427) acc 40.6250 (47.1875) lr 4.8943e-05 eta 0:02:56
epoch [182/200] batch [15/50] time 0.086 (0.160) data 0.000 (0.074) loss 1.5713 (1.8221) acc 50.0000 (47.0833) lr 4.8943e-05 eta 0:02:29
epoch [182/200] batch [20/50] time 0.086 (0.141) data 0.000 (0.056) loss 2.1152 (1.8380) acc 37.5000 (45.9375) lr 4.8943e-05 eta 0:02:11
epoch [182/200] batch [25/50] time 0.086 (0.135) data 0.000 (0.050) loss 1.9053 (1.8787) acc 46.8750 (45.2500) lr 4.8943e-05 eta 0:02:04
epoch [182/200] batch [30/50] time 0.154 (0.133) data 0.069 (0.047) loss 1.9600 (1.8890) acc 43.7500 (46.2500) lr 4.8943e-05 eta 0:02:02
epoch [182/200] batch [35/50] time 0.085 (0.126) data 0.001 (0.041) loss 2.4414 (1.8962) acc 37.5000 (45.7143) lr 4.8943e-05 eta 0:01:55
epoch [182/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.040) loss 1.9141 (1.9050) acc 46.8750 (45.7812) lr 4.8943e-05 eta 0:01:53
epoch [182/200] batch [45/50] time 0.083 (0.120) data 0.000 (0.035) loss 2.0332 (1.9187) acc 31.2500 (45.0000) lr 4.8943e-05 eta 0:01:48
epoch [182/200] batch [50/50] time 0.084 (0.118) data 0.000 (0.034) loss 1.6650 (1.9174) acc 53.1250 (44.8750) lr 4.4207e-05 eta 0:01:46
epoch [183/200] batch [5/50] time 0.086 (0.267) data 0.000 (0.181) loss 1.6797 (1.8223) acc 34.3750 (38.1250) lr 4.4207e-05 eta 0:03:58
epoch [183/200] batch [10/50] time 0.086 (0.179) data 0.001 (0.094) loss 1.5771 (1.7803) acc 50.0000 (43.7500) lr 4.4207e-05 eta 0:02:39
epoch [183/200] batch [15/50] time 0.086 (0.148) data 0.000 (0.063) loss 1.8359 (1.8833) acc 53.1250 (41.8750) lr 4.4207e-05 eta 0:02:11
epoch [183/200] batch [20/50] time 0.085 (0.132) data 0.000 (0.047) loss 1.7832 (1.8547) acc 34.3750 (43.4375) lr 4.4207e-05 eta 0:01:56
epoch [183/200] batch [25/50] time 0.086 (0.123) data 0.001 (0.038) loss 1.8525 (1.8498) acc 37.5000 (43.8750) lr 4.4207e-05 eta 0:01:47
epoch [183/200] batch [30/50] time 0.087 (0.119) data 0.000 (0.033) loss 1.8232 (1.8494) acc 56.2500 (44.8958) lr 4.4207e-05 eta 0:01:43
epoch [183/200] batch [35/50] time 0.085 (0.114) data 0.001 (0.029) loss 2.1113 (1.8194) acc 34.3750 (46.2500) lr 4.4207e-05 eta 0:01:38
epoch [183/200] batch [40/50] time 0.085 (0.113) data 0.000 (0.028) loss 2.1758 (1.8519) acc 34.3750 (45.4688) lr 4.4207e-05 eta 0:01:37
epoch [183/200] batch [45/50] time 0.084 (0.113) data 0.000 (0.028) loss 1.7197 (1.8470) acc 56.2500 (45.6250) lr 4.4207e-05 eta 0:01:36
epoch [183/200] batch [50/50] time 0.083 (0.110) data 0.000 (0.025) loss 2.3320 (1.8496) acc 34.3750 (45.6250) lr 3.9706e-05 eta 0:01:33
epoch [184/200] batch [5/50] time 0.087 (0.308) data 0.000 (0.222) loss 1.5957 (1.8783) acc 53.1250 (41.2500) lr 3.9706e-05 eta 0:04:20
epoch [184/200] batch [10/50] time 0.086 (0.201) data 0.001 (0.116) loss 2.1191 (1.8980) acc 40.6250 (41.8750) lr 3.9706e-05 eta 0:02:49
epoch [184/200] batch [15/50] time 0.083 (0.162) data 0.000 (0.077) loss 1.7334 (1.8844) acc 56.2500 (43.9583) lr 3.9706e-05 eta 0:02:15
epoch [184/200] batch [20/50] time 0.086 (0.149) data 0.002 (0.064) loss 1.9121 (1.8482) acc 46.8750 (45.0000) lr 3.9706e-05 eta 0:02:03
epoch [184/200] batch [25/50] time 0.084 (0.136) data 0.000 (0.051) loss 1.8838 (1.8376) acc 43.7500 (45.5000) lr 3.9706e-05 eta 0:01:51
epoch [184/200] batch [30/50] time 0.084 (0.132) data 0.000 (0.047) loss 1.9170 (1.8681) acc 46.8750 (45.6250) lr 3.9706e-05 eta 0:01:47
epoch [184/200] batch [35/50] time 0.086 (0.125) data 0.000 (0.040) loss 1.8760 (1.8667) acc 34.3750 (45.0000) lr 3.9706e-05 eta 0:01:41
epoch [184/200] batch [40/50] time 0.083 (0.124) data 0.000 (0.040) loss 1.6553 (1.8649) acc 62.5000 (45.3125) lr 3.9706e-05 eta 0:01:40
epoch [184/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.8496 (1.8710) acc 50.0000 (45.4167) lr 3.9706e-05 eta 0:01:40
epoch [184/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.5986 (1.8717) acc 50.0000 (45.1250) lr 3.5443e-05 eta 0:01:36
epoch [185/200] batch [5/50] time 0.084 (0.303) data 0.000 (0.219) loss 2.0410 (2.0314) acc 46.8750 (43.1250) lr 3.5443e-05 eta 0:04:00
epoch [185/200] batch [10/50] time 0.317 (0.217) data 0.234 (0.133) loss 2.2871 (2.0009) acc 31.2500 (43.7500) lr 3.5443e-05 eta 0:02:51
epoch [185/200] batch [15/50] time 0.084 (0.173) data 0.000 (0.089) loss 1.6738 (1.9228) acc 50.0000 (46.4583) lr 3.5443e-05 eta 0:02:15
epoch [185/200] batch [20/50] time 0.087 (0.151) data 0.000 (0.067) loss 2.1133 (1.9071) acc 43.7500 (45.7812) lr 3.5443e-05 eta 0:01:57
epoch [185/200] batch [25/50] time 0.085 (0.138) data 0.000 (0.053) loss 2.1133 (1.9095) acc 37.5000 (45.1250) lr 3.5443e-05 eta 0:01:46
epoch [185/200] batch [30/50] time 0.084 (0.129) data 0.000 (0.045) loss 1.6533 (1.8889) acc 46.8750 (45.6250) lr 3.5443e-05 eta 0:01:39
epoch [185/200] batch [35/50] time 0.085 (0.123) data 0.001 (0.038) loss 2.2949 (1.8823) acc 31.2500 (46.0714) lr 3.5443e-05 eta 0:01:33
epoch [185/200] batch [40/50] time 0.085 (0.118) data 0.000 (0.034) loss 1.7773 (1.8655) acc 37.5000 (46.2500) lr 3.5443e-05 eta 0:01:29
epoch [185/200] batch [45/50] time 0.083 (0.117) data 0.000 (0.033) loss 1.6504 (1.8596) acc 59.3750 (46.5278) lr 3.5443e-05 eta 0:01:28
epoch [185/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.029) loss 1.9160 (1.8521) acc 31.2500 (46.3750) lr 3.1417e-05 eta 0:01:25
epoch [186/200] batch [5/50] time 0.083 (0.332) data 0.001 (0.248) loss 1.9072 (1.8561) acc 53.1250 (42.5000) lr 3.1417e-05 eta 0:04:07
epoch [186/200] batch [10/50] time 0.083 (0.213) data 0.000 (0.129) loss 1.9209 (1.8507) acc 43.7500 (44.3750) lr 3.1417e-05 eta 0:02:37
epoch [186/200] batch [15/50] time 0.084 (0.170) data 0.000 (0.086) loss 1.7715 (1.8824) acc 43.7500 (42.0833) lr 3.1417e-05 eta 0:02:05
epoch [186/200] batch [20/50] time 0.084 (0.154) data 0.000 (0.071) loss 1.7969 (1.9116) acc 34.3750 (41.8750) lr 3.1417e-05 eta 0:01:52
epoch [186/200] batch [25/50] time 0.084 (0.140) data 0.000 (0.057) loss 2.1895 (1.9189) acc 34.3750 (42.8750) lr 3.1417e-05 eta 0:01:41
epoch [186/200] batch [30/50] time 0.084 (0.133) data 0.000 (0.049) loss 2.1387 (1.9361) acc 34.3750 (41.7708) lr 3.1417e-05 eta 0:01:35
epoch [186/200] batch [35/50] time 0.085 (0.126) data 0.001 (0.042) loss 1.7705 (1.9051) acc 50.0000 (43.6607) lr 3.1417e-05 eta 0:01:30
epoch [186/200] batch [40/50] time 0.084 (0.121) data 0.000 (0.037) loss 1.6025 (1.8961) acc 50.0000 (43.4375) lr 3.1417e-05 eta 0:01:25
epoch [186/200] batch [45/50] time 0.083 (0.117) data 0.000 (0.033) loss 1.6445 (1.8708) acc 59.3750 (44.5139) lr 3.1417e-05 eta 0:01:22
epoch [186/200] batch [50/50] time 0.085 (0.113) data 0.000 (0.030) loss 2.0430 (1.8700) acc 50.0000 (44.5000) lr 2.7630e-05 eta 0:01:19
epoch [187/200] batch [5/50] time 0.085 (0.285) data 0.000 (0.200) loss 2.1738 (1.8795) acc 37.5000 (50.0000) lr 2.7630e-05 eta 0:03:18
epoch [187/200] batch [10/50] time 0.086 (0.186) data 0.000 (0.100) loss 1.9014 (1.8233) acc 34.3750 (49.0625) lr 2.7630e-05 eta 0:02:08
epoch [187/200] batch [15/50] time 0.086 (0.153) data 0.001 (0.067) loss 1.4648 (1.8226) acc 56.2500 (49.3750) lr 2.7630e-05 eta 0:01:44
epoch [187/200] batch [20/50] time 0.086 (0.136) data 0.000 (0.050) loss 2.0254 (1.8272) acc 40.6250 (49.0625) lr 2.7630e-05 eta 0:01:32
epoch [187/200] batch [25/50] time 0.086 (0.126) data 0.000 (0.040) loss 2.3027 (1.8639) acc 40.6250 (48.6250) lr 2.7630e-05 eta 0:01:24
epoch [187/200] batch [30/50] time 0.086 (0.119) data 0.001 (0.034) loss 1.8135 (1.8628) acc 53.1250 (48.3333) lr 2.7630e-05 eta 0:01:19
epoch [187/200] batch [35/50] time 0.088 (0.115) data 0.001 (0.029) loss 1.8818 (1.8652) acc 46.8750 (48.0357) lr 2.7630e-05 eta 0:01:16
epoch [187/200] batch [40/50] time 0.085 (0.111) data 0.000 (0.025) loss 2.3711 (1.8763) acc 25.0000 (46.9531) lr 2.7630e-05 eta 0:01:13
epoch [187/200] batch [45/50] time 0.084 (0.108) data 0.000 (0.023) loss 1.5195 (1.8731) acc 40.6250 (46.4583) lr 2.7630e-05 eta 0:01:10
epoch [187/200] batch [50/50] time 0.084 (0.106) data 0.000 (0.021) loss 1.6162 (1.8553) acc 56.2500 (46.8125) lr 2.4083e-05 eta 0:01:08
epoch [188/200] batch [5/50] time 0.085 (0.323) data 0.000 (0.235) loss 1.6768 (1.8018) acc 46.8750 (45.0000) lr 2.4083e-05 eta 0:03:28
epoch [188/200] batch [10/50] time 0.084 (0.204) data 0.000 (0.118) loss 1.5957 (1.8793) acc 46.8750 (44.0625) lr 2.4083e-05 eta 0:02:10
epoch [188/200] batch [15/50] time 0.084 (0.165) data 0.000 (0.080) loss 1.9893 (1.9061) acc 37.5000 (41.8750) lr 2.4083e-05 eta 0:01:44
epoch [188/200] batch [20/50] time 0.084 (0.149) data 0.000 (0.064) loss 2.0195 (1.8957) acc 43.7500 (42.0312) lr 2.4083e-05 eta 0:01:33
epoch [188/200] batch [25/50] time 0.119 (0.138) data 0.035 (0.053) loss 1.9277 (1.8798) acc 34.3750 (43.2500) lr 2.4083e-05 eta 0:01:25
epoch [188/200] batch [30/50] time 0.086 (0.129) data 0.000 (0.044) loss 1.9170 (1.8730) acc 46.8750 (43.8542) lr 2.4083e-05 eta 0:01:19
epoch [188/200] batch [35/50] time 0.084 (0.122) data 0.000 (0.038) loss 2.0234 (1.8583) acc 43.7500 (45.0000) lr 2.4083e-05 eta 0:01:15
epoch [188/200] batch [40/50] time 0.083 (0.118) data 0.000 (0.033) loss 1.5146 (1.8582) acc 56.2500 (45.3906) lr 2.4083e-05 eta 0:01:11
epoch [188/200] batch [45/50] time 0.084 (0.114) data 0.000 (0.029) loss 2.1367 (1.8544) acc 37.5000 (44.9306) lr 2.4083e-05 eta 0:01:08
epoch [188/200] batch [50/50] time 0.084 (0.111) data 0.000 (0.026) loss 2.1270 (1.8557) acc 46.8750 (45.5625) lr 2.0777e-05 eta 0:01:06
epoch [189/200] batch [5/50] time 0.084 (0.370) data 0.000 (0.286) loss 1.6250 (1.7100) acc 46.8750 (46.8750) lr 2.0777e-05 eta 0:03:40
epoch [189/200] batch [10/50] time 0.193 (0.239) data 0.109 (0.154) loss 2.1699 (1.8652) acc 28.1250 (45.6250) lr 2.0777e-05 eta 0:02:20
epoch [189/200] batch [15/50] time 0.085 (0.187) data 0.000 (0.103) loss 1.8721 (1.8441) acc 40.6250 (45.4167) lr 2.0777e-05 eta 0:01:49
epoch [189/200] batch [20/50] time 0.085 (0.171) data 0.000 (0.086) loss 2.0781 (1.8416) acc 34.3750 (45.7812) lr 2.0777e-05 eta 0:01:39
epoch [189/200] batch [25/50] time 0.085 (0.154) data 0.000 (0.069) loss 2.2578 (1.8624) acc 34.3750 (45.5000) lr 2.0777e-05 eta 0:01:28
epoch [189/200] batch [30/50] time 0.085 (0.146) data 0.000 (0.061) loss 1.8896 (1.8532) acc 28.1250 (44.6875) lr 2.0777e-05 eta 0:01:23
epoch [189/200] batch [35/50] time 0.085 (0.142) data 0.000 (0.057) loss 1.8887 (1.8619) acc 31.2500 (44.4643) lr 2.0777e-05 eta 0:01:20
epoch [189/200] batch [40/50] time 0.083 (0.135) data 0.000 (0.050) loss 1.5508 (1.8570) acc 50.0000 (44.9219) lr 2.0777e-05 eta 0:01:15
epoch [189/200] batch [45/50] time 0.083 (0.132) data 0.000 (0.048) loss 1.9053 (1.8519) acc 46.8750 (45.3472) lr 2.0777e-05 eta 0:01:13
epoch [189/200] batch [50/50] time 0.083 (0.127) data 0.000 (0.043) loss 1.9668 (1.8450) acc 43.7500 (45.8125) lr 1.7713e-05 eta 0:01:10
epoch [190/200] batch [5/50] time 0.084 (0.344) data 0.000 (0.260) loss 1.6826 (1.8805) acc 46.8750 (42.5000) lr 1.7713e-05 eta 0:03:07
epoch [190/200] batch [10/50] time 0.086 (0.225) data 0.000 (0.141) loss 1.9248 (1.8401) acc 46.8750 (46.2500) lr 1.7713e-05 eta 0:02:01
epoch [190/200] batch [15/50] time 0.086 (0.179) data 0.000 (0.094) loss 1.7773 (1.8644) acc 43.7500 (46.0417) lr 1.7713e-05 eta 0:01:35
epoch [190/200] batch [20/50] time 0.085 (0.162) data 0.000 (0.078) loss 1.8447 (1.8784) acc 34.3750 (44.6875) lr 1.7713e-05 eta 0:01:26
epoch [190/200] batch [25/50] time 0.274 (0.154) data 0.189 (0.070) loss 2.0156 (1.8582) acc 50.0000 (45.1250) lr 1.7713e-05 eta 0:01:21
epoch [190/200] batch [30/50] time 0.087 (0.143) data 0.000 (0.058) loss 1.6172 (1.8617) acc 46.8750 (45.0000) lr 1.7713e-05 eta 0:01:14
epoch [190/200] batch [35/50] time 0.084 (0.139) data 0.000 (0.054) loss 1.6377 (1.8410) acc 62.5000 (45.4464) lr 1.7713e-05 eta 0:01:11
epoch [190/200] batch [40/50] time 0.082 (0.132) data 0.000 (0.047) loss 1.7354 (1.8625) acc 46.8750 (44.8438) lr 1.7713e-05 eta 0:01:07
epoch [190/200] batch [45/50] time 0.082 (0.130) data 0.000 (0.045) loss 2.0820 (1.8791) acc 25.0000 (44.5833) lr 1.7713e-05 eta 0:01:05
epoch [190/200] batch [50/50] time 0.082 (0.125) data 0.000 (0.041) loss 1.7080 (1.8731) acc 53.1250 (44.5000) lr 1.4891e-05 eta 0:01:02
epoch [191/200] batch [5/50] time 0.084 (0.359) data 0.000 (0.274) loss 1.7432 (2.0154) acc 46.8750 (41.2500) lr 1.4891e-05 eta 0:02:57
epoch [191/200] batch [10/50] time 0.085 (0.222) data 0.000 (0.137) loss 1.8760 (1.9214) acc 43.7500 (45.0000) lr 1.4891e-05 eta 0:01:48
epoch [191/200] batch [15/50] time 0.086 (0.179) data 0.000 (0.094) loss 1.7354 (1.8714) acc 37.5000 (45.8333) lr 1.4891e-05 eta 0:01:26
epoch [191/200] batch [20/50] time 0.084 (0.163) data 0.000 (0.078) loss 1.8682 (1.8290) acc 40.6250 (47.1875) lr 1.4891e-05 eta 0:01:18
epoch [191/200] batch [25/50] time 0.085 (0.147) data 0.000 (0.063) loss 1.6924 (1.8133) acc 50.0000 (48.3750) lr 1.4891e-05 eta 0:01:09
epoch [191/200] batch [30/50] time 0.084 (0.145) data 0.000 (0.060) loss 1.7705 (1.7951) acc 59.3750 (49.1667) lr 1.4891e-05 eta 0:01:07
epoch [191/200] batch [35/50] time 0.280 (0.142) data 0.197 (0.057) loss 2.3906 (1.8300) acc 46.8750 (48.0357) lr 1.4891e-05 eta 0:01:05
epoch [191/200] batch [40/50] time 0.083 (0.134) data 0.000 (0.050) loss 2.1016 (1.8332) acc 34.3750 (47.4219) lr 1.4891e-05 eta 0:01:01
epoch [191/200] batch [45/50] time 0.083 (0.132) data 0.000 (0.048) loss 2.5820 (1.8561) acc 34.3750 (46.8750) lr 1.4891e-05 eta 0:01:00
epoch [191/200] batch [50/50] time 0.083 (0.127) data 0.000 (0.043) loss 1.9707 (1.8563) acc 59.3750 (47.0625) lr 1.2312e-05 eta 0:00:57
epoch [192/200] batch [5/50] time 0.086 (0.298) data 0.000 (0.213) loss 2.4238 (2.1535) acc 34.3750 (37.5000) lr 1.2312e-05 eta 0:02:12
epoch [192/200] batch [10/50] time 0.111 (0.195) data 0.026 (0.109) loss 1.5342 (1.9771) acc 46.8750 (41.5625) lr 1.2312e-05 eta 0:01:25
epoch [192/200] batch [15/50] time 0.086 (0.158) data 0.000 (0.073) loss 1.6924 (1.9667) acc 31.2500 (41.0417) lr 1.2312e-05 eta 0:01:08
epoch [192/200] batch [20/50] time 0.086 (0.146) data 0.000 (0.061) loss 1.5527 (1.9183) acc 59.3750 (43.5938) lr 1.2312e-05 eta 0:01:02
epoch [192/200] batch [25/50] time 0.218 (0.139) data 0.134 (0.054) loss 1.5244 (1.9398) acc 56.2500 (43.5000) lr 1.2312e-05 eta 0:00:59
epoch [192/200] batch [30/50] time 0.088 (0.132) data 0.001 (0.046) loss 2.1465 (1.9604) acc 43.7500 (42.9167) lr 1.2312e-05 eta 0:00:55
epoch [192/200] batch [35/50] time 0.119 (0.128) data 0.034 (0.043) loss 1.5684 (1.9371) acc 46.8750 (43.7500) lr 1.2312e-05 eta 0:00:53
epoch [192/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.038) loss 1.7852 (1.9127) acc 37.5000 (43.9062) lr 1.2312e-05 eta 0:00:50
epoch [192/200] batch [45/50] time 0.085 (0.123) data 0.001 (0.038) loss 1.6777 (1.9144) acc 56.2500 (44.1667) lr 1.2312e-05 eta 0:00:49
epoch [192/200] batch [50/50] time 0.085 (0.119) data 0.000 (0.034) loss 1.5713 (1.9032) acc 46.8750 (43.8750) lr 9.9763e-06 eta 0:00:47
epoch [193/200] batch [5/50] time 0.083 (0.329) data 0.000 (0.245) loss 1.9023 (1.9037) acc 50.0000 (45.0000) lr 9.9763e-06 eta 0:02:10
epoch [193/200] batch [10/50] time 0.083 (0.207) data 0.000 (0.123) loss 1.9580 (1.9090) acc 37.5000 (45.0000) lr 9.9763e-06 eta 0:01:20
epoch [193/200] batch [15/50] time 0.084 (0.166) data 0.000 (0.082) loss 1.8271 (1.9355) acc 37.5000 (42.2917) lr 9.9763e-06 eta 0:01:03
epoch [193/200] batch [20/50] time 0.084 (0.154) data 0.000 (0.071) loss 1.5205 (1.8923) acc 53.1250 (43.7500) lr 9.9763e-06 eta 0:00:58
epoch [193/200] batch [25/50] time 0.083 (0.140) data 0.000 (0.057) loss 1.7490 (1.8462) acc 53.1250 (45.1250) lr 9.9763e-06 eta 0:00:52
epoch [193/200] batch [30/50] time 0.084 (0.132) data 0.000 (0.048) loss 1.9678 (1.8664) acc 46.8750 (45.4167) lr 9.9763e-06 eta 0:00:48
epoch [193/200] batch [35/50] time 0.226 (0.129) data 0.142 (0.046) loss 1.5303 (1.8599) acc 62.5000 (45.8036) lr 9.9763e-06 eta 0:00:47
epoch [193/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.9727 (1.8584) acc 59.3750 (46.3281) lr 9.9763e-06 eta 0:00:44
epoch [193/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.040) loss 2.1719 (1.8587) acc 37.5000 (46.3194) lr 9.9763e-06 eta 0:00:43
epoch [193/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.036) loss 1.6895 (1.8530) acc 50.0000 (46.5625) lr 7.8853e-06 eta 0:00:41
epoch [194/200] batch [5/50] time 0.086 (0.304) data 0.000 (0.218) loss 1.6289 (1.6443) acc 40.6250 (49.3750) lr 7.8853e-06 eta 0:01:44
epoch [194/200] batch [10/50] time 0.087 (0.203) data 0.000 (0.117) loss 2.1660 (1.8318) acc 34.3750 (46.2500) lr 7.8853e-06 eta 0:01:08
epoch [194/200] batch [15/50] time 0.086 (0.164) data 0.001 (0.078) loss 2.1973 (1.8579) acc 37.5000 (45.6250) lr 7.8853e-06 eta 0:00:54
epoch [194/200] batch [20/50] time 0.086 (0.152) data 0.000 (0.066) loss 2.5000 (1.8797) acc 25.0000 (45.4688) lr 7.8853e-06 eta 0:00:50
epoch [194/200] batch [25/50] time 0.086 (0.138) data 0.000 (0.053) loss 2.2969 (1.8710) acc 31.2500 (46.0000) lr 7.8853e-06 eta 0:00:44
epoch [194/200] batch [30/50] time 0.084 (0.138) data 0.000 (0.052) loss 1.5615 (1.8508) acc 50.0000 (47.0833) lr 7.8853e-06 eta 0:00:44
epoch [194/200] batch [35/50] time 0.084 (0.135) data 0.000 (0.050) loss 1.7939 (1.8346) acc 50.0000 (47.1429) lr 7.8853e-06 eta 0:00:42
epoch [194/200] batch [40/50] time 0.083 (0.129) data 0.000 (0.044) loss 1.8682 (1.8551) acc 40.6250 (46.7188) lr 7.8853e-06 eta 0:00:39
epoch [194/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.041) loss 1.9775 (1.8472) acc 43.7500 (46.8750) lr 7.8853e-06 eta 0:00:38
epoch [194/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.7637 (1.8587) acc 43.7500 (46.7500) lr 6.0390e-06 eta 0:00:36
epoch [195/200] batch [5/50] time 0.177 (0.283) data 0.093 (0.197) loss 2.0176 (1.7375) acc 31.2500 (48.7500) lr 6.0390e-06 eta 0:01:23
epoch [195/200] batch [10/50] time 0.085 (0.184) data 0.000 (0.099) loss 1.8184 (1.7897) acc 43.7500 (46.8750) lr 6.0390e-06 eta 0:00:53
epoch [195/200] batch [15/50] time 0.086 (0.161) data 0.000 (0.075) loss 1.9131 (1.8046) acc 40.6250 (47.2917) lr 6.0390e-06 eta 0:00:45
epoch [195/200] batch [20/50] time 0.086 (0.142) data 0.000 (0.056) loss 1.9375 (1.8007) acc 31.2500 (46.2500) lr 6.0390e-06 eta 0:00:39
epoch [195/200] batch [25/50] time 0.085 (0.137) data 0.000 (0.052) loss 1.7539 (1.7974) acc 40.6250 (46.3750) lr 6.0390e-06 eta 0:00:37
epoch [195/200] batch [30/50] time 0.085 (0.135) data 0.001 (0.050) loss 1.4404 (1.8118) acc 53.1250 (46.8750) lr 6.0390e-06 eta 0:00:36
epoch [195/200] batch [35/50] time 0.085 (0.128) data 0.000 (0.043) loss 1.9619 (1.8202) acc 50.0000 (47.2321) lr 6.0390e-06 eta 0:00:33
epoch [195/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.040) loss 1.3516 (1.8056) acc 62.5000 (47.6562) lr 6.0390e-06 eta 0:00:32
epoch [195/200] batch [45/50] time 0.127 (0.121) data 0.043 (0.037) loss 1.9668 (1.8300) acc 40.6250 (47.0139) lr 6.0390e-06 eta 0:00:30
epoch [195/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.033) loss 1.8809 (1.8272) acc 46.8750 (47.0000) lr 4.4380e-06 eta 0:00:29
epoch [196/200] batch [5/50] time 0.086 (0.303) data 0.000 (0.217) loss 1.4346 (1.8156) acc 68.7500 (47.5000) lr 4.4380e-06 eta 0:01:14
epoch [196/200] batch [10/50] time 0.086 (0.197) data 0.000 (0.111) loss 1.6504 (1.8428) acc 62.5000 (45.6250) lr 4.4380e-06 eta 0:00:47
epoch [196/200] batch [15/50] time 0.086 (0.160) data 0.000 (0.074) loss 1.9990 (1.8724) acc 43.7500 (45.0000) lr 4.4380e-06 eta 0:00:37
epoch [196/200] batch [20/50] time 0.084 (0.152) data 0.000 (0.067) loss 2.0039 (1.9226) acc 37.5000 (42.8125) lr 4.4380e-06 eta 0:00:35
epoch [196/200] batch [25/50] time 0.287 (0.147) data 0.204 (0.062) loss 1.3936 (1.8791) acc 59.3750 (43.6250) lr 4.4380e-06 eta 0:00:33
epoch [196/200] batch [30/50] time 0.084 (0.136) data 0.000 (0.051) loss 1.4150 (1.8608) acc 59.3750 (44.6875) lr 4.4380e-06 eta 0:00:30
epoch [196/200] batch [35/50] time 0.084 (0.136) data 0.000 (0.051) loss 1.9736 (1.8617) acc 34.3750 (44.2857) lr 4.4380e-06 eta 0:00:29
epoch [196/200] batch [40/50] time 0.083 (0.129) data 0.000 (0.044) loss 2.0801 (1.8656) acc 31.2500 (44.0625) lr 4.4380e-06 eta 0:00:27
epoch [196/200] batch [45/50] time 0.083 (0.127) data 0.000 (0.043) loss 2.0176 (1.8662) acc 37.5000 (44.8611) lr 4.4380e-06 eta 0:00:26
epoch [196/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.038) loss 1.4434 (1.8459) acc 62.5000 (45.5625) lr 3.0827e-06 eta 0:00:24
epoch [197/200] batch [5/50] time 0.085 (0.304) data 0.000 (0.218) loss 1.7520 (1.7121) acc 50.0000 (45.6250) lr 3.0827e-06 eta 0:00:59
epoch [197/200] batch [10/50] time 0.087 (0.199) data 0.000 (0.114) loss 1.6846 (1.7245) acc 56.2500 (46.5625) lr 3.0827e-06 eta 0:00:37
epoch [197/200] batch [15/50] time 0.085 (0.161) data 0.000 (0.076) loss 1.7471 (1.7839) acc 50.0000 (45.6250) lr 3.0827e-06 eta 0:00:29
epoch [197/200] batch [20/50] time 0.085 (0.143) data 0.000 (0.058) loss 2.1543 (1.7689) acc 34.3750 (45.3125) lr 3.0827e-06 eta 0:00:25
epoch [197/200] batch [25/50] time 0.084 (0.131) data 0.000 (0.046) loss 2.3926 (1.7905) acc 28.1250 (44.8750) lr 3.0827e-06 eta 0:00:22
epoch [197/200] batch [30/50] time 0.085 (0.127) data 0.000 (0.042) loss 1.4678 (1.8043) acc 56.2500 (44.2708) lr 3.0827e-06 eta 0:00:21
epoch [197/200] batch [35/50] time 0.086 (0.121) data 0.001 (0.036) loss 1.8252 (1.8230) acc 53.1250 (45.0000) lr 3.0827e-06 eta 0:00:19
epoch [197/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.035) loss 1.6592 (1.8218) acc 40.6250 (44.8438) lr 3.0827e-06 eta 0:00:19
epoch [197/200] batch [45/50] time 0.084 (0.116) data 0.000 (0.031) loss 2.6465 (1.8710) acc 28.1250 (44.0972) lr 3.0827e-06 eta 0:00:17
epoch [197/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 1.8984 (1.8799) acc 40.6250 (44.3125) lr 1.9733e-06 eta 0:00:17
epoch [198/200] batch [5/50] time 0.084 (0.370) data 0.000 (0.286) loss 2.0449 (1.8670) acc 50.0000 (46.8750) lr 1.9733e-06 eta 0:00:53
epoch [198/200] batch [10/50] time 0.085 (0.228) data 0.000 (0.143) loss 1.9268 (1.8945) acc 34.3750 (43.7500) lr 1.9733e-06 eta 0:00:31
epoch [198/200] batch [15/50] time 0.086 (0.188) data 0.000 (0.103) loss 1.6035 (1.9267) acc 50.0000 (43.5417) lr 1.9733e-06 eta 0:00:25
epoch [198/200] batch [20/50] time 0.084 (0.174) data 0.000 (0.089) loss 1.5820 (1.8665) acc 59.3750 (45.1562) lr 1.9733e-06 eta 0:00:22
epoch [198/200] batch [25/50] time 0.084 (0.156) data 0.000 (0.072) loss 2.0371 (1.8454) acc 50.0000 (46.7500) lr 1.9733e-06 eta 0:00:19
epoch [198/200] batch [30/50] time 0.085 (0.151) data 0.000 (0.066) loss 1.9199 (1.8305) acc 43.7500 (47.0833) lr 1.9733e-06 eta 0:00:18
epoch [198/200] batch [35/50] time 0.294 (0.147) data 0.209 (0.063) loss 2.1035 (1.8421) acc 50.0000 (47.1429) lr 1.9733e-06 eta 0:00:16
epoch [198/200] batch [40/50] time 0.084 (0.139) data 0.000 (0.055) loss 2.1758 (1.8499) acc 34.3750 (47.1875) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [45/50] time 0.082 (0.136) data 0.000 (0.052) loss 1.8906 (1.8446) acc 40.6250 (47.1528) lr 1.9733e-06 eta 0:00:14
epoch [198/200] batch [50/50] time 0.083 (0.131) data 0.000 (0.047) loss 2.1191 (1.8554) acc 46.8750 (47.3125) lr 1.1101e-06 eta 0:00:13
epoch [199/200] batch [5/50] time 0.085 (0.301) data 0.000 (0.216) loss 2.0977 (1.9176) acc 43.7500 (44.3750) lr 1.1101e-06 eta 0:00:28
epoch [199/200] batch [10/50] time 0.190 (0.203) data 0.104 (0.118) loss 2.2676 (1.8998) acc 28.1250 (44.6875) lr 1.1101e-06 eta 0:00:18
epoch [199/200] batch [15/50] time 0.087 (0.164) data 0.000 (0.079) loss 1.8613 (1.8702) acc 46.8750 (45.2083) lr 1.1101e-06 eta 0:00:13
epoch [199/200] batch [20/50] time 0.086 (0.147) data 0.001 (0.062) loss 1.5430 (1.8526) acc 53.1250 (44.8438) lr 1.1101e-06 eta 0:00:11
epoch [199/200] batch [25/50] time 0.086 (0.135) data 0.000 (0.050) loss 2.0449 (1.8617) acc 50.0000 (44.3750) lr 1.1101e-06 eta 0:00:10
epoch [199/200] batch [30/50] time 0.087 (0.131) data 0.000 (0.045) loss 1.9727 (1.8937) acc 43.7500 (44.1667) lr 1.1101e-06 eta 0:00:09
epoch [199/200] batch [35/50] time 0.086 (0.127) data 0.001 (0.042) loss 2.0625 (1.8933) acc 37.5000 (44.4643) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.038) loss 1.7305 (1.8880) acc 53.1250 (44.5312) lr 1.1101e-06 eta 0:00:07
epoch [199/200] batch [45/50] time 0.084 (0.123) data 0.000 (0.038) loss 2.2207 (1.8954) acc 34.3750 (44.3750) lr 1.1101e-06 eta 0:00:06
epoch [199/200] batch [50/50] time 0.082 (0.119) data 0.000 (0.034) loss 1.8926 (1.8982) acc 46.8750 (44.5625) lr 4.9344e-07 eta 0:00:05
epoch [200/200] batch [5/50] time 0.086 (0.317) data 0.000 (0.232) loss 2.5078 (1.9184) acc 34.3750 (43.1250) lr 4.9344e-07 eta 0:00:14
epoch [200/200] batch [10/50] time 0.083 (0.222) data 0.000 (0.137) loss 1.6875 (1.8300) acc 59.3750 (48.4375) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [15/50] time 0.083 (0.176) data 0.000 (0.092) loss 1.6094 (1.8176) acc 50.0000 (46.6667) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [20/50] time 0.084 (0.163) data 0.000 (0.078) loss 1.6396 (1.8467) acc 59.3750 (47.8125) lr 4.9344e-07 eta 0:00:04
epoch [200/200] batch [25/50] time 0.085 (0.147) data 0.000 (0.063) loss 2.0078 (1.8602) acc 43.7500 (47.0000) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [30/50] time 0.086 (0.137) data 0.000 (0.052) loss 1.8564 (1.8335) acc 37.5000 (47.1875) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [35/50] time 0.085 (0.129) data 0.000 (0.045) loss 1.8115 (1.8193) acc 46.8750 (46.7857) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [40/50] time 0.083 (0.124) data 0.000 (0.039) loss 1.6689 (1.8289) acc 53.1250 (46.7969) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [45/50] time 0.083 (0.119) data 0.000 (0.035) loss 1.6465 (1.8216) acc 46.8750 (46.4583) lr 4.9344e-07 eta 0:00:00
epoch [200/200] batch [50/50] time 0.085 (0.116) data 0.000 (0.032) loss 1.5205 (1.8020) acc 59.3750 (47.1250) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/34 [00:00<?, ?it/s]  3%|▎         | 1/34 [00:03<01:43,  3.13s/it]  6%|▌         | 2/34 [00:03<00:43,  1.36s/it]  9%|▉         | 3/34 [00:03<00:27,  1.15it/s] 12%|█▏        | 4/34 [00:03<00:18,  1.59it/s] 15%|█▍        | 5/34 [00:03<00:12,  2.24it/s] 18%|█▊        | 6/34 [00:04<00:09,  2.97it/s] 21%|██        | 7/34 [00:04<00:07,  3.75it/s] 24%|██▎       | 8/34 [00:04<00:05,  4.53it/s] 26%|██▋       | 9/34 [00:05<00:14,  1.77it/s] 29%|██▉       | 10/34 [00:05<00:10,  2.33it/s] 32%|███▏      | 11/34 [00:05<00:07,  2.98it/s] 35%|███▌      | 12/34 [00:07<00:13,  1.64it/s] 38%|███▊      | 13/34 [00:07<00:09,  2.17it/s] 41%|████      | 14/34 [00:07<00:07,  2.79it/s] 44%|████▍     | 15/34 [00:07<00:05,  3.48it/s] 47%|████▋     | 16/34 [00:07<00:04,  4.21it/s] 50%|█████     | 17/34 [00:08<00:08,  1.99it/s] 53%|█████▎    | 18/34 [00:08<00:06,  2.58it/s] 56%|█████▌    | 19/34 [00:08<00:04,  3.25it/s] 59%|█████▉    | 20/34 [00:10<00:08,  1.58it/s] 62%|██████▏   | 21/34 [00:10<00:06,  2.09it/s] 65%|██████▍   | 22/34 [00:10<00:04,  2.69it/s] 68%|██████▊   | 23/34 [00:10<00:03,  3.37it/s] 71%|███████   | 24/34 [00:10<00:02,  4.10it/s] 74%|███████▎  | 25/34 [00:11<00:02,  3.23it/s] 76%|███████▋  | 26/34 [00:11<00:02,  3.95it/s] 79%|███████▉  | 27/34 [00:11<00:01,  4.68it/s] 82%|████████▏ | 28/34 [00:12<00:02,  2.08it/s] 85%|████████▌ | 29/34 [00:12<00:01,  2.68it/s] 88%|████████▊ | 30/34 [00:12<00:01,  3.36it/s] 91%|█████████ | 31/34 [00:12<00:00,  4.09it/s] 94%|█████████▍| 32/34 [00:13<00:00,  4.82it/s] 97%|█████████▋| 33/34 [00:13<00:00,  4.19it/s]100%|██████████| 34/34 [00:13<00:00,  2.50it/s]
=> result
* total: 3,333
* correct: 1,366
* accuracy: 41.0%
* error: 59.0%
* macro_f1: 39.7%
Elapsed: 0:20:03
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_train.sh fgvc_aircraft 3 0 vit_b16_ctxv1 16 CoOp
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_3.pkl
1600 3333 3333
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  100
# train_x  1,600
# val      3,333
# test     3,333
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/200] batch [5/50] time 0.083 (0.719) data 0.000 (0.265) loss 3.2930 (3.0680) acc 18.7500 (22.5000) lr 1.0000e-05 eta 1:59:48
epoch [1/200] batch [10/50] time 0.084 (0.403) data 0.000 (0.134) loss 2.8457 (3.0207) acc 34.3750 (20.9375) lr 1.0000e-05 eta 1:07:07
epoch [1/200] batch [15/50] time 0.082 (0.296) data 0.000 (0.089) loss 3.2930 (3.0191) acc 25.0000 (20.6250) lr 1.0000e-05 eta 0:49:18
epoch [1/200] batch [20/50] time 0.084 (0.243) data 0.000 (0.067) loss 3.1152 (3.0598) acc 21.8750 (20.0000) lr 1.0000e-05 eta 0:40:24
epoch [1/200] batch [25/50] time 0.083 (0.211) data 0.000 (0.054) loss 3.5176 (3.0976) acc 25.0000 (20.6250) lr 1.0000e-05 eta 0:35:04
epoch [1/200] batch [30/50] time 0.084 (0.190) data 0.000 (0.045) loss 2.9141 (3.0562) acc 31.2500 (21.5625) lr 1.0000e-05 eta 0:31:31
epoch [1/200] batch [35/50] time 0.084 (0.175) data 0.000 (0.038) loss 2.8047 (3.0533) acc 12.5000 (20.8036) lr 1.0000e-05 eta 0:28:59
epoch [1/200] batch [40/50] time 0.084 (0.163) data 0.000 (0.034) loss 2.7500 (3.0338) acc 34.3750 (21.9531) lr 1.0000e-05 eta 0:27:05
epoch [1/200] batch [45/50] time 0.082 (0.154) data 0.000 (0.030) loss 2.7012 (3.0100) acc 25.0000 (21.9444) lr 1.0000e-05 eta 0:25:35
epoch [1/200] batch [50/50] time 0.082 (0.147) data 0.000 (0.027) loss 2.7305 (3.0192) acc 31.2500 (21.9375) lr 2.0000e-03 eta 0:24:22
epoch [2/200] batch [5/50] time 0.084 (0.324) data 0.000 (0.240) loss 2.8535 (2.9688) acc 15.6250 (21.2500) lr 2.0000e-03 eta 0:53:43
epoch [2/200] batch [10/50] time 0.085 (0.212) data 0.000 (0.128) loss 2.6816 (2.8182) acc 25.0000 (21.5625) lr 2.0000e-03 eta 0:35:10
epoch [2/200] batch [15/50] time 0.085 (0.170) data 0.000 (0.086) loss 2.9316 (2.7846) acc 21.8750 (21.2500) lr 2.0000e-03 eta 0:28:11
epoch [2/200] batch [20/50] time 0.083 (0.153) data 0.000 (0.068) loss 3.0586 (2.7896) acc 12.5000 (21.5625) lr 2.0000e-03 eta 0:25:16
epoch [2/200] batch [25/50] time 0.084 (0.139) data 0.000 (0.055) loss 3.1621 (2.7477) acc 25.0000 (22.7500) lr 2.0000e-03 eta 0:22:58
epoch [2/200] batch [30/50] time 0.084 (0.138) data 0.000 (0.054) loss 2.3613 (2.7368) acc 34.3750 (23.4375) lr 2.0000e-03 eta 0:22:51
epoch [2/200] batch [35/50] time 0.084 (0.134) data 0.000 (0.050) loss 2.6055 (2.7554) acc 18.7500 (23.2143) lr 2.0000e-03 eta 0:22:06
epoch [2/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.044) loss 2.8965 (2.7386) acc 15.6250 (23.5156) lr 2.0000e-03 eta 0:21:02
epoch [2/200] batch [45/50] time 0.082 (0.124) data 0.000 (0.041) loss 2.7910 (2.7316) acc 31.2500 (23.6806) lr 2.0000e-03 eta 0:20:32
epoch [2/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.037) loss 3.2676 (2.7466) acc 21.8750 (23.6250) lr 1.9999e-03 eta 0:19:50
epoch [3/200] batch [5/50] time 0.085 (0.322) data 0.000 (0.237) loss 2.8457 (2.5582) acc 21.8750 (28.1250) lr 1.9999e-03 eta 0:53:03
epoch [3/200] batch [10/50] time 0.083 (0.203) data 0.000 (0.119) loss 2.9453 (2.6418) acc 15.6250 (27.1875) lr 1.9999e-03 eta 0:33:24
epoch [3/200] batch [15/50] time 0.083 (0.163) data 0.000 (0.079) loss 2.3320 (2.5367) acc 25.0000 (28.1250) lr 1.9999e-03 eta 0:26:52
epoch [3/200] batch [20/50] time 0.084 (0.154) data 0.000 (0.070) loss 2.8652 (2.5469) acc 25.0000 (27.5000) lr 1.9999e-03 eta 0:25:26
epoch [3/200] batch [25/50] time 0.085 (0.141) data 0.001 (0.056) loss 2.2910 (2.5517) acc 28.1250 (27.2500) lr 1.9999e-03 eta 0:23:08
epoch [3/200] batch [30/50] time 0.085 (0.135) data 0.000 (0.051) loss 2.6660 (2.5815) acc 15.6250 (25.8333) lr 1.9999e-03 eta 0:22:11
epoch [3/200] batch [35/50] time 0.084 (0.135) data 0.001 (0.050) loss 3.2031 (2.6000) acc 25.0000 (25.7143) lr 1.9999e-03 eta 0:22:08
epoch [3/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.044) loss 2.3633 (2.5907) acc 21.8750 (25.2344) lr 1.9999e-03 eta 0:21:04
epoch [3/200] batch [45/50] time 0.083 (0.128) data 0.000 (0.044) loss 2.3965 (2.5683) acc 31.2500 (25.8333) lr 1.9999e-03 eta 0:20:57
epoch [3/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.4336 (2.5625) acc 25.0000 (25.8750) lr 1.9995e-03 eta 0:20:13
epoch [4/200] batch [5/50] time 0.085 (0.279) data 0.000 (0.194) loss 2.6270 (2.3203) acc 25.0000 (31.2500) lr 1.9995e-03 eta 0:45:44
epoch [4/200] batch [10/50] time 0.136 (0.188) data 0.052 (0.104) loss 2.4727 (2.4137) acc 25.0000 (30.6250) lr 1.9995e-03 eta 0:30:53
epoch [4/200] batch [15/50] time 0.087 (0.156) data 0.000 (0.071) loss 2.7812 (2.5229) acc 15.6250 (25.8333) lr 1.9995e-03 eta 0:25:30
epoch [4/200] batch [20/50] time 0.085 (0.138) data 0.000 (0.053) loss 2.5137 (2.5287) acc 28.1250 (26.0938) lr 1.9995e-03 eta 0:22:36
epoch [4/200] batch [25/50] time 0.085 (0.135) data 0.000 (0.051) loss 2.7559 (2.5002) acc 21.8750 (26.5000) lr 1.9995e-03 eta 0:22:08
epoch [4/200] batch [30/50] time 0.085 (0.134) data 0.000 (0.049) loss 1.9209 (2.4961) acc 34.3750 (26.7708) lr 1.9995e-03 eta 0:21:52
epoch [4/200] batch [35/50] time 0.085 (0.127) data 0.001 (0.042) loss 2.2949 (2.5205) acc 25.0000 (26.5179) lr 1.9995e-03 eta 0:20:44
epoch [4/200] batch [40/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.7109 (2.5211) acc 18.7500 (26.5625) lr 1.9995e-03 eta 0:20:20
epoch [4/200] batch [45/50] time 0.278 (0.124) data 0.195 (0.040) loss 2.2246 (2.5192) acc 37.5000 (26.8056) lr 1.9995e-03 eta 0:20:18
epoch [4/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 2.3184 (2.5097) acc 34.3750 (27.0000) lr 1.9989e-03 eta 0:19:37
epoch [5/200] batch [5/50] time 0.085 (0.323) data 0.000 (0.239) loss 1.7314 (2.3857) acc 46.8750 (30.0000) lr 1.9989e-03 eta 0:52:46
epoch [5/200] batch [10/50] time 0.247 (0.220) data 0.165 (0.136) loss 2.2168 (2.4483) acc 34.3750 (27.5000) lr 1.9989e-03 eta 0:35:53
epoch [5/200] batch [15/50] time 0.083 (0.175) data 0.000 (0.091) loss 2.4082 (2.4896) acc 40.6250 (27.0833) lr 1.9989e-03 eta 0:28:28
epoch [5/200] batch [20/50] time 0.085 (0.157) data 0.001 (0.073) loss 2.5117 (2.4702) acc 21.8750 (26.4062) lr 1.9989e-03 eta 0:25:32
epoch [5/200] batch [25/50] time 0.084 (0.142) data 0.000 (0.058) loss 2.0469 (2.4409) acc 37.5000 (27.7500) lr 1.9989e-03 eta 0:23:09
epoch [5/200] batch [30/50] time 0.085 (0.137) data 0.000 (0.054) loss 2.5039 (2.4356) acc 28.1250 (28.4375) lr 1.9989e-03 eta 0:22:22
epoch [5/200] batch [35/50] time 0.085 (0.134) data 0.001 (0.050) loss 2.5078 (2.4312) acc 18.7500 (28.6607) lr 1.9989e-03 eta 0:21:53
epoch [5/200] batch [40/50] time 0.084 (0.128) data 0.000 (0.044) loss 2.6582 (2.4358) acc 18.7500 (28.1250) lr 1.9989e-03 eta 0:20:49
epoch [5/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 1.9648 (2.4297) acc 37.5000 (28.2639) lr 1.9989e-03 eta 0:20:08
epoch [5/200] batch [50/50] time 0.084 (0.120) data 0.000 (0.036) loss 2.6445 (2.4385) acc 18.7500 (28.0625) lr 1.9980e-03 eta 0:19:28
epoch [6/200] batch [5/50] time 0.085 (0.333) data 0.000 (0.248) loss 2.0352 (2.4199) acc 28.1250 (28.1250) lr 1.9980e-03 eta 0:54:02
epoch [6/200] batch [10/50] time 0.084 (0.218) data 0.000 (0.133) loss 2.2871 (2.4414) acc 37.5000 (28.1250) lr 1.9980e-03 eta 0:35:18
epoch [6/200] batch [15/50] time 0.085 (0.173) data 0.000 (0.089) loss 2.2773 (2.4577) acc 34.3750 (27.9167) lr 1.9980e-03 eta 0:28:04
epoch [6/200] batch [20/50] time 0.083 (0.156) data 0.000 (0.072) loss 2.3887 (2.4255) acc 34.3750 (28.9062) lr 1.9980e-03 eta 0:25:15
epoch [6/200] batch [25/50] time 0.234 (0.147) data 0.150 (0.063) loss 2.3691 (2.4103) acc 25.0000 (28.6250) lr 1.9980e-03 eta 0:23:53
epoch [6/200] batch [30/50] time 0.084 (0.137) data 0.000 (0.053) loss 2.3301 (2.3934) acc 25.0000 (28.4375) lr 1.9980e-03 eta 0:22:11
epoch [6/200] batch [35/50] time 0.084 (0.134) data 0.000 (0.050) loss 2.0410 (2.3813) acc 40.6250 (28.5714) lr 1.9980e-03 eta 0:21:44
epoch [6/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.044) loss 2.4883 (2.3811) acc 28.1250 (28.8281) lr 1.9980e-03 eta 0:20:42
epoch [6/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.042) loss 2.9219 (2.3997) acc 21.8750 (28.8194) lr 1.9980e-03 eta 0:20:24
epoch [6/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.038) loss 2.1836 (2.4192) acc 31.2500 (28.4375) lr 1.9969e-03 eta 0:19:42
epoch [7/200] batch [5/50] time 0.085 (0.313) data 0.000 (0.228) loss 2.5645 (2.5395) acc 25.0000 (20.6250) lr 1.9969e-03 eta 0:50:35
epoch [7/200] batch [10/50] time 0.174 (0.208) data 0.089 (0.123) loss 2.4199 (2.4943) acc 21.8750 (24.0625) lr 1.9969e-03 eta 0:33:38
epoch [7/200] batch [15/50] time 0.085 (0.167) data 0.000 (0.082) loss 2.4805 (2.4543) acc 28.1250 (25.6250) lr 1.9969e-03 eta 0:27:02
epoch [7/200] batch [20/50] time 0.087 (0.153) data 0.001 (0.067) loss 2.0527 (2.4428) acc 37.5000 (26.5625) lr 1.9969e-03 eta 0:24:39
epoch [7/200] batch [25/50] time 0.086 (0.139) data 0.001 (0.054) loss 2.5566 (2.4398) acc 18.7500 (27.0000) lr 1.9969e-03 eta 0:22:27
epoch [7/200] batch [30/50] time 0.085 (0.134) data 0.000 (0.049) loss 2.1191 (2.4258) acc 21.8750 (27.5000) lr 1.9969e-03 eta 0:21:34
epoch [7/200] batch [35/50] time 0.086 (0.127) data 0.000 (0.042) loss 2.4199 (2.3823) acc 37.5000 (28.5714) lr 1.9969e-03 eta 0:20:27
epoch [7/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.2227 (2.3780) acc 28.1250 (28.2812) lr 1.9969e-03 eta 0:20:15
epoch [7/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.3320 (2.3750) acc 37.5000 (28.5417) lr 1.9969e-03 eta 0:20:09
epoch [7/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.036) loss 2.3262 (2.3701) acc 34.3750 (28.5000) lr 1.9956e-03 eta 0:19:27
epoch [8/200] batch [5/50] time 0.085 (0.314) data 0.000 (0.230) loss 2.5918 (2.1947) acc 18.7500 (31.8750) lr 1.9956e-03 eta 0:50:32
epoch [8/200] batch [10/50] time 0.085 (0.199) data 0.000 (0.115) loss 1.9551 (2.1266) acc 40.6250 (33.4375) lr 1.9956e-03 eta 0:32:02
epoch [8/200] batch [15/50] time 0.085 (0.161) data 0.000 (0.077) loss 2.4219 (2.2507) acc 31.2500 (30.8333) lr 1.9956e-03 eta 0:25:53
epoch [8/200] batch [20/50] time 0.085 (0.142) data 0.000 (0.058) loss 2.6680 (2.3158) acc 34.3750 (30.3125) lr 1.9956e-03 eta 0:22:48
epoch [8/200] batch [25/50] time 0.087 (0.131) data 0.000 (0.046) loss 2.1699 (2.3101) acc 37.5000 (30.2500) lr 1.9956e-03 eta 0:20:57
epoch [8/200] batch [30/50] time 0.084 (0.123) data 0.000 (0.039) loss 2.5469 (2.3402) acc 28.1250 (29.8958) lr 1.9956e-03 eta 0:19:44
epoch [8/200] batch [35/50] time 0.086 (0.118) data 0.001 (0.033) loss 2.0625 (2.3480) acc 43.7500 (30.8036) lr 1.9956e-03 eta 0:18:51
epoch [8/200] batch [40/50] time 0.084 (0.115) data 0.000 (0.031) loss 2.9141 (2.3635) acc 15.6250 (30.3906) lr 1.9956e-03 eta 0:18:28
epoch [8/200] batch [45/50] time 0.083 (0.114) data 0.000 (0.030) loss 2.1562 (2.3785) acc 37.5000 (30.3472) lr 1.9956e-03 eta 0:18:18
epoch [8/200] batch [50/50] time 0.083 (0.111) data 0.000 (0.027) loss 2.2480 (2.3654) acc 31.2500 (30.8125) lr 1.9940e-03 eta 0:17:47
epoch [9/200] batch [5/50] time 0.085 (0.320) data 0.001 (0.236) loss 2.3262 (2.1039) acc 40.6250 (31.2500) lr 1.9940e-03 eta 0:51:12
epoch [9/200] batch [10/50] time 0.159 (0.210) data 0.076 (0.126) loss 2.2246 (2.2371) acc 37.5000 (32.5000) lr 1.9940e-03 eta 0:33:30
epoch [9/200] batch [15/50] time 0.085 (0.168) data 0.000 (0.084) loss 2.3633 (2.3164) acc 21.8750 (31.4583) lr 1.9940e-03 eta 0:26:49
epoch [9/200] batch [20/50] time 0.084 (0.152) data 0.000 (0.068) loss 2.6348 (2.3480) acc 12.5000 (30.3125) lr 1.9940e-03 eta 0:24:14
epoch [9/200] batch [25/50] time 0.084 (0.138) data 0.000 (0.054) loss 2.1289 (2.3139) acc 40.6250 (30.7500) lr 1.9940e-03 eta 0:22:04
epoch [9/200] batch [30/50] time 0.085 (0.129) data 0.000 (0.045) loss 2.0469 (2.2943) acc 31.2500 (31.1458) lr 1.9940e-03 eta 0:20:37
epoch [9/200] batch [35/50] time 0.084 (0.123) data 0.000 (0.039) loss 2.8164 (2.2989) acc 15.6250 (30.8929) lr 1.9940e-03 eta 0:19:35
epoch [9/200] batch [40/50] time 0.084 (0.118) data 0.000 (0.034) loss 2.1953 (2.2901) acc 40.6250 (31.1719) lr 1.9940e-03 eta 0:18:48
epoch [9/200] batch [45/50] time 0.082 (0.114) data 0.000 (0.030) loss 1.9590 (2.2791) acc 37.5000 (31.4583) lr 1.9940e-03 eta 0:18:10
epoch [9/200] batch [50/50] time 0.083 (0.111) data 0.000 (0.027) loss 2.6328 (2.3034) acc 18.7500 (31.0625) lr 1.9921e-03 eta 0:17:40
epoch [10/200] batch [5/50] time 0.083 (0.318) data 0.000 (0.235) loss 2.1875 (2.2979) acc 37.5000 (31.2500) lr 1.9921e-03 eta 0:50:38
epoch [10/200] batch [10/50] time 0.166 (0.209) data 0.083 (0.126) loss 1.8848 (2.2513) acc 40.6250 (33.1250) lr 1.9921e-03 eta 0:33:16
epoch [10/200] batch [15/50] time 0.084 (0.167) data 0.000 (0.084) loss 2.3203 (2.3443) acc 18.7500 (29.1667) lr 1.9921e-03 eta 0:26:35
epoch [10/200] batch [20/50] time 0.083 (0.154) data 0.000 (0.070) loss 3.1211 (2.3644) acc 12.5000 (28.5938) lr 1.9921e-03 eta 0:24:26
epoch [10/200] batch [25/50] time 0.084 (0.140) data 0.000 (0.056) loss 2.1387 (2.3489) acc 37.5000 (28.7500) lr 1.9921e-03 eta 0:22:13
epoch [10/200] batch [30/50] time 0.085 (0.133) data 0.000 (0.049) loss 2.4512 (2.3269) acc 40.6250 (30.0000) lr 1.9921e-03 eta 0:21:03
epoch [10/200] batch [35/50] time 0.084 (0.126) data 0.000 (0.042) loss 2.3184 (2.3232) acc 25.0000 (30.3571) lr 1.9921e-03 eta 0:19:56
epoch [10/200] batch [40/50] time 0.083 (0.122) data 0.000 (0.038) loss 2.7148 (2.3190) acc 15.6250 (30.3125) lr 1.9921e-03 eta 0:19:17
epoch [10/200] batch [45/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.8623 (2.3084) acc 43.7500 (31.1111) lr 1.9921e-03 eta 0:19:09
epoch [10/200] batch [50/50] time 0.085 (0.117) data 0.000 (0.033) loss 1.8750 (2.2940) acc 43.7500 (31.3750) lr 1.9900e-03 eta 0:18:32
epoch [11/200] batch [5/50] time 0.086 (0.318) data 0.000 (0.233) loss 2.0898 (2.2191) acc 34.3750 (31.8750) lr 1.9900e-03 eta 0:50:22
epoch [11/200] batch [10/50] time 0.085 (0.201) data 0.000 (0.117) loss 3.0332 (2.3401) acc 18.7500 (31.2500) lr 1.9900e-03 eta 0:31:52
epoch [11/200] batch [15/50] time 0.084 (0.163) data 0.000 (0.078) loss 1.8867 (2.3046) acc 31.2500 (31.0417) lr 1.9900e-03 eta 0:25:42
epoch [11/200] batch [20/50] time 0.084 (0.143) data 0.000 (0.059) loss 2.3496 (2.2894) acc 25.0000 (31.2500) lr 1.9900e-03 eta 0:22:35
epoch [11/200] batch [25/50] time 0.084 (0.131) data 0.000 (0.047) loss 2.0859 (2.3335) acc 40.6250 (31.5000) lr 1.9900e-03 eta 0:20:43
epoch [11/200] batch [30/50] time 0.084 (0.123) data 0.000 (0.039) loss 2.2480 (2.3288) acc 25.0000 (31.3542) lr 1.9900e-03 eta 0:19:29
epoch [11/200] batch [35/50] time 0.083 (0.121) data 0.000 (0.036) loss 2.1934 (2.3164) acc 43.7500 (31.6964) lr 1.9900e-03 eta 0:19:02
epoch [11/200] batch [40/50] time 0.086 (0.116) data 0.000 (0.032) loss 2.2344 (2.3323) acc 37.5000 (31.0938) lr 1.9900e-03 eta 0:18:17
epoch [11/200] batch [45/50] time 0.084 (0.112) data 0.000 (0.028) loss 2.3633 (2.3138) acc 21.8750 (30.9722) lr 1.9900e-03 eta 0:17:42
epoch [11/200] batch [50/50] time 0.083 (0.109) data 0.000 (0.026) loss 2.1152 (2.2998) acc 37.5000 (31.6875) lr 1.9877e-03 eta 0:17:14
epoch [12/200] batch [5/50] time 0.086 (0.309) data 0.000 (0.224) loss 2.2500 (2.2828) acc 31.2500 (32.5000) lr 1.9877e-03 eta 0:48:37
epoch [12/200] batch [10/50] time 0.183 (0.207) data 0.099 (0.122) loss 2.3184 (2.2370) acc 28.1250 (34.6875) lr 1.9877e-03 eta 0:32:36
epoch [12/200] batch [15/50] time 0.085 (0.167) data 0.001 (0.081) loss 2.4180 (2.2743) acc 31.2500 (33.3333) lr 1.9877e-03 eta 0:26:12
epoch [12/200] batch [20/50] time 0.086 (0.152) data 0.001 (0.067) loss 2.8984 (2.3230) acc 18.7500 (31.2500) lr 1.9877e-03 eta 0:23:53
epoch [12/200] batch [25/50] time 0.084 (0.139) data 0.000 (0.054) loss 2.1602 (2.3126) acc 43.7500 (32.2500) lr 1.9877e-03 eta 0:21:45
epoch [12/200] batch [30/50] time 0.086 (0.134) data 0.000 (0.049) loss 1.9404 (2.2705) acc 40.6250 (33.0208) lr 1.9877e-03 eta 0:20:59
epoch [12/200] batch [35/50] time 0.086 (0.131) data 0.001 (0.046) loss 2.4238 (2.2803) acc 31.2500 (32.6786) lr 1.9877e-03 eta 0:20:30
epoch [12/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.6797 (2.3031) acc 18.7500 (32.4219) lr 1.9877e-03 eta 0:19:35
epoch [12/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.2539 (2.2902) acc 28.1250 (32.3611) lr 1.9877e-03 eta 0:19:17
epoch [12/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.1680 (2.2877) acc 34.3750 (32.5000) lr 1.9851e-03 eta 0:18:39
epoch [13/200] batch [5/50] time 0.084 (0.334) data 0.000 (0.248) loss 2.0391 (2.1506) acc 34.3750 (35.6250) lr 1.9851e-03 eta 0:52:17
epoch [13/200] batch [10/50] time 0.085 (0.220) data 0.000 (0.135) loss 2.0332 (2.2018) acc 37.5000 (32.5000) lr 1.9851e-03 eta 0:34:23
epoch [13/200] batch [15/50] time 0.085 (0.175) data 0.000 (0.090) loss 2.4004 (2.1896) acc 34.3750 (32.9167) lr 1.9851e-03 eta 0:27:23
epoch [13/200] batch [20/50] time 0.086 (0.153) data 0.000 (0.068) loss 2.1641 (2.2458) acc 31.2500 (30.7812) lr 1.9851e-03 eta 0:23:53
epoch [13/200] batch [25/50] time 0.086 (0.139) data 0.000 (0.054) loss 2.1250 (2.2402) acc 34.3750 (31.7500) lr 1.9851e-03 eta 0:21:47
epoch [13/200] batch [30/50] time 0.086 (0.130) data 0.000 (0.045) loss 2.1074 (2.2197) acc 28.1250 (32.2917) lr 1.9851e-03 eta 0:20:22
epoch [13/200] batch [35/50] time 0.086 (0.126) data 0.001 (0.041) loss 2.2969 (2.2184) acc 31.2500 (32.4107) lr 1.9851e-03 eta 0:19:39
epoch [13/200] batch [40/50] time 0.085 (0.121) data 0.000 (0.036) loss 1.8398 (2.2325) acc 50.0000 (32.1094) lr 1.9851e-03 eta 0:18:51
epoch [13/200] batch [45/50] time 0.084 (0.117) data 0.000 (0.032) loss 2.2148 (2.2368) acc 37.5000 (31.5278) lr 1.9851e-03 eta 0:18:12
epoch [13/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.029) loss 2.0527 (2.2434) acc 25.0000 (30.8125) lr 1.9823e-03 eta 0:17:41
epoch [14/200] batch [5/50] time 0.085 (0.320) data 0.000 (0.234) loss 2.2305 (2.1816) acc 34.3750 (35.0000) lr 1.9823e-03 eta 0:49:46
epoch [14/200] batch [10/50] time 0.196 (0.213) data 0.113 (0.129) loss 2.2969 (2.3328) acc 25.0000 (31.5625) lr 1.9823e-03 eta 0:33:11
epoch [14/200] batch [15/50] time 0.084 (0.170) data 0.000 (0.086) loss 2.2676 (2.3077) acc 31.2500 (31.2500) lr 1.9823e-03 eta 0:26:28
epoch [14/200] batch [20/50] time 0.084 (0.155) data 0.000 (0.071) loss 2.1602 (2.3066) acc 31.2500 (31.0938) lr 1.9823e-03 eta 0:24:07
epoch [14/200] batch [25/50] time 0.084 (0.141) data 0.000 (0.057) loss 2.5918 (2.2780) acc 25.0000 (31.7500) lr 1.9823e-03 eta 0:21:53
epoch [14/200] batch [30/50] time 0.084 (0.137) data 0.000 (0.054) loss 2.4121 (2.2871) acc 31.2500 (31.4583) lr 1.9823e-03 eta 0:21:21
epoch [14/200] batch [35/50] time 0.084 (0.137) data 0.000 (0.053) loss 1.7529 (2.2895) acc 43.7500 (30.8036) lr 1.9823e-03 eta 0:21:17
epoch [14/200] batch [40/50] time 0.084 (0.131) data 0.000 (0.047) loss 1.9346 (2.2868) acc 43.7500 (31.0938) lr 1.9823e-03 eta 0:20:15
epoch [14/200] batch [45/50] time 0.083 (0.127) data 0.000 (0.044) loss 2.4121 (2.2772) acc 25.0000 (31.4583) lr 1.9823e-03 eta 0:19:45
epoch [14/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.039) loss 1.7783 (2.2566) acc 31.2500 (31.1875) lr 1.9792e-03 eta 0:19:04
epoch [15/200] batch [5/50] time 0.084 (0.321) data 0.000 (0.237) loss 2.0820 (2.2199) acc 37.5000 (33.7500) lr 1.9792e-03 eta 0:49:42
epoch [15/200] batch [10/50] time 0.141 (0.218) data 0.058 (0.134) loss 2.0586 (2.1437) acc 40.6250 (35.6250) lr 1.9792e-03 eta 0:33:41
epoch [15/200] batch [15/50] time 0.085 (0.173) data 0.000 (0.089) loss 2.1953 (2.1320) acc 28.1250 (35.2083) lr 1.9792e-03 eta 0:26:46
epoch [15/200] batch [20/50] time 0.083 (0.161) data 0.000 (0.078) loss 2.2578 (2.1793) acc 34.3750 (33.7500) lr 1.9792e-03 eta 0:24:57
epoch [15/200] batch [25/50] time 0.084 (0.146) data 0.000 (0.062) loss 2.3340 (2.2054) acc 34.3750 (33.1250) lr 1.9792e-03 eta 0:22:33
epoch [15/200] batch [30/50] time 0.085 (0.142) data 0.000 (0.058) loss 2.3301 (2.1942) acc 31.2500 (34.0625) lr 1.9792e-03 eta 0:21:54
epoch [15/200] batch [35/50] time 0.176 (0.136) data 0.093 (0.053) loss 2.5664 (2.2353) acc 37.5000 (33.5714) lr 1.9792e-03 eta 0:21:01
epoch [15/200] batch [40/50] time 0.083 (0.130) data 0.000 (0.046) loss 2.3711 (2.2518) acc 31.2500 (33.1250) lr 1.9792e-03 eta 0:19:59
epoch [15/200] batch [45/50] time 0.083 (0.127) data 0.000 (0.044) loss 2.4277 (2.2591) acc 28.1250 (32.5694) lr 1.9792e-03 eta 0:19:39
epoch [15/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.040) loss 2.0664 (2.2563) acc 31.2500 (32.7500) lr 1.9759e-03 eta 0:18:58
epoch [16/200] batch [5/50] time 0.085 (0.359) data 0.000 (0.273) loss 1.9219 (2.1059) acc 43.7500 (38.7500) lr 1.9759e-03 eta 0:55:14
epoch [16/200] batch [10/50] time 0.099 (0.233) data 0.013 (0.148) loss 2.1621 (2.1766) acc 31.2500 (38.7500) lr 1.9759e-03 eta 0:35:49
epoch [16/200] batch [15/50] time 0.085 (0.184) data 0.000 (0.099) loss 2.0938 (2.2180) acc 43.7500 (36.4583) lr 1.9759e-03 eta 0:28:14
epoch [16/200] batch [20/50] time 0.085 (0.165) data 0.000 (0.080) loss 2.4785 (2.2573) acc 21.8750 (34.8438) lr 1.9759e-03 eta 0:25:19
epoch [16/200] batch [25/50] time 0.246 (0.155) data 0.161 (0.070) loss 2.3770 (2.2442) acc 31.2500 (34.8750) lr 1.9759e-03 eta 0:23:51
epoch [16/200] batch [30/50] time 0.085 (0.145) data 0.000 (0.060) loss 2.0938 (2.2191) acc 31.2500 (34.5833) lr 1.9759e-03 eta 0:22:16
epoch [16/200] batch [35/50] time 0.084 (0.142) data 0.000 (0.057) loss 2.1836 (2.2179) acc 37.5000 (35.0000) lr 1.9759e-03 eta 0:21:48
epoch [16/200] batch [40/50] time 0.084 (0.135) data 0.000 (0.050) loss 2.1719 (2.2101) acc 25.0000 (34.8438) lr 1.9759e-03 eta 0:20:40
epoch [16/200] batch [45/50] time 0.083 (0.132) data 0.000 (0.048) loss 2.0410 (2.2167) acc 37.5000 (34.2361) lr 1.9759e-03 eta 0:20:17
epoch [16/200] batch [50/50] time 0.201 (0.130) data 0.118 (0.046) loss 2.3262 (2.2166) acc 18.7500 (33.4375) lr 1.9724e-03 eta 0:19:53
epoch [17/200] batch [5/50] time 0.085 (0.308) data 0.000 (0.224) loss 2.4727 (2.2150) acc 34.3750 (36.8750) lr 1.9724e-03 eta 0:47:14
epoch [17/200] batch [10/50] time 0.085 (0.197) data 0.000 (0.112) loss 2.1074 (2.1336) acc 28.1250 (37.1875) lr 1.9724e-03 eta 0:30:08
epoch [17/200] batch [15/50] time 0.084 (0.170) data 0.000 (0.085) loss 2.0078 (2.1470) acc 46.8750 (37.2917) lr 1.9724e-03 eta 0:25:57
epoch [17/200] batch [20/50] time 0.085 (0.154) data 0.000 (0.069) loss 2.1973 (2.2356) acc 25.0000 (33.7500) lr 1.9724e-03 eta 0:23:34
epoch [17/200] batch [25/50] time 0.086 (0.140) data 0.000 (0.056) loss 2.2676 (2.2337) acc 40.6250 (34.6250) lr 1.9724e-03 eta 0:21:26
epoch [17/200] batch [30/50] time 0.084 (0.137) data 0.000 (0.052) loss 2.4512 (2.2327) acc 25.0000 (34.5833) lr 1.9724e-03 eta 0:20:54
epoch [17/200] batch [35/50] time 0.301 (0.136) data 0.218 (0.051) loss 2.0820 (2.2218) acc 34.3750 (34.2857) lr 1.9724e-03 eta 0:20:42
epoch [17/200] batch [40/50] time 0.083 (0.129) data 0.000 (0.045) loss 2.0156 (2.2091) acc 34.3750 (34.5312) lr 1.9724e-03 eta 0:19:41
epoch [17/200] batch [45/50] time 0.082 (0.127) data 0.000 (0.043) loss 2.1836 (2.1925) acc 31.2500 (34.6528) lr 1.9724e-03 eta 0:19:20
epoch [17/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.038) loss 2.4707 (2.2028) acc 31.2500 (34.7500) lr 1.9686e-03 eta 0:18:39
epoch [18/200] batch [5/50] time 0.085 (0.290) data 0.000 (0.205) loss 1.8496 (2.2098) acc 43.7500 (37.5000) lr 1.9686e-03 eta 0:44:10
epoch [18/200] batch [10/50] time 0.085 (0.187) data 0.000 (0.103) loss 2.5645 (2.1834) acc 34.3750 (36.5625) lr 1.9686e-03 eta 0:28:32
epoch [18/200] batch [15/50] time 0.087 (0.154) data 0.000 (0.069) loss 2.0039 (2.1750) acc 43.7500 (36.2500) lr 1.9686e-03 eta 0:23:22
epoch [18/200] batch [20/50] time 0.086 (0.144) data 0.000 (0.059) loss 2.2129 (2.1911) acc 31.2500 (35.9375) lr 1.9686e-03 eta 0:21:53
epoch [18/200] batch [25/50] time 0.086 (0.132) data 0.001 (0.047) loss 1.7822 (2.1779) acc 53.1250 (36.2500) lr 1.9686e-03 eta 0:20:06
epoch [18/200] batch [30/50] time 0.086 (0.126) data 0.000 (0.041) loss 2.6406 (2.1880) acc 25.0000 (36.4583) lr 1.9686e-03 eta 0:19:10
epoch [18/200] batch [35/50] time 0.086 (0.122) data 0.000 (0.037) loss 1.9668 (2.1719) acc 37.5000 (36.6964) lr 1.9686e-03 eta 0:18:29
epoch [18/200] batch [40/50] time 0.086 (0.118) data 0.001 (0.033) loss 2.1992 (2.1903) acc 37.5000 (35.7031) lr 1.9686e-03 eta 0:17:54
epoch [18/200] batch [45/50] time 0.084 (0.114) data 0.000 (0.029) loss 2.4238 (2.1889) acc 28.1250 (35.9028) lr 1.9686e-03 eta 0:17:19
epoch [18/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.029) loss 2.2910 (2.2042) acc 31.2500 (35.3125) lr 1.9646e-03 eta 0:17:19
epoch [19/200] batch [5/50] time 0.086 (0.288) data 0.000 (0.202) loss 2.5117 (2.3965) acc 34.3750 (30.0000) lr 1.9646e-03 eta 0:43:36
epoch [19/200] batch [10/50] time 0.084 (0.186) data 0.000 (0.101) loss 2.3184 (2.3473) acc 28.1250 (30.6250) lr 1.9646e-03 eta 0:28:14
epoch [19/200] batch [15/50] time 0.085 (0.163) data 0.000 (0.078) loss 2.2324 (2.3256) acc 37.5000 (30.8333) lr 1.9646e-03 eta 0:24:40
epoch [19/200] batch [20/50] time 0.257 (0.152) data 0.173 (0.067) loss 2.5430 (2.3029) acc 21.8750 (31.8750) lr 1.9646e-03 eta 0:23:01
epoch [19/200] batch [25/50] time 0.086 (0.139) data 0.000 (0.054) loss 2.5859 (2.3243) acc 21.8750 (30.8750) lr 1.9646e-03 eta 0:21:00
epoch [19/200] batch [30/50] time 0.085 (0.134) data 0.000 (0.049) loss 1.9707 (2.3112) acc 34.3750 (31.5625) lr 1.9646e-03 eta 0:20:19
epoch [19/200] batch [35/50] time 0.085 (0.128) data 0.000 (0.042) loss 1.9307 (2.2900) acc 40.6250 (31.9643) lr 1.9646e-03 eta 0:19:16
epoch [19/200] batch [40/50] time 0.084 (0.125) data 0.000 (0.040) loss 2.3516 (2.2730) acc 34.3750 (32.1875) lr 1.9646e-03 eta 0:18:49
epoch [19/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.038) loss 2.9316 (2.2588) acc 28.1250 (32.7778) lr 1.9646e-03 eta 0:18:35
epoch [19/200] batch [50/50] time 0.084 (0.119) data 0.000 (0.035) loss 2.0723 (2.2414) acc 34.3750 (33.3750) lr 1.9603e-03 eta 0:17:58
epoch [20/200] batch [5/50] time 0.086 (0.311) data 0.000 (0.226) loss 2.7305 (2.3617) acc 28.1250 (30.6250) lr 1.9603e-03 eta 0:46:51
epoch [20/200] batch [10/50] time 0.084 (0.203) data 0.000 (0.119) loss 2.1348 (2.3674) acc 43.7500 (31.5625) lr 1.9603e-03 eta 0:30:35
epoch [20/200] batch [15/50] time 0.087 (0.164) data 0.001 (0.079) loss 1.9707 (2.3083) acc 37.5000 (33.1250) lr 1.9603e-03 eta 0:24:39
epoch [20/200] batch [20/50] time 0.085 (0.144) data 0.000 (0.060) loss 2.0137 (2.2987) acc 37.5000 (33.2812) lr 1.9603e-03 eta 0:21:40
epoch [20/200] batch [25/50] time 0.085 (0.132) data 0.000 (0.048) loss 2.0430 (2.3095) acc 43.7500 (32.6250) lr 1.9603e-03 eta 0:19:53
epoch [20/200] batch [30/50] time 0.084 (0.125) data 0.000 (0.040) loss 2.4141 (2.2721) acc 28.1250 (33.7500) lr 1.9603e-03 eta 0:18:44
epoch [20/200] batch [35/50] time 0.085 (0.119) data 0.001 (0.035) loss 2.4043 (2.2508) acc 25.0000 (34.4643) lr 1.9603e-03 eta 0:17:53
epoch [20/200] batch [40/50] time 0.083 (0.118) data 0.000 (0.033) loss 2.0742 (2.2539) acc 25.0000 (34.4531) lr 1.9603e-03 eta 0:17:42
epoch [20/200] batch [45/50] time 0.145 (0.115) data 0.062 (0.031) loss 2.2109 (2.2527) acc 40.6250 (34.5139) lr 1.9603e-03 eta 0:17:19
epoch [20/200] batch [50/50] time 0.084 (0.113) data 0.000 (0.029) loss 1.8770 (2.2443) acc 43.7500 (34.9375) lr 1.9558e-03 eta 0:16:56
epoch [21/200] batch [5/50] time 0.084 (0.337) data 0.000 (0.253) loss 2.7305 (2.0939) acc 28.1250 (38.7500) lr 1.9558e-03 eta 0:50:28
epoch [21/200] batch [10/50] time 0.096 (0.212) data 0.012 (0.128) loss 2.2246 (2.1020) acc 34.3750 (38.1250) lr 1.9558e-03 eta 0:31:43
epoch [21/200] batch [15/50] time 0.086 (0.170) data 0.000 (0.085) loss 2.0801 (2.1445) acc 37.5000 (36.8750) lr 1.9558e-03 eta 0:25:23
epoch [21/200] batch [20/50] time 0.084 (0.149) data 0.000 (0.064) loss 2.3496 (2.1743) acc 28.1250 (35.4688) lr 1.9558e-03 eta 0:22:14
epoch [21/200] batch [25/50] time 0.087 (0.136) data 0.000 (0.051) loss 2.0547 (2.1932) acc 34.3750 (34.1250) lr 1.9558e-03 eta 0:20:22
epoch [21/200] batch [30/50] time 0.087 (0.128) data 0.000 (0.043) loss 2.5508 (2.2432) acc 28.1250 (33.1250) lr 1.9558e-03 eta 0:19:06
epoch [21/200] batch [35/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.3750 (2.2272) acc 31.2500 (34.3750) lr 1.9558e-03 eta 0:18:45
epoch [21/200] batch [40/50] time 0.083 (0.120) data 0.000 (0.036) loss 2.3770 (2.2274) acc 15.6250 (33.8281) lr 1.9558e-03 eta 0:17:58
epoch [21/200] batch [45/50] time 0.083 (0.118) data 0.000 (0.033) loss 2.2656 (2.2324) acc 34.3750 (33.4028) lr 1.9558e-03 eta 0:17:35
epoch [21/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.030) loss 2.3887 (2.2487) acc 34.3750 (33.6875) lr 1.9511e-03 eta 0:17:04
epoch [22/200] batch [5/50] time 0.086 (0.310) data 0.000 (0.224) loss 2.1016 (2.2633) acc 40.6250 (36.8750) lr 1.9511e-03 eta 0:46:15
epoch [22/200] batch [10/50] time 0.176 (0.207) data 0.092 (0.121) loss 2.2188 (2.2117) acc 37.5000 (37.8125) lr 1.9511e-03 eta 0:30:52
epoch [22/200] batch [15/50] time 0.086 (0.167) data 0.000 (0.081) loss 1.5732 (2.2632) acc 59.3750 (37.7083) lr 1.9511e-03 eta 0:24:49
epoch [22/200] batch [20/50] time 0.084 (0.156) data 0.000 (0.071) loss 2.1992 (2.2329) acc 37.5000 (38.1250) lr 1.9511e-03 eta 0:23:14
epoch [22/200] batch [25/50] time 0.083 (0.142) data 0.000 (0.057) loss 1.8262 (2.2011) acc 40.6250 (38.3750) lr 1.9511e-03 eta 0:21:05
epoch [22/200] batch [30/50] time 0.084 (0.140) data 0.000 (0.055) loss 1.9072 (2.1849) acc 53.1250 (38.4375) lr 1.9511e-03 eta 0:20:45
epoch [22/200] batch [35/50] time 0.086 (0.132) data 0.000 (0.047) loss 2.6699 (2.2122) acc 18.7500 (37.5000) lr 1.9511e-03 eta 0:19:36
epoch [22/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.043) loss 2.4766 (2.2263) acc 31.2500 (37.0312) lr 1.9511e-03 eta 0:18:57
epoch [22/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.8789 (2.2418) acc 15.6250 (36.1806) lr 1.9511e-03 eta 0:18:35
epoch [22/200] batch [50/50] time 0.085 (0.121) data 0.000 (0.036) loss 3.0293 (2.2541) acc 28.1250 (35.6250) lr 1.9461e-03 eta 0:17:57
epoch [23/200] batch [5/50] time 0.085 (0.305) data 0.000 (0.220) loss 2.0039 (1.8990) acc 37.5000 (43.7500) lr 1.9461e-03 eta 0:45:14
epoch [23/200] batch [10/50] time 0.084 (0.202) data 0.000 (0.117) loss 1.5801 (2.0200) acc 56.2500 (41.8750) lr 1.9461e-03 eta 0:29:54
epoch [23/200] batch [15/50] time 0.085 (0.163) data 0.000 (0.078) loss 1.9639 (2.0408) acc 37.5000 (40.6250) lr 1.9461e-03 eta 0:24:06
epoch [23/200] batch [20/50] time 0.086 (0.144) data 0.000 (0.060) loss 2.5410 (2.1178) acc 40.6250 (39.3750) lr 1.9461e-03 eta 0:21:20
epoch [23/200] batch [25/50] time 0.085 (0.132) data 0.000 (0.048) loss 2.2188 (2.1535) acc 40.6250 (38.5000) lr 1.9461e-03 eta 0:19:33
epoch [23/200] batch [30/50] time 0.085 (0.125) data 0.000 (0.040) loss 2.6855 (2.1733) acc 28.1250 (37.9167) lr 1.9461e-03 eta 0:18:26
epoch [23/200] batch [35/50] time 0.084 (0.119) data 0.000 (0.035) loss 1.9277 (2.1603) acc 40.6250 (38.2143) lr 1.9461e-03 eta 0:17:34
epoch [23/200] batch [40/50] time 0.084 (0.118) data 0.000 (0.033) loss 2.2988 (2.1666) acc 28.1250 (37.3438) lr 1.9461e-03 eta 0:17:24
epoch [23/200] batch [45/50] time 0.171 (0.116) data 0.087 (0.032) loss 2.3027 (2.1769) acc 40.6250 (37.2222) lr 1.9461e-03 eta 0:17:06
epoch [23/200] batch [50/50] time 0.083 (0.113) data 0.000 (0.029) loss 1.7852 (2.1726) acc 40.6250 (36.9375) lr 1.9409e-03 eta 0:16:37
epoch [24/200] batch [5/50] time 0.084 (0.288) data 0.000 (0.203) loss 2.0762 (2.1754) acc 37.5000 (31.2500) lr 1.9409e-03 eta 0:42:29
epoch [24/200] batch [10/50] time 0.085 (0.187) data 0.000 (0.102) loss 2.4551 (2.2821) acc 37.5000 (32.1875) lr 1.9409e-03 eta 0:27:30
epoch [24/200] batch [15/50] time 0.084 (0.164) data 0.000 (0.079) loss 1.9775 (2.2451) acc 40.6250 (33.7500) lr 1.9409e-03 eta 0:24:06
epoch [24/200] batch [20/50] time 0.085 (0.151) data 0.000 (0.066) loss 2.2930 (2.2891) acc 40.6250 (33.5938) lr 1.9409e-03 eta 0:22:14
epoch [24/200] batch [25/50] time 0.085 (0.138) data 0.000 (0.053) loss 1.9590 (2.2462) acc 37.5000 (34.1250) lr 1.9409e-03 eta 0:20:16
epoch [24/200] batch [30/50] time 0.085 (0.136) data 0.000 (0.052) loss 2.4512 (2.2814) acc 37.5000 (33.7500) lr 1.9409e-03 eta 0:20:02
epoch [24/200] batch [35/50] time 0.344 (0.136) data 0.261 (0.052) loss 2.3984 (2.2506) acc 28.1250 (34.6429) lr 1.9409e-03 eta 0:20:02
epoch [24/200] batch [40/50] time 0.082 (0.130) data 0.000 (0.045) loss 2.1621 (2.2408) acc 31.2500 (34.7656) lr 1.9409e-03 eta 0:19:03
epoch [24/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 2.5156 (2.2545) acc 21.8750 (34.3056) lr 1.9409e-03 eta 0:18:22
epoch [24/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 2.2109 (2.2436) acc 37.5000 (34.6250) lr 1.9354e-03 eta 0:17:44
epoch [25/200] batch [5/50] time 0.086 (0.324) data 0.000 (0.238) loss 2.2891 (2.2209) acc 34.3750 (33.7500) lr 1.9354e-03 eta 0:47:30
epoch [25/200] batch [10/50] time 0.086 (0.205) data 0.000 (0.119) loss 2.3105 (2.2089) acc 25.0000 (32.8125) lr 1.9354e-03 eta 0:29:59
epoch [25/200] batch [15/50] time 0.086 (0.165) data 0.000 (0.080) loss 1.9775 (2.1553) acc 37.5000 (34.5833) lr 1.9354e-03 eta 0:24:10
epoch [25/200] batch [20/50] time 0.086 (0.145) data 0.000 (0.060) loss 2.2891 (2.1628) acc 28.1250 (34.0625) lr 1.9354e-03 eta 0:21:16
epoch [25/200] batch [25/50] time 0.086 (0.134) data 0.000 (0.048) loss 1.9121 (2.1741) acc 37.5000 (33.5000) lr 1.9354e-03 eta 0:19:32
epoch [25/200] batch [30/50] time 0.087 (0.129) data 0.000 (0.043) loss 1.9883 (2.1545) acc 40.6250 (33.8542) lr 1.9354e-03 eta 0:18:48
epoch [25/200] batch [35/50] time 0.087 (0.123) data 0.000 (0.037) loss 2.1074 (2.1600) acc 40.6250 (34.4643) lr 1.9354e-03 eta 0:17:53
epoch [25/200] batch [40/50] time 0.085 (0.120) data 0.000 (0.035) loss 2.5293 (2.1604) acc 15.6250 (34.3750) lr 1.9354e-03 eta 0:17:33
epoch [25/200] batch [45/50] time 0.083 (0.120) data 0.000 (0.034) loss 2.2090 (2.1638) acc 43.7500 (34.5833) lr 1.9354e-03 eta 0:17:27
epoch [25/200] batch [50/50] time 0.085 (0.116) data 0.000 (0.031) loss 1.8770 (2.1621) acc 37.5000 (34.4375) lr 1.9298e-03 eta 0:16:55
epoch [26/200] batch [5/50] time 0.085 (0.285) data 0.000 (0.200) loss 2.1133 (2.1605) acc 34.3750 (40.0000) lr 1.9298e-03 eta 0:41:30
epoch [26/200] batch [10/50] time 0.085 (0.185) data 0.000 (0.100) loss 2.0234 (2.1199) acc 46.8750 (40.0000) lr 1.9298e-03 eta 0:26:55
epoch [26/200] batch [15/50] time 0.086 (0.153) data 0.000 (0.068) loss 2.1582 (2.1585) acc 31.2500 (39.1667) lr 1.9298e-03 eta 0:22:16
epoch [26/200] batch [20/50] time 0.085 (0.136) data 0.000 (0.051) loss 2.0703 (2.1156) acc 40.6250 (40.1562) lr 1.9298e-03 eta 0:19:47
epoch [26/200] batch [25/50] time 0.085 (0.130) data 0.000 (0.045) loss 2.0332 (2.1163) acc 40.6250 (39.7500) lr 1.9298e-03 eta 0:18:52
epoch [26/200] batch [30/50] time 0.085 (0.126) data 0.000 (0.042) loss 1.6230 (2.1479) acc 50.0000 (38.1250) lr 1.9298e-03 eta 0:18:21
epoch [26/200] batch [35/50] time 0.085 (0.120) data 0.000 (0.036) loss 2.4023 (2.1540) acc 25.0000 (37.6786) lr 1.9298e-03 eta 0:17:29
epoch [26/200] batch [40/50] time 0.083 (0.121) data 0.000 (0.037) loss 2.0957 (2.1277) acc 34.3750 (38.2812) lr 1.9298e-03 eta 0:17:35
epoch [26/200] batch [45/50] time 0.196 (0.119) data 0.112 (0.035) loss 2.0723 (2.1500) acc 46.8750 (37.7778) lr 1.9298e-03 eta 0:17:19
epoch [26/200] batch [50/50] time 0.084 (0.116) data 0.000 (0.031) loss 1.9824 (2.1751) acc 50.0000 (37.5000) lr 1.9239e-03 eta 0:16:47
epoch [27/200] batch [5/50] time 0.087 (0.306) data 0.000 (0.221) loss 2.6836 (2.2408) acc 25.0000 (30.0000) lr 1.9239e-03 eta 0:44:24
epoch [27/200] batch [10/50] time 0.086 (0.196) data 0.000 (0.111) loss 2.0586 (2.2468) acc 34.3750 (30.6250) lr 1.9239e-03 eta 0:28:24
epoch [27/200] batch [15/50] time 0.085 (0.163) data 0.001 (0.077) loss 2.2305 (2.2508) acc 37.5000 (32.7083) lr 1.9239e-03 eta 0:23:34
epoch [27/200] batch [20/50] time 0.086 (0.152) data 0.000 (0.067) loss 1.9258 (2.2120) acc 46.8750 (34.0625) lr 1.9239e-03 eta 0:22:01
epoch [27/200] batch [25/50] time 0.084 (0.139) data 0.000 (0.054) loss 1.9502 (2.1909) acc 34.3750 (34.2500) lr 1.9239e-03 eta 0:20:04
epoch [27/200] batch [30/50] time 0.085 (0.132) data 0.000 (0.047) loss 2.1680 (2.1836) acc 40.6250 (35.0000) lr 1.9239e-03 eta 0:19:02
epoch [27/200] batch [35/50] time 0.237 (0.130) data 0.153 (0.044) loss 1.6797 (2.1518) acc 46.8750 (36.1607) lr 1.9239e-03 eta 0:18:42
epoch [27/200] batch [40/50] time 0.085 (0.124) data 0.000 (0.039) loss 1.8037 (2.1390) acc 40.6250 (36.9531) lr 1.9239e-03 eta 0:17:53
epoch [27/200] batch [45/50] time 0.083 (0.122) data 0.001 (0.037) loss 1.9863 (2.1532) acc 40.6250 (36.7361) lr 1.9239e-03 eta 0:17:34
epoch [27/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.033) loss 2.5195 (2.1929) acc 28.1250 (35.8125) lr 1.9178e-03 eta 0:17:00
epoch [28/200] batch [5/50] time 0.087 (0.311) data 0.000 (0.225) loss 1.8398 (2.1584) acc 40.6250 (38.7500) lr 1.9178e-03 eta 0:44:47
epoch [28/200] batch [10/50] time 0.085 (0.205) data 0.001 (0.119) loss 2.0059 (2.1397) acc 37.5000 (38.4375) lr 1.9178e-03 eta 0:29:31
epoch [28/200] batch [15/50] time 0.085 (0.165) data 0.000 (0.080) loss 1.9814 (2.1672) acc 37.5000 (36.4583) lr 1.9178e-03 eta 0:23:46
epoch [28/200] batch [20/50] time 0.085 (0.152) data 0.000 (0.066) loss 2.3418 (2.1715) acc 31.2500 (37.5000) lr 1.9178e-03 eta 0:21:48
epoch [28/200] batch [25/50] time 0.247 (0.145) data 0.164 (0.060) loss 2.2637 (2.1259) acc 28.1250 (38.3750) lr 1.9178e-03 eta 0:20:49
epoch [28/200] batch [30/50] time 0.084 (0.135) data 0.000 (0.050) loss 1.6992 (2.1200) acc 56.2500 (38.4375) lr 1.9178e-03 eta 0:19:23
epoch [28/200] batch [35/50] time 0.086 (0.132) data 0.001 (0.047) loss 1.9531 (2.1100) acc 37.5000 (39.0179) lr 1.9178e-03 eta 0:18:56
epoch [28/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.041) loss 1.9609 (2.1082) acc 37.5000 (38.5156) lr 1.9178e-03 eta 0:18:04
epoch [28/200] batch [45/50] time 0.084 (0.124) data 0.000 (0.039) loss 2.4824 (2.1245) acc 25.0000 (38.4028) lr 1.9178e-03 eta 0:17:46
epoch [28/200] batch [50/50] time 0.084 (0.120) data 0.000 (0.035) loss 1.7324 (2.1266) acc 50.0000 (38.5625) lr 1.9114e-03 eta 0:17:11
epoch [29/200] batch [5/50] time 0.084 (0.312) data 0.000 (0.227) loss 1.8662 (2.0391) acc 34.3750 (36.8750) lr 1.9114e-03 eta 0:44:39
epoch [29/200] batch [10/50] time 0.084 (0.198) data 0.000 (0.114) loss 2.1582 (2.0440) acc 31.2500 (37.5000) lr 1.9114e-03 eta 0:28:21
epoch [29/200] batch [15/50] time 0.085 (0.160) data 0.000 (0.076) loss 1.4502 (2.0123) acc 62.5000 (38.9583) lr 1.9114e-03 eta 0:22:54
epoch [29/200] batch [20/50] time 0.193 (0.147) data 0.110 (0.063) loss 2.1465 (2.0546) acc 37.5000 (39.2188) lr 1.9114e-03 eta 0:20:57
epoch [29/200] batch [25/50] time 0.085 (0.134) data 0.000 (0.050) loss 2.6543 (2.0831) acc 34.3750 (39.2500) lr 1.9114e-03 eta 0:19:10
epoch [29/200] batch [30/50] time 0.084 (0.130) data 0.000 (0.046) loss 1.7002 (2.0893) acc 46.8750 (39.0625) lr 1.9114e-03 eta 0:18:33
epoch [29/200] batch [35/50] time 0.085 (0.123) data 0.000 (0.039) loss 2.4863 (2.1173) acc 40.6250 (38.4821) lr 1.9114e-03 eta 0:17:37
epoch [29/200] batch [40/50] time 0.084 (0.121) data 0.000 (0.037) loss 1.8887 (2.1254) acc 46.8750 (37.8125) lr 1.9114e-03 eta 0:17:18
epoch [29/200] batch [45/50] time 0.105 (0.118) data 0.022 (0.034) loss 1.7803 (2.1274) acc 43.7500 (37.6389) lr 1.9114e-03 eta 0:16:46
epoch [29/200] batch [50/50] time 0.084 (0.114) data 0.000 (0.030) loss 2.5605 (2.1387) acc 18.7500 (37.0000) lr 1.9048e-03 eta 0:16:16
epoch [30/200] batch [5/50] time 0.084 (0.332) data 0.000 (0.248) loss 2.7852 (1.9771) acc 28.1250 (44.3750) lr 1.9048e-03 eta 0:47:18
epoch [30/200] batch [10/50] time 0.207 (0.221) data 0.124 (0.136) loss 1.8652 (2.0515) acc 56.2500 (41.5625) lr 1.9048e-03 eta 0:31:24
epoch [30/200] batch [15/50] time 0.085 (0.175) data 0.000 (0.091) loss 2.2324 (2.1020) acc 37.5000 (41.0417) lr 1.9048e-03 eta 0:24:56
epoch [30/200] batch [20/50] time 0.086 (0.163) data 0.000 (0.079) loss 2.3887 (2.1314) acc 25.0000 (38.2812) lr 1.9048e-03 eta 0:23:11
epoch [30/200] batch [25/50] time 0.086 (0.147) data 0.000 (0.063) loss 2.4551 (2.1640) acc 34.3750 (36.6250) lr 1.9048e-03 eta 0:20:56
epoch [30/200] batch [30/50] time 0.085 (0.141) data 0.000 (0.056) loss 2.3320 (2.1775) acc 31.2500 (35.8333) lr 1.9048e-03 eta 0:19:58
epoch [30/200] batch [35/50] time 0.084 (0.137) data 0.000 (0.052) loss 2.2090 (2.1681) acc 40.6250 (36.6071) lr 1.9048e-03 eta 0:19:23
epoch [30/200] batch [40/50] time 0.083 (0.130) data 0.000 (0.046) loss 2.1133 (2.1722) acc 46.8750 (36.4844) lr 1.9048e-03 eta 0:18:26
epoch [30/200] batch [45/50] time 0.084 (0.128) data 0.000 (0.044) loss 1.8379 (2.1410) acc 50.0000 (37.6389) lr 1.9048e-03 eta 0:18:07
epoch [30/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.4199 (2.1567) acc 25.0000 (36.8125) lr 1.8980e-03 eta 0:17:28
epoch [31/200] batch [5/50] time 0.086 (0.296) data 0.000 (0.210) loss 2.1836 (2.2176) acc 43.7500 (36.2500) lr 1.8980e-03 eta 0:41:51
epoch [31/200] batch [10/50] time 0.087 (0.191) data 0.000 (0.105) loss 1.9258 (2.1890) acc 34.3750 (35.6250) lr 1.8980e-03 eta 0:27:00
epoch [31/200] batch [15/50] time 0.085 (0.156) data 0.000 (0.071) loss 2.2168 (2.1744) acc 31.2500 (35.6250) lr 1.8980e-03 eta 0:22:05
epoch [31/200] batch [20/50] time 0.084 (0.145) data 0.000 (0.059) loss 2.2949 (2.1817) acc 31.2500 (35.1562) lr 1.8980e-03 eta 0:20:27
epoch [31/200] batch [25/50] time 0.084 (0.133) data 0.000 (0.048) loss 2.1465 (2.1721) acc 43.7500 (35.1250) lr 1.8980e-03 eta 0:18:43
epoch [31/200] batch [30/50] time 0.084 (0.131) data 0.000 (0.046) loss 1.8311 (2.1634) acc 40.6250 (34.6875) lr 1.8980e-03 eta 0:18:31
epoch [31/200] batch [35/50] time 0.284 (0.130) data 0.201 (0.046) loss 1.8076 (2.1409) acc 43.7500 (35.4464) lr 1.8980e-03 eta 0:18:24
epoch [31/200] batch [40/50] time 0.084 (0.125) data 0.000 (0.040) loss 2.3438 (2.1358) acc 40.6250 (35.3906) lr 1.8980e-03 eta 0:17:34
epoch [31/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.1758 (2.1497) acc 25.0000 (35.2083) lr 1.8980e-03 eta 0:17:35
epoch [31/200] batch [50/50] time 0.084 (0.121) data 0.000 (0.036) loss 2.3145 (2.1457) acc 37.5000 (35.8125) lr 1.8910e-03 eta 0:17:00
epoch [32/200] batch [5/50] time 0.088 (0.315) data 0.001 (0.228) loss 1.7598 (1.9828) acc 40.6250 (39.3750) lr 1.8910e-03 eta 0:44:16
epoch [32/200] batch [10/50] time 0.085 (0.203) data 0.000 (0.117) loss 1.7656 (1.9467) acc 50.0000 (40.9375) lr 1.8910e-03 eta 0:28:33
epoch [32/200] batch [15/50] time 0.085 (0.169) data 0.000 (0.083) loss 2.4746 (2.0186) acc 31.2500 (40.4167) lr 1.8910e-03 eta 0:23:45
epoch [32/200] batch [20/50] time 0.084 (0.151) data 0.000 (0.066) loss 1.7822 (2.0862) acc 34.3750 (37.5000) lr 1.8910e-03 eta 0:21:15
epoch [32/200] batch [25/50] time 0.085 (0.138) data 0.000 (0.053) loss 2.6074 (2.1332) acc 28.1250 (37.5000) lr 1.8910e-03 eta 0:19:22
epoch [32/200] batch [30/50] time 0.085 (0.129) data 0.000 (0.044) loss 2.0215 (2.1648) acc 56.2500 (37.6042) lr 1.8910e-03 eta 0:18:06
epoch [32/200] batch [35/50] time 0.085 (0.123) data 0.000 (0.038) loss 2.0020 (2.1706) acc 43.7500 (37.9464) lr 1.8910e-03 eta 0:17:12
epoch [32/200] batch [40/50] time 0.083 (0.118) data 0.000 (0.033) loss 1.7490 (2.1743) acc 50.0000 (38.4375) lr 1.8910e-03 eta 0:16:31
epoch [32/200] batch [45/50] time 0.085 (0.114) data 0.000 (0.030) loss 2.2207 (2.1643) acc 34.3750 (38.2639) lr 1.8910e-03 eta 0:15:58
epoch [32/200] batch [50/50] time 0.084 (0.111) data 0.000 (0.027) loss 2.3105 (2.1500) acc 31.2500 (38.4375) lr 1.8838e-03 eta 0:15:32
epoch [33/200] batch [5/50] time 0.084 (0.348) data 0.000 (0.264) loss 2.3418 (2.0736) acc 25.0000 (35.6250) lr 1.8838e-03 eta 0:48:43
epoch [33/200] batch [10/50] time 0.085 (0.219) data 0.000 (0.134) loss 1.9668 (2.0196) acc 43.7500 (39.6875) lr 1.8838e-03 eta 0:30:34
epoch [33/200] batch [15/50] time 0.085 (0.174) data 0.000 (0.090) loss 2.2461 (2.0422) acc 40.6250 (40.2083) lr 1.8838e-03 eta 0:24:19
epoch [33/200] batch [20/50] time 0.085 (0.154) data 0.000 (0.070) loss 2.3203 (2.1228) acc 43.7500 (38.2812) lr 1.8838e-03 eta 0:21:31
epoch [33/200] batch [25/50] time 0.136 (0.142) data 0.051 (0.058) loss 2.3496 (2.1211) acc 28.1250 (38.2500) lr 1.8838e-03 eta 0:19:51
epoch [33/200] batch [30/50] time 0.088 (0.133) data 0.000 (0.048) loss 1.9590 (2.1095) acc 40.6250 (38.1250) lr 1.8838e-03 eta 0:18:30
epoch [33/200] batch [35/50] time 0.085 (0.126) data 0.001 (0.042) loss 1.7891 (2.1250) acc 43.7500 (37.5000) lr 1.8838e-03 eta 0:17:33
epoch [33/200] batch [40/50] time 0.086 (0.121) data 0.000 (0.036) loss 2.2520 (2.1333) acc 46.8750 (37.7344) lr 1.8838e-03 eta 0:16:49
epoch [33/200] batch [45/50] time 0.083 (0.117) data 0.000 (0.032) loss 2.5137 (2.1386) acc 21.8750 (37.2222) lr 1.8838e-03 eta 0:16:14
epoch [33/200] batch [50/50] time 0.083 (0.113) data 0.000 (0.029) loss 2.2559 (2.1434) acc 37.5000 (37.1875) lr 1.8763e-03 eta 0:15:46
epoch [34/200] batch [5/50] time 0.086 (0.316) data 0.000 (0.231) loss 2.0723 (2.3328) acc 31.2500 (30.0000) lr 1.8763e-03 eta 0:44:00
epoch [34/200] batch [10/50] time 0.240 (0.216) data 0.157 (0.132) loss 2.1406 (2.2064) acc 40.6250 (33.1250) lr 1.8763e-03 eta 0:30:05
epoch [34/200] batch [15/50] time 0.085 (0.172) data 0.001 (0.088) loss 2.0957 (2.1674) acc 31.2500 (35.4167) lr 1.8763e-03 eta 0:23:56
epoch [34/200] batch [20/50] time 0.084 (0.158) data 0.000 (0.073) loss 2.1270 (2.1454) acc 46.8750 (37.5000) lr 1.8763e-03 eta 0:21:52
epoch [34/200] batch [25/50] time 0.085 (0.143) data 0.000 (0.059) loss 1.9795 (2.1373) acc 21.8750 (36.5000) lr 1.8763e-03 eta 0:19:49
epoch [34/200] batch [30/50] time 0.085 (0.138) data 0.000 (0.054) loss 2.3965 (2.1285) acc 40.6250 (36.6667) lr 1.8763e-03 eta 0:19:08
epoch [34/200] batch [35/50] time 0.086 (0.133) data 0.000 (0.049) loss 2.2188 (2.1132) acc 34.3750 (37.3214) lr 1.8763e-03 eta 0:18:29
epoch [34/200] batch [40/50] time 0.085 (0.127) data 0.000 (0.043) loss 1.9053 (2.1155) acc 40.6250 (36.8750) lr 1.8763e-03 eta 0:17:37
epoch [34/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.2188 (2.1015) acc 31.2500 (36.5278) lr 1.8763e-03 eta 0:17:12
epoch [34/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 2.4355 (2.1077) acc 40.6250 (36.9375) lr 1.8686e-03 eta 0:16:37
epoch [35/200] batch [5/50] time 0.085 (0.305) data 0.000 (0.219) loss 3.0000 (2.4590) acc 28.1250 (33.7500) lr 1.8686e-03 eta 0:42:13
epoch [35/200] batch [10/50] time 0.086 (0.212) data 0.000 (0.127) loss 3.0508 (2.2814) acc 18.7500 (36.2500) lr 1.8686e-03 eta 0:29:16
epoch [35/200] batch [15/50] time 0.085 (0.169) data 0.000 (0.084) loss 2.2246 (2.2587) acc 37.5000 (35.6250) lr 1.8686e-03 eta 0:23:23
epoch [35/200] batch [20/50] time 0.084 (0.155) data 0.000 (0.070) loss 1.9580 (2.1904) acc 37.5000 (36.8750) lr 1.8686e-03 eta 0:21:24
epoch [35/200] batch [25/50] time 0.085 (0.141) data 0.001 (0.056) loss 1.9902 (2.1799) acc 43.7500 (35.8750) lr 1.8686e-03 eta 0:19:28
epoch [35/200] batch [30/50] time 0.087 (0.132) data 0.000 (0.047) loss 1.9697 (2.1459) acc 31.2500 (36.3542) lr 1.8686e-03 eta 0:18:11
epoch [35/200] batch [35/50] time 0.086 (0.125) data 0.001 (0.040) loss 2.2598 (2.1563) acc 25.0000 (36.1607) lr 1.8686e-03 eta 0:17:16
epoch [35/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.035) loss 2.3809 (2.1592) acc 37.5000 (36.4844) lr 1.8686e-03 eta 0:16:33
epoch [35/200] batch [45/50] time 0.084 (0.116) data 0.000 (0.032) loss 2.1035 (2.1622) acc 31.2500 (36.8750) lr 1.8686e-03 eta 0:15:59
epoch [35/200] batch [50/50] time 0.084 (0.113) data 0.000 (0.028) loss 2.1016 (2.1488) acc 31.2500 (36.9375) lr 1.8607e-03 eta 0:15:31
epoch [36/200] batch [5/50] time 0.085 (0.328) data 0.000 (0.243) loss 2.1387 (2.1744) acc 25.0000 (33.1250) lr 1.8607e-03 eta 0:45:05
epoch [36/200] batch [10/50] time 0.086 (0.217) data 0.000 (0.132) loss 2.3789 (2.1163) acc 25.0000 (35.6250) lr 1.8607e-03 eta 0:29:51
epoch [36/200] batch [15/50] time 0.086 (0.173) data 0.000 (0.088) loss 1.9258 (2.0872) acc 50.0000 (37.7083) lr 1.8607e-03 eta 0:23:48
epoch [36/200] batch [20/50] time 0.085 (0.158) data 0.000 (0.072) loss 1.8828 (2.0787) acc 43.7500 (37.5000) lr 1.8607e-03 eta 0:21:38
epoch [36/200] batch [25/50] time 0.196 (0.148) data 0.111 (0.062) loss 2.2207 (2.0879) acc 40.6250 (37.8750) lr 1.8607e-03 eta 0:20:15
epoch [36/200] batch [30/50] time 0.086 (0.138) data 0.000 (0.052) loss 2.0527 (2.0882) acc 25.0000 (38.2292) lr 1.8607e-03 eta 0:18:50
epoch [36/200] batch [35/50] time 0.085 (0.139) data 0.001 (0.053) loss 2.2910 (2.1054) acc 34.3750 (37.9464) lr 1.8607e-03 eta 0:18:57
epoch [36/200] batch [40/50] time 0.083 (0.132) data 0.000 (0.047) loss 1.9590 (2.1140) acc 50.0000 (37.6562) lr 1.8607e-03 eta 0:18:01
epoch [36/200] batch [45/50] time 0.083 (0.129) data 0.000 (0.044) loss 2.4492 (2.1294) acc 34.3750 (37.1528) lr 1.8607e-03 eta 0:17:38
epoch [36/200] batch [50/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.1465 (2.1332) acc 34.3750 (37.0625) lr 1.8526e-03 eta 0:17:00
epoch [37/200] batch [5/50] time 0.086 (0.299) data 0.000 (0.214) loss 2.1836 (2.1959) acc 43.7500 (36.8750) lr 1.8526e-03 eta 0:40:54
epoch [37/200] batch [10/50] time 0.086 (0.192) data 0.000 (0.107) loss 1.9717 (2.1938) acc 43.7500 (35.0000) lr 1.8526e-03 eta 0:26:14
epoch [37/200] batch [15/50] time 0.084 (0.163) data 0.000 (0.079) loss 2.3008 (2.1973) acc 34.3750 (36.2500) lr 1.8526e-03 eta 0:22:16
epoch [37/200] batch [20/50] time 0.155 (0.147) data 0.072 (0.063) loss 2.1914 (2.2283) acc 43.7500 (35.7812) lr 1.8526e-03 eta 0:20:03
epoch [37/200] batch [25/50] time 0.084 (0.135) data 0.000 (0.050) loss 2.0352 (2.2005) acc 40.6250 (35.8750) lr 1.8526e-03 eta 0:18:19
epoch [37/200] batch [30/50] time 0.085 (0.132) data 0.000 (0.048) loss 1.8584 (2.2092) acc 37.5000 (35.9375) lr 1.8526e-03 eta 0:17:58
epoch [37/200] batch [35/50] time 0.085 (0.125) data 0.001 (0.041) loss 2.3730 (2.1927) acc 25.0000 (36.3393) lr 1.8526e-03 eta 0:17:03
epoch [37/200] batch [40/50] time 0.083 (0.123) data 0.000 (0.038) loss 1.9492 (2.1845) acc 46.8750 (36.6406) lr 1.8526e-03 eta 0:16:41
epoch [37/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.039) loss 1.9775 (2.1658) acc 34.3750 (36.3889) lr 1.8526e-03 eta 0:16:43
epoch [37/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.3047 (2.1610) acc 28.1250 (36.5000) lr 1.8443e-03 eta 0:16:10
epoch [38/200] batch [5/50] time 0.087 (0.293) data 0.000 (0.207) loss 1.7754 (2.0443) acc 46.8750 (40.0000) lr 1.8443e-03 eta 0:39:43
epoch [38/200] batch [10/50] time 0.085 (0.189) data 0.000 (0.104) loss 2.2852 (2.0768) acc 31.2500 (40.0000) lr 1.8443e-03 eta 0:25:40
epoch [38/200] batch [15/50] time 0.084 (0.158) data 0.000 (0.072) loss 1.9619 (2.0890) acc 43.7500 (38.9583) lr 1.8443e-03 eta 0:21:22
epoch [38/200] batch [20/50] time 0.086 (0.140) data 0.000 (0.054) loss 2.2598 (2.0796) acc 34.3750 (38.2812) lr 1.8443e-03 eta 0:18:55
epoch [38/200] batch [25/50] time 0.085 (0.133) data 0.000 (0.048) loss 2.1094 (2.0856) acc 43.7500 (38.6250) lr 1.8443e-03 eta 0:17:59
epoch [38/200] batch [30/50] time 0.084 (0.131) data 0.000 (0.046) loss 2.5352 (2.0964) acc 31.2500 (38.4375) lr 1.8443e-03 eta 0:17:47
epoch [38/200] batch [35/50] time 0.085 (0.125) data 0.000 (0.040) loss 2.0234 (2.1004) acc 28.1250 (37.5893) lr 1.8443e-03 eta 0:16:52
epoch [38/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.2559 (2.1067) acc 25.0000 (36.9531) lr 1.8443e-03 eta 0:16:50
epoch [38/200] batch [45/50] time 0.083 (0.120) data 0.000 (0.035) loss 1.7773 (2.1045) acc 37.5000 (36.8056) lr 1.8443e-03 eta 0:16:12
epoch [38/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.032) loss 2.0566 (2.1105) acc 40.6250 (37.1875) lr 1.8358e-03 eta 0:15:41
epoch [39/200] batch [5/50] time 0.181 (0.309) data 0.096 (0.224) loss 1.9160 (2.1000) acc 34.3750 (36.8750) lr 1.8358e-03 eta 0:41:38
epoch [39/200] batch [10/50] time 0.086 (0.205) data 0.001 (0.120) loss 1.9873 (2.1022) acc 40.6250 (37.5000) lr 1.8358e-03 eta 0:27:41
epoch [39/200] batch [15/50] time 0.085 (0.172) data 0.000 (0.087) loss 2.8223 (2.1473) acc 18.7500 (37.0833) lr 1.8358e-03 eta 0:23:10
epoch [39/200] batch [20/50] time 0.085 (0.150) data 0.000 (0.065) loss 1.8779 (2.0952) acc 50.0000 (39.3750) lr 1.8358e-03 eta 0:20:14
epoch [39/200] batch [25/50] time 0.085 (0.144) data 0.000 (0.059) loss 1.7715 (2.0942) acc 46.8750 (40.0000) lr 1.8358e-03 eta 0:19:21
epoch [39/200] batch [30/50] time 0.085 (0.139) data 0.000 (0.054) loss 2.4629 (2.0950) acc 31.2500 (40.0000) lr 1.8358e-03 eta 0:18:39
epoch [39/200] batch [35/50] time 0.084 (0.131) data 0.000 (0.046) loss 2.0996 (2.0943) acc 34.3750 (39.6429) lr 1.8358e-03 eta 0:17:36
epoch [39/200] batch [40/50] time 0.082 (0.130) data 0.000 (0.045) loss 2.0938 (2.1075) acc 37.5000 (38.9844) lr 1.8358e-03 eta 0:17:28
epoch [39/200] batch [45/50] time 0.228 (0.128) data 0.148 (0.044) loss 2.1953 (2.0888) acc 37.5000 (39.0972) lr 1.8358e-03 eta 0:17:11
epoch [39/200] batch [50/50] time 0.082 (0.123) data 0.000 (0.039) loss 1.9404 (2.0866) acc 43.7500 (38.6875) lr 1.8271e-03 eta 0:16:34
epoch [40/200] batch [5/50] time 0.086 (0.314) data 0.000 (0.228) loss 2.0684 (2.0691) acc 37.5000 (36.2500) lr 1.8271e-03 eta 0:42:02
epoch [40/200] batch [10/50] time 0.086 (0.200) data 0.000 (0.114) loss 2.0684 (2.0816) acc 34.3750 (35.6250) lr 1.8271e-03 eta 0:26:45
epoch [40/200] batch [15/50] time 0.085 (0.171) data 0.000 (0.085) loss 1.7969 (2.0546) acc 56.2500 (37.9167) lr 1.8271e-03 eta 0:22:50
epoch [40/200] batch [20/50] time 0.086 (0.157) data 0.000 (0.072) loss 1.9092 (2.1159) acc 46.8750 (38.2812) lr 1.8271e-03 eta 0:21:03
epoch [40/200] batch [25/50] time 0.087 (0.143) data 0.000 (0.058) loss 1.9326 (2.1018) acc 43.7500 (39.1250) lr 1.8271e-03 eta 0:19:08
epoch [40/200] batch [30/50] time 0.086 (0.139) data 0.000 (0.054) loss 2.4453 (2.1264) acc 25.0000 (37.5000) lr 1.8271e-03 eta 0:18:37
epoch [40/200] batch [35/50] time 0.180 (0.134) data 0.096 (0.049) loss 2.5586 (2.1475) acc 25.0000 (37.0536) lr 1.8271e-03 eta 0:17:55
epoch [40/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.043) loss 2.1914 (2.1546) acc 25.0000 (36.4844) lr 1.8271e-03 eta 0:17:04
epoch [40/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.4434 (2.1426) acc 34.3750 (36.3889) lr 1.8271e-03 eta 0:16:36
epoch [40/200] batch [50/50] time 0.084 (0.120) data 0.000 (0.036) loss 2.4355 (2.1522) acc 34.3750 (36.0000) lr 1.8181e-03 eta 0:16:03
epoch [41/200] batch [5/50] time 0.085 (0.318) data 0.000 (0.233) loss 2.1680 (2.0670) acc 37.5000 (36.8750) lr 1.8181e-03 eta 0:42:18
epoch [41/200] batch [10/50] time 0.128 (0.205) data 0.045 (0.121) loss 1.6074 (2.0689) acc 46.8750 (35.6250) lr 1.8181e-03 eta 0:27:21
epoch [41/200] batch [15/50] time 0.085 (0.165) data 0.000 (0.081) loss 2.0293 (2.0867) acc 37.5000 (36.4583) lr 1.8181e-03 eta 0:21:58
epoch [41/200] batch [20/50] time 0.085 (0.145) data 0.000 (0.061) loss 1.9668 (2.0656) acc 43.7500 (36.7188) lr 1.8181e-03 eta 0:19:16
epoch [41/200] batch [25/50] time 0.084 (0.133) data 0.000 (0.049) loss 2.3789 (2.1140) acc 25.0000 (35.2500) lr 1.8181e-03 eta 0:17:41
epoch [41/200] batch [30/50] time 0.085 (0.125) data 0.000 (0.041) loss 2.1855 (2.1179) acc 43.7500 (36.3542) lr 1.8181e-03 eta 0:16:37
epoch [41/200] batch [35/50] time 0.085 (0.119) data 0.000 (0.035) loss 1.9717 (2.0922) acc 40.6250 (36.9643) lr 1.8181e-03 eta 0:15:50
epoch [41/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.035) loss 1.7959 (2.0688) acc 40.6250 (37.8906) lr 1.8181e-03 eta 0:15:52
epoch [41/200] batch [45/50] time 0.084 (0.116) data 0.000 (0.031) loss 2.2129 (2.0685) acc 18.7500 (37.3611) lr 1.8181e-03 eta 0:15:20
epoch [41/200] batch [50/50] time 0.084 (0.113) data 0.000 (0.029) loss 2.3203 (2.0787) acc 37.5000 (37.3125) lr 1.8090e-03 eta 0:14:58
epoch [42/200] batch [5/50] time 0.087 (0.312) data 0.000 (0.227) loss 2.3555 (2.1125) acc 37.5000 (38.1250) lr 1.8090e-03 eta 0:41:21
epoch [42/200] batch [10/50] time 0.108 (0.201) data 0.023 (0.116) loss 2.0508 (2.1284) acc 46.8750 (37.8125) lr 1.8090e-03 eta 0:26:39
epoch [42/200] batch [15/50] time 0.087 (0.167) data 0.000 (0.082) loss 1.8945 (2.1017) acc 25.0000 (37.7083) lr 1.8090e-03 eta 0:22:09
epoch [42/200] batch [20/50] time 0.086 (0.157) data 0.000 (0.071) loss 2.0840 (2.0750) acc 46.8750 (38.4375) lr 1.8090e-03 eta 0:20:41
epoch [42/200] batch [25/50] time 0.087 (0.142) data 0.000 (0.057) loss 2.0273 (2.0870) acc 46.8750 (37.3750) lr 1.8090e-03 eta 0:18:48
epoch [42/200] batch [30/50] time 0.087 (0.133) data 0.000 (0.047) loss 1.5840 (2.1012) acc 40.6250 (37.1875) lr 1.8090e-03 eta 0:17:33
epoch [42/200] batch [35/50] time 0.086 (0.126) data 0.001 (0.041) loss 1.7646 (2.1040) acc 43.7500 (37.5893) lr 1.8090e-03 eta 0:16:39
epoch [42/200] batch [40/50] time 0.085 (0.121) data 0.000 (0.036) loss 2.0938 (2.1122) acc 37.5000 (37.3438) lr 1.8090e-03 eta 0:15:58
epoch [42/200] batch [45/50] time 0.090 (0.117) data 0.006 (0.032) loss 1.6299 (2.1162) acc 43.7500 (37.4306) lr 1.8090e-03 eta 0:15:26
epoch [42/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.029) loss 2.2676 (2.1404) acc 40.6250 (37.0000) lr 1.7997e-03 eta 0:14:59
epoch [43/200] batch [5/50] time 0.086 (0.281) data 0.000 (0.195) loss 2.3398 (2.1801) acc 28.1250 (36.2500) lr 1.7997e-03 eta 0:37:00
epoch [43/200] batch [10/50] time 0.087 (0.184) data 0.000 (0.098) loss 2.1484 (2.1927) acc 37.5000 (37.5000) lr 1.7997e-03 eta 0:24:09
epoch [43/200] batch [15/50] time 0.086 (0.151) data 0.001 (0.065) loss 1.8281 (2.1467) acc 43.7500 (37.2917) lr 1.7997e-03 eta 0:19:52
epoch [43/200] batch [20/50] time 0.084 (0.135) data 0.000 (0.049) loss 2.2207 (2.1099) acc 37.5000 (38.1250) lr 1.7997e-03 eta 0:17:41
epoch [43/200] batch [25/50] time 0.086 (0.129) data 0.000 (0.043) loss 2.1504 (2.1031) acc 34.3750 (37.8750) lr 1.7997e-03 eta 0:16:54
epoch [43/200] batch [30/50] time 0.268 (0.128) data 0.184 (0.042) loss 2.1016 (2.1041) acc 46.8750 (38.2292) lr 1.7997e-03 eta 0:16:45
epoch [43/200] batch [35/50] time 0.086 (0.122) data 0.001 (0.036) loss 2.0078 (2.0929) acc 37.5000 (38.3929) lr 1.7997e-03 eta 0:15:57
epoch [43/200] batch [40/50] time 0.085 (0.119) data 0.000 (0.033) loss 2.2793 (2.1111) acc 34.3750 (37.9688) lr 1.7997e-03 eta 0:15:34
epoch [43/200] batch [45/50] time 0.083 (0.115) data 0.000 (0.030) loss 1.8643 (2.1177) acc 46.8750 (37.9167) lr 1.7997e-03 eta 0:15:02
epoch [43/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.030) loss 1.9658 (2.1099) acc 40.6250 (37.9375) lr 1.7902e-03 eta 0:15:03
epoch [44/200] batch [5/50] time 0.086 (0.299) data 0.000 (0.213) loss 2.2383 (2.1426) acc 40.6250 (41.2500) lr 1.7902e-03 eta 0:39:01
epoch [44/200] batch [10/50] time 0.086 (0.206) data 0.001 (0.120) loss 2.0000 (2.0473) acc 37.5000 (39.3750) lr 1.7902e-03 eta 0:26:56
epoch [44/200] batch [15/50] time 0.086 (0.166) data 0.000 (0.080) loss 1.9434 (2.0330) acc 34.3750 (37.5000) lr 1.7902e-03 eta 0:21:41
epoch [44/200] batch [20/50] time 0.097 (0.152) data 0.000 (0.066) loss 2.4785 (2.0507) acc 37.5000 (38.4375) lr 1.7902e-03 eta 0:19:52
epoch [44/200] batch [25/50] time 0.084 (0.139) data 0.000 (0.053) loss 1.7646 (2.0476) acc 53.1250 (38.6250) lr 1.7902e-03 eta 0:18:06
epoch [44/200] batch [30/50] time 0.143 (0.134) data 0.060 (0.049) loss 2.4492 (2.0558) acc 25.0000 (38.6458) lr 1.7902e-03 eta 0:17:31
epoch [44/200] batch [35/50] time 0.083 (0.127) data 0.000 (0.042) loss 2.3945 (2.0651) acc 25.0000 (37.9464) lr 1.7902e-03 eta 0:16:34
epoch [44/200] batch [40/50] time 0.084 (0.125) data 0.000 (0.039) loss 2.3945 (2.0875) acc 34.3750 (37.7344) lr 1.7902e-03 eta 0:16:12
epoch [44/200] batch [45/50] time 0.084 (0.120) data 0.000 (0.035) loss 2.0723 (2.0725) acc 31.2500 (37.9861) lr 1.7902e-03 eta 0:15:37
epoch [44/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.032) loss 1.9326 (2.0719) acc 46.8750 (38.0625) lr 1.7804e-03 eta 0:15:07
epoch [45/200] batch [5/50] time 0.086 (0.316) data 0.000 (0.230) loss 2.2891 (2.1668) acc 21.8750 (32.5000) lr 1.7804e-03 eta 0:41:03
epoch [45/200] batch [10/50] time 0.253 (0.218) data 0.169 (0.132) loss 2.4766 (2.0992) acc 25.0000 (36.2500) lr 1.7804e-03 eta 0:28:16
epoch [45/200] batch [15/50] time 0.085 (0.174) data 0.000 (0.088) loss 1.8164 (2.1033) acc 43.7500 (37.5000) lr 1.7804e-03 eta 0:22:31
epoch [45/200] batch [20/50] time 0.084 (0.161) data 0.000 (0.075) loss 2.4258 (2.1251) acc 28.1250 (37.5000) lr 1.7804e-03 eta 0:20:49
epoch [45/200] batch [25/50] time 0.085 (0.145) data 0.000 (0.060) loss 2.7207 (2.1540) acc 31.2500 (37.0000) lr 1.7804e-03 eta 0:18:50
epoch [45/200] batch [30/50] time 0.084 (0.144) data 0.000 (0.059) loss 2.3770 (2.1888) acc 40.6250 (36.6667) lr 1.7804e-03 eta 0:18:36
epoch [45/200] batch [35/50] time 0.084 (0.138) data 0.001 (0.053) loss 1.9141 (2.1725) acc 40.6250 (36.9643) lr 1.7804e-03 eta 0:17:48
epoch [45/200] batch [40/50] time 0.083 (0.131) data 0.000 (0.046) loss 2.1367 (2.1659) acc 40.6250 (37.3438) lr 1.7804e-03 eta 0:16:54
epoch [45/200] batch [45/50] time 0.083 (0.128) data 0.000 (0.044) loss 2.0918 (2.1609) acc 43.7500 (37.3611) lr 1.7804e-03 eta 0:16:31
epoch [45/200] batch [50/50] time 0.084 (0.123) data 0.000 (0.039) loss 2.0723 (2.1443) acc 40.6250 (37.3750) lr 1.7705e-03 eta 0:15:56
epoch [46/200] batch [5/50] time 0.082 (0.298) data 0.000 (0.215) loss 1.9639 (2.0146) acc 46.8750 (44.3750) lr 1.7705e-03 eta 0:38:31
epoch [46/200] batch [10/50] time 0.230 (0.206) data 0.148 (0.122) loss 2.1914 (2.0771) acc 43.7500 (40.3125) lr 1.7705e-03 eta 0:26:32
epoch [46/200] batch [15/50] time 0.084 (0.165) data 0.000 (0.082) loss 2.2461 (2.1270) acc 40.6250 (39.1667) lr 1.7705e-03 eta 0:21:17
epoch [46/200] batch [20/50] time 0.083 (0.151) data 0.000 (0.068) loss 2.2246 (2.1220) acc 34.3750 (38.7500) lr 1.7705e-03 eta 0:19:29
epoch [46/200] batch [25/50] time 0.083 (0.138) data 0.000 (0.054) loss 1.9473 (2.1464) acc 43.7500 (38.1250) lr 1.7705e-03 eta 0:17:43
epoch [46/200] batch [30/50] time 0.083 (0.136) data 0.000 (0.053) loss 2.2930 (2.1531) acc 34.3750 (37.9167) lr 1.7705e-03 eta 0:17:29
epoch [46/200] batch [35/50] time 0.083 (0.133) data 0.001 (0.050) loss 2.2676 (2.1360) acc 46.8750 (38.4821) lr 1.7705e-03 eta 0:17:08
epoch [46/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.044) loss 1.8164 (2.1260) acc 43.7500 (38.2031) lr 1.7705e-03 eta 0:16:19
epoch [46/200] batch [45/50] time 0.084 (0.122) data 0.000 (0.039) loss 2.2637 (2.1359) acc 34.3750 (37.7083) lr 1.7705e-03 eta 0:15:41
epoch [46/200] batch [50/50] time 0.084 (0.118) data 0.000 (0.035) loss 1.4795 (2.1313) acc 50.0000 (37.3125) lr 1.7604e-03 eta 0:15:11
epoch [47/200] batch [5/50] time 0.086 (0.300) data 0.000 (0.213) loss 2.2656 (2.0359) acc 34.3750 (40.6250) lr 1.7604e-03 eta 0:38:26
epoch [47/200] batch [10/50] time 0.085 (0.193) data 0.000 (0.107) loss 1.9365 (2.0556) acc 46.8750 (40.6250) lr 1.7604e-03 eta 0:24:43
epoch [47/200] batch [15/50] time 0.084 (0.157) data 0.000 (0.071) loss 1.8818 (2.0296) acc 46.8750 (41.2500) lr 1.7604e-03 eta 0:20:05
epoch [47/200] batch [20/50] time 0.173 (0.146) data 0.089 (0.060) loss 1.6973 (2.0307) acc 56.2500 (40.6250) lr 1.7604e-03 eta 0:18:37
epoch [47/200] batch [25/50] time 0.090 (0.133) data 0.000 (0.048) loss 2.2109 (2.0352) acc 28.1250 (40.6250) lr 1.7604e-03 eta 0:17:04
epoch [47/200] batch [30/50] time 0.084 (0.130) data 0.000 (0.046) loss 2.1270 (2.0443) acc 43.7500 (40.1042) lr 1.7604e-03 eta 0:16:40
epoch [47/200] batch [35/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.2637 (2.0538) acc 25.0000 (39.5536) lr 1.7604e-03 eta 0:16:04
epoch [47/200] batch [40/50] time 0.083 (0.121) data 0.000 (0.036) loss 2.2148 (2.0389) acc 40.6250 (40.1562) lr 1.7604e-03 eta 0:15:23
epoch [47/200] batch [45/50] time 0.085 (0.119) data 0.000 (0.035) loss 1.9766 (2.0559) acc 40.6250 (40.2778) lr 1.7604e-03 eta 0:15:13
epoch [47/200] batch [50/50] time 0.084 (0.116) data 0.000 (0.031) loss 1.9619 (2.0435) acc 37.5000 (40.6250) lr 1.7501e-03 eta 0:14:45
epoch [48/200] batch [5/50] time 0.085 (0.305) data 0.000 (0.220) loss 1.2598 (1.9002) acc 62.5000 (45.0000) lr 1.7501e-03 eta 0:38:50
epoch [48/200] batch [10/50] time 0.085 (0.195) data 0.000 (0.110) loss 2.0801 (2.0747) acc 37.5000 (39.6875) lr 1.7501e-03 eta 0:24:46
epoch [48/200] batch [15/50] time 0.084 (0.168) data 0.000 (0.084) loss 1.9863 (2.0535) acc 43.7500 (40.6250) lr 1.7501e-03 eta 0:21:24
epoch [48/200] batch [20/50] time 0.085 (0.156) data 0.000 (0.071) loss 2.0195 (2.0427) acc 34.3750 (40.3125) lr 1.7501e-03 eta 0:19:47
epoch [48/200] batch [25/50] time 0.085 (0.142) data 0.000 (0.057) loss 2.4785 (2.0416) acc 18.7500 (39.7500) lr 1.7501e-03 eta 0:17:59
epoch [48/200] batch [30/50] time 0.086 (0.135) data 0.000 (0.050) loss 2.2637 (2.0410) acc 31.2500 (39.7917) lr 1.7501e-03 eta 0:17:07
epoch [48/200] batch [35/50] time 0.195 (0.131) data 0.111 (0.046) loss 1.6689 (2.0435) acc 43.7500 (38.8393) lr 1.7501e-03 eta 0:16:37
epoch [48/200] batch [40/50] time 0.085 (0.125) data 0.000 (0.041) loss 2.2266 (2.0844) acc 34.3750 (38.5156) lr 1.7501e-03 eta 0:15:52
epoch [48/200] batch [45/50] time 0.084 (0.123) data 0.000 (0.039) loss 1.9678 (2.1006) acc 37.5000 (37.9167) lr 1.7501e-03 eta 0:15:39
epoch [48/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 1.8232 (2.0984) acc 43.7500 (38.0625) lr 1.7396e-03 eta 0:15:07
epoch [49/200] batch [5/50] time 0.084 (0.300) data 0.000 (0.216) loss 2.1543 (2.0543) acc 37.5000 (36.8750) lr 1.7396e-03 eta 0:38:00
epoch [49/200] batch [10/50] time 0.085 (0.192) data 0.000 (0.108) loss 1.8320 (2.1295) acc 50.0000 (37.1875) lr 1.7396e-03 eta 0:24:20
epoch [49/200] batch [15/50] time 0.084 (0.164) data 0.000 (0.080) loss 2.1582 (2.0899) acc 31.2500 (37.7083) lr 1.7396e-03 eta 0:20:42
epoch [49/200] batch [20/50] time 0.084 (0.148) data 0.000 (0.064) loss 1.9297 (2.0966) acc 40.6250 (37.1875) lr 1.7396e-03 eta 0:18:44
epoch [49/200] batch [25/50] time 0.084 (0.136) data 0.000 (0.051) loss 2.0215 (2.0725) acc 34.3750 (37.0000) lr 1.7396e-03 eta 0:17:06
epoch [49/200] batch [30/50] time 0.084 (0.133) data 0.000 (0.048) loss 2.0117 (2.0779) acc 46.8750 (37.3958) lr 1.7396e-03 eta 0:16:44
epoch [49/200] batch [35/50] time 0.226 (0.130) data 0.142 (0.046) loss 2.1562 (2.1318) acc 31.2500 (35.9821) lr 1.7396e-03 eta 0:16:21
epoch [49/200] batch [40/50] time 0.083 (0.124) data 0.000 (0.040) loss 1.9355 (2.1088) acc 46.8750 (37.5000) lr 1.7396e-03 eta 0:15:38
epoch [49/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.0879 (2.0805) acc 25.0000 (37.9167) lr 1.7396e-03 eta 0:15:38
epoch [49/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.8047 (2.0836) acc 46.8750 (37.6250) lr 1.7290e-03 eta 0:15:06
epoch [50/200] batch [5/50] time 0.084 (0.337) data 0.000 (0.252) loss 2.2773 (2.0654) acc 31.2500 (37.5000) lr 1.7290e-03 eta 0:42:20
epoch [50/200] batch [10/50] time 0.210 (0.223) data 0.126 (0.139) loss 2.0996 (2.0245) acc 40.6250 (39.6875) lr 1.7290e-03 eta 0:28:03
epoch [50/200] batch [15/50] time 0.087 (0.177) data 0.000 (0.093) loss 2.1309 (2.0040) acc 28.1250 (38.3333) lr 1.7290e-03 eta 0:22:15
epoch [50/200] batch [20/50] time 0.085 (0.163) data 0.000 (0.078) loss 2.0527 (1.9800) acc 40.6250 (40.1562) lr 1.7290e-03 eta 0:20:25
epoch [50/200] batch [25/50] time 0.086 (0.147) data 0.000 (0.063) loss 2.2480 (2.0254) acc 40.6250 (39.2500) lr 1.7290e-03 eta 0:18:27
epoch [50/200] batch [30/50] time 0.086 (0.141) data 0.000 (0.056) loss 2.2930 (2.0611) acc 43.7500 (39.1667) lr 1.7290e-03 eta 0:17:37
epoch [50/200] batch [35/50] time 0.085 (0.136) data 0.000 (0.051) loss 2.0703 (2.0617) acc 46.8750 (40.3571) lr 1.7290e-03 eta 0:17:00
epoch [50/200] batch [40/50] time 0.083 (0.129) data 0.000 (0.045) loss 1.9307 (2.0567) acc 46.8750 (39.9219) lr 1.7290e-03 eta 0:16:10
epoch [50/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.6348 (2.0247) acc 43.7500 (40.2778) lr 1.7290e-03 eta 0:15:40
epoch [50/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 2.1348 (2.0299) acc 37.5000 (39.8125) lr 1.7181e-03 eta 0:15:08
epoch [51/200] batch [5/50] time 0.085 (0.327) data 0.000 (0.243) loss 1.7500 (1.8762) acc 40.6250 (40.6250) lr 1.7181e-03 eta 0:40:54
epoch [51/200] batch [10/50] time 0.085 (0.218) data 0.000 (0.133) loss 2.3223 (2.0547) acc 28.1250 (37.5000) lr 1.7181e-03 eta 0:27:10
epoch [51/200] batch [15/50] time 0.084 (0.174) data 0.000 (0.089) loss 1.4717 (2.0305) acc 56.2500 (39.7917) lr 1.7181e-03 eta 0:21:39
epoch [51/200] batch [20/50] time 0.085 (0.151) data 0.001 (0.067) loss 2.3418 (2.0495) acc 40.6250 (39.6875) lr 1.7181e-03 eta 0:18:52
epoch [51/200] batch [25/50] time 0.085 (0.138) data 0.000 (0.054) loss 2.1562 (2.0585) acc 40.6250 (39.8750) lr 1.7181e-03 eta 0:17:11
epoch [51/200] batch [30/50] time 0.085 (0.129) data 0.000 (0.045) loss 1.5518 (2.0219) acc 46.8750 (40.1042) lr 1.7181e-03 eta 0:16:04
epoch [51/200] batch [35/50] time 0.084 (0.123) data 0.000 (0.038) loss 2.2070 (2.0480) acc 34.3750 (39.6429) lr 1.7181e-03 eta 0:15:16
epoch [51/200] batch [40/50] time 0.084 (0.118) data 0.000 (0.034) loss 1.9307 (2.0473) acc 25.0000 (39.1406) lr 1.7181e-03 eta 0:14:39
epoch [51/200] batch [45/50] time 0.083 (0.114) data 0.000 (0.030) loss 1.7129 (2.0582) acc 40.6250 (39.0972) lr 1.7181e-03 eta 0:14:11
epoch [51/200] batch [50/50] time 0.083 (0.111) data 0.000 (0.027) loss 1.8760 (2.0501) acc 37.5000 (39.6250) lr 1.7071e-03 eta 0:13:47
epoch [52/200] batch [5/50] time 0.085 (0.344) data 0.000 (0.259) loss 1.9785 (2.1088) acc 53.1250 (41.2500) lr 1.7071e-03 eta 0:42:39
epoch [52/200] batch [10/50] time 0.141 (0.220) data 0.056 (0.135) loss 1.9229 (2.1053) acc 34.3750 (38.1250) lr 1.7071e-03 eta 0:27:17
epoch [52/200] batch [15/50] time 0.085 (0.175) data 0.000 (0.090) loss 2.2520 (2.0768) acc 37.5000 (38.5417) lr 1.7071e-03 eta 0:21:42
epoch [52/200] batch [20/50] time 0.085 (0.159) data 0.000 (0.073) loss 2.2871 (2.0981) acc 37.5000 (36.8750) lr 1.7071e-03 eta 0:19:37
epoch [52/200] batch [25/50] time 0.086 (0.144) data 0.000 (0.059) loss 2.1699 (2.0890) acc 37.5000 (38.3750) lr 1.7071e-03 eta 0:17:47
epoch [52/200] batch [30/50] time 0.085 (0.140) data 0.001 (0.055) loss 1.9316 (2.0734) acc 40.6250 (38.4375) lr 1.7071e-03 eta 0:17:16
epoch [52/200] batch [35/50] time 0.086 (0.135) data 0.000 (0.050) loss 2.1387 (2.0540) acc 50.0000 (39.4643) lr 1.7071e-03 eta 0:16:39
epoch [52/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.044) loss 1.7881 (2.0377) acc 56.2500 (39.7656) lr 1.7071e-03 eta 0:15:51
epoch [52/200] batch [45/50] time 0.084 (0.126) data 0.000 (0.042) loss 1.7158 (2.0415) acc 43.7500 (39.1667) lr 1.7071e-03 eta 0:15:35
epoch [52/200] batch [50/50] time 0.084 (0.122) data 0.000 (0.038) loss 2.4668 (2.0670) acc 40.6250 (38.6875) lr 1.6959e-03 eta 0:15:04
epoch [53/200] batch [5/50] time 0.086 (0.319) data 0.000 (0.233) loss 2.1406 (1.8160) acc 43.7500 (46.2500) lr 1.6959e-03 eta 0:39:15
epoch [53/200] batch [10/50] time 0.181 (0.211) data 0.097 (0.126) loss 2.1641 (1.9510) acc 31.2500 (41.8750) lr 1.6959e-03 eta 0:26:00
epoch [53/200] batch [15/50] time 0.085 (0.169) data 0.000 (0.084) loss 1.8857 (2.0221) acc 46.8750 (40.2083) lr 1.6959e-03 eta 0:20:48
epoch [53/200] batch [20/50] time 0.085 (0.155) data 0.000 (0.071) loss 2.1543 (2.0718) acc 34.3750 (38.7500) lr 1.6959e-03 eta 0:19:07
epoch [53/200] batch [25/50] time 0.087 (0.141) data 0.000 (0.057) loss 2.0840 (2.0586) acc 40.6250 (38.3750) lr 1.6959e-03 eta 0:17:22
epoch [53/200] batch [30/50] time 0.085 (0.136) data 0.001 (0.051) loss 2.2480 (2.0785) acc 40.6250 (38.3333) lr 1.6959e-03 eta 0:16:40
epoch [53/200] batch [35/50] time 0.085 (0.134) data 0.000 (0.049) loss 2.4219 (2.1122) acc 34.3750 (37.8571) lr 1.6959e-03 eta 0:16:24
epoch [53/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.043) loss 2.0312 (2.0858) acc 40.6250 (38.5938) lr 1.6959e-03 eta 0:15:38
epoch [53/200] batch [45/50] time 0.085 (0.124) data 0.000 (0.039) loss 2.4473 (2.1122) acc 28.1250 (37.7083) lr 1.6959e-03 eta 0:15:09
epoch [53/200] batch [50/50] time 0.084 (0.120) data 0.000 (0.035) loss 2.1465 (2.0926) acc 34.3750 (38.3125) lr 1.6845e-03 eta 0:14:39
epoch [54/200] batch [5/50] time 0.086 (0.316) data 0.000 (0.230) loss 1.9893 (1.9912) acc 31.2500 (40.6250) lr 1.6845e-03 eta 0:38:42
epoch [54/200] batch [10/50] time 0.171 (0.210) data 0.085 (0.124) loss 1.9512 (1.9892) acc 43.7500 (42.8125) lr 1.6845e-03 eta 0:25:39
epoch [54/200] batch [15/50] time 0.085 (0.168) data 0.000 (0.083) loss 2.5996 (2.0401) acc 31.2500 (39.7917) lr 1.6845e-03 eta 0:20:33
epoch [54/200] batch [20/50] time 0.085 (0.152) data 0.000 (0.067) loss 1.7451 (2.0326) acc 37.5000 (39.3750) lr 1.6845e-03 eta 0:18:35
epoch [54/200] batch [25/50] time 0.085 (0.139) data 0.001 (0.054) loss 2.2266 (2.0651) acc 40.6250 (39.2500) lr 1.6845e-03 eta 0:16:56
epoch [54/200] batch [30/50] time 0.085 (0.138) data 0.000 (0.053) loss 1.9795 (2.0805) acc 34.3750 (38.4375) lr 1.6845e-03 eta 0:16:47
epoch [54/200] batch [35/50] time 0.085 (0.133) data 0.001 (0.049) loss 2.4277 (2.0933) acc 31.2500 (37.8571) lr 1.6845e-03 eta 0:16:14
epoch [54/200] batch [40/50] time 0.084 (0.127) data 0.000 (0.043) loss 2.4551 (2.0869) acc 34.3750 (38.0469) lr 1.6845e-03 eta 0:15:29
epoch [54/200] batch [45/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.1445 (2.0793) acc 37.5000 (38.2639) lr 1.6845e-03 eta 0:15:18
epoch [54/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.037) loss 1.7109 (2.0685) acc 56.2500 (38.6250) lr 1.6730e-03 eta 0:14:47
epoch [55/200] batch [5/50] time 0.088 (0.307) data 0.001 (0.221) loss 1.8926 (2.2268) acc 46.8750 (36.8750) lr 1.6730e-03 eta 0:37:20
epoch [55/200] batch [10/50] time 0.085 (0.196) data 0.000 (0.111) loss 2.3691 (2.1543) acc 31.2500 (39.6875) lr 1.6730e-03 eta 0:23:51
epoch [55/200] batch [15/50] time 0.085 (0.166) data 0.000 (0.080) loss 2.1973 (2.0749) acc 43.7500 (40.6250) lr 1.6730e-03 eta 0:20:07
epoch [55/200] batch [20/50] time 0.220 (0.152) data 0.136 (0.067) loss 1.8740 (2.0468) acc 37.5000 (41.2500) lr 1.6730e-03 eta 0:18:28
epoch [55/200] batch [25/50] time 0.086 (0.139) data 0.001 (0.054) loss 2.3203 (2.0517) acc 37.5000 (41.2500) lr 1.6730e-03 eta 0:16:49
epoch [55/200] batch [30/50] time 0.085 (0.136) data 0.000 (0.051) loss 2.0469 (2.0376) acc 43.7500 (41.3542) lr 1.6730e-03 eta 0:16:31
epoch [55/200] batch [35/50] time 0.084 (0.129) data 0.000 (0.044) loss 2.0488 (2.0407) acc 43.7500 (41.3393) lr 1.6730e-03 eta 0:15:37
epoch [55/200] batch [40/50] time 0.084 (0.127) data 0.000 (0.042) loss 2.3516 (2.0593) acc 34.3750 (40.7812) lr 1.6730e-03 eta 0:15:18
epoch [55/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.1250 (2.0753) acc 37.5000 (40.2778) lr 1.6730e-03 eta 0:15:00
epoch [55/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.7324 (2.0623) acc 50.0000 (40.1250) lr 1.6613e-03 eta 0:14:30
epoch [56/200] batch [5/50] time 0.136 (0.316) data 0.053 (0.231) loss 1.3613 (2.0637) acc 56.2500 (38.1250) lr 1.6613e-03 eta 0:38:10
epoch [56/200] batch [10/50] time 0.166 (0.209) data 0.082 (0.124) loss 1.9453 (2.0183) acc 37.5000 (37.1875) lr 1.6613e-03 eta 0:25:09
epoch [56/200] batch [15/50] time 0.085 (0.167) data 0.000 (0.083) loss 1.8232 (2.0707) acc 50.0000 (38.1250) lr 1.6613e-03 eta 0:20:11
epoch [56/200] batch [20/50] time 0.084 (0.147) data 0.000 (0.062) loss 1.9902 (2.0723) acc 46.8750 (38.5938) lr 1.6613e-03 eta 0:17:40
epoch [56/200] batch [25/50] time 0.113 (0.138) data 0.030 (0.054) loss 2.2461 (2.0819) acc 34.3750 (38.6250) lr 1.6613e-03 eta 0:16:35
epoch [56/200] batch [30/50] time 0.084 (0.129) data 0.000 (0.045) loss 1.6270 (2.0529) acc 56.2500 (39.4792) lr 1.6613e-03 eta 0:15:30
epoch [56/200] batch [35/50] time 0.085 (0.127) data 0.000 (0.043) loss 1.9961 (2.0639) acc 40.6250 (39.3750) lr 1.6613e-03 eta 0:15:16
epoch [56/200] batch [40/50] time 0.085 (0.122) data 0.000 (0.037) loss 1.7969 (2.0612) acc 46.8750 (39.5312) lr 1.6613e-03 eta 0:14:37
epoch [56/200] batch [45/50] time 0.083 (0.119) data 0.000 (0.034) loss 2.3418 (2.0631) acc 28.1250 (39.3056) lr 1.6613e-03 eta 0:14:14
epoch [56/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.031) loss 2.1562 (2.0779) acc 34.3750 (39.1875) lr 1.6494e-03 eta 0:13:48
epoch [57/200] batch [5/50] time 0.085 (0.323) data 0.000 (0.238) loss 2.0352 (1.9033) acc 25.0000 (40.0000) lr 1.6494e-03 eta 0:38:46
epoch [57/200] batch [10/50] time 0.086 (0.213) data 0.000 (0.127) loss 2.0410 (2.0372) acc 31.2500 (40.0000) lr 1.6494e-03 eta 0:25:28
epoch [57/200] batch [15/50] time 0.086 (0.171) data 0.000 (0.085) loss 2.1484 (2.1077) acc 40.6250 (37.9167) lr 1.6494e-03 eta 0:20:25
epoch [57/200] batch [20/50] time 0.086 (0.159) data 0.000 (0.073) loss 2.0586 (2.1244) acc 46.8750 (37.1875) lr 1.6494e-03 eta 0:18:58
epoch [57/200] batch [25/50] time 0.150 (0.147) data 0.064 (0.061) loss 2.0234 (2.1411) acc 31.2500 (36.2500) lr 1.6494e-03 eta 0:17:31
epoch [57/200] batch [30/50] time 0.084 (0.136) data 0.000 (0.051) loss 2.1621 (2.0939) acc 50.0000 (37.7083) lr 1.6494e-03 eta 0:16:15
epoch [57/200] batch [35/50] time 0.084 (0.135) data 0.000 (0.049) loss 2.5000 (2.0954) acc 31.2500 (37.9464) lr 1.6494e-03 eta 0:16:03
epoch [57/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.043) loss 2.3379 (2.1004) acc 28.1250 (37.5000) lr 1.6494e-03 eta 0:15:17
epoch [57/200] batch [45/50] time 0.084 (0.127) data 0.000 (0.043) loss 2.0703 (2.1109) acc 34.3750 (37.2917) lr 1.6494e-03 eta 0:15:12
epoch [57/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.039) loss 1.7158 (2.0837) acc 46.8750 (38.0000) lr 1.6374e-03 eta 0:14:40
epoch [58/200] batch [5/50] time 0.085 (0.291) data 0.000 (0.207) loss 2.4062 (2.1252) acc 40.6250 (43.1250) lr 1.6374e-03 eta 0:34:42
epoch [58/200] batch [10/50] time 0.085 (0.201) data 0.001 (0.116) loss 1.8652 (2.1251) acc 46.8750 (40.9375) lr 1.6374e-03 eta 0:23:54
epoch [58/200] batch [15/50] time 0.085 (0.162) data 0.001 (0.078) loss 1.8545 (2.0533) acc 40.6250 (40.6250) lr 1.6374e-03 eta 0:19:18
epoch [58/200] batch [20/50] time 0.085 (0.146) data 0.000 (0.062) loss 2.5176 (2.0769) acc 18.7500 (40.0000) lr 1.6374e-03 eta 0:17:24
epoch [58/200] batch [25/50] time 0.164 (0.137) data 0.081 (0.053) loss 2.2930 (2.0703) acc 34.3750 (39.5000) lr 1.6374e-03 eta 0:16:18
epoch [58/200] batch [30/50] time 0.085 (0.129) data 0.000 (0.044) loss 1.7676 (2.0420) acc 34.3750 (39.6875) lr 1.6374e-03 eta 0:15:16
epoch [58/200] batch [35/50] time 0.085 (0.122) data 0.000 (0.038) loss 1.8535 (2.0601) acc 40.6250 (39.1071) lr 1.6374e-03 eta 0:14:31
epoch [58/200] batch [40/50] time 0.083 (0.118) data 0.000 (0.033) loss 1.8799 (2.0323) acc 46.8750 (39.6875) lr 1.6374e-03 eta 0:13:56
epoch [58/200] batch [45/50] time 0.084 (0.114) data 0.000 (0.029) loss 2.0664 (2.0315) acc 37.5000 (39.7917) lr 1.6374e-03 eta 0:13:29
epoch [58/200] batch [50/50] time 0.084 (0.111) data 0.000 (0.027) loss 1.8906 (2.0338) acc 53.1250 (39.9375) lr 1.6252e-03 eta 0:13:06
epoch [59/200] batch [5/50] time 0.085 (0.346) data 0.000 (0.261) loss 2.0742 (2.0447) acc 31.2500 (36.8750) lr 1.6252e-03 eta 0:40:53
epoch [59/200] batch [10/50] time 0.084 (0.218) data 0.000 (0.134) loss 2.0762 (2.0122) acc 37.5000 (40.0000) lr 1.6252e-03 eta 0:25:43
epoch [59/200] batch [15/50] time 0.084 (0.173) data 0.000 (0.089) loss 2.1797 (2.0229) acc 34.3750 (40.0000) lr 1.6252e-03 eta 0:20:27
epoch [59/200] batch [20/50] time 0.084 (0.151) data 0.000 (0.067) loss 2.0352 (2.0333) acc 34.3750 (39.6875) lr 1.6252e-03 eta 0:17:50
epoch [59/200] batch [25/50] time 0.085 (0.138) data 0.000 (0.054) loss 1.9512 (2.0136) acc 50.0000 (41.0000) lr 1.6252e-03 eta 0:16:16
epoch [59/200] batch [30/50] time 0.085 (0.129) data 0.000 (0.045) loss 2.1016 (2.0181) acc 34.3750 (41.0417) lr 1.6252e-03 eta 0:15:13
epoch [59/200] batch [35/50] time 0.084 (0.123) data 0.001 (0.038) loss 2.3027 (2.0505) acc 31.2500 (40.9821) lr 1.6252e-03 eta 0:14:27
epoch [59/200] batch [40/50] time 0.085 (0.118) data 0.000 (0.034) loss 2.0586 (2.0211) acc 34.3750 (41.2500) lr 1.6252e-03 eta 0:13:52
epoch [59/200] batch [45/50] time 0.083 (0.114) data 0.000 (0.030) loss 2.2070 (2.0201) acc 21.8750 (40.9722) lr 1.6252e-03 eta 0:13:25
epoch [59/200] batch [50/50] time 0.084 (0.111) data 0.000 (0.027) loss 1.7754 (2.0419) acc 43.7500 (40.3750) lr 1.6129e-03 eta 0:13:03
epoch [60/200] batch [5/50] time 0.084 (0.289) data 0.000 (0.204) loss 2.0605 (1.7572) acc 21.8750 (44.3750) lr 1.6129e-03 eta 0:33:54
epoch [60/200] batch [10/50] time 0.084 (0.192) data 0.000 (0.108) loss 2.1953 (1.9249) acc 40.6250 (41.5625) lr 1.6129e-03 eta 0:22:33
epoch [60/200] batch [15/50] time 0.084 (0.156) data 0.000 (0.072) loss 2.2109 (1.9530) acc 40.6250 (40.2083) lr 1.6129e-03 eta 0:18:19
epoch [60/200] batch [20/50] time 0.088 (0.145) data 0.001 (0.060) loss 2.5078 (2.0057) acc 34.3750 (39.0625) lr 1.6129e-03 eta 0:17:00
epoch [60/200] batch [25/50] time 0.086 (0.133) data 0.000 (0.048) loss 2.3223 (2.0161) acc 21.8750 (38.8750) lr 1.6129e-03 eta 0:15:37
epoch [60/200] batch [30/50] time 0.132 (0.127) data 0.046 (0.042) loss 2.0918 (2.0152) acc 37.5000 (39.3750) lr 1.6129e-03 eta 0:14:51
epoch [60/200] batch [35/50] time 0.086 (0.121) data 0.001 (0.036) loss 2.1250 (2.0436) acc 40.6250 (38.7500) lr 1.6129e-03 eta 0:14:09
epoch [60/200] batch [40/50] time 0.084 (0.119) data 0.000 (0.034) loss 2.4844 (2.0503) acc 21.8750 (38.5938) lr 1.6129e-03 eta 0:13:57
epoch [60/200] batch [45/50] time 0.084 (0.116) data 0.000 (0.031) loss 2.3184 (2.0735) acc 31.2500 (37.6389) lr 1.6129e-03 eta 0:13:29
epoch [60/200] batch [50/50] time 0.084 (0.115) data 0.000 (0.030) loss 2.3418 (2.0630) acc 31.2500 (38.3125) lr 1.6004e-03 eta 0:13:23
epoch [61/200] batch [5/50] time 0.087 (0.334) data 0.000 (0.248) loss 1.9951 (2.1414) acc 37.5000 (39.3750) lr 1.6004e-03 eta 0:38:56
epoch [61/200] batch [10/50] time 0.085 (0.210) data 0.000 (0.124) loss 1.8027 (2.0995) acc 46.8750 (38.1250) lr 1.6004e-03 eta 0:24:25
epoch [61/200] batch [15/50] time 0.085 (0.177) data 0.000 (0.092) loss 1.9434 (2.0431) acc 43.7500 (39.1667) lr 1.6004e-03 eta 0:20:39
epoch [61/200] batch [20/50] time 0.085 (0.161) data 0.000 (0.076) loss 2.4473 (2.0280) acc 21.8750 (39.2188) lr 1.6004e-03 eta 0:18:45
epoch [61/200] batch [25/50] time 0.084 (0.146) data 0.000 (0.061) loss 2.2559 (2.0599) acc 31.2500 (39.0000) lr 1.6004e-03 eta 0:16:58
epoch [61/200] batch [30/50] time 0.084 (0.140) data 0.000 (0.055) loss 2.0566 (2.0795) acc 28.1250 (38.5417) lr 1.6004e-03 eta 0:16:12
epoch [61/200] batch [35/50] time 0.150 (0.134) data 0.066 (0.049) loss 2.1270 (2.0661) acc 46.8750 (38.9286) lr 1.6004e-03 eta 0:15:30
epoch [61/200] batch [40/50] time 0.084 (0.127) data 0.000 (0.043) loss 2.2773 (2.0756) acc 21.8750 (38.5938) lr 1.6004e-03 eta 0:14:47
epoch [61/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.5449 (2.0732) acc 31.2500 (38.8889) lr 1.6004e-03 eta 0:14:27
epoch [61/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.036) loss 2.7109 (2.0723) acc 31.2500 (39.3125) lr 1.5878e-03 eta 0:13:57
epoch [62/200] batch [5/50] time 0.086 (0.305) data 0.000 (0.220) loss 2.0879 (2.0727) acc 43.7500 (35.6250) lr 1.5878e-03 eta 0:35:20
epoch [62/200] batch [10/50] time 0.086 (0.196) data 0.001 (0.110) loss 2.0645 (2.0801) acc 37.5000 (37.8125) lr 1.5878e-03 eta 0:22:37
epoch [62/200] batch [15/50] time 0.085 (0.166) data 0.000 (0.081) loss 1.9102 (2.0486) acc 37.5000 (37.9167) lr 1.5878e-03 eta 0:19:13
epoch [62/200] batch [20/50] time 0.085 (0.154) data 0.001 (0.069) loss 2.0137 (1.9975) acc 28.1250 (39.5312) lr 1.5878e-03 eta 0:17:47
epoch [62/200] batch [25/50] time 0.087 (0.140) data 0.000 (0.055) loss 2.1816 (2.0216) acc 37.5000 (39.1250) lr 1.5878e-03 eta 0:16:12
epoch [62/200] batch [30/50] time 0.086 (0.134) data 0.000 (0.049) loss 2.3926 (2.0144) acc 34.3750 (39.3750) lr 1.5878e-03 eta 0:15:29
epoch [62/200] batch [35/50] time 0.198 (0.131) data 0.114 (0.045) loss 1.9424 (2.0277) acc 56.2500 (39.0179) lr 1.5878e-03 eta 0:15:03
epoch [62/200] batch [40/50] time 0.085 (0.125) data 0.000 (0.040) loss 1.9463 (2.0466) acc 43.7500 (39.0625) lr 1.5878e-03 eta 0:14:23
epoch [62/200] batch [45/50] time 0.083 (0.122) data 0.000 (0.037) loss 2.1992 (2.0500) acc 40.6250 (38.8194) lr 1.5878e-03 eta 0:14:03
epoch [62/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.033) loss 1.9014 (2.0556) acc 40.6250 (38.9375) lr 1.5750e-03 eta 0:13:36
epoch [63/200] batch [5/50] time 0.087 (0.303) data 0.000 (0.218) loss 2.1289 (2.0268) acc 43.7500 (38.7500) lr 1.5750e-03 eta 0:34:52
epoch [63/200] batch [10/50] time 0.085 (0.194) data 0.001 (0.109) loss 2.1699 (2.0982) acc 46.8750 (39.3750) lr 1.5750e-03 eta 0:22:19
epoch [63/200] batch [15/50] time 0.086 (0.165) data 0.000 (0.079) loss 2.5000 (2.0997) acc 28.1250 (38.9583) lr 1.5750e-03 eta 0:18:53
epoch [63/200] batch [20/50] time 0.086 (0.145) data 0.000 (0.060) loss 2.1719 (2.0635) acc 21.8750 (39.0625) lr 1.5750e-03 eta 0:16:35
epoch [63/200] batch [25/50] time 0.086 (0.138) data 0.000 (0.053) loss 2.5293 (2.0754) acc 31.2500 (39.3750) lr 1.5750e-03 eta 0:15:48
epoch [63/200] batch [30/50] time 0.086 (0.131) data 0.000 (0.046) loss 2.3652 (2.0710) acc 25.0000 (39.6875) lr 1.5750e-03 eta 0:15:03
epoch [63/200] batch [35/50] time 0.085 (0.128) data 0.001 (0.043) loss 2.3184 (2.0884) acc 34.3750 (39.1964) lr 1.5750e-03 eta 0:14:39
epoch [63/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.042) loss 2.1211 (2.0781) acc 28.1250 (39.2969) lr 1.5750e-03 eta 0:14:30
epoch [63/200] batch [45/50] time 0.130 (0.123) data 0.047 (0.038) loss 1.8945 (2.0616) acc 43.7500 (39.2361) lr 1.5750e-03 eta 0:14:02
epoch [63/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.8223 (2.0572) acc 50.0000 (39.6250) lr 1.5621e-03 eta 0:13:42
epoch [64/200] batch [5/50] time 0.564 (0.388) data 0.479 (0.303) loss 1.6250 (1.9602) acc 50.0000 (38.1250) lr 1.5621e-03 eta 0:44:18
epoch [64/200] batch [10/50] time 0.085 (0.237) data 0.000 (0.152) loss 2.3047 (2.0508) acc 43.7500 (40.9375) lr 1.5621e-03 eta 0:27:01
epoch [64/200] batch [15/50] time 0.086 (0.187) data 0.000 (0.101) loss 1.8809 (2.0720) acc 43.7500 (39.5833) lr 1.5621e-03 eta 0:21:15
epoch [64/200] batch [20/50] time 0.084 (0.161) data 0.000 (0.076) loss 1.8994 (2.0339) acc 37.5000 (40.4688) lr 1.5621e-03 eta 0:18:21
epoch [64/200] batch [25/50] time 0.085 (0.151) data 0.000 (0.066) loss 2.0020 (2.0089) acc 37.5000 (40.6250) lr 1.5621e-03 eta 0:17:12
epoch [64/200] batch [30/50] time 0.104 (0.141) data 0.019 (0.056) loss 2.4473 (2.0301) acc 34.3750 (40.4167) lr 1.5621e-03 eta 0:16:01
epoch [64/200] batch [35/50] time 0.086 (0.133) data 0.000 (0.048) loss 2.1113 (2.0229) acc 43.7500 (40.8929) lr 1.5621e-03 eta 0:15:06
epoch [64/200] batch [40/50] time 0.084 (0.130) data 0.000 (0.045) loss 2.4531 (2.0267) acc 25.0000 (40.3125) lr 1.5621e-03 eta 0:14:44
epoch [64/200] batch [45/50] time 0.082 (0.125) data 0.000 (0.040) loss 1.7246 (2.0170) acc 56.2500 (41.0417) lr 1.5621e-03 eta 0:14:08
epoch [64/200] batch [50/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.0391 (2.0366) acc 40.6250 (40.8125) lr 1.5490e-03 eta 0:14:08
epoch [65/200] batch [5/50] time 0.086 (0.326) data 0.000 (0.240) loss 2.3711 (2.0350) acc 37.5000 (37.5000) lr 1.5490e-03 eta 0:36:51
epoch [65/200] batch [10/50] time 0.085 (0.206) data 0.000 (0.120) loss 2.1504 (2.1263) acc 40.6250 (35.0000) lr 1.5490e-03 eta 0:23:15
epoch [65/200] batch [15/50] time 0.087 (0.173) data 0.001 (0.087) loss 1.6006 (2.0812) acc 37.5000 (37.0833) lr 1.5490e-03 eta 0:19:30
epoch [65/200] batch [20/50] time 0.161 (0.155) data 0.077 (0.069) loss 2.1152 (2.0793) acc 34.3750 (38.2812) lr 1.5490e-03 eta 0:17:27
epoch [65/200] batch [25/50] time 0.086 (0.141) data 0.000 (0.055) loss 2.3770 (2.0895) acc 31.2500 (38.3750) lr 1.5490e-03 eta 0:15:53
epoch [65/200] batch [30/50] time 0.085 (0.135) data 0.001 (0.050) loss 1.9473 (2.0778) acc 53.1250 (38.5417) lr 1.5490e-03 eta 0:15:14
epoch [65/200] batch [35/50] time 0.087 (0.128) data 0.001 (0.043) loss 1.6377 (2.0789) acc 46.8750 (38.8393) lr 1.5490e-03 eta 0:14:25
epoch [65/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.3398 (2.0715) acc 31.2500 (38.9062) lr 1.5490e-03 eta 0:14:10
epoch [65/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.9902 (2.0641) acc 43.7500 (38.8194) lr 1.5490e-03 eta 0:14:07
epoch [65/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 2.2656 (2.0614) acc 31.2500 (38.6250) lr 1.5358e-03 eta 0:13:38
epoch [66/200] batch [5/50] time 0.086 (0.331) data 0.000 (0.245) loss 2.0078 (1.9398) acc 43.7500 (46.2500) lr 1.5358e-03 eta 0:37:14
epoch [66/200] batch [10/50] time 0.086 (0.209) data 0.000 (0.123) loss 2.2949 (2.0371) acc 40.6250 (42.5000) lr 1.5358e-03 eta 0:23:28
epoch [66/200] batch [15/50] time 0.084 (0.168) data 0.000 (0.082) loss 2.0098 (2.1013) acc 46.8750 (41.0417) lr 1.5358e-03 eta 0:18:48
epoch [66/200] batch [20/50] time 0.085 (0.152) data 0.001 (0.066) loss 2.0703 (2.0615) acc 43.7500 (41.5625) lr 1.5358e-03 eta 0:17:00
epoch [66/200] batch [25/50] time 0.084 (0.138) data 0.000 (0.053) loss 1.7461 (2.0503) acc 50.0000 (41.5000) lr 1.5358e-03 eta 0:15:29
epoch [66/200] batch [30/50] time 0.084 (0.133) data 0.000 (0.048) loss 2.2715 (2.0731) acc 28.1250 (39.7917) lr 1.5358e-03 eta 0:14:50
epoch [66/200] batch [35/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.0703 (2.0714) acc 31.2500 (39.1071) lr 1.5358e-03 eta 0:14:03
epoch [66/200] batch [40/50] time 0.084 (0.120) data 0.000 (0.036) loss 1.8037 (2.0584) acc 46.8750 (39.5312) lr 1.5358e-03 eta 0:13:27
epoch [66/200] batch [45/50] time 0.084 (0.118) data 0.000 (0.034) loss 1.7832 (2.0530) acc 50.0000 (39.5139) lr 1.5358e-03 eta 0:13:14
epoch [66/200] batch [50/50] time 0.084 (0.115) data 0.000 (0.031) loss 2.0039 (2.0477) acc 31.2500 (39.4375) lr 1.5225e-03 eta 0:12:50
epoch [67/200] batch [5/50] time 0.084 (0.283) data 0.000 (0.198) loss 1.9268 (1.6498) acc 53.1250 (51.8750) lr 1.5225e-03 eta 0:31:35
epoch [67/200] batch [10/50] time 0.121 (0.188) data 0.037 (0.103) loss 2.0371 (1.9247) acc 34.3750 (45.6250) lr 1.5225e-03 eta 0:20:56
epoch [67/200] batch [15/50] time 0.087 (0.157) data 0.000 (0.072) loss 1.8447 (1.9160) acc 46.8750 (44.7917) lr 1.5225e-03 eta 0:17:32
epoch [67/200] batch [20/50] time 0.086 (0.141) data 0.000 (0.056) loss 1.7734 (1.9318) acc 43.7500 (43.5938) lr 1.5225e-03 eta 0:15:42
epoch [67/200] batch [25/50] time 0.087 (0.135) data 0.000 (0.050) loss 2.4492 (1.9802) acc 31.2500 (41.5000) lr 1.5225e-03 eta 0:15:01
epoch [67/200] batch [30/50] time 0.086 (0.131) data 0.001 (0.046) loss 1.8252 (2.0219) acc 40.6250 (39.8958) lr 1.5225e-03 eta 0:14:33
epoch [67/200] batch [35/50] time 0.084 (0.124) data 0.000 (0.039) loss 2.6641 (2.0359) acc 34.3750 (40.3571) lr 1.5225e-03 eta 0:13:49
epoch [67/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.038) loss 1.9795 (2.0327) acc 43.7500 (40.0000) lr 1.5225e-03 eta 0:13:40
epoch [67/200] batch [45/50] time 0.187 (0.121) data 0.104 (0.036) loss 1.8535 (2.0355) acc 43.7500 (40.3472) lr 1.5225e-03 eta 0:13:25
epoch [67/200] batch [50/50] time 0.087 (0.117) data 0.000 (0.032) loss 2.3418 (2.0349) acc 21.8750 (40.3125) lr 1.5090e-03 eta 0:13:00
epoch [68/200] batch [5/50] time 0.085 (0.316) data 0.000 (0.231) loss 2.5723 (2.1064) acc 21.8750 (43.1250) lr 1.5090e-03 eta 0:35:01
epoch [68/200] batch [10/50] time 0.085 (0.201) data 0.000 (0.116) loss 1.9766 (2.1059) acc 40.6250 (38.1250) lr 1.5090e-03 eta 0:22:15
epoch [68/200] batch [15/50] time 0.086 (0.166) data 0.000 (0.081) loss 1.9717 (2.0141) acc 53.1250 (40.6250) lr 1.5090e-03 eta 0:18:20
epoch [68/200] batch [20/50] time 0.086 (0.149) data 0.000 (0.064) loss 2.4453 (2.1057) acc 25.0000 (38.2812) lr 1.5090e-03 eta 0:16:29
epoch [68/200] batch [25/50] time 0.086 (0.140) data 0.000 (0.054) loss 2.3066 (2.0904) acc 37.5000 (38.5000) lr 1.5090e-03 eta 0:15:24
epoch [68/200] batch [30/50] time 0.086 (0.135) data 0.000 (0.050) loss 2.1348 (2.0821) acc 43.7500 (38.2292) lr 1.5090e-03 eta 0:14:54
epoch [68/200] batch [35/50] time 0.084 (0.128) data 0.000 (0.043) loss 2.5586 (2.0992) acc 31.2500 (37.9464) lr 1.5090e-03 eta 0:14:06
epoch [68/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.041) loss 2.1055 (2.0876) acc 34.3750 (37.8125) lr 1.5090e-03 eta 0:13:52
epoch [68/200] batch [45/50] time 0.187 (0.124) data 0.103 (0.039) loss 1.8467 (2.1012) acc 46.8750 (38.1944) lr 1.5090e-03 eta 0:13:36
epoch [68/200] batch [50/50] time 0.084 (0.120) data 0.000 (0.035) loss 1.9062 (2.0902) acc 43.7500 (38.1875) lr 1.4955e-03 eta 0:13:09
epoch [69/200] batch [5/50] time 0.085 (0.306) data 0.000 (0.221) loss 2.1055 (2.0883) acc 40.6250 (38.1250) lr 1.4955e-03 eta 0:33:38
epoch [69/200] batch [10/50] time 0.085 (0.196) data 0.000 (0.110) loss 1.9766 (2.0421) acc 46.8750 (40.0000) lr 1.4955e-03 eta 0:21:29
epoch [69/200] batch [15/50] time 0.086 (0.166) data 0.000 (0.081) loss 2.3398 (2.0830) acc 31.2500 (38.3333) lr 1.4955e-03 eta 0:18:15
epoch [69/200] batch [20/50] time 0.086 (0.150) data 0.000 (0.065) loss 1.8564 (2.0131) acc 37.5000 (39.5312) lr 1.4955e-03 eta 0:16:29
epoch [69/200] batch [25/50] time 0.085 (0.137) data 0.000 (0.052) loss 2.1719 (2.0094) acc 43.7500 (40.5000) lr 1.4955e-03 eta 0:15:03
epoch [69/200] batch [30/50] time 0.087 (0.133) data 0.000 (0.047) loss 1.9131 (1.9961) acc 46.8750 (40.2083) lr 1.4955e-03 eta 0:14:30
epoch [69/200] batch [35/50] time 0.168 (0.128) data 0.083 (0.043) loss 2.6914 (2.0291) acc 15.6250 (39.1964) lr 1.4955e-03 eta 0:14:01
epoch [69/200] batch [40/50] time 0.085 (0.123) data 0.000 (0.038) loss 2.7402 (2.0456) acc 34.3750 (39.2188) lr 1.4955e-03 eta 0:13:26
epoch [69/200] batch [45/50] time 0.082 (0.122) data 0.000 (0.037) loss 2.7188 (2.0726) acc 28.1250 (38.6111) lr 1.4955e-03 eta 0:13:18
epoch [69/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.033) loss 1.9805 (2.0631) acc 31.2500 (38.9375) lr 1.4818e-03 eta 0:12:52
epoch [70/200] batch [5/50] time 0.086 (0.325) data 0.000 (0.240) loss 1.7529 (1.8814) acc 46.8750 (43.7500) lr 1.4818e-03 eta 0:35:30
epoch [70/200] batch [10/50] time 0.084 (0.205) data 0.000 (0.120) loss 2.2285 (1.9248) acc 43.7500 (43.4375) lr 1.4818e-03 eta 0:22:20
epoch [70/200] batch [15/50] time 0.086 (0.165) data 0.000 (0.080) loss 2.2812 (1.9484) acc 46.8750 (42.2917) lr 1.4818e-03 eta 0:17:57
epoch [70/200] batch [20/50] time 0.085 (0.145) data 0.000 (0.060) loss 1.8955 (1.9077) acc 43.7500 (44.0625) lr 1.4818e-03 eta 0:15:45
epoch [70/200] batch [25/50] time 0.084 (0.135) data 0.000 (0.051) loss 1.9238 (1.9508) acc 46.8750 (43.2500) lr 1.4818e-03 eta 0:14:42
epoch [70/200] batch [30/50] time 0.249 (0.132) data 0.166 (0.048) loss 1.9531 (1.9537) acc 34.3750 (42.7083) lr 1.4818e-03 eta 0:14:21
epoch [70/200] batch [35/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.9092 (1.9494) acc 34.3750 (41.8750) lr 1.4818e-03 eta 0:13:35
epoch [70/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.043) loss 2.1836 (1.9851) acc 40.6250 (40.7812) lr 1.4818e-03 eta 0:13:44
epoch [70/200] batch [45/50] time 0.083 (0.122) data 0.000 (0.038) loss 1.9346 (2.0110) acc 46.8750 (40.9028) lr 1.4818e-03 eta 0:13:12
epoch [70/200] batch [50/50] time 0.085 (0.119) data 0.000 (0.035) loss 1.9639 (2.0306) acc 34.3750 (40.3125) lr 1.4679e-03 eta 0:12:54
epoch [71/200] batch [5/50] time 0.085 (0.337) data 0.000 (0.252) loss 1.7998 (1.8461) acc 37.5000 (44.3750) lr 1.4679e-03 eta 0:36:26
epoch [71/200] batch [10/50] time 0.085 (0.211) data 0.000 (0.126) loss 1.8945 (1.8785) acc 37.5000 (44.0625) lr 1.4679e-03 eta 0:22:47
epoch [71/200] batch [15/50] time 0.084 (0.175) data 0.000 (0.090) loss 2.2949 (1.9204) acc 31.2500 (43.3333) lr 1.4679e-03 eta 0:18:54
epoch [71/200] batch [20/50] time 0.231 (0.160) data 0.147 (0.075) loss 2.4023 (2.0083) acc 28.1250 (40.3125) lr 1.4679e-03 eta 0:17:15
epoch [71/200] batch [25/50] time 0.085 (0.145) data 0.001 (0.060) loss 2.3223 (2.0448) acc 37.5000 (39.7500) lr 1.4679e-03 eta 0:15:38
epoch [71/200] batch [30/50] time 0.084 (0.141) data 0.000 (0.057) loss 2.0488 (2.0750) acc 31.2500 (38.8542) lr 1.4679e-03 eta 0:15:12
epoch [71/200] batch [35/50] time 0.085 (0.133) data 0.000 (0.049) loss 2.0586 (2.0458) acc 40.6250 (39.2857) lr 1.4679e-03 eta 0:14:20
epoch [71/200] batch [40/50] time 0.084 (0.130) data 0.000 (0.045) loss 2.3535 (2.0712) acc 18.7500 (38.1250) lr 1.4679e-03 eta 0:13:57
epoch [71/200] batch [45/50] time 0.083 (0.127) data 0.000 (0.043) loss 1.9316 (2.0689) acc 43.7500 (38.1250) lr 1.4679e-03 eta 0:13:38
epoch [71/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.038) loss 1.5098 (2.0461) acc 50.0000 (38.9375) lr 1.4540e-03 eta 0:13:09
epoch [72/200] batch [5/50] time 0.086 (0.332) data 0.001 (0.245) loss 1.9824 (2.1025) acc 50.0000 (43.1250) lr 1.4540e-03 eta 0:35:36
epoch [72/200] batch [10/50] time 0.094 (0.210) data 0.008 (0.124) loss 2.0234 (2.1740) acc 46.8750 (39.0625) lr 1.4540e-03 eta 0:22:30
epoch [72/200] batch [15/50] time 0.085 (0.168) data 0.000 (0.083) loss 1.7168 (2.0978) acc 56.2500 (40.4167) lr 1.4540e-03 eta 0:18:03
epoch [72/200] batch [20/50] time 0.085 (0.148) data 0.000 (0.062) loss 1.8672 (2.0978) acc 34.3750 (38.7500) lr 1.4540e-03 eta 0:15:50
epoch [72/200] batch [25/50] time 0.085 (0.135) data 0.000 (0.050) loss 2.0996 (2.1091) acc 43.7500 (38.6250) lr 1.4540e-03 eta 0:14:28
epoch [72/200] batch [30/50] time 0.318 (0.135) data 0.234 (0.049) loss 1.7129 (2.0442) acc 50.0000 (40.7292) lr 1.4540e-03 eta 0:14:25
epoch [72/200] batch [35/50] time 0.086 (0.128) data 0.001 (0.042) loss 2.2188 (2.0369) acc 50.0000 (40.8036) lr 1.4540e-03 eta 0:13:40
epoch [72/200] batch [40/50] time 0.084 (0.125) data 0.000 (0.040) loss 2.0664 (2.0409) acc 31.2500 (40.2344) lr 1.4540e-03 eta 0:13:23
epoch [72/200] batch [45/50] time 0.085 (0.121) data 0.000 (0.036) loss 2.0586 (2.0533) acc 43.7500 (39.8611) lr 1.4540e-03 eta 0:12:53
epoch [72/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.032) loss 2.4473 (2.0613) acc 28.1250 (39.1250) lr 1.4399e-03 eta 0:12:29
epoch [73/200] batch [5/50] time 0.084 (0.335) data 0.000 (0.250) loss 1.8447 (2.1064) acc 37.5000 (39.3750) lr 1.4399e-03 eta 0:35:42
epoch [73/200] batch [10/50] time 0.084 (0.228) data 0.000 (0.143) loss 2.0625 (1.9841) acc 34.3750 (40.0000) lr 1.4399e-03 eta 0:24:15
epoch [73/200] batch [15/50] time 0.085 (0.180) data 0.000 (0.096) loss 1.7402 (1.9368) acc 56.2500 (42.5000) lr 1.4399e-03 eta 0:19:09
epoch [73/200] batch [20/50] time 0.085 (0.164) data 0.000 (0.080) loss 2.2734 (1.9344) acc 40.6250 (43.2812) lr 1.4399e-03 eta 0:17:26
epoch [73/200] batch [25/50] time 0.085 (0.148) data 0.000 (0.064) loss 1.8037 (1.9454) acc 43.7500 (42.2500) lr 1.4399e-03 eta 0:15:44
epoch [73/200] batch [30/50] time 0.086 (0.138) data 0.000 (0.053) loss 2.0820 (1.9583) acc 37.5000 (42.2917) lr 1.4399e-03 eta 0:14:37
epoch [73/200] batch [35/50] time 0.086 (0.130) data 0.001 (0.046) loss 2.2207 (1.9540) acc 40.6250 (42.6786) lr 1.4399e-03 eta 0:13:49
epoch [73/200] batch [40/50] time 0.084 (0.125) data 0.000 (0.040) loss 2.1719 (1.9912) acc 31.2500 (41.4062) lr 1.4399e-03 eta 0:13:12
epoch [73/200] batch [45/50] time 0.083 (0.122) data 0.000 (0.038) loss 2.0938 (2.0216) acc 28.1250 (40.2083) lr 1.4399e-03 eta 0:12:55
epoch [73/200] batch [50/50] time 0.085 (0.118) data 0.000 (0.034) loss 1.9736 (2.0175) acc 46.8750 (40.3125) lr 1.4258e-03 eta 0:12:30
epoch [74/200] batch [5/50] time 0.083 (0.334) data 0.000 (0.250) loss 1.6924 (1.9076) acc 50.0000 (48.1250) lr 1.4258e-03 eta 0:35:18
epoch [74/200] batch [10/50] time 0.083 (0.222) data 0.000 (0.138) loss 2.1836 (1.9037) acc 37.5000 (46.5625) lr 1.4258e-03 eta 0:23:28
epoch [74/200] batch [15/50] time 0.084 (0.176) data 0.000 (0.092) loss 2.1895 (1.9305) acc 34.3750 (44.3750) lr 1.4258e-03 eta 0:18:34
epoch [74/200] batch [20/50] time 0.084 (0.160) data 0.000 (0.076) loss 2.3535 (2.0083) acc 21.8750 (43.1250) lr 1.4258e-03 eta 0:16:51
epoch [74/200] batch [25/50] time 0.245 (0.151) data 0.162 (0.067) loss 1.9219 (1.9753) acc 37.5000 (42.8750) lr 1.4258e-03 eta 0:15:55
epoch [74/200] batch [30/50] time 0.085 (0.140) data 0.000 (0.056) loss 1.5137 (1.9635) acc 65.6250 (42.6042) lr 1.4258e-03 eta 0:14:44
epoch [74/200] batch [35/50] time 0.085 (0.132) data 0.001 (0.048) loss 2.0371 (1.9784) acc 31.2500 (42.0536) lr 1.4258e-03 eta 0:13:54
epoch [74/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.042) loss 1.6289 (1.9676) acc 59.3750 (42.0312) lr 1.4258e-03 eta 0:13:15
epoch [74/200] batch [45/50] time 0.083 (0.121) data 0.000 (0.038) loss 2.5762 (2.0011) acc 18.7500 (41.4583) lr 1.4258e-03 eta 0:12:45
epoch [74/200] batch [50/50] time 0.084 (0.118) data 0.000 (0.034) loss 1.8486 (1.9992) acc 46.8750 (41.2500) lr 1.4115e-03 eta 0:12:20
epoch [75/200] batch [5/50] time 0.084 (0.308) data 0.000 (0.224) loss 1.9775 (2.1260) acc 40.6250 (36.2500) lr 1.4115e-03 eta 0:32:21
epoch [75/200] batch [10/50] time 0.112 (0.199) data 0.027 (0.115) loss 1.9385 (2.1216) acc 43.7500 (37.5000) lr 1.4115e-03 eta 0:20:52
epoch [75/200] batch [15/50] time 0.083 (0.164) data 0.000 (0.080) loss 1.7910 (2.0669) acc 43.7500 (40.2083) lr 1.4115e-03 eta 0:17:09
epoch [75/200] batch [20/50] time 0.084 (0.153) data 0.000 (0.069) loss 2.2070 (2.0496) acc 40.6250 (40.1562) lr 1.4115e-03 eta 0:16:00
epoch [75/200] batch [25/50] time 0.084 (0.139) data 0.000 (0.055) loss 2.0410 (2.0603) acc 40.6250 (39.3750) lr 1.4115e-03 eta 0:14:33
epoch [75/200] batch [30/50] time 0.085 (0.139) data 0.000 (0.055) loss 2.1016 (2.0617) acc 43.7500 (38.2292) lr 1.4115e-03 eta 0:14:31
epoch [75/200] batch [35/50] time 0.127 (0.135) data 0.043 (0.051) loss 1.8115 (2.0352) acc 46.8750 (38.9286) lr 1.4115e-03 eta 0:14:03
epoch [75/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.044) loss 1.7881 (2.0236) acc 40.6250 (39.0625) lr 1.4115e-03 eta 0:13:23
epoch [75/200] batch [45/50] time 0.084 (0.128) data 0.000 (0.044) loss 1.8301 (2.0150) acc 31.2500 (39.1667) lr 1.4115e-03 eta 0:13:21
epoch [75/200] batch [50/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.2734 (2.0296) acc 31.2500 (38.7500) lr 1.3971e-03 eta 0:12:52
epoch [76/200] batch [5/50] time 0.085 (0.343) data 0.000 (0.257) loss 2.1289 (2.0324) acc 40.6250 (45.0000) lr 1.3971e-03 eta 0:35:39
epoch [76/200] batch [10/50] time 0.186 (0.225) data 0.101 (0.139) loss 1.8594 (2.0139) acc 43.7500 (43.1250) lr 1.3971e-03 eta 0:23:23
epoch [76/200] batch [15/50] time 0.086 (0.179) data 0.000 (0.093) loss 1.8672 (2.0242) acc 43.7500 (42.7083) lr 1.3971e-03 eta 0:18:33
epoch [76/200] batch [20/50] time 0.084 (0.161) data 0.000 (0.075) loss 1.9150 (2.0264) acc 53.1250 (41.7188) lr 1.3971e-03 eta 0:16:42
epoch [76/200] batch [25/50] time 0.084 (0.146) data 0.000 (0.060) loss 1.8242 (2.0104) acc 40.6250 (42.1250) lr 1.3971e-03 eta 0:15:06
epoch [76/200] batch [30/50] time 0.085 (0.140) data 0.000 (0.055) loss 2.5859 (2.0524) acc 21.8750 (40.8333) lr 1.3971e-03 eta 0:14:28
epoch [76/200] batch [35/50] time 0.085 (0.135) data 0.000 (0.050) loss 2.1133 (2.0724) acc 37.5000 (39.7321) lr 1.3971e-03 eta 0:13:57
epoch [76/200] batch [40/50] time 0.084 (0.128) data 0.000 (0.044) loss 2.2422 (2.0616) acc 53.1250 (40.3906) lr 1.3971e-03 eta 0:13:17
epoch [76/200] batch [45/50] time 0.083 (0.127) data 0.000 (0.042) loss 2.0332 (2.0492) acc 34.3750 (40.4167) lr 1.3971e-03 eta 0:13:07
epoch [76/200] batch [50/50] time 0.084 (0.123) data 0.000 (0.038) loss 1.8311 (2.0358) acc 34.3750 (41.0000) lr 1.3827e-03 eta 0:12:40
epoch [77/200] batch [5/50] time 0.084 (0.337) data 0.000 (0.253) loss 1.9033 (1.8887) acc 43.7500 (46.2500) lr 1.3827e-03 eta 0:34:47
epoch [77/200] batch [10/50] time 0.141 (0.216) data 0.057 (0.132) loss 1.8809 (1.9238) acc 37.5000 (44.3750) lr 1.3827e-03 eta 0:22:18
epoch [77/200] batch [15/50] time 0.083 (0.172) data 0.000 (0.088) loss 2.2539 (1.9777) acc 37.5000 (41.4583) lr 1.3827e-03 eta 0:17:43
epoch [77/200] batch [20/50] time 0.086 (0.160) data 0.000 (0.076) loss 2.1367 (1.9702) acc 40.6250 (42.0312) lr 1.3827e-03 eta 0:16:28
epoch [77/200] batch [25/50] time 0.084 (0.145) data 0.000 (0.061) loss 1.8643 (1.9809) acc 50.0000 (41.7500) lr 1.3827e-03 eta 0:14:54
epoch [77/200] batch [30/50] time 0.084 (0.138) data 0.000 (0.054) loss 2.0762 (1.9893) acc 37.5000 (41.8750) lr 1.3827e-03 eta 0:14:11
epoch [77/200] batch [35/50] time 0.084 (0.134) data 0.000 (0.050) loss 2.1777 (1.9742) acc 34.3750 (41.8750) lr 1.3827e-03 eta 0:13:44
epoch [77/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.044) loss 2.2148 (1.9935) acc 28.1250 (41.1719) lr 1.3827e-03 eta 0:13:05
epoch [77/200] batch [45/50] time 0.084 (0.126) data 0.000 (0.042) loss 2.0859 (1.9976) acc 53.1250 (41.5972) lr 1.3827e-03 eta 0:12:53
epoch [77/200] batch [50/50] time 0.084 (0.122) data 0.000 (0.038) loss 2.1973 (2.0034) acc 40.6250 (41.5625) lr 1.3681e-03 eta 0:12:27
epoch [78/200] batch [5/50] time 0.084 (0.342) data 0.000 (0.258) loss 2.1328 (2.1344) acc 34.3750 (38.1250) lr 1.3681e-03 eta 0:35:03
epoch [78/200] batch [10/50] time 0.084 (0.225) data 0.000 (0.141) loss 2.3809 (2.0642) acc 21.8750 (39.6875) lr 1.3681e-03 eta 0:23:02
epoch [78/200] batch [15/50] time 0.084 (0.178) data 0.000 (0.094) loss 2.0332 (2.0394) acc 50.0000 (41.2500) lr 1.3681e-03 eta 0:18:12
epoch [78/200] batch [20/50] time 0.086 (0.161) data 0.000 (0.077) loss 1.7168 (1.9978) acc 40.6250 (42.8125) lr 1.3681e-03 eta 0:16:28
epoch [78/200] batch [25/50] time 0.259 (0.153) data 0.173 (0.068) loss 1.6807 (1.9881) acc 37.5000 (41.8750) lr 1.3681e-03 eta 0:15:37
epoch [78/200] batch [30/50] time 0.086 (0.142) data 0.000 (0.057) loss 2.0742 (2.0029) acc 34.3750 (40.5208) lr 1.3681e-03 eta 0:14:26
epoch [78/200] batch [35/50] time 0.086 (0.138) data 0.001 (0.053) loss 2.0605 (2.0152) acc 37.5000 (40.1786) lr 1.3681e-03 eta 0:14:02
epoch [78/200] batch [40/50] time 0.084 (0.131) data 0.000 (0.047) loss 1.6943 (2.0123) acc 40.6250 (39.8438) lr 1.3681e-03 eta 0:13:20
epoch [78/200] batch [45/50] time 0.083 (0.129) data 0.000 (0.045) loss 1.8232 (2.0064) acc 50.0000 (40.1389) lr 1.3681e-03 eta 0:13:07
epoch [78/200] batch [50/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.0293 (1.9991) acc 34.3750 (40.3750) lr 1.3535e-03 eta 0:12:39
epoch [79/200] batch [5/50] time 0.087 (0.326) data 0.000 (0.239) loss 2.2637 (2.1361) acc 28.1250 (40.6250) lr 1.3535e-03 eta 0:33:04
epoch [79/200] batch [10/50] time 0.109 (0.208) data 0.024 (0.122) loss 1.6338 (2.1091) acc 56.2500 (42.1875) lr 1.3535e-03 eta 0:21:07
epoch [79/200] batch [15/50] time 0.084 (0.168) data 0.000 (0.082) loss 1.9883 (2.0814) acc 37.5000 (40.0000) lr 1.3535e-03 eta 0:17:01
epoch [79/200] batch [20/50] time 0.176 (0.152) data 0.093 (0.067) loss 2.1035 (2.1033) acc 21.8750 (39.2188) lr 1.3535e-03 eta 0:15:23
epoch [79/200] batch [25/50] time 0.084 (0.138) data 0.000 (0.053) loss 1.7266 (2.0597) acc 59.3750 (41.7500) lr 1.3535e-03 eta 0:14:00
epoch [79/200] batch [30/50] time 0.083 (0.136) data 0.000 (0.051) loss 2.1133 (2.0304) acc 37.5000 (41.5625) lr 1.3535e-03 eta 0:13:43
epoch [79/200] batch [35/50] time 0.084 (0.128) data 0.000 (0.044) loss 1.9727 (2.0070) acc 53.1250 (42.0536) lr 1.3535e-03 eta 0:12:58
epoch [79/200] batch [40/50] time 0.083 (0.125) data 0.000 (0.041) loss 2.2754 (2.0195) acc 31.2500 (41.6406) lr 1.3535e-03 eta 0:12:40
epoch [79/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.8105 (2.0104) acc 34.3750 (41.6667) lr 1.3535e-03 eta 0:12:37
epoch [79/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.7227 (1.9957) acc 53.1250 (41.6875) lr 1.3387e-03 eta 0:12:11
epoch [80/200] batch [5/50] time 0.088 (0.328) data 0.001 (0.242) loss 1.8564 (1.8709) acc 43.7500 (43.1250) lr 1.3387e-03 eta 0:33:03
epoch [80/200] batch [10/50] time 0.208 (0.219) data 0.124 (0.134) loss 2.3203 (1.9937) acc 34.3750 (40.9375) lr 1.3387e-03 eta 0:22:04
epoch [80/200] batch [15/50] time 0.086 (0.175) data 0.000 (0.089) loss 1.7939 (1.9613) acc 43.7500 (42.2917) lr 1.3387e-03 eta 0:17:35
epoch [80/200] batch [20/50] time 0.085 (0.159) data 0.000 (0.074) loss 1.9258 (1.9673) acc 34.3750 (41.8750) lr 1.3387e-03 eta 0:16:00
epoch [80/200] batch [25/50] time 0.086 (0.145) data 0.000 (0.059) loss 2.1582 (1.9871) acc 34.3750 (42.0000) lr 1.3387e-03 eta 0:14:31
epoch [80/200] batch [30/50] time 0.086 (0.146) data 0.000 (0.061) loss 1.6172 (2.0206) acc 43.7500 (41.2500) lr 1.3387e-03 eta 0:14:39
epoch [80/200] batch [35/50] time 0.084 (0.138) data 0.000 (0.052) loss 2.0742 (2.0184) acc 37.5000 (41.1607) lr 1.3387e-03 eta 0:13:47
epoch [80/200] batch [40/50] time 0.084 (0.131) data 0.000 (0.046) loss 2.3398 (2.0384) acc 21.8750 (40.2344) lr 1.3387e-03 eta 0:13:06
epoch [80/200] batch [45/50] time 0.094 (0.126) data 0.009 (0.041) loss 2.0254 (2.0385) acc 34.3750 (39.5139) lr 1.3387e-03 eta 0:12:35
epoch [80/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.037) loss 2.1406 (2.0270) acc 28.1250 (39.4375) lr 1.3239e-03 eta 0:12:09
epoch [81/200] batch [5/50] time 0.085 (0.346) data 0.000 (0.260) loss 1.6123 (1.9654) acc 50.0000 (43.7500) lr 1.3239e-03 eta 0:34:32
epoch [81/200] batch [10/50] time 0.085 (0.236) data 0.000 (0.151) loss 1.9189 (1.9343) acc 34.3750 (43.4375) lr 1.3239e-03 eta 0:23:31
epoch [81/200] batch [15/50] time 0.084 (0.185) data 0.000 (0.101) loss 2.0859 (2.0052) acc 46.8750 (42.7083) lr 1.3239e-03 eta 0:18:28
epoch [81/200] batch [20/50] time 0.084 (0.165) data 0.001 (0.081) loss 2.0605 (1.9687) acc 43.7500 (43.7500) lr 1.3239e-03 eta 0:16:27
epoch [81/200] batch [25/50] time 0.262 (0.156) data 0.178 (0.072) loss 2.0059 (1.9838) acc 31.2500 (42.7500) lr 1.3239e-03 eta 0:15:32
epoch [81/200] batch [30/50] time 0.084 (0.144) data 0.000 (0.060) loss 2.3906 (1.9684) acc 31.2500 (42.8125) lr 1.3239e-03 eta 0:14:21
epoch [81/200] batch [35/50] time 0.084 (0.139) data 0.000 (0.055) loss 2.4141 (1.9799) acc 37.5000 (42.6786) lr 1.3239e-03 eta 0:13:51
epoch [81/200] batch [40/50] time 0.084 (0.133) data 0.000 (0.048) loss 2.4121 (1.9912) acc 31.2500 (42.0312) lr 1.3239e-03 eta 0:13:09
epoch [81/200] batch [45/50] time 0.083 (0.128) data 0.000 (0.044) loss 1.6523 (1.9865) acc 56.2500 (42.5694) lr 1.3239e-03 eta 0:12:42
epoch [81/200] batch [50/50] time 0.083 (0.124) data 0.000 (0.039) loss 2.9629 (2.0241) acc 18.7500 (42.0625) lr 1.3090e-03 eta 0:12:14
epoch [82/200] batch [5/50] time 0.086 (0.289) data 0.001 (0.203) loss 2.0762 (2.0641) acc 34.3750 (36.2500) lr 1.3090e-03 eta 0:28:36
epoch [82/200] batch [10/50] time 0.086 (0.192) data 0.000 (0.106) loss 2.3730 (2.1222) acc 31.2500 (35.6250) lr 1.3090e-03 eta 0:19:01
epoch [82/200] batch [15/50] time 0.087 (0.163) data 0.000 (0.078) loss 2.1309 (2.1206) acc 53.1250 (38.7500) lr 1.3090e-03 eta 0:16:10
epoch [82/200] batch [20/50] time 0.258 (0.153) data 0.171 (0.067) loss 1.6533 (2.0529) acc 46.8750 (39.2188) lr 1.3090e-03 eta 0:15:04
epoch [82/200] batch [25/50] time 0.085 (0.139) data 0.000 (0.054) loss 1.8564 (2.0480) acc 37.5000 (39.6250) lr 1.3090e-03 eta 0:13:44
epoch [82/200] batch [30/50] time 0.085 (0.136) data 0.000 (0.051) loss 2.0566 (2.0499) acc 43.7500 (40.1042) lr 1.3090e-03 eta 0:13:26
epoch [82/200] batch [35/50] time 0.085 (0.129) data 0.001 (0.044) loss 2.2188 (2.0717) acc 25.0000 (39.1964) lr 1.3090e-03 eta 0:12:43
epoch [82/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.043) loss 1.7100 (2.0500) acc 43.7500 (39.9219) lr 1.3090e-03 eta 0:12:38
epoch [82/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.041) loss 2.0957 (2.0264) acc 34.3750 (40.1389) lr 1.3090e-03 eta 0:12:23
epoch [82/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.037) loss 1.7158 (1.9969) acc 53.1250 (41.1250) lr 1.2940e-03 eta 0:11:57
epoch [83/200] batch [5/50] time 0.085 (0.340) data 0.000 (0.255) loss 2.2695 (1.9602) acc 34.3750 (41.8750) lr 1.2940e-03 eta 0:33:25
epoch [83/200] batch [10/50] time 0.084 (0.212) data 0.000 (0.128) loss 2.4922 (2.0645) acc 28.1250 (39.3750) lr 1.2940e-03 eta 0:20:50
epoch [83/200] batch [15/50] time 0.083 (0.169) data 0.000 (0.085) loss 2.1367 (2.0445) acc 28.1250 (39.7917) lr 1.2940e-03 eta 0:16:36
epoch [83/200] batch [20/50] time 0.085 (0.152) data 0.000 (0.068) loss 1.6094 (2.0718) acc 59.3750 (39.6875) lr 1.2940e-03 eta 0:14:52
epoch [83/200] batch [25/50] time 0.085 (0.138) data 0.000 (0.054) loss 2.3164 (2.0636) acc 34.3750 (40.6250) lr 1.2940e-03 eta 0:13:32
epoch [83/200] batch [30/50] time 0.084 (0.133) data 0.000 (0.049) loss 2.1973 (2.0555) acc 34.3750 (40.8333) lr 1.2940e-03 eta 0:12:59
epoch [83/200] batch [35/50] time 0.288 (0.132) data 0.207 (0.048) loss 2.2168 (2.0650) acc 31.2500 (40.6250) lr 1.2940e-03 eta 0:12:52
epoch [83/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.042) loss 2.6289 (2.0916) acc 37.5000 (40.6250) lr 1.2940e-03 eta 0:12:16
epoch [83/200] batch [45/50] time 0.084 (0.126) data 0.000 (0.042) loss 2.0156 (2.0696) acc 37.5000 (40.7639) lr 1.2940e-03 eta 0:12:17
epoch [83/200] batch [50/50] time 0.084 (0.122) data 0.000 (0.038) loss 1.7910 (2.0702) acc 56.2500 (41.3125) lr 1.2790e-03 eta 0:11:51
epoch [84/200] batch [5/50] time 0.085 (0.357) data 0.000 (0.271) loss 1.7734 (1.7756) acc 53.1250 (51.2500) lr 1.2790e-03 eta 0:34:44
epoch [84/200] batch [10/50] time 0.086 (0.224) data 0.000 (0.139) loss 2.1387 (1.8892) acc 37.5000 (47.1875) lr 1.2790e-03 eta 0:21:46
epoch [84/200] batch [15/50] time 0.086 (0.178) data 0.000 (0.093) loss 1.9971 (1.9110) acc 37.5000 (45.6250) lr 1.2790e-03 eta 0:17:16
epoch [84/200] batch [20/50] time 0.086 (0.165) data 0.000 (0.080) loss 2.2637 (1.9583) acc 43.7500 (44.5312) lr 1.2790e-03 eta 0:16:03
epoch [84/200] batch [25/50] time 0.267 (0.156) data 0.184 (0.072) loss 1.9873 (1.9520) acc 40.6250 (44.1250) lr 1.2790e-03 eta 0:15:11
epoch [84/200] batch [30/50] time 0.085 (0.145) data 0.000 (0.060) loss 2.0176 (1.9722) acc 43.7500 (43.0208) lr 1.2790e-03 eta 0:14:01
epoch [84/200] batch [35/50] time 0.085 (0.139) data 0.001 (0.055) loss 2.1602 (1.9662) acc 34.3750 (42.9464) lr 1.2790e-03 eta 0:13:31
epoch [84/200] batch [40/50] time 0.083 (0.133) data 0.000 (0.048) loss 2.0996 (1.9858) acc 40.6250 (41.7188) lr 1.2790e-03 eta 0:12:50
epoch [84/200] batch [45/50] time 0.084 (0.130) data 0.000 (0.045) loss 1.7695 (1.9808) acc 34.3750 (41.1806) lr 1.2790e-03 eta 0:12:33
epoch [84/200] batch [50/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.7939 (1.9852) acc 53.1250 (41.3750) lr 1.2639e-03 eta 0:12:05
epoch [85/200] batch [5/50] time 0.086 (0.325) data 0.000 (0.239) loss 1.8105 (1.9590) acc 37.5000 (40.6250) lr 1.2639e-03 eta 0:31:24
epoch [85/200] batch [10/50] time 0.088 (0.206) data 0.001 (0.120) loss 1.9668 (2.0729) acc 31.2500 (37.8125) lr 1.2639e-03 eta 0:19:52
epoch [85/200] batch [15/50] time 0.086 (0.166) data 0.000 (0.080) loss 2.0156 (2.0583) acc 40.6250 (37.9167) lr 1.2639e-03 eta 0:16:00
epoch [85/200] batch [20/50] time 0.085 (0.150) data 0.000 (0.065) loss 1.9072 (2.0324) acc 40.6250 (39.2188) lr 1.2639e-03 eta 0:14:29
epoch [85/200] batch [25/50] time 0.299 (0.146) data 0.216 (0.060) loss 1.5996 (2.0067) acc 50.0000 (39.1250) lr 1.2639e-03 eta 0:14:04
epoch [85/200] batch [30/50] time 0.085 (0.136) data 0.000 (0.050) loss 2.2500 (2.0237) acc 37.5000 (38.8542) lr 1.2639e-03 eta 0:13:04
epoch [85/200] batch [35/50] time 0.085 (0.129) data 0.001 (0.043) loss 2.1836 (2.0165) acc 34.3750 (39.1964) lr 1.2639e-03 eta 0:12:21
epoch [85/200] batch [40/50] time 0.085 (0.123) data 0.000 (0.038) loss 2.4492 (2.0510) acc 28.1250 (38.6719) lr 1.2639e-03 eta 0:11:49
epoch [85/200] batch [45/50] time 0.083 (0.119) data 0.000 (0.034) loss 1.7715 (2.0287) acc 40.6250 (39.5833) lr 1.2639e-03 eta 0:11:24
epoch [85/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.030) loss 2.1484 (2.0246) acc 46.8750 (39.8750) lr 1.2487e-03 eta 0:11:03
epoch [86/200] batch [5/50] time 0.084 (0.305) data 0.000 (0.219) loss 1.6045 (1.9604) acc 56.2500 (43.7500) lr 1.2487e-03 eta 0:29:11
epoch [86/200] batch [10/50] time 0.255 (0.212) data 0.171 (0.127) loss 1.9375 (2.0009) acc 40.6250 (42.1875) lr 1.2487e-03 eta 0:20:16
epoch [86/200] batch [15/50] time 0.084 (0.170) data 0.000 (0.085) loss 1.6113 (1.9752) acc 53.1250 (41.4583) lr 1.2487e-03 eta 0:16:14
epoch [86/200] batch [20/50] time 0.085 (0.155) data 0.000 (0.070) loss 2.3984 (1.9916) acc 34.3750 (42.1875) lr 1.2487e-03 eta 0:14:50
epoch [86/200] batch [25/50] time 0.084 (0.141) data 0.000 (0.056) loss 2.2188 (1.9837) acc 31.2500 (43.0000) lr 1.2487e-03 eta 0:13:29
epoch [86/200] batch [30/50] time 0.085 (0.138) data 0.000 (0.053) loss 1.7129 (1.9856) acc 46.8750 (43.0208) lr 1.2487e-03 eta 0:13:09
epoch [86/200] batch [35/50] time 0.083 (0.136) data 0.000 (0.051) loss 2.0977 (1.9611) acc 40.6250 (43.6607) lr 1.2487e-03 eta 0:12:55
epoch [86/200] batch [40/50] time 0.083 (0.129) data 0.000 (0.045) loss 1.6982 (1.9713) acc 40.6250 (43.2031) lr 1.2487e-03 eta 0:12:17
epoch [86/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.042) loss 2.8301 (1.9990) acc 25.0000 (42.4306) lr 1.2487e-03 eta 0:11:59
epoch [86/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.037) loss 2.1680 (1.9858) acc 34.3750 (42.9375) lr 1.2334e-03 eta 0:11:34
epoch [87/200] batch [5/50] time 0.084 (0.403) data 0.000 (0.317) loss 1.9688 (1.9018) acc 46.8750 (44.3750) lr 1.2334e-03 eta 0:38:13
epoch [87/200] batch [10/50] time 0.084 (0.243) data 0.000 (0.159) loss 1.9512 (1.9789) acc 31.2500 (41.8750) lr 1.2334e-03 eta 0:23:04
epoch [87/200] batch [15/50] time 0.084 (0.197) data 0.000 (0.113) loss 1.7012 (1.9540) acc 43.7500 (41.2500) lr 1.2334e-03 eta 0:18:40
epoch [87/200] batch [20/50] time 0.272 (0.178) data 0.185 (0.094) loss 1.9043 (1.9271) acc 37.5000 (41.2500) lr 1.2334e-03 eta 0:16:52
epoch [87/200] batch [25/50] time 0.085 (0.160) data 0.000 (0.075) loss 1.9707 (1.9578) acc 37.5000 (40.3750) lr 1.2334e-03 eta 0:15:05
epoch [87/200] batch [30/50] time 0.084 (0.150) data 0.000 (0.065) loss 2.2168 (1.9715) acc 40.6250 (40.3125) lr 1.2334e-03 eta 0:14:09
epoch [87/200] batch [35/50] time 0.085 (0.141) data 0.000 (0.056) loss 1.8154 (1.9842) acc 40.6250 (39.4643) lr 1.2334e-03 eta 0:13:15
epoch [87/200] batch [40/50] time 0.083 (0.136) data 0.000 (0.052) loss 1.8477 (2.0077) acc 37.5000 (39.1406) lr 1.2334e-03 eta 0:12:52
epoch [87/200] batch [45/50] time 0.084 (0.133) data 0.000 (0.048) loss 1.9668 (2.0171) acc 46.8750 (39.0278) lr 1.2334e-03 eta 0:12:29
epoch [87/200] batch [50/50] time 0.083 (0.128) data 0.000 (0.044) loss 1.9873 (2.0217) acc 43.7500 (39.5000) lr 1.2181e-03 eta 0:12:01
epoch [88/200] batch [5/50] time 0.083 (0.319) data 0.000 (0.236) loss 2.0176 (2.0166) acc 46.8750 (41.2500) lr 1.2181e-03 eta 0:30:03
epoch [88/200] batch [10/50] time 0.340 (0.227) data 0.256 (0.144) loss 1.7627 (1.9288) acc 37.5000 (43.1250) lr 1.2181e-03 eta 0:21:22
epoch [88/200] batch [15/50] time 0.086 (0.184) data 0.001 (0.100) loss 2.7090 (1.9725) acc 15.6250 (40.8333) lr 1.2181e-03 eta 0:17:15
epoch [88/200] batch [20/50] time 0.084 (0.168) data 0.000 (0.084) loss 1.9258 (2.0378) acc 37.5000 (39.0625) lr 1.2181e-03 eta 0:15:46
epoch [88/200] batch [25/50] time 0.085 (0.152) data 0.000 (0.067) loss 1.6748 (2.0365) acc 59.3750 (39.2500) lr 1.2181e-03 eta 0:14:13
epoch [88/200] batch [30/50] time 0.086 (0.147) data 0.000 (0.062) loss 1.6963 (2.0306) acc 43.7500 (39.4792) lr 1.2181e-03 eta 0:13:43
epoch [88/200] batch [35/50] time 0.177 (0.140) data 0.093 (0.056) loss 2.0566 (2.0104) acc 40.6250 (40.2679) lr 1.2181e-03 eta 0:13:07
epoch [88/200] batch [40/50] time 0.084 (0.133) data 0.000 (0.049) loss 2.5488 (2.0123) acc 28.1250 (40.4688) lr 1.2181e-03 eta 0:12:27
epoch [88/200] batch [45/50] time 0.083 (0.131) data 0.000 (0.047) loss 1.7822 (2.0099) acc 53.1250 (40.8333) lr 1.2181e-03 eta 0:12:17
epoch [88/200] batch [50/50] time 0.083 (0.127) data 0.000 (0.043) loss 2.0859 (2.0159) acc 31.2500 (40.2500) lr 1.2028e-03 eta 0:11:48
epoch [89/200] batch [5/50] time 0.086 (0.318) data 0.000 (0.233) loss 1.9385 (1.9662) acc 37.5000 (46.2500) lr 1.2028e-03 eta 0:29:37
epoch [89/200] batch [10/50] time 0.084 (0.202) data 0.000 (0.117) loss 2.1953 (2.0058) acc 37.5000 (41.5625) lr 1.2028e-03 eta 0:18:47
epoch [89/200] batch [15/50] time 0.086 (0.169) data 0.000 (0.084) loss 1.6807 (1.9267) acc 46.8750 (42.5000) lr 1.2028e-03 eta 0:15:46
epoch [89/200] batch [20/50] time 0.270 (0.158) data 0.185 (0.073) loss 2.1055 (1.9235) acc 46.8750 (42.6562) lr 1.2028e-03 eta 0:14:40
epoch [89/200] batch [25/50] time 0.085 (0.143) data 0.000 (0.058) loss 2.4199 (1.9620) acc 28.1250 (42.3750) lr 1.2028e-03 eta 0:13:18
epoch [89/200] batch [30/50] time 0.085 (0.137) data 0.000 (0.052) loss 2.1992 (1.9976) acc 34.3750 (41.6667) lr 1.2028e-03 eta 0:12:42
epoch [89/200] batch [35/50] time 0.086 (0.130) data 0.000 (0.044) loss 1.7061 (1.9775) acc 43.7500 (41.2500) lr 1.2028e-03 eta 0:12:01
epoch [89/200] batch [40/50] time 0.084 (0.129) data 0.000 (0.044) loss 2.1387 (1.9754) acc 37.5000 (41.0938) lr 1.2028e-03 eta 0:11:55
epoch [89/200] batch [45/50] time 0.175 (0.126) data 0.092 (0.041) loss 2.5098 (1.9749) acc 37.5000 (41.5972) lr 1.2028e-03 eta 0:11:38
epoch [89/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 2.0762 (1.9825) acc 34.3750 (41.4375) lr 1.1874e-03 eta 0:11:13
epoch [90/200] batch [5/50] time 0.085 (0.337) data 0.000 (0.252) loss 1.8643 (1.8961) acc 37.5000 (46.2500) lr 1.1874e-03 eta 0:31:07
epoch [90/200] batch [10/50] time 0.085 (0.237) data 0.000 (0.153) loss 2.6035 (1.9787) acc 40.6250 (45.6250) lr 1.1874e-03 eta 0:21:55
epoch [90/200] batch [15/50] time 0.084 (0.186) data 0.000 (0.102) loss 1.6445 (1.9622) acc 56.2500 (43.3333) lr 1.1874e-03 eta 0:17:10
epoch [90/200] batch [20/50] time 0.085 (0.165) data 0.000 (0.080) loss 1.8330 (2.0118) acc 50.0000 (41.2500) lr 1.1874e-03 eta 0:15:09
epoch [90/200] batch [25/50] time 0.085 (0.149) data 0.000 (0.064) loss 2.3398 (2.0189) acc 31.2500 (40.2500) lr 1.1874e-03 eta 0:13:41
epoch [90/200] batch [30/50] time 0.086 (0.138) data 0.000 (0.054) loss 2.0078 (2.0210) acc 37.5000 (40.5208) lr 1.1874e-03 eta 0:12:42
epoch [90/200] batch [35/50] time 0.085 (0.131) data 0.001 (0.046) loss 1.9141 (2.0146) acc 34.3750 (40.4464) lr 1.1874e-03 eta 0:12:00
epoch [90/200] batch [40/50] time 0.085 (0.125) data 0.000 (0.040) loss 1.9961 (2.0155) acc 40.6250 (40.3906) lr 1.1874e-03 eta 0:11:28
epoch [90/200] batch [45/50] time 0.084 (0.122) data 0.000 (0.038) loss 2.4141 (2.0223) acc 21.8750 (40.4167) lr 1.1874e-03 eta 0:11:13
epoch [90/200] batch [50/50] time 0.084 (0.118) data 0.000 (0.034) loss 1.6934 (2.0288) acc 50.0000 (40.8125) lr 1.1719e-03 eta 0:10:51
epoch [91/200] batch [5/50] time 0.085 (0.306) data 0.000 (0.222) loss 2.2246 (2.2115) acc 40.6250 (33.7500) lr 1.1719e-03 eta 0:28:01
epoch [91/200] batch [10/50] time 0.084 (0.195) data 0.000 (0.111) loss 1.7002 (2.1045) acc 46.8750 (35.9375) lr 1.1719e-03 eta 0:17:50
epoch [91/200] batch [15/50] time 0.084 (0.163) data 0.000 (0.079) loss 1.9160 (2.0633) acc 56.2500 (39.1667) lr 1.1719e-03 eta 0:14:51
epoch [91/200] batch [20/50] time 0.201 (0.150) data 0.117 (0.066) loss 2.0820 (2.0461) acc 34.3750 (38.5938) lr 1.1719e-03 eta 0:13:40
epoch [91/200] batch [25/50] time 0.085 (0.137) data 0.000 (0.053) loss 1.7129 (2.0105) acc 43.7500 (40.2500) lr 1.1719e-03 eta 0:12:27
epoch [91/200] batch [30/50] time 0.084 (0.135) data 0.000 (0.051) loss 1.9453 (2.0061) acc 43.7500 (40.9375) lr 1.1719e-03 eta 0:12:19
epoch [91/200] batch [35/50] time 0.084 (0.128) data 0.000 (0.044) loss 2.1719 (2.0027) acc 37.5000 (41.3393) lr 1.1719e-03 eta 0:11:38
epoch [91/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.042) loss 2.1797 (1.9946) acc 37.5000 (41.7188) lr 1.1719e-03 eta 0:11:27
epoch [91/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 2.1133 (2.0023) acc 21.8750 (41.0417) lr 1.1719e-03 eta 0:11:19
epoch [91/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.037) loss 1.6221 (2.0072) acc 46.8750 (41.2500) lr 1.1564e-03 eta 0:10:55
epoch [92/200] batch [5/50] time 0.084 (0.344) data 0.000 (0.259) loss 2.1191 (2.0314) acc 46.8750 (41.8750) lr 1.1564e-03 eta 0:31:10
epoch [92/200] batch [10/50] time 0.084 (0.224) data 0.000 (0.139) loss 1.6680 (1.9291) acc 56.2500 (44.3750) lr 1.1564e-03 eta 0:20:17
epoch [92/200] batch [15/50] time 0.085 (0.177) data 0.000 (0.093) loss 1.8008 (1.9964) acc 40.6250 (42.9167) lr 1.1564e-03 eta 0:16:04
epoch [92/200] batch [20/50] time 0.097 (0.155) data 0.012 (0.071) loss 2.1035 (1.9717) acc 34.3750 (42.5000) lr 1.1564e-03 eta 0:14:01
epoch [92/200] batch [25/50] time 0.085 (0.141) data 0.001 (0.056) loss 1.8896 (1.9606) acc 37.5000 (43.2500) lr 1.1564e-03 eta 0:12:44
epoch [92/200] batch [30/50] time 0.086 (0.132) data 0.000 (0.047) loss 2.4141 (1.9761) acc 34.3750 (43.3333) lr 1.1564e-03 eta 0:11:53
epoch [92/200] batch [35/50] time 0.085 (0.125) data 0.001 (0.040) loss 1.8506 (1.9650) acc 43.7500 (42.8571) lr 1.1564e-03 eta 0:11:17
epoch [92/200] batch [40/50] time 0.085 (0.120) data 0.000 (0.035) loss 1.9648 (1.9721) acc 53.1250 (42.8125) lr 1.1564e-03 eta 0:10:49
epoch [92/200] batch [45/50] time 0.083 (0.116) data 0.000 (0.032) loss 1.8672 (1.9613) acc 40.6250 (43.0556) lr 1.1564e-03 eta 0:10:27
epoch [92/200] batch [50/50] time 0.083 (0.113) data 0.000 (0.028) loss 2.1465 (1.9897) acc 40.6250 (42.0000) lr 1.1409e-03 eta 0:10:08
epoch [93/200] batch [5/50] time 0.085 (0.325) data 0.001 (0.238) loss 2.2246 (1.9475) acc 43.7500 (49.3750) lr 1.1409e-03 eta 0:29:11
epoch [93/200] batch [10/50] time 0.197 (0.216) data 0.113 (0.131) loss 1.8848 (1.8972) acc 37.5000 (46.2500) lr 1.1409e-03 eta 0:19:25
epoch [93/200] batch [15/50] time 0.085 (0.173) data 0.001 (0.087) loss 2.2285 (1.9467) acc 31.2500 (45.0000) lr 1.1409e-03 eta 0:15:29
epoch [93/200] batch [20/50] time 0.087 (0.156) data 0.000 (0.071) loss 1.7822 (1.9581) acc 46.8750 (44.3750) lr 1.1409e-03 eta 0:14:01
epoch [93/200] batch [25/50] time 0.086 (0.142) data 0.000 (0.057) loss 1.7295 (1.9554) acc 37.5000 (43.1250) lr 1.1409e-03 eta 0:12:44
epoch [93/200] batch [30/50] time 0.085 (0.138) data 0.000 (0.053) loss 2.0117 (1.9592) acc 46.8750 (43.2292) lr 1.1409e-03 eta 0:12:22
epoch [93/200] batch [35/50] time 0.085 (0.135) data 0.001 (0.050) loss 2.2969 (1.9848) acc 34.3750 (42.7679) lr 1.1409e-03 eta 0:12:03
epoch [93/200] batch [40/50] time 0.082 (0.128) data 0.000 (0.044) loss 1.6279 (1.9609) acc 56.2500 (43.2812) lr 1.1409e-03 eta 0:11:28
epoch [93/200] batch [45/50] time 0.083 (0.128) data 0.000 (0.043) loss 1.9443 (1.9514) acc 25.0000 (42.8472) lr 1.1409e-03 eta 0:11:24
epoch [93/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.039) loss 2.2148 (1.9735) acc 28.1250 (41.9375) lr 1.1253e-03 eta 0:10:59
epoch [94/200] batch [5/50] time 0.085 (0.311) data 0.000 (0.226) loss 2.0801 (1.8967) acc 46.8750 (45.0000) lr 1.1253e-03 eta 0:27:40
epoch [94/200] batch [10/50] time 0.085 (0.198) data 0.000 (0.113) loss 1.8018 (1.8516) acc 40.6250 (45.6250) lr 1.1253e-03 eta 0:17:37
epoch [94/200] batch [15/50] time 0.086 (0.168) data 0.001 (0.083) loss 1.9297 (1.9141) acc 43.7500 (45.2083) lr 1.1253e-03 eta 0:14:58
epoch [94/200] batch [20/50] time 0.085 (0.148) data 0.000 (0.063) loss 1.9902 (1.9236) acc 50.0000 (44.2188) lr 1.1253e-03 eta 0:13:09
epoch [94/200] batch [25/50] time 0.086 (0.136) data 0.000 (0.050) loss 2.0117 (1.9480) acc 37.5000 (42.8750) lr 1.1253e-03 eta 0:12:02
epoch [94/200] batch [30/50] time 0.086 (0.127) data 0.000 (0.042) loss 1.9121 (1.9504) acc 40.6250 (42.3958) lr 1.1253e-03 eta 0:11:17
epoch [94/200] batch [35/50] time 0.086 (0.121) data 0.001 (0.036) loss 2.0605 (1.9504) acc 46.8750 (42.0536) lr 1.1253e-03 eta 0:10:45
epoch [94/200] batch [40/50] time 0.084 (0.119) data 0.000 (0.034) loss 1.8887 (1.9713) acc 40.6250 (41.1719) lr 1.1253e-03 eta 0:10:31
epoch [94/200] batch [45/50] time 0.083 (0.115) data 0.000 (0.030) loss 1.5713 (1.9693) acc 46.8750 (41.5278) lr 1.1253e-03 eta 0:10:10
epoch [94/200] batch [50/50] time 0.083 (0.115) data 0.000 (0.030) loss 1.7256 (1.9615) acc 59.3750 (42.0000) lr 1.1097e-03 eta 0:10:07
epoch [95/200] batch [5/50] time 0.086 (0.321) data 0.000 (0.236) loss 2.2188 (1.9822) acc 43.7500 (45.0000) lr 1.1097e-03 eta 0:28:20
epoch [95/200] batch [10/50] time 0.214 (0.216) data 0.130 (0.131) loss 2.5977 (2.0395) acc 15.6250 (40.6250) lr 1.1097e-03 eta 0:19:04
epoch [95/200] batch [15/50] time 0.086 (0.173) data 0.000 (0.087) loss 1.9248 (2.0614) acc 43.7500 (39.7917) lr 1.1097e-03 eta 0:15:13
epoch [95/200] batch [20/50] time 0.084 (0.151) data 0.000 (0.066) loss 1.7432 (2.0438) acc 46.8750 (40.1562) lr 1.1097e-03 eta 0:13:16
epoch [95/200] batch [25/50] time 0.084 (0.138) data 0.000 (0.053) loss 1.7852 (2.0298) acc 53.1250 (40.3750) lr 1.1097e-03 eta 0:12:06
epoch [95/200] batch [30/50] time 0.085 (0.129) data 0.000 (0.044) loss 2.0645 (2.0357) acc 40.6250 (40.5208) lr 1.1097e-03 eta 0:11:19
epoch [95/200] batch [35/50] time 0.086 (0.123) data 0.001 (0.038) loss 2.0078 (2.0201) acc 37.5000 (40.8929) lr 1.1097e-03 eta 0:10:46
epoch [95/200] batch [40/50] time 0.085 (0.118) data 0.000 (0.033) loss 2.0918 (1.9940) acc 25.0000 (41.1719) lr 1.1097e-03 eta 0:10:20
epoch [95/200] batch [45/50] time 0.085 (0.114) data 0.000 (0.029) loss 2.1035 (1.9952) acc 31.2500 (40.9028) lr 1.1097e-03 eta 0:10:00
epoch [95/200] batch [50/50] time 0.083 (0.111) data 0.000 (0.026) loss 2.0176 (1.9970) acc 34.3750 (40.9375) lr 1.0941e-03 eta 0:09:44
epoch [96/200] batch [5/50] time 0.084 (0.332) data 0.000 (0.248) loss 2.0859 (1.9158) acc 40.6250 (41.8750) lr 1.0941e-03 eta 0:29:03
epoch [96/200] batch [10/50] time 0.084 (0.208) data 0.000 (0.124) loss 1.7988 (1.9774) acc 37.5000 (38.7500) lr 1.0941e-03 eta 0:18:12
epoch [96/200] batch [15/50] time 0.086 (0.167) data 0.000 (0.083) loss 1.8457 (2.0003) acc 40.6250 (39.3750) lr 1.0941e-03 eta 0:14:34
epoch [96/200] batch [20/50] time 0.084 (0.146) data 0.000 (0.062) loss 2.1973 (2.0255) acc 40.6250 (38.5938) lr 1.0941e-03 eta 0:12:45
epoch [96/200] batch [25/50] time 0.084 (0.134) data 0.000 (0.050) loss 1.7490 (2.0365) acc 43.7500 (38.1250) lr 1.0941e-03 eta 0:11:39
epoch [96/200] batch [30/50] time 0.109 (0.126) data 0.025 (0.042) loss 2.3887 (2.0464) acc 31.2500 (38.5417) lr 1.0941e-03 eta 0:10:59
epoch [96/200] batch [35/50] time 0.084 (0.122) data 0.000 (0.038) loss 1.8574 (2.0156) acc 46.8750 (39.1071) lr 1.0941e-03 eta 0:10:34
epoch [96/200] batch [40/50] time 0.161 (0.120) data 0.079 (0.036) loss 1.9170 (2.0087) acc 43.7500 (39.6875) lr 1.0941e-03 eta 0:10:24
epoch [96/200] batch [45/50] time 0.085 (0.117) data 0.000 (0.033) loss 2.2656 (2.0213) acc 40.6250 (39.3056) lr 1.0941e-03 eta 0:10:07
epoch [96/200] batch [50/50] time 0.084 (0.113) data 0.000 (0.030) loss 1.6318 (2.0068) acc 62.5000 (40.1875) lr 1.0785e-03 eta 0:09:50
epoch [97/200] batch [5/50] time 0.085 (0.324) data 0.001 (0.239) loss 1.7334 (1.9473) acc 56.2500 (42.5000) lr 1.0785e-03 eta 0:28:02
epoch [97/200] batch [10/50] time 0.086 (0.225) data 0.000 (0.140) loss 2.3203 (2.0748) acc 31.2500 (37.8125) lr 1.0785e-03 eta 0:19:28
epoch [97/200] batch [15/50] time 0.085 (0.179) data 0.000 (0.094) loss 1.9736 (2.0908) acc 40.6250 (38.5417) lr 1.0785e-03 eta 0:15:26
epoch [97/200] batch [20/50] time 0.086 (0.156) data 0.000 (0.071) loss 1.7979 (2.0575) acc 46.8750 (38.7500) lr 1.0785e-03 eta 0:13:28
epoch [97/200] batch [25/50] time 0.086 (0.142) data 0.000 (0.057) loss 1.8457 (2.0062) acc 40.6250 (40.2500) lr 1.0785e-03 eta 0:12:15
epoch [97/200] batch [30/50] time 0.085 (0.133) data 0.001 (0.047) loss 1.7061 (1.9754) acc 43.7500 (41.1458) lr 1.0785e-03 eta 0:11:26
epoch [97/200] batch [35/50] time 0.086 (0.126) data 0.001 (0.041) loss 1.8115 (1.9602) acc 43.7500 (42.1429) lr 1.0785e-03 eta 0:10:50
epoch [97/200] batch [40/50] time 0.084 (0.121) data 0.000 (0.036) loss 1.8193 (1.9699) acc 37.5000 (41.7969) lr 1.0785e-03 eta 0:10:23
epoch [97/200] batch [45/50] time 0.084 (0.121) data 0.000 (0.036) loss 1.9209 (1.9623) acc 46.8750 (41.9444) lr 1.0785e-03 eta 0:10:22
epoch [97/200] batch [50/50] time 0.084 (0.117) data 0.000 (0.032) loss 2.1836 (1.9774) acc 46.8750 (42.4375) lr 1.0628e-03 eta 0:10:02
epoch [98/200] batch [5/50] time 0.084 (0.354) data 0.000 (0.269) loss 2.1582 (2.0016) acc 34.3750 (36.8750) lr 1.0628e-03 eta 0:30:23
epoch [98/200] batch [10/50] time 0.084 (0.221) data 0.000 (0.137) loss 2.0020 (2.0240) acc 40.6250 (38.7500) lr 1.0628e-03 eta 0:18:58
epoch [98/200] batch [15/50] time 0.086 (0.176) data 0.000 (0.091) loss 1.5371 (1.9868) acc 56.2500 (40.8333) lr 1.0628e-03 eta 0:15:03
epoch [98/200] batch [20/50] time 0.084 (0.153) data 0.000 (0.069) loss 1.9561 (1.9579) acc 34.3750 (41.0938) lr 1.0628e-03 eta 0:13:04
epoch [98/200] batch [25/50] time 0.084 (0.139) data 0.000 (0.055) loss 2.0801 (1.9643) acc 43.7500 (41.7500) lr 1.0628e-03 eta 0:11:53
epoch [98/200] batch [30/50] time 0.085 (0.130) data 0.000 (0.046) loss 1.8486 (1.9592) acc 43.7500 (42.2917) lr 1.0628e-03 eta 0:11:06
epoch [98/200] batch [35/50] time 0.084 (0.124) data 0.000 (0.039) loss 1.8223 (1.9703) acc 40.6250 (41.6071) lr 1.0628e-03 eta 0:10:33
epoch [98/200] batch [40/50] time 0.083 (0.119) data 0.000 (0.034) loss 1.9561 (1.9775) acc 28.1250 (41.4844) lr 1.0628e-03 eta 0:10:07
epoch [98/200] batch [45/50] time 0.085 (0.115) data 0.000 (0.031) loss 2.2695 (1.9876) acc 34.3750 (41.5972) lr 1.0628e-03 eta 0:09:46
epoch [98/200] batch [50/50] time 0.103 (0.112) data 0.020 (0.028) loss 2.3555 (1.9962) acc 34.3750 (41.1250) lr 1.0471e-03 eta 0:09:32
epoch [99/200] batch [5/50] time 0.084 (0.350) data 0.000 (0.266) loss 1.9180 (1.8381) acc 40.6250 (45.6250) lr 1.0471e-03 eta 0:29:45
epoch [99/200] batch [10/50] time 0.085 (0.217) data 0.001 (0.133) loss 2.0488 (1.9445) acc 53.1250 (45.6250) lr 1.0471e-03 eta 0:18:26
epoch [99/200] batch [15/50] time 0.084 (0.173) data 0.000 (0.089) loss 2.1699 (1.9299) acc 37.5000 (46.0417) lr 1.0471e-03 eta 0:14:39
epoch [99/200] batch [20/50] time 0.084 (0.151) data 0.001 (0.067) loss 1.9004 (1.9666) acc 50.0000 (45.6250) lr 1.0471e-03 eta 0:12:47
epoch [99/200] batch [25/50] time 0.086 (0.138) data 0.000 (0.054) loss 2.0039 (1.9490) acc 40.6250 (44.8750) lr 1.0471e-03 eta 0:11:39
epoch [99/200] batch [30/50] time 0.085 (0.129) data 0.000 (0.045) loss 2.3516 (1.9839) acc 18.7500 (43.5417) lr 1.0471e-03 eta 0:10:53
epoch [99/200] batch [35/50] time 0.084 (0.123) data 0.000 (0.038) loss 2.0527 (1.9754) acc 37.5000 (43.2143) lr 1.0471e-03 eta 0:10:21
epoch [99/200] batch [40/50] time 0.084 (0.118) data 0.000 (0.034) loss 2.0293 (1.9761) acc 46.8750 (43.5156) lr 1.0471e-03 eta 0:09:56
epoch [99/200] batch [45/50] time 0.084 (0.117) data 0.000 (0.032) loss 2.5801 (1.9768) acc 37.5000 (44.0278) lr 1.0471e-03 eta 0:09:49
epoch [99/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.029) loss 2.0059 (1.9765) acc 43.7500 (43.7500) lr 1.0314e-03 eta 0:09:33
epoch [100/200] batch [5/50] time 0.086 (0.339) data 0.001 (0.252) loss 1.6113 (1.8729) acc 56.2500 (44.3750) lr 1.0314e-03 eta 0:28:28
epoch [100/200] batch [10/50] time 0.083 (0.235) data 0.000 (0.150) loss 2.1387 (1.9790) acc 34.3750 (40.9375) lr 1.0314e-03 eta 0:19:45
epoch [100/200] batch [15/50] time 0.086 (0.185) data 0.000 (0.100) loss 2.4141 (2.0725) acc 28.1250 (38.3333) lr 1.0314e-03 eta 0:15:30
epoch [100/200] batch [20/50] time 0.084 (0.167) data 0.000 (0.083) loss 1.6436 (2.0158) acc 46.8750 (40.6250) lr 1.0314e-03 eta 0:14:00
epoch [100/200] batch [25/50] time 0.084 (0.151) data 0.000 (0.066) loss 2.1992 (2.0340) acc 34.3750 (40.0000) lr 1.0314e-03 eta 0:12:38
epoch [100/200] batch [30/50] time 0.085 (0.140) data 0.000 (0.055) loss 1.6982 (1.9953) acc 40.6250 (40.9375) lr 1.0314e-03 eta 0:11:42
epoch [100/200] batch [35/50] time 0.086 (0.132) data 0.001 (0.047) loss 2.2402 (2.0217) acc 31.2500 (40.2679) lr 1.0314e-03 eta 0:11:02
epoch [100/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.041) loss 1.8135 (2.0443) acc 37.5000 (39.3750) lr 1.0314e-03 eta 0:10:32
epoch [100/200] batch [45/50] time 0.085 (0.121) data 0.000 (0.037) loss 2.0156 (2.0349) acc 43.7500 (39.8611) lr 1.0314e-03 eta 0:10:08
epoch [100/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.033) loss 1.9580 (2.0417) acc 50.0000 (40.1250) lr 1.0157e-03 eta 0:09:48
epoch [101/200] batch [5/50] time 0.085 (0.297) data 0.000 (0.211) loss 1.7939 (1.8842) acc 43.7500 (44.3750) lr 1.0157e-03 eta 0:24:42
epoch [101/200] batch [10/50] time 0.085 (0.208) data 0.000 (0.124) loss 1.9424 (1.9532) acc 50.0000 (44.6875) lr 1.0157e-03 eta 0:17:18
epoch [101/200] batch [15/50] time 0.083 (0.167) data 0.000 (0.083) loss 1.8652 (1.9484) acc 46.8750 (43.7500) lr 1.0157e-03 eta 0:13:51
epoch [101/200] batch [20/50] time 0.083 (0.161) data 0.000 (0.077) loss 1.9482 (1.9688) acc 37.5000 (42.9688) lr 1.0157e-03 eta 0:13:23
epoch [101/200] batch [25/50] time 0.116 (0.147) data 0.034 (0.063) loss 2.2012 (1.9723) acc 15.6250 (41.8750) lr 1.0157e-03 eta 0:12:12
epoch [101/200] batch [30/50] time 0.084 (0.137) data 0.000 (0.053) loss 1.8428 (1.9686) acc 46.8750 (41.5625) lr 1.0157e-03 eta 0:11:19
epoch [101/200] batch [35/50] time 0.083 (0.129) data 0.000 (0.045) loss 1.8604 (1.9697) acc 43.7500 (41.1607) lr 1.0157e-03 eta 0:10:40
epoch [101/200] batch [40/50] time 0.084 (0.124) data 0.000 (0.040) loss 2.0117 (1.9561) acc 43.7500 (41.7188) lr 1.0157e-03 eta 0:10:13
epoch [101/200] batch [45/50] time 0.083 (0.121) data 0.000 (0.037) loss 2.2910 (1.9449) acc 28.1250 (41.9444) lr 1.0157e-03 eta 0:10:00
epoch [101/200] batch [50/50] time 0.084 (0.117) data 0.000 (0.034) loss 2.1504 (1.9568) acc 25.0000 (41.6250) lr 1.0000e-03 eta 0:09:40
epoch [102/200] batch [5/50] time 0.085 (0.343) data 0.001 (0.257) loss 1.4502 (1.9369) acc 62.5000 (41.8750) lr 1.0000e-03 eta 0:28:17
epoch [102/200] batch [10/50] time 0.085 (0.225) data 0.000 (0.140) loss 1.6611 (1.9866) acc 53.1250 (41.2500) lr 1.0000e-03 eta 0:18:33
epoch [102/200] batch [15/50] time 0.084 (0.178) data 0.000 (0.094) loss 1.9678 (2.0011) acc 43.7500 (41.6667) lr 1.0000e-03 eta 0:14:40
epoch [102/200] batch [20/50] time 0.084 (0.158) data 0.000 (0.074) loss 2.2852 (1.9938) acc 37.5000 (41.5625) lr 1.0000e-03 eta 0:12:59
epoch [102/200] batch [25/50] time 0.270 (0.151) data 0.186 (0.066) loss 1.4336 (1.9958) acc 62.5000 (41.8750) lr 1.0000e-03 eta 0:12:22
epoch [102/200] batch [30/50] time 0.085 (0.140) data 0.000 (0.055) loss 1.8652 (2.0024) acc 34.3750 (41.3542) lr 1.0000e-03 eta 0:11:27
epoch [102/200] batch [35/50] time 0.088 (0.134) data 0.001 (0.049) loss 2.4688 (1.9985) acc 34.3750 (41.7857) lr 1.0000e-03 eta 0:10:57
epoch [102/200] batch [40/50] time 0.085 (0.128) data 0.000 (0.043) loss 1.5605 (1.9824) acc 40.6250 (41.5625) lr 1.0000e-03 eta 0:10:27
epoch [102/200] batch [45/50] time 0.082 (0.123) data 0.000 (0.038) loss 1.9365 (1.9749) acc 34.3750 (41.5278) lr 1.0000e-03 eta 0:10:02
epoch [102/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.035) loss 2.0781 (1.9831) acc 40.6250 (41.6250) lr 9.8429e-04 eta 0:09:42
epoch [103/200] batch [5/50] time 0.083 (0.349) data 0.000 (0.265) loss 1.9170 (1.8656) acc 34.3750 (41.2500) lr 9.8429e-04 eta 0:28:28
epoch [103/200] batch [10/50] time 0.084 (0.229) data 0.000 (0.146) loss 1.5010 (1.7605) acc 53.1250 (44.6875) lr 9.8429e-04 eta 0:18:41
epoch [103/200] batch [15/50] time 0.085 (0.181) data 0.000 (0.097) loss 1.9033 (1.8558) acc 34.3750 (43.3333) lr 9.8429e-04 eta 0:14:44
epoch [103/200] batch [20/50] time 0.085 (0.168) data 0.000 (0.084) loss 1.8545 (1.8929) acc 43.7500 (42.1875) lr 9.8429e-04 eta 0:13:40
epoch [103/200] batch [25/50] time 0.110 (0.152) data 0.026 (0.068) loss 1.9248 (1.8911) acc 37.5000 (41.2500) lr 9.8429e-04 eta 0:12:23
epoch [103/200] batch [30/50] time 0.087 (0.141) data 0.000 (0.057) loss 2.3359 (1.9317) acc 34.3750 (40.2083) lr 9.8429e-04 eta 0:11:28
epoch [103/200] batch [35/50] time 0.087 (0.133) data 0.001 (0.049) loss 1.9629 (1.9306) acc 43.7500 (41.3393) lr 9.8429e-04 eta 0:10:49
epoch [103/200] batch [40/50] time 0.084 (0.127) data 0.000 (0.043) loss 2.3145 (1.9395) acc 34.3750 (41.4844) lr 9.8429e-04 eta 0:10:19
epoch [103/200] batch [45/50] time 0.085 (0.123) data 0.000 (0.038) loss 1.9580 (1.9619) acc 34.3750 (40.6944) lr 9.8429e-04 eta 0:09:55
epoch [103/200] batch [50/50] time 0.084 (0.119) data 0.000 (0.034) loss 2.3672 (1.9744) acc 28.1250 (40.6250) lr 9.6859e-04 eta 0:09:35
epoch [104/200] batch [5/50] time 0.087 (0.328) data 0.001 (0.242) loss 2.1348 (1.8592) acc 43.7500 (42.5000) lr 9.6859e-04 eta 0:26:30
epoch [104/200] batch [10/50] time 0.171 (0.216) data 0.085 (0.130) loss 1.8701 (1.8858) acc 43.7500 (44.0625) lr 9.6859e-04 eta 0:17:25
epoch [104/200] batch [15/50] time 0.087 (0.173) data 0.001 (0.087) loss 2.0859 (1.9135) acc 37.5000 (43.7500) lr 9.6859e-04 eta 0:13:56
epoch [104/200] batch [20/50] time 0.085 (0.164) data 0.000 (0.078) loss 1.6123 (1.9066) acc 53.1250 (44.6875) lr 9.6859e-04 eta 0:13:14
epoch [104/200] batch [25/50] time 0.087 (0.149) data 0.001 (0.063) loss 1.4082 (1.9164) acc 53.1250 (43.8750) lr 9.6859e-04 eta 0:11:58
epoch [104/200] batch [30/50] time 0.083 (0.141) data 0.000 (0.056) loss 1.7803 (1.9254) acc 46.8750 (43.6458) lr 9.6859e-04 eta 0:11:21
epoch [104/200] batch [35/50] time 0.084 (0.133) data 0.000 (0.048) loss 2.2090 (1.9511) acc 37.5000 (43.1250) lr 9.6859e-04 eta 0:10:41
epoch [104/200] batch [40/50] time 0.084 (0.127) data 0.000 (0.042) loss 1.7393 (1.9326) acc 43.7500 (43.5938) lr 9.6859e-04 eta 0:10:11
epoch [104/200] batch [45/50] time 0.084 (0.122) data 0.000 (0.037) loss 2.4844 (1.9539) acc 18.7500 (43.1250) lr 9.6859e-04 eta 0:09:47
epoch [104/200] batch [50/50] time 0.084 (0.118) data 0.000 (0.033) loss 1.7559 (1.9615) acc 50.0000 (42.3125) lr 9.5289e-04 eta 0:09:28
epoch [105/200] batch [5/50] time 0.087 (0.326) data 0.000 (0.240) loss 1.7178 (1.9885) acc 53.1250 (40.6250) lr 9.5289e-04 eta 0:26:03
epoch [105/200] batch [10/50] time 0.085 (0.206) data 0.000 (0.120) loss 1.6895 (1.9242) acc 40.6250 (44.6875) lr 9.5289e-04 eta 0:16:26
epoch [105/200] batch [15/50] time 0.088 (0.172) data 0.001 (0.086) loss 2.2148 (1.9135) acc 31.2500 (43.7500) lr 9.5289e-04 eta 0:13:43
epoch [105/200] batch [20/50] time 0.142 (0.153) data 0.056 (0.067) loss 1.8545 (1.8775) acc 40.6250 (44.3750) lr 9.5289e-04 eta 0:12:11
epoch [105/200] batch [25/50] time 0.086 (0.140) data 0.001 (0.054) loss 1.6855 (1.8874) acc 46.8750 (44.2500) lr 9.5289e-04 eta 0:11:07
epoch [105/200] batch [30/50] time 0.085 (0.135) data 0.000 (0.050) loss 2.0586 (1.9192) acc 21.8750 (43.1250) lr 9.5289e-04 eta 0:10:45
epoch [105/200] batch [35/50] time 0.086 (0.128) data 0.001 (0.043) loss 1.9277 (1.9212) acc 40.6250 (43.3036) lr 9.5289e-04 eta 0:10:11
epoch [105/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.041) loss 1.9980 (1.9266) acc 43.7500 (42.8906) lr 9.5289e-04 eta 0:09:59
epoch [105/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.041) loss 2.3535 (1.9380) acc 28.1250 (42.4306) lr 9.5289e-04 eta 0:09:59
epoch [105/200] batch [50/50] time 0.083 (0.122) data 0.000 (0.037) loss 2.2070 (1.9573) acc 40.6250 (42.3750) lr 9.3721e-04 eta 0:09:38
epoch [106/200] batch [5/50] time 0.321 (0.325) data 0.235 (0.239) loss 1.5537 (1.6850) acc 43.7500 (51.2500) lr 9.3721e-04 eta 0:25:44
epoch [106/200] batch [10/50] time 0.086 (0.206) data 0.000 (0.120) loss 1.8477 (1.8433) acc 40.6250 (45.3125) lr 9.3721e-04 eta 0:16:15
epoch [106/200] batch [15/50] time 0.085 (0.176) data 0.000 (0.090) loss 2.1680 (1.9357) acc 31.2500 (42.7083) lr 9.3721e-04 eta 0:13:51
epoch [106/200] batch [20/50] time 0.084 (0.153) data 0.000 (0.067) loss 1.9932 (1.9171) acc 43.7500 (43.2812) lr 9.3721e-04 eta 0:12:05
epoch [106/200] batch [25/50] time 0.086 (0.146) data 0.000 (0.060) loss 2.3652 (1.9284) acc 28.1250 (42.0000) lr 9.3721e-04 eta 0:11:29
epoch [106/200] batch [30/50] time 0.126 (0.140) data 0.042 (0.055) loss 1.8691 (1.9660) acc 50.0000 (41.4583) lr 9.3721e-04 eta 0:11:02
epoch [106/200] batch [35/50] time 0.084 (0.132) data 0.000 (0.047) loss 1.8926 (1.9760) acc 37.5000 (41.4286) lr 9.3721e-04 eta 0:10:24
epoch [106/200] batch [40/50] time 0.084 (0.130) data 0.000 (0.044) loss 1.7695 (1.9844) acc 46.8750 (41.5625) lr 9.3721e-04 eta 0:10:09
epoch [106/200] batch [45/50] time 0.306 (0.129) data 0.223 (0.044) loss 2.0469 (1.9726) acc 46.8750 (42.2222) lr 9.3721e-04 eta 0:10:08
epoch [106/200] batch [50/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.3223 (1.9835) acc 34.3750 (41.5625) lr 9.2154e-04 eta 0:09:45
epoch [107/200] batch [5/50] time 0.085 (0.313) data 0.000 (0.227) loss 1.9434 (2.0238) acc 50.0000 (43.1250) lr 9.2154e-04 eta 0:24:28
epoch [107/200] batch [10/50] time 0.085 (0.200) data 0.000 (0.114) loss 1.6357 (1.9657) acc 62.5000 (45.6250) lr 9.2154e-04 eta 0:15:37
epoch [107/200] batch [15/50] time 0.086 (0.162) data 0.001 (0.076) loss 2.1543 (1.9745) acc 34.3750 (43.7500) lr 9.2154e-04 eta 0:12:38
epoch [107/200] batch [20/50] time 0.086 (0.143) data 0.000 (0.057) loss 1.9062 (1.9582) acc 34.3750 (43.1250) lr 9.2154e-04 eta 0:11:08
epoch [107/200] batch [25/50] time 0.085 (0.131) data 0.000 (0.046) loss 2.4238 (1.9423) acc 31.2500 (43.6250) lr 9.2154e-04 eta 0:10:14
epoch [107/200] batch [30/50] time 0.085 (0.124) data 0.000 (0.038) loss 1.9912 (1.9317) acc 37.5000 (43.7500) lr 9.2154e-04 eta 0:09:38
epoch [107/200] batch [35/50] time 0.086 (0.118) data 0.000 (0.033) loss 1.8447 (1.9164) acc 46.8750 (44.1071) lr 9.2154e-04 eta 0:09:12
epoch [107/200] batch [40/50] time 0.084 (0.118) data 0.000 (0.032) loss 2.1777 (1.9043) acc 18.7500 (44.1406) lr 9.2154e-04 eta 0:09:09
epoch [107/200] batch [45/50] time 0.085 (0.117) data 0.000 (0.032) loss 2.0273 (1.9339) acc 43.7500 (43.1944) lr 9.2154e-04 eta 0:09:06
epoch [107/200] batch [50/50] time 0.083 (0.114) data 0.000 (0.029) loss 2.0488 (1.9395) acc 37.5000 (42.8750) lr 9.0589e-04 eta 0:08:49
epoch [108/200] batch [5/50] time 0.087 (0.320) data 0.000 (0.234) loss 2.1641 (2.0242) acc 28.1250 (35.6250) lr 9.0589e-04 eta 0:24:46
epoch [108/200] batch [10/50] time 0.248 (0.219) data 0.165 (0.134) loss 1.6826 (1.9402) acc 43.7500 (39.3750) lr 9.0589e-04 eta 0:16:57
epoch [108/200] batch [15/50] time 0.085 (0.175) data 0.000 (0.089) loss 2.1211 (1.9174) acc 37.5000 (42.0833) lr 9.0589e-04 eta 0:13:29
epoch [108/200] batch [20/50] time 0.086 (0.152) data 0.000 (0.067) loss 2.3125 (1.9549) acc 31.2500 (41.8750) lr 9.0589e-04 eta 0:11:45
epoch [108/200] batch [25/50] time 0.086 (0.139) data 0.001 (0.054) loss 2.0059 (1.9482) acc 40.6250 (42.1250) lr 9.0589e-04 eta 0:10:42
epoch [108/200] batch [30/50] time 0.085 (0.130) data 0.001 (0.045) loss 1.6094 (1.9725) acc 59.3750 (40.8333) lr 9.0589e-04 eta 0:10:01
epoch [108/200] batch [35/50] time 0.085 (0.124) data 0.001 (0.038) loss 2.0137 (1.9581) acc 43.7500 (41.6071) lr 9.0589e-04 eta 0:09:31
epoch [108/200] batch [40/50] time 0.084 (0.119) data 0.000 (0.034) loss 2.2656 (1.9620) acc 40.6250 (41.4844) lr 9.0589e-04 eta 0:09:08
epoch [108/200] batch [45/50] time 0.084 (0.115) data 0.000 (0.030) loss 1.9180 (1.9538) acc 40.6250 (41.9444) lr 9.0589e-04 eta 0:08:50
epoch [108/200] batch [50/50] time 0.084 (0.112) data 0.000 (0.027) loss 2.1426 (1.9546) acc 34.3750 (41.5625) lr 8.9027e-04 eta 0:08:34
epoch [109/200] batch [5/50] time 0.085 (0.341) data 0.000 (0.254) loss 1.8369 (1.9211) acc 50.0000 (47.5000) lr 8.9027e-04 eta 0:26:04
epoch [109/200] batch [10/50] time 0.130 (0.228) data 0.046 (0.142) loss 1.9902 (1.9341) acc 46.8750 (46.8750) lr 8.9027e-04 eta 0:17:24
epoch [109/200] batch [15/50] time 0.085 (0.180) data 0.001 (0.095) loss 2.2422 (1.8926) acc 43.7500 (46.0417) lr 8.9027e-04 eta 0:13:47
epoch [109/200] batch [20/50] time 0.085 (0.162) data 0.000 (0.077) loss 1.9082 (1.9583) acc 34.3750 (43.1250) lr 8.9027e-04 eta 0:12:21
epoch [109/200] batch [25/50] time 0.086 (0.147) data 0.001 (0.061) loss 2.0098 (1.9781) acc 43.7500 (42.5000) lr 8.9027e-04 eta 0:11:10
epoch [109/200] batch [30/50] time 0.084 (0.140) data 0.000 (0.055) loss 2.3223 (1.9850) acc 40.6250 (43.0208) lr 8.9027e-04 eta 0:10:39
epoch [109/200] batch [35/50] time 0.086 (0.132) data 0.001 (0.047) loss 1.7852 (1.9735) acc 56.2500 (43.2143) lr 8.9027e-04 eta 0:10:03
epoch [109/200] batch [40/50] time 0.085 (0.126) data 0.000 (0.041) loss 2.1445 (1.9808) acc 43.7500 (42.9688) lr 8.9027e-04 eta 0:09:36
epoch [109/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.038) loss 1.8457 (1.9757) acc 43.7500 (43.4028) lr 8.9027e-04 eta 0:09:18
epoch [109/200] batch [50/50] time 0.084 (0.119) data 0.000 (0.034) loss 2.1016 (1.9705) acc 34.3750 (43.5625) lr 8.7467e-04 eta 0:08:59
epoch [110/200] batch [5/50] time 0.084 (0.338) data 0.000 (0.253) loss 2.1641 (1.8766) acc 37.5000 (44.3750) lr 8.7467e-04 eta 0:25:37
epoch [110/200] batch [10/50] time 0.084 (0.230) data 0.001 (0.146) loss 1.4014 (1.7568) acc 62.5000 (49.6875) lr 8.7467e-04 eta 0:17:26
epoch [110/200] batch [15/50] time 0.083 (0.182) data 0.000 (0.097) loss 1.8066 (1.8115) acc 34.3750 (46.6667) lr 8.7467e-04 eta 0:13:43
epoch [110/200] batch [20/50] time 0.084 (0.164) data 0.000 (0.079) loss 1.6348 (1.8474) acc 53.1250 (45.7812) lr 8.7467e-04 eta 0:12:21
epoch [110/200] batch [25/50] time 0.086 (0.148) data 0.000 (0.064) loss 2.2051 (1.8646) acc 31.2500 (44.8750) lr 8.7467e-04 eta 0:11:09
epoch [110/200] batch [30/50] time 0.086 (0.141) data 0.001 (0.057) loss 1.9180 (1.9035) acc 46.8750 (44.3750) lr 8.7467e-04 eta 0:10:38
epoch [110/200] batch [35/50] time 0.085 (0.140) data 0.001 (0.055) loss 2.2109 (1.8992) acc 31.2500 (44.7321) lr 8.7467e-04 eta 0:10:30
epoch [110/200] batch [40/50] time 0.084 (0.133) data 0.000 (0.048) loss 1.8770 (1.9066) acc 34.3750 (44.0625) lr 8.7467e-04 eta 0:09:58
epoch [110/200] batch [45/50] time 0.084 (0.130) data 0.000 (0.046) loss 2.2109 (1.9247) acc 37.5000 (43.5417) lr 8.7467e-04 eta 0:09:47
epoch [110/200] batch [50/50] time 0.084 (0.126) data 0.000 (0.041) loss 1.8418 (1.9223) acc 37.5000 (43.4375) lr 8.5910e-04 eta 0:09:25
epoch [111/200] batch [5/50] time 0.085 (0.302) data 0.000 (0.216) loss 2.4062 (1.9658) acc 40.6250 (43.1250) lr 8.5910e-04 eta 0:22:38
epoch [111/200] batch [10/50] time 0.087 (0.198) data 0.000 (0.112) loss 1.8594 (1.9407) acc 59.3750 (42.8125) lr 8.5910e-04 eta 0:14:47
epoch [111/200] batch [15/50] time 0.088 (0.166) data 0.001 (0.080) loss 1.5918 (1.9367) acc 50.0000 (42.2917) lr 8.5910e-04 eta 0:12:26
epoch [111/200] batch [20/50] time 0.087 (0.150) data 0.001 (0.064) loss 1.6045 (1.9227) acc 37.5000 (41.2500) lr 8.5910e-04 eta 0:11:10
epoch [111/200] batch [25/50] time 0.265 (0.144) data 0.180 (0.058) loss 1.7227 (1.9371) acc 34.3750 (41.0000) lr 8.5910e-04 eta 0:10:44
epoch [111/200] batch [30/50] time 0.085 (0.134) data 0.000 (0.049) loss 2.0586 (1.9383) acc 31.2500 (41.2500) lr 8.5910e-04 eta 0:10:00
epoch [111/200] batch [35/50] time 0.085 (0.131) data 0.000 (0.046) loss 2.2383 (1.9310) acc 34.3750 (41.2500) lr 8.5910e-04 eta 0:09:47
epoch [111/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.040) loss 2.2012 (1.9308) acc 28.1250 (40.9375) lr 8.5910e-04 eta 0:09:19
epoch [111/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.040) loss 2.4141 (1.9440) acc 34.3750 (40.9722) lr 8.5910e-04 eta 0:09:15
epoch [111/200] batch [50/50] time 0.086 (0.121) data 0.000 (0.036) loss 2.1348 (1.9540) acc 43.7500 (41.0625) lr 8.4357e-04 eta 0:08:56
epoch [112/200] batch [5/50] time 0.085 (0.290) data 0.000 (0.205) loss 1.8828 (1.9195) acc 53.1250 (46.2500) lr 8.4357e-04 eta 0:21:30
epoch [112/200] batch [10/50] time 0.086 (0.188) data 0.000 (0.103) loss 2.3457 (1.9866) acc 21.8750 (42.5000) lr 8.4357e-04 eta 0:13:54
epoch [112/200] batch [15/50] time 0.085 (0.154) data 0.000 (0.069) loss 2.2441 (1.9608) acc 34.3750 (43.5417) lr 8.4357e-04 eta 0:11:20
epoch [112/200] batch [20/50] time 0.085 (0.136) data 0.000 (0.052) loss 1.8428 (1.9415) acc 40.6250 (41.8750) lr 8.4357e-04 eta 0:10:04
epoch [112/200] batch [25/50] time 0.136 (0.128) data 0.051 (0.043) loss 1.5830 (1.9172) acc 68.7500 (43.3750) lr 8.4357e-04 eta 0:09:26
epoch [112/200] batch [30/50] time 0.085 (0.121) data 0.000 (0.036) loss 1.9756 (1.9433) acc 43.7500 (43.0208) lr 8.4357e-04 eta 0:08:54
epoch [112/200] batch [35/50] time 0.084 (0.123) data 0.000 (0.038) loss 1.7656 (1.9330) acc 40.6250 (42.9464) lr 8.4357e-04 eta 0:09:02
epoch [112/200] batch [40/50] time 0.084 (0.118) data 0.000 (0.033) loss 2.2148 (1.9542) acc 31.2500 (42.1875) lr 8.4357e-04 eta 0:08:40
epoch [112/200] batch [45/50] time 0.084 (0.116) data 0.000 (0.032) loss 1.7285 (1.9492) acc 43.7500 (42.5000) lr 8.4357e-04 eta 0:08:32
epoch [112/200] batch [50/50] time 0.083 (0.113) data 0.000 (0.029) loss 1.6416 (1.9271) acc 53.1250 (42.3125) lr 8.2807e-04 eta 0:08:17
epoch [113/200] batch [5/50] time 0.086 (0.336) data 0.001 (0.249) loss 1.5840 (1.7836) acc 56.2500 (47.5000) lr 8.2807e-04 eta 0:24:35
epoch [113/200] batch [10/50] time 0.101 (0.213) data 0.017 (0.127) loss 1.9414 (1.8638) acc 43.7500 (45.6250) lr 8.2807e-04 eta 0:15:33
epoch [113/200] batch [15/50] time 0.085 (0.181) data 0.000 (0.096) loss 2.2949 (1.9568) acc 31.2500 (42.2917) lr 8.2807e-04 eta 0:13:12
epoch [113/200] batch [20/50] time 0.085 (0.160) data 0.001 (0.075) loss 2.0938 (1.9321) acc 37.5000 (42.0312) lr 8.2807e-04 eta 0:11:39
epoch [113/200] batch [25/50] time 0.094 (0.145) data 0.001 (0.060) loss 1.9316 (1.9475) acc 31.2500 (41.1250) lr 8.2807e-04 eta 0:10:34
epoch [113/200] batch [30/50] time 0.084 (0.137) data 0.000 (0.051) loss 1.5947 (1.9515) acc 43.7500 (40.6250) lr 8.2807e-04 eta 0:09:57
epoch [113/200] batch [35/50] time 0.086 (0.134) data 0.001 (0.049) loss 1.6016 (1.9317) acc 50.0000 (41.6071) lr 8.2807e-04 eta 0:09:47
epoch [113/200] batch [40/50] time 0.084 (0.128) data 0.000 (0.043) loss 1.8545 (1.9372) acc 56.2500 (42.5000) lr 8.2807e-04 eta 0:09:19
epoch [113/200] batch [45/50] time 0.084 (0.125) data 0.000 (0.040) loss 2.7461 (1.9646) acc 31.2500 (42.0139) lr 8.2807e-04 eta 0:09:05
epoch [113/200] batch [50/50] time 0.085 (0.121) data 0.000 (0.036) loss 1.5791 (1.9651) acc 56.2500 (41.7500) lr 8.1262e-04 eta 0:08:47
epoch [114/200] batch [5/50] time 0.084 (0.324) data 0.000 (0.239) loss 1.9512 (1.7607) acc 43.7500 (41.8750) lr 8.1262e-04 eta 0:23:26
epoch [114/200] batch [10/50] time 0.120 (0.208) data 0.037 (0.124) loss 1.4619 (1.7435) acc 53.1250 (45.6250) lr 8.1262e-04 eta 0:15:01
epoch [114/200] batch [15/50] time 0.085 (0.169) data 0.000 (0.085) loss 1.9092 (1.8029) acc 40.6250 (45.8333) lr 8.1262e-04 eta 0:12:14
epoch [114/200] batch [20/50] time 0.084 (0.152) data 0.000 (0.067) loss 1.6943 (1.8812) acc 46.8750 (43.2812) lr 8.1262e-04 eta 0:10:56
epoch [114/200] batch [25/50] time 0.087 (0.139) data 0.001 (0.054) loss 2.1055 (1.9011) acc 40.6250 (43.6250) lr 8.1262e-04 eta 0:09:59
epoch [114/200] batch [30/50] time 0.084 (0.134) data 0.000 (0.050) loss 1.9619 (1.8962) acc 40.6250 (43.2292) lr 8.1262e-04 eta 0:09:40
epoch [114/200] batch [35/50] time 0.087 (0.131) data 0.001 (0.047) loss 1.8750 (1.9090) acc 40.6250 (42.7679) lr 8.1262e-04 eta 0:09:27
epoch [114/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.041) loss 1.8955 (1.9270) acc 50.0000 (42.6562) lr 8.1262e-04 eta 0:09:01
epoch [114/200] batch [45/50] time 0.084 (0.123) data 0.000 (0.039) loss 2.1152 (1.9288) acc 43.7500 (42.7083) lr 8.1262e-04 eta 0:08:50
epoch [114/200] batch [50/50] time 0.084 (0.119) data 0.000 (0.035) loss 2.0195 (1.9587) acc 28.1250 (41.8125) lr 7.9721e-04 eta 0:08:32
epoch [115/200] batch [5/50] time 0.123 (0.351) data 0.038 (0.264) loss 2.2168 (2.0084) acc 37.5000 (37.5000) lr 7.9721e-04 eta 0:25:07
epoch [115/200] batch [10/50] time 0.086 (0.219) data 0.000 (0.132) loss 1.9824 (1.9638) acc 37.5000 (40.0000) lr 7.9721e-04 eta 0:15:37
epoch [115/200] batch [15/50] time 0.086 (0.178) data 0.000 (0.092) loss 1.5586 (1.9736) acc 53.1250 (38.9583) lr 7.9721e-04 eta 0:12:43
epoch [115/200] batch [20/50] time 0.086 (0.158) data 0.000 (0.072) loss 2.1289 (1.9685) acc 50.0000 (39.5312) lr 7.9721e-04 eta 0:11:16
epoch [115/200] batch [25/50] time 0.086 (0.144) data 0.000 (0.057) loss 1.9766 (1.9495) acc 37.5000 (39.2500) lr 7.9721e-04 eta 0:10:13
epoch [115/200] batch [30/50] time 0.084 (0.140) data 0.000 (0.054) loss 2.2422 (1.9348) acc 28.1250 (39.1667) lr 7.9721e-04 eta 0:09:56
epoch [115/200] batch [35/50] time 0.224 (0.136) data 0.139 (0.050) loss 1.9727 (1.9302) acc 46.8750 (39.6429) lr 7.9721e-04 eta 0:09:39
epoch [115/200] batch [40/50] time 0.084 (0.129) data 0.000 (0.044) loss 1.6924 (1.9227) acc 43.7500 (40.1562) lr 7.9721e-04 eta 0:09:11
epoch [115/200] batch [45/50] time 0.084 (0.128) data 0.000 (0.043) loss 1.7480 (1.9029) acc 53.1250 (40.9028) lr 7.9721e-04 eta 0:09:05
epoch [115/200] batch [50/50] time 0.083 (0.124) data 0.000 (0.039) loss 1.4238 (1.9040) acc 71.8750 (41.6875) lr 7.8186e-04 eta 0:08:45
epoch [116/200] batch [5/50] time 0.087 (0.320) data 0.001 (0.233) loss 1.8223 (1.8834) acc 43.7500 (46.2500) lr 7.8186e-04 eta 0:22:37
epoch [116/200] batch [10/50] time 0.094 (0.204) data 0.006 (0.117) loss 1.7822 (1.9317) acc 40.6250 (43.4375) lr 7.8186e-04 eta 0:14:25
epoch [116/200] batch [15/50] time 0.085 (0.168) data 0.001 (0.082) loss 1.5508 (1.9512) acc 62.5000 (43.1250) lr 7.8186e-04 eta 0:11:52
epoch [116/200] batch [20/50] time 0.083 (0.156) data 0.001 (0.071) loss 1.9668 (1.9752) acc 37.5000 (42.9688) lr 7.8186e-04 eta 0:11:00
epoch [116/200] batch [25/50] time 0.084 (0.142) data 0.001 (0.057) loss 1.9492 (1.9602) acc 40.6250 (44.6250) lr 7.8186e-04 eta 0:09:58
epoch [116/200] batch [30/50] time 0.083 (0.138) data 0.000 (0.053) loss 1.9854 (1.9671) acc 43.7500 (44.2708) lr 7.8186e-04 eta 0:09:42
epoch [116/200] batch [35/50] time 0.159 (0.132) data 0.076 (0.048) loss 1.8682 (1.9498) acc 50.0000 (44.6429) lr 7.8186e-04 eta 0:09:18
epoch [116/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.043) loss 1.9424 (1.9662) acc 50.0000 (44.2969) lr 7.8186e-04 eta 0:08:55
epoch [116/200] batch [45/50] time 0.083 (0.127) data 0.000 (0.043) loss 2.0801 (1.9724) acc 31.2500 (43.5417) lr 7.8186e-04 eta 0:08:54
epoch [116/200] batch [50/50] time 0.084 (0.123) data 0.000 (0.038) loss 1.9541 (1.9744) acc 46.8750 (43.5000) lr 7.6655e-04 eta 0:08:35
epoch [117/200] batch [5/50] time 0.087 (0.335) data 0.000 (0.248) loss 2.2324 (1.9232) acc 28.1250 (40.0000) lr 7.6655e-04 eta 0:23:23
epoch [117/200] batch [10/50] time 0.089 (0.214) data 0.001 (0.127) loss 2.1855 (1.9376) acc 31.2500 (41.2500) lr 7.6655e-04 eta 0:14:58
epoch [117/200] batch [15/50] time 0.085 (0.172) data 0.000 (0.085) loss 1.5908 (1.9277) acc 50.0000 (41.0417) lr 7.6655e-04 eta 0:11:57
epoch [117/200] batch [20/50] time 0.085 (0.157) data 0.000 (0.071) loss 2.0312 (1.9230) acc 40.6250 (42.0312) lr 7.6655e-04 eta 0:10:55
epoch [117/200] batch [25/50] time 0.085 (0.142) data 0.001 (0.057) loss 2.0000 (1.9288) acc 46.8750 (42.0000) lr 7.6655e-04 eta 0:09:54
epoch [117/200] batch [30/50] time 0.084 (0.134) data 0.000 (0.048) loss 2.1875 (1.9197) acc 34.3750 (41.8750) lr 7.6655e-04 eta 0:09:16
epoch [117/200] batch [35/50] time 0.085 (0.126) data 0.000 (0.041) loss 2.0156 (1.9106) acc 46.8750 (42.5893) lr 7.6655e-04 eta 0:08:46
epoch [117/200] batch [40/50] time 0.084 (0.123) data 0.000 (0.039) loss 1.9395 (1.9237) acc 50.0000 (42.8125) lr 7.6655e-04 eta 0:08:33
epoch [117/200] batch [45/50] time 0.084 (0.121) data 0.000 (0.036) loss 2.2539 (1.9167) acc 28.1250 (42.9167) lr 7.6655e-04 eta 0:08:20
epoch [117/200] batch [50/50] time 0.084 (0.117) data 0.000 (0.033) loss 1.5645 (1.9201) acc 56.2500 (42.8750) lr 7.5131e-04 eta 0:08:06
epoch [118/200] batch [5/50] time 0.084 (0.330) data 0.000 (0.246) loss 1.9238 (1.9299) acc 37.5000 (40.0000) lr 7.5131e-04 eta 0:22:48
epoch [118/200] batch [10/50] time 0.215 (0.221) data 0.132 (0.136) loss 1.5576 (1.8489) acc 40.6250 (42.8125) lr 7.5131e-04 eta 0:15:13
epoch [118/200] batch [15/50] time 0.086 (0.175) data 0.000 (0.091) loss 1.8125 (1.8749) acc 40.6250 (42.9167) lr 7.5131e-04 eta 0:12:05
epoch [118/200] batch [20/50] time 0.086 (0.162) data 0.000 (0.078) loss 1.9609 (1.8760) acc 43.7500 (43.7500) lr 7.5131e-04 eta 0:11:10
epoch [118/200] batch [25/50] time 0.086 (0.147) data 0.000 (0.062) loss 2.0957 (1.9145) acc 25.0000 (42.6250) lr 7.5131e-04 eta 0:10:06
epoch [118/200] batch [30/50] time 0.085 (0.144) data 0.000 (0.059) loss 1.8701 (1.9140) acc 50.0000 (42.9167) lr 7.5131e-04 eta 0:09:52
epoch [118/200] batch [35/50] time 0.084 (0.140) data 0.000 (0.055) loss 2.0684 (1.9116) acc 34.3750 (43.2143) lr 7.5131e-04 eta 0:09:35
epoch [118/200] batch [40/50] time 0.083 (0.133) data 0.000 (0.048) loss 2.1719 (1.9148) acc 37.5000 (42.5781) lr 7.5131e-04 eta 0:09:06
epoch [118/200] batch [45/50] time 0.083 (0.130) data 0.000 (0.046) loss 1.6768 (1.9227) acc 46.8750 (41.9444) lr 7.5131e-04 eta 0:08:53
epoch [118/200] batch [50/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.9004 (1.9384) acc 43.7500 (41.5625) lr 7.3613e-04 eta 0:08:33
epoch [119/200] batch [5/50] time 0.086 (0.324) data 0.000 (0.238) loss 1.9561 (1.7525) acc 53.1250 (54.3750) lr 7.3613e-04 eta 0:22:06
epoch [119/200] batch [10/50] time 0.258 (0.222) data 0.173 (0.136) loss 2.1289 (1.8098) acc 40.6250 (50.3125) lr 7.3613e-04 eta 0:15:09
epoch [119/200] batch [15/50] time 0.085 (0.177) data 0.000 (0.091) loss 1.8164 (1.8549) acc 40.6250 (47.2917) lr 7.3613e-04 eta 0:12:02
epoch [119/200] batch [20/50] time 0.084 (0.159) data 0.000 (0.074) loss 1.9482 (1.8876) acc 46.8750 (46.8750) lr 7.3613e-04 eta 0:10:49
epoch [119/200] batch [25/50] time 0.085 (0.144) data 0.000 (0.059) loss 1.7539 (1.8964) acc 43.7500 (45.5000) lr 7.3613e-04 eta 0:09:47
epoch [119/200] batch [30/50] time 0.087 (0.134) data 0.001 (0.049) loss 1.9189 (1.8902) acc 50.0000 (45.4167) lr 7.3613e-04 eta 0:09:06
epoch [119/200] batch [35/50] time 0.085 (0.129) data 0.000 (0.044) loss 2.4062 (1.8937) acc 37.5000 (44.5536) lr 7.3613e-04 eta 0:08:46
epoch [119/200] batch [40/50] time 0.086 (0.124) data 0.000 (0.039) loss 1.6475 (1.9214) acc 53.1250 (43.6719) lr 7.3613e-04 eta 0:08:23
epoch [119/200] batch [45/50] time 0.085 (0.120) data 0.000 (0.035) loss 1.5195 (1.9439) acc 62.5000 (43.8194) lr 7.3613e-04 eta 0:08:08
epoch [119/200] batch [50/50] time 0.084 (0.117) data 0.000 (0.032) loss 1.7930 (1.9452) acc 37.5000 (43.4375) lr 7.2101e-04 eta 0:07:52
epoch [120/200] batch [5/50] time 0.085 (0.356) data 0.000 (0.271) loss 1.4902 (1.8389) acc 53.1250 (47.5000) lr 7.2101e-04 eta 0:24:01
epoch [120/200] batch [10/50] time 0.086 (0.221) data 0.000 (0.136) loss 1.6562 (1.8396) acc 50.0000 (45.6250) lr 7.2101e-04 eta 0:14:51
epoch [120/200] batch [15/50] time 0.088 (0.175) data 0.000 (0.091) loss 1.8604 (1.9043) acc 37.5000 (44.5833) lr 7.2101e-04 eta 0:11:47
epoch [120/200] batch [20/50] time 0.180 (0.157) data 0.097 (0.073) loss 1.9326 (1.8877) acc 40.6250 (44.8438) lr 7.2101e-04 eta 0:10:34
epoch [120/200] batch [25/50] time 0.084 (0.143) data 0.000 (0.058) loss 2.0625 (1.9042) acc 46.8750 (45.2500) lr 7.2101e-04 eta 0:09:35
epoch [120/200] batch [30/50] time 0.084 (0.141) data 0.000 (0.056) loss 1.7783 (1.9085) acc 53.1250 (45.0000) lr 7.2101e-04 eta 0:09:25
epoch [120/200] batch [35/50] time 0.086 (0.133) data 0.001 (0.048) loss 1.4756 (1.8794) acc 53.1250 (45.5357) lr 7.2101e-04 eta 0:08:52
epoch [120/200] batch [40/50] time 0.084 (0.130) data 0.000 (0.046) loss 2.1836 (1.9089) acc 34.3750 (44.4531) lr 7.2101e-04 eta 0:08:42
epoch [120/200] batch [45/50] time 0.084 (0.128) data 0.000 (0.044) loss 2.0723 (1.9258) acc 40.6250 (43.8889) lr 7.2101e-04 eta 0:08:32
epoch [120/200] batch [50/50] time 0.083 (0.124) data 0.000 (0.039) loss 1.8164 (1.9337) acc 37.5000 (43.5000) lr 7.0596e-04 eta 0:08:14
epoch [121/200] batch [5/50] time 0.085 (0.345) data 0.000 (0.260) loss 2.2852 (2.0967) acc 37.5000 (39.3750) lr 7.0596e-04 eta 0:22:59
epoch [121/200] batch [10/50] time 0.086 (0.236) data 0.001 (0.150) loss 1.9766 (2.0376) acc 25.0000 (36.2500) lr 7.0596e-04 eta 0:15:40
epoch [121/200] batch [15/50] time 0.086 (0.186) data 0.001 (0.100) loss 2.3594 (2.0451) acc 25.0000 (36.4583) lr 7.0596e-04 eta 0:12:19
epoch [121/200] batch [20/50] time 0.084 (0.169) data 0.000 (0.084) loss 1.8779 (1.9824) acc 43.7500 (39.0625) lr 7.0596e-04 eta 0:11:12
epoch [121/200] batch [25/50] time 0.117 (0.153) data 0.033 (0.069) loss 1.5273 (1.9197) acc 50.0000 (41.1250) lr 7.0596e-04 eta 0:10:09
epoch [121/200] batch [30/50] time 0.085 (0.142) data 0.000 (0.057) loss 1.6533 (1.9083) acc 46.8750 (40.9375) lr 7.0596e-04 eta 0:09:23
epoch [121/200] batch [35/50] time 0.085 (0.140) data 0.000 (0.055) loss 1.9854 (1.9131) acc 50.0000 (41.2500) lr 7.0596e-04 eta 0:09:14
epoch [121/200] batch [40/50] time 0.084 (0.133) data 0.000 (0.048) loss 1.8926 (1.9062) acc 46.8750 (41.6406) lr 7.0596e-04 eta 0:08:45
epoch [121/200] batch [45/50] time 0.084 (0.128) data 0.000 (0.043) loss 2.1133 (1.8998) acc 34.3750 (42.2222) lr 7.0596e-04 eta 0:08:25
epoch [121/200] batch [50/50] time 0.085 (0.123) data 0.000 (0.039) loss 1.8311 (1.8992) acc 46.8750 (42.8125) lr 6.9098e-04 eta 0:08:07
epoch [122/200] batch [5/50] time 0.089 (0.336) data 0.001 (0.249) loss 2.0488 (1.7768) acc 34.3750 (43.7500) lr 6.9098e-04 eta 0:22:04
epoch [122/200] batch [10/50] time 0.156 (0.218) data 0.071 (0.132) loss 1.8994 (1.9358) acc 53.1250 (45.0000) lr 6.9098e-04 eta 0:14:19
epoch [122/200] batch [15/50] time 0.088 (0.174) data 0.000 (0.088) loss 1.9980 (1.9315) acc 31.2500 (42.7083) lr 6.9098e-04 eta 0:11:26
epoch [122/200] batch [20/50] time 0.088 (0.163) data 0.001 (0.077) loss 1.5127 (1.9256) acc 50.0000 (42.5000) lr 6.9098e-04 eta 0:10:41
epoch [122/200] batch [25/50] time 0.085 (0.148) data 0.000 (0.062) loss 2.0723 (1.9191) acc 34.3750 (43.1250) lr 6.9098e-04 eta 0:09:40
epoch [122/200] batch [30/50] time 0.085 (0.144) data 0.000 (0.059) loss 2.0703 (1.8973) acc 34.3750 (43.9583) lr 6.9098e-04 eta 0:09:25
epoch [122/200] batch [35/50] time 0.084 (0.139) data 0.000 (0.054) loss 1.9854 (1.8907) acc 40.6250 (44.0179) lr 6.9098e-04 eta 0:09:06
epoch [122/200] batch [40/50] time 0.084 (0.133) data 0.000 (0.047) loss 1.8750 (1.9064) acc 37.5000 (42.9688) lr 6.9098e-04 eta 0:08:38
epoch [122/200] batch [45/50] time 0.082 (0.130) data 0.000 (0.045) loss 2.4219 (1.9117) acc 40.6250 (42.6389) lr 6.9098e-04 eta 0:08:28
epoch [122/200] batch [50/50] time 0.083 (0.126) data 0.000 (0.041) loss 1.9883 (1.9225) acc 37.5000 (42.5625) lr 6.7608e-04 eta 0:08:09
epoch [123/200] batch [5/50] time 0.084 (0.362) data 0.000 (0.278) loss 1.7158 (1.8758) acc 34.3750 (41.8750) lr 6.7608e-04 eta 0:23:31
epoch [123/200] batch [10/50] time 0.085 (0.223) data 0.000 (0.139) loss 1.8789 (1.9456) acc 53.1250 (43.1250) lr 6.7608e-04 eta 0:14:29
epoch [123/200] batch [15/50] time 0.083 (0.177) data 0.000 (0.093) loss 1.9668 (1.9535) acc 40.6250 (42.9167) lr 6.7608e-04 eta 0:11:26
epoch [123/200] batch [20/50] time 0.084 (0.158) data 0.000 (0.075) loss 1.4443 (1.9052) acc 53.1250 (44.5312) lr 6.7608e-04 eta 0:10:14
epoch [123/200] batch [25/50] time 0.260 (0.151) data 0.175 (0.067) loss 2.1641 (1.9280) acc 50.0000 (43.8750) lr 6.7608e-04 eta 0:09:43
epoch [123/200] batch [30/50] time 0.085 (0.140) data 0.001 (0.056) loss 1.5088 (1.9296) acc 50.0000 (43.9583) lr 6.7608e-04 eta 0:09:00
epoch [123/200] batch [35/50] time 0.146 (0.135) data 0.063 (0.051) loss 2.2227 (1.9599) acc 34.3750 (42.8571) lr 6.7608e-04 eta 0:08:43
epoch [123/200] batch [40/50] time 0.084 (0.129) data 0.000 (0.045) loss 1.9219 (1.9483) acc 34.3750 (42.5781) lr 6.7608e-04 eta 0:08:18
epoch [123/200] batch [45/50] time 0.083 (0.128) data 0.000 (0.044) loss 2.4121 (1.9660) acc 40.6250 (42.7778) lr 6.7608e-04 eta 0:08:14
epoch [123/200] batch [50/50] time 0.084 (0.124) data 0.000 (0.040) loss 1.8457 (1.9625) acc 43.7500 (42.5625) lr 6.6126e-04 eta 0:07:56
epoch [124/200] batch [5/50] time 0.086 (0.331) data 0.000 (0.246) loss 1.5557 (1.9477) acc 56.2500 (45.6250) lr 6.6126e-04 eta 0:21:13
epoch [124/200] batch [10/50] time 0.247 (0.224) data 0.164 (0.140) loss 2.4922 (1.9451) acc 43.7500 (45.6250) lr 6.6126e-04 eta 0:14:21
epoch [124/200] batch [15/50] time 0.085 (0.178) data 0.000 (0.093) loss 1.5674 (1.9961) acc 46.8750 (41.8750) lr 6.6126e-04 eta 0:11:22
epoch [124/200] batch [20/50] time 0.085 (0.161) data 0.000 (0.077) loss 1.9238 (1.9608) acc 43.7500 (42.9688) lr 6.6126e-04 eta 0:10:17
epoch [124/200] batch [25/50] time 0.086 (0.146) data 0.000 (0.061) loss 1.7900 (1.9657) acc 46.8750 (42.2500) lr 6.6126e-04 eta 0:09:18
epoch [124/200] batch [30/50] time 0.085 (0.144) data 0.000 (0.059) loss 1.6924 (1.9438) acc 43.7500 (42.5000) lr 6.6126e-04 eta 0:09:08
epoch [124/200] batch [35/50] time 0.085 (0.140) data 0.001 (0.056) loss 2.2305 (1.9586) acc 34.3750 (41.9643) lr 6.6126e-04 eta 0:08:55
epoch [124/200] batch [40/50] time 0.082 (0.133) data 0.000 (0.049) loss 1.7920 (1.9657) acc 50.0000 (41.6406) lr 6.6126e-04 eta 0:08:27
epoch [124/200] batch [45/50] time 0.083 (0.130) data 0.000 (0.045) loss 1.9697 (1.9359) acc 34.3750 (41.8750) lr 6.6126e-04 eta 0:08:13
epoch [124/200] batch [50/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.7920 (1.9333) acc 43.7500 (42.0000) lr 6.4653e-04 eta 0:07:54
epoch [125/200] batch [5/50] time 0.085 (0.345) data 0.000 (0.260) loss 2.1211 (1.9438) acc 34.3750 (38.1250) lr 6.4653e-04 eta 0:21:49
epoch [125/200] batch [10/50] time 0.084 (0.218) data 0.000 (0.133) loss 1.8047 (1.8854) acc 56.2500 (44.6875) lr 6.4653e-04 eta 0:13:47
epoch [125/200] batch [15/50] time 0.084 (0.174) data 0.000 (0.089) loss 1.5938 (1.8649) acc 50.0000 (43.9583) lr 6.4653e-04 eta 0:10:57
epoch [125/200] batch [20/50] time 0.085 (0.155) data 0.001 (0.070) loss 1.7979 (1.8995) acc 43.7500 (44.3750) lr 6.4653e-04 eta 0:09:45
epoch [125/200] batch [25/50] time 0.085 (0.141) data 0.000 (0.056) loss 2.3613 (1.9312) acc 31.2500 (44.1250) lr 6.4653e-04 eta 0:08:51
epoch [125/200] batch [30/50] time 0.084 (0.132) data 0.000 (0.047) loss 2.0996 (1.9316) acc 37.5000 (43.3333) lr 6.4653e-04 eta 0:08:16
epoch [125/200] batch [35/50] time 0.085 (0.127) data 0.001 (0.042) loss 1.9277 (1.9440) acc 46.8750 (43.3036) lr 6.4653e-04 eta 0:07:56
epoch [125/200] batch [40/50] time 0.084 (0.124) data 0.000 (0.039) loss 2.3887 (1.9417) acc 40.6250 (43.6719) lr 6.4653e-04 eta 0:07:44
epoch [125/200] batch [45/50] time 0.094 (0.119) data 0.011 (0.035) loss 1.7617 (1.9217) acc 46.8750 (43.9583) lr 6.4653e-04 eta 0:07:28
epoch [125/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.032) loss 1.4707 (1.9292) acc 59.3750 (43.7500) lr 6.3188e-04 eta 0:07:14
epoch [126/200] batch [5/50] time 0.086 (0.315) data 0.001 (0.229) loss 2.4824 (2.1313) acc 28.1250 (39.3750) lr 6.3188e-04 eta 0:19:39
epoch [126/200] batch [10/50] time 0.087 (0.201) data 0.001 (0.115) loss 1.7061 (1.9253) acc 53.1250 (41.8750) lr 6.3188e-04 eta 0:12:30
epoch [126/200] batch [15/50] time 0.086 (0.172) data 0.000 (0.086) loss 2.0879 (1.9327) acc 43.7500 (42.9167) lr 6.3188e-04 eta 0:10:41
epoch [126/200] batch [20/50] time 0.086 (0.158) data 0.000 (0.072) loss 1.7852 (1.9373) acc 37.5000 (41.5625) lr 6.3188e-04 eta 0:09:48
epoch [126/200] batch [25/50] time 0.085 (0.143) data 0.000 (0.058) loss 1.7393 (1.9346) acc 50.0000 (42.1250) lr 6.3188e-04 eta 0:08:53
epoch [126/200] batch [30/50] time 0.085 (0.138) data 0.000 (0.053) loss 1.7832 (1.9231) acc 46.8750 (42.7083) lr 6.3188e-04 eta 0:08:32
epoch [126/200] batch [35/50] time 0.213 (0.134) data 0.128 (0.049) loss 1.9229 (1.9159) acc 43.7500 (42.9464) lr 6.3188e-04 eta 0:08:17
epoch [126/200] batch [40/50] time 0.084 (0.128) data 0.000 (0.043) loss 1.9883 (1.8995) acc 37.5000 (43.4375) lr 6.3188e-04 eta 0:07:54
epoch [126/200] batch [45/50] time 0.084 (0.127) data 0.000 (0.042) loss 1.8467 (1.9186) acc 34.3750 (42.3611) lr 6.3188e-04 eta 0:07:48
epoch [126/200] batch [50/50] time 0.084 (0.122) data 0.000 (0.037) loss 2.2715 (1.9251) acc 34.3750 (42.0625) lr 6.1732e-04 eta 0:07:32
epoch [127/200] batch [5/50] time 0.086 (0.341) data 0.001 (0.255) loss 2.0664 (1.9287) acc 46.8750 (41.2500) lr 6.1732e-04 eta 0:21:00
epoch [127/200] batch [10/50] time 0.087 (0.216) data 0.001 (0.129) loss 1.8574 (1.9428) acc 50.0000 (43.1250) lr 6.1732e-04 eta 0:13:17
epoch [127/200] batch [15/50] time 0.087 (0.173) data 0.001 (0.086) loss 1.8291 (1.9540) acc 56.2500 (43.7500) lr 6.1732e-04 eta 0:10:37
epoch [127/200] batch [20/50] time 0.090 (0.160) data 0.001 (0.074) loss 1.9082 (1.9254) acc 40.6250 (44.0625) lr 6.1732e-04 eta 0:09:49
epoch [127/200] batch [25/50] time 0.213 (0.150) data 0.127 (0.064) loss 2.3320 (1.9657) acc 34.3750 (42.8750) lr 6.1732e-04 eta 0:09:12
epoch [127/200] batch [30/50] time 0.087 (0.140) data 0.000 (0.053) loss 1.6895 (1.9590) acc 59.3750 (43.2292) lr 6.1732e-04 eta 0:08:32
epoch [127/200] batch [35/50] time 0.084 (0.137) data 0.000 (0.050) loss 1.7588 (1.9761) acc 43.7500 (42.3214) lr 6.1732e-04 eta 0:08:20
epoch [127/200] batch [40/50] time 0.085 (0.130) data 0.000 (0.044) loss 1.7070 (1.9708) acc 37.5000 (42.2656) lr 6.1732e-04 eta 0:07:56
epoch [127/200] batch [45/50] time 0.085 (0.125) data 0.000 (0.039) loss 1.8271 (1.9577) acc 34.3750 (42.3611) lr 6.1732e-04 eta 0:07:36
epoch [127/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.035) loss 1.7236 (1.9652) acc 46.8750 (42.3125) lr 6.0285e-04 eta 0:07:21
epoch [128/200] batch [5/50] time 0.084 (0.339) data 0.000 (0.254) loss 1.5508 (1.8086) acc 59.3750 (47.5000) lr 6.0285e-04 eta 0:20:37
epoch [128/200] batch [10/50] time 0.083 (0.237) data 0.000 (0.153) loss 1.6396 (1.8146) acc 50.0000 (48.4375) lr 6.0285e-04 eta 0:14:22
epoch [128/200] batch [15/50] time 0.085 (0.186) data 0.000 (0.102) loss 2.1016 (1.9186) acc 28.1250 (44.1667) lr 6.0285e-04 eta 0:11:15
epoch [128/200] batch [20/50] time 0.085 (0.167) data 0.000 (0.083) loss 1.9658 (1.9052) acc 37.5000 (44.2188) lr 6.0285e-04 eta 0:10:06
epoch [128/200] batch [25/50] time 0.282 (0.158) data 0.199 (0.074) loss 1.9639 (1.9047) acc 40.6250 (44.3750) lr 6.0285e-04 eta 0:09:34
epoch [128/200] batch [30/50] time 0.085 (0.146) data 0.000 (0.062) loss 1.6270 (1.8771) acc 46.8750 (45.0000) lr 6.0285e-04 eta 0:08:49
epoch [128/200] batch [35/50] time 0.084 (0.144) data 0.000 (0.060) loss 1.6943 (1.8589) acc 53.1250 (45.3571) lr 6.0285e-04 eta 0:08:41
epoch [128/200] batch [40/50] time 0.084 (0.137) data 0.000 (0.053) loss 1.9355 (1.8882) acc 28.1250 (44.6094) lr 6.0285e-04 eta 0:08:13
epoch [128/200] batch [45/50] time 0.083 (0.131) data 0.000 (0.047) loss 1.9473 (1.9076) acc 37.5000 (44.0972) lr 6.0285e-04 eta 0:07:51
epoch [128/200] batch [50/50] time 0.083 (0.126) data 0.000 (0.042) loss 1.5996 (1.9079) acc 53.1250 (44.0000) lr 5.8849e-04 eta 0:07:33
epoch [129/200] batch [5/50] time 0.085 (0.329) data 0.000 (0.245) loss 2.2480 (1.9094) acc 25.0000 (42.5000) lr 5.8849e-04 eta 0:19:43
epoch [129/200] batch [10/50] time 0.106 (0.209) data 0.022 (0.125) loss 1.9336 (1.9262) acc 46.8750 (42.1875) lr 5.8849e-04 eta 0:12:30
epoch [129/200] batch [15/50] time 0.085 (0.169) data 0.000 (0.084) loss 1.6836 (1.8988) acc 50.0000 (44.1667) lr 5.8849e-04 eta 0:10:05
epoch [129/200] batch [20/50] time 0.311 (0.159) data 0.227 (0.075) loss 2.0391 (1.9201) acc 46.8750 (43.2812) lr 5.8849e-04 eta 0:09:29
epoch [129/200] batch [25/50] time 0.090 (0.145) data 0.000 (0.060) loss 1.9150 (1.9470) acc 43.7500 (42.7500) lr 5.8849e-04 eta 0:08:36
epoch [129/200] batch [30/50] time 0.087 (0.140) data 0.000 (0.056) loss 1.5059 (1.9156) acc 46.8750 (43.1250) lr 5.8849e-04 eta 0:08:20
epoch [129/200] batch [35/50] time 0.084 (0.132) data 0.001 (0.048) loss 1.8086 (1.9198) acc 46.8750 (43.2143) lr 5.8849e-04 eta 0:07:52
epoch [129/200] batch [40/50] time 0.084 (0.133) data 0.000 (0.048) loss 1.9365 (1.9337) acc 46.8750 (43.6719) lr 5.8849e-04 eta 0:07:52
epoch [129/200] batch [45/50] time 0.083 (0.128) data 0.000 (0.044) loss 2.0645 (1.9248) acc 31.2500 (43.2639) lr 5.8849e-04 eta 0:07:35
epoch [129/200] batch [50/50] time 0.084 (0.124) data 0.000 (0.039) loss 1.7314 (1.9460) acc 46.8750 (43.0000) lr 5.7422e-04 eta 0:07:19
epoch [130/200] batch [5/50] time 0.085 (0.306) data 0.000 (0.219) loss 2.1094 (1.9889) acc 18.7500 (36.2500) lr 5.7422e-04 eta 0:18:05
epoch [130/200] batch [10/50] time 0.086 (0.212) data 0.001 (0.125) loss 1.7305 (1.9019) acc 46.8750 (42.5000) lr 5.7422e-04 eta 0:12:29
epoch [130/200] batch [15/50] time 0.087 (0.170) data 0.001 (0.084) loss 1.7080 (1.9367) acc 40.6250 (42.2917) lr 5.7422e-04 eta 0:10:01
epoch [130/200] batch [20/50] time 0.086 (0.161) data 0.000 (0.074) loss 1.5752 (1.8993) acc 46.8750 (42.9688) lr 5.7422e-04 eta 0:09:26
epoch [130/200] batch [25/50] time 0.086 (0.146) data 0.000 (0.060) loss 1.9268 (1.8919) acc 37.5000 (43.0000) lr 5.7422e-04 eta 0:08:33
epoch [130/200] batch [30/50] time 0.086 (0.136) data 0.000 (0.050) loss 2.2520 (1.9306) acc 37.5000 (42.5000) lr 5.7422e-04 eta 0:07:58
epoch [130/200] batch [35/50] time 0.087 (0.129) data 0.001 (0.043) loss 1.9355 (1.9078) acc 50.0000 (43.3036) lr 5.7422e-04 eta 0:07:33
epoch [130/200] batch [40/50] time 0.087 (0.124) data 0.000 (0.037) loss 1.9326 (1.9065) acc 34.3750 (42.9688) lr 5.7422e-04 eta 0:07:13
epoch [130/200] batch [45/50] time 0.083 (0.119) data 0.000 (0.033) loss 2.1230 (1.9224) acc 37.5000 (43.1944) lr 5.7422e-04 eta 0:06:57
epoch [130/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.030) loss 1.7578 (1.9117) acc 50.0000 (43.2500) lr 5.6006e-04 eta 0:06:44
epoch [131/200] batch [5/50] time 0.085 (0.318) data 0.000 (0.232) loss 1.6709 (1.8041) acc 50.0000 (46.2500) lr 5.6006e-04 eta 0:18:31
epoch [131/200] batch [10/50] time 0.085 (0.202) data 0.000 (0.116) loss 1.9434 (1.8280) acc 43.7500 (46.2500) lr 5.6006e-04 eta 0:11:44
epoch [131/200] batch [15/50] time 0.087 (0.164) data 0.000 (0.078) loss 2.0293 (1.8352) acc 40.6250 (45.6250) lr 5.6006e-04 eta 0:09:29
epoch [131/200] batch [20/50] time 0.087 (0.144) data 0.000 (0.058) loss 2.0273 (1.9062) acc 25.0000 (43.5938) lr 5.6006e-04 eta 0:08:21
epoch [131/200] batch [25/50] time 0.085 (0.133) data 0.000 (0.047) loss 2.0371 (1.9158) acc 25.0000 (42.6250) lr 5.6006e-04 eta 0:07:41
epoch [131/200] batch [30/50] time 0.086 (0.131) data 0.000 (0.045) loss 2.1797 (1.9419) acc 25.0000 (41.2500) lr 5.6006e-04 eta 0:07:33
epoch [131/200] batch [35/50] time 0.307 (0.130) data 0.222 (0.045) loss 2.0391 (1.9267) acc 28.1250 (41.3393) lr 5.6006e-04 eta 0:07:32
epoch [131/200] batch [40/50] time 0.084 (0.125) data 0.000 (0.039) loss 1.4277 (1.9077) acc 43.7500 (41.7969) lr 5.6006e-04 eta 0:07:11
epoch [131/200] batch [45/50] time 0.084 (0.123) data 0.000 (0.038) loss 1.6807 (1.8993) acc 59.3750 (42.2222) lr 5.6006e-04 eta 0:07:05
epoch [131/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.034) loss 1.9443 (1.8880) acc 37.5000 (42.5000) lr 5.4601e-04 eta 0:06:50
epoch [132/200] batch [5/50] time 0.085 (0.321) data 0.000 (0.237) loss 1.8740 (1.9338) acc 53.1250 (47.5000) lr 5.4601e-04 eta 0:18:26
epoch [132/200] batch [10/50] time 0.247 (0.219) data 0.163 (0.135) loss 2.3594 (1.9715) acc 28.1250 (43.4375) lr 5.4601e-04 eta 0:12:33
epoch [132/200] batch [15/50] time 0.084 (0.174) data 0.000 (0.090) loss 2.0137 (1.9206) acc 43.7500 (44.5833) lr 5.4601e-04 eta 0:09:58
epoch [132/200] batch [20/50] time 0.083 (0.163) data 0.000 (0.079) loss 1.3232 (1.9175) acc 56.2500 (45.1562) lr 5.4601e-04 eta 0:09:19
epoch [132/200] batch [25/50] time 0.084 (0.147) data 0.000 (0.063) loss 1.3701 (1.9050) acc 62.5000 (44.8750) lr 5.4601e-04 eta 0:08:25
epoch [132/200] batch [30/50] time 0.084 (0.144) data 0.000 (0.060) loss 2.2090 (1.9390) acc 40.6250 (43.9583) lr 5.4601e-04 eta 0:08:12
epoch [132/200] batch [35/50] time 0.086 (0.140) data 0.001 (0.055) loss 1.6250 (1.9440) acc 50.0000 (43.3929) lr 5.4601e-04 eta 0:07:56
epoch [132/200] batch [40/50] time 0.084 (0.133) data 0.000 (0.049) loss 1.8408 (1.9362) acc 43.7500 (43.3594) lr 5.4601e-04 eta 0:07:31
epoch [132/200] batch [45/50] time 0.084 (0.127) data 0.000 (0.043) loss 2.1270 (1.9337) acc 34.3750 (43.6806) lr 5.4601e-04 eta 0:07:12
epoch [132/200] batch [50/50] time 0.084 (0.123) data 0.000 (0.039) loss 1.9346 (1.9400) acc 40.6250 (43.6250) lr 5.3207e-04 eta 0:06:57
epoch [133/200] batch [5/50] time 0.087 (0.349) data 0.001 (0.263) loss 1.9668 (2.0395) acc 50.0000 (43.1250) lr 5.3207e-04 eta 0:19:45
epoch [133/200] batch [10/50] time 0.087 (0.218) data 0.001 (0.132) loss 2.0918 (2.0378) acc 40.6250 (44.6875) lr 5.3207e-04 eta 0:12:20
epoch [133/200] batch [15/50] time 0.088 (0.178) data 0.001 (0.092) loss 2.0254 (2.0434) acc 46.8750 (45.6250) lr 5.3207e-04 eta 0:10:03
epoch [133/200] batch [20/50] time 0.087 (0.155) data 0.001 (0.069) loss 2.5605 (2.0518) acc 21.8750 (43.2812) lr 5.3207e-04 eta 0:08:45
epoch [133/200] batch [25/50] time 0.087 (0.142) data 0.001 (0.055) loss 1.9697 (2.0298) acc 40.6250 (42.2500) lr 5.3207e-04 eta 0:07:58
epoch [133/200] batch [30/50] time 0.086 (0.133) data 0.000 (0.046) loss 1.9785 (1.9826) acc 46.8750 (43.1250) lr 5.3207e-04 eta 0:07:27
epoch [133/200] batch [35/50] time 0.085 (0.126) data 0.001 (0.040) loss 2.0098 (2.0030) acc 37.5000 (42.5893) lr 5.3207e-04 eta 0:07:03
epoch [133/200] batch [40/50] time 0.086 (0.122) data 0.000 (0.036) loss 1.7344 (1.9874) acc 46.8750 (42.7344) lr 5.3207e-04 eta 0:06:49
epoch [133/200] batch [45/50] time 0.085 (0.118) data 0.000 (0.032) loss 1.7676 (1.9637) acc 46.8750 (42.9861) lr 5.3207e-04 eta 0:06:34
epoch [133/200] batch [50/50] time 0.085 (0.116) data 0.000 (0.031) loss 1.8037 (1.9582) acc 46.8750 (43.0000) lr 5.1825e-04 eta 0:06:30
epoch [134/200] batch [5/50] time 0.086 (0.304) data 0.000 (0.217) loss 1.9111 (1.8273) acc 46.8750 (43.7500) lr 5.1825e-04 eta 0:16:55
epoch [134/200] batch [10/50] time 0.089 (0.196) data 0.000 (0.109) loss 1.9434 (1.9555) acc 46.8750 (40.9375) lr 5.1825e-04 eta 0:10:53
epoch [134/200] batch [15/50] time 0.086 (0.167) data 0.000 (0.081) loss 2.1504 (1.9523) acc 40.6250 (40.4167) lr 5.1825e-04 eta 0:09:17
epoch [134/200] batch [20/50] time 0.203 (0.153) data 0.119 (0.067) loss 1.4746 (1.8833) acc 62.5000 (43.5938) lr 5.1825e-04 eta 0:08:28
epoch [134/200] batch [25/50] time 0.086 (0.140) data 0.001 (0.054) loss 2.2480 (1.9161) acc 31.2500 (42.3750) lr 5.1825e-04 eta 0:07:44
epoch [134/200] batch [30/50] time 0.085 (0.132) data 0.000 (0.046) loss 1.9248 (1.8863) acc 37.5000 (43.9583) lr 5.1825e-04 eta 0:07:17
epoch [134/200] batch [35/50] time 0.084 (0.125) data 0.001 (0.040) loss 1.7979 (1.8837) acc 40.6250 (44.0179) lr 5.1825e-04 eta 0:06:55
epoch [134/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.041) loss 1.9756 (1.8699) acc 40.6250 (43.6719) lr 5.1825e-04 eta 0:06:57
epoch [134/200] batch [45/50] time 0.086 (0.121) data 0.000 (0.036) loss 2.0391 (1.8788) acc 46.8750 (44.2361) lr 5.1825e-04 eta 0:06:41
epoch [134/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.033) loss 2.4492 (1.8914) acc 34.3750 (43.7500) lr 5.0454e-04 eta 0:06:28
epoch [135/200] batch [5/50] time 0.086 (0.324) data 0.000 (0.238) loss 2.0449 (1.8807) acc 46.8750 (48.1250) lr 5.0454e-04 eta 0:17:48
epoch [135/200] batch [10/50] time 0.087 (0.205) data 0.000 (0.119) loss 1.6992 (1.8904) acc 50.0000 (45.9375) lr 5.0454e-04 eta 0:11:15
epoch [135/200] batch [15/50] time 0.087 (0.166) data 0.000 (0.080) loss 2.1895 (1.8815) acc 34.3750 (44.5833) lr 5.0454e-04 eta 0:09:05
epoch [135/200] batch [20/50] time 0.087 (0.147) data 0.000 (0.061) loss 2.1660 (1.8832) acc 43.7500 (44.3750) lr 5.0454e-04 eta 0:08:03
epoch [135/200] batch [25/50] time 0.085 (0.138) data 0.000 (0.052) loss 1.4951 (1.8776) acc 56.2500 (44.3750) lr 5.0454e-04 eta 0:07:31
epoch [135/200] batch [30/50] time 0.085 (0.133) data 0.000 (0.047) loss 1.7646 (1.8614) acc 37.5000 (44.7917) lr 5.0454e-04 eta 0:07:14
epoch [135/200] batch [35/50] time 0.245 (0.131) data 0.163 (0.045) loss 2.3242 (1.9003) acc 37.5000 (43.3929) lr 5.0454e-04 eta 0:07:06
epoch [135/200] batch [40/50] time 0.084 (0.125) data 0.000 (0.039) loss 2.0723 (1.9002) acc 43.7500 (43.9844) lr 5.0454e-04 eta 0:06:46
epoch [135/200] batch [45/50] time 0.083 (0.120) data 0.000 (0.035) loss 1.9688 (1.9033) acc 40.6250 (43.6111) lr 5.0454e-04 eta 0:06:32
epoch [135/200] batch [50/50] time 0.085 (0.117) data 0.000 (0.032) loss 1.8896 (1.9052) acc 43.7500 (43.5000) lr 4.9096e-04 eta 0:06:19
epoch [136/200] batch [5/50] time 0.085 (0.330) data 0.001 (0.244) loss 1.6445 (1.6719) acc 50.0000 (50.0000) lr 4.9096e-04 eta 0:17:52
epoch [136/200] batch [10/50] time 0.084 (0.209) data 0.000 (0.123) loss 2.1191 (1.8756) acc 37.5000 (45.0000) lr 4.9096e-04 eta 0:11:16
epoch [136/200] batch [15/50] time 0.085 (0.168) data 0.000 (0.082) loss 1.9375 (1.9243) acc 50.0000 (46.2500) lr 4.9096e-04 eta 0:09:02
epoch [136/200] batch [20/50] time 0.085 (0.151) data 0.000 (0.066) loss 2.3848 (1.9392) acc 28.1250 (45.7812) lr 4.9096e-04 eta 0:08:08
epoch [136/200] batch [25/50] time 0.086 (0.138) data 0.000 (0.053) loss 2.1387 (1.9472) acc 50.0000 (44.3750) lr 4.9096e-04 eta 0:07:25
epoch [136/200] batch [30/50] time 0.086 (0.129) data 0.000 (0.044) loss 1.8516 (1.9597) acc 46.8750 (43.4375) lr 4.9096e-04 eta 0:06:56
epoch [136/200] batch [35/50] time 0.086 (0.123) data 0.001 (0.038) loss 2.0508 (1.9743) acc 53.1250 (43.3036) lr 4.9096e-04 eta 0:06:36
epoch [136/200] batch [40/50] time 0.085 (0.119) data 0.000 (0.033) loss 1.7666 (1.9651) acc 40.6250 (43.2812) lr 4.9096e-04 eta 0:06:20
epoch [136/200] batch [45/50] time 0.085 (0.116) data 0.000 (0.031) loss 2.0098 (1.9784) acc 40.6250 (42.9167) lr 4.9096e-04 eta 0:06:11
epoch [136/200] batch [50/50] time 0.084 (0.113) data 0.000 (0.028) loss 1.9287 (1.9695) acc 46.8750 (43.2500) lr 4.7750e-04 eta 0:06:01
epoch [137/200] batch [5/50] time 0.085 (0.327) data 0.000 (0.242) loss 2.0664 (2.0762) acc 34.3750 (40.0000) lr 4.7750e-04 eta 0:17:23
epoch [137/200] batch [10/50] time 0.089 (0.219) data 0.000 (0.134) loss 1.9648 (1.9441) acc 50.0000 (43.1250) lr 4.7750e-04 eta 0:11:40
epoch [137/200] batch [15/50] time 0.086 (0.175) data 0.000 (0.089) loss 1.4385 (1.9011) acc 65.6250 (44.7917) lr 4.7750e-04 eta 0:09:17
epoch [137/200] batch [20/50] time 0.086 (0.154) data 0.000 (0.068) loss 2.4395 (1.9610) acc 31.2500 (42.6562) lr 4.7750e-04 eta 0:08:08
epoch [137/200] batch [25/50] time 0.085 (0.140) data 0.000 (0.054) loss 1.8359 (1.9486) acc 46.8750 (42.8750) lr 4.7750e-04 eta 0:07:24
epoch [137/200] batch [30/50] time 0.142 (0.135) data 0.059 (0.049) loss 1.6416 (1.9563) acc 59.3750 (43.0208) lr 4.7750e-04 eta 0:07:07
epoch [137/200] batch [35/50] time 0.084 (0.128) data 0.000 (0.043) loss 1.7988 (1.9595) acc 50.0000 (42.6786) lr 4.7750e-04 eta 0:06:45
epoch [137/200] batch [40/50] time 0.083 (0.128) data 0.000 (0.043) loss 2.1230 (1.9520) acc 43.7500 (42.8125) lr 4.7750e-04 eta 0:06:44
epoch [137/200] batch [45/50] time 0.083 (0.123) data 0.000 (0.038) loss 1.8936 (1.9589) acc 40.6250 (42.6389) lr 4.7750e-04 eta 0:06:27
epoch [137/200] batch [50/50] time 0.084 (0.120) data 0.000 (0.035) loss 1.6006 (1.9413) acc 56.2500 (43.3125) lr 4.6417e-04 eta 0:06:17
epoch [138/200] batch [5/50] time 0.084 (0.338) data 0.000 (0.254) loss 1.9824 (1.9574) acc 40.6250 (39.3750) lr 4.6417e-04 eta 0:17:42
epoch [138/200] batch [10/50] time 0.085 (0.211) data 0.000 (0.127) loss 1.9346 (1.9580) acc 37.5000 (40.3125) lr 4.6417e-04 eta 0:11:03
epoch [138/200] batch [15/50] time 0.085 (0.169) data 0.000 (0.085) loss 1.9648 (1.9693) acc 43.7500 (40.6250) lr 4.6417e-04 eta 0:08:50
epoch [138/200] batch [20/50] time 0.085 (0.152) data 0.000 (0.067) loss 1.7217 (1.9425) acc 53.1250 (42.8125) lr 4.6417e-04 eta 0:07:55
epoch [138/200] batch [25/50] time 0.095 (0.139) data 0.011 (0.054) loss 1.8496 (1.9336) acc 50.0000 (44.0000) lr 4.6417e-04 eta 0:07:13
epoch [138/200] batch [30/50] time 0.084 (0.130) data 0.000 (0.045) loss 1.5557 (1.9145) acc 46.8750 (43.7500) lr 4.6417e-04 eta 0:06:44
epoch [138/200] batch [35/50] time 0.151 (0.125) data 0.067 (0.041) loss 1.8945 (1.9097) acc 40.6250 (43.3929) lr 4.6417e-04 eta 0:06:29
epoch [138/200] batch [40/50] time 0.084 (0.122) data 0.000 (0.037) loss 2.1074 (1.9271) acc 40.6250 (43.5156) lr 4.6417e-04 eta 0:06:18
epoch [138/200] batch [45/50] time 0.084 (0.117) data 0.000 (0.033) loss 1.7227 (1.9170) acc 43.7500 (43.6806) lr 4.6417e-04 eta 0:06:04
epoch [138/200] batch [50/50] time 0.083 (0.116) data 0.000 (0.032) loss 1.3105 (1.9183) acc 50.0000 (43.1875) lr 4.5098e-04 eta 0:05:58
epoch [139/200] batch [5/50] time 0.084 (0.318) data 0.000 (0.233) loss 1.9873 (1.8713) acc 53.1250 (50.0000) lr 4.5098e-04 eta 0:16:23
epoch [139/200] batch [10/50] time 0.084 (0.201) data 0.000 (0.117) loss 2.2461 (1.9104) acc 40.6250 (45.0000) lr 4.5098e-04 eta 0:10:21
epoch [139/200] batch [15/50] time 0.084 (0.162) data 0.000 (0.078) loss 2.0352 (1.8785) acc 43.7500 (44.7917) lr 4.5098e-04 eta 0:08:20
epoch [139/200] batch [20/50] time 0.179 (0.151) data 0.094 (0.067) loss 2.2090 (1.8967) acc 31.2500 (44.0625) lr 4.5098e-04 eta 0:07:44
epoch [139/200] batch [25/50] time 0.084 (0.137) data 0.000 (0.053) loss 2.0547 (1.8893) acc 46.8750 (44.2500) lr 4.5098e-04 eta 0:07:02
epoch [139/200] batch [30/50] time 0.085 (0.136) data 0.000 (0.052) loss 1.7393 (1.9125) acc 53.1250 (43.4375) lr 4.5098e-04 eta 0:06:56
epoch [139/200] batch [35/50] time 0.085 (0.129) data 0.000 (0.044) loss 1.9512 (1.9195) acc 37.5000 (43.0357) lr 4.5098e-04 eta 0:06:33
epoch [139/200] batch [40/50] time 0.085 (0.126) data 0.000 (0.042) loss 2.0762 (1.9139) acc 40.6250 (43.6719) lr 4.5098e-04 eta 0:06:26
epoch [139/200] batch [45/50] time 0.083 (0.122) data 0.000 (0.037) loss 1.9336 (1.9115) acc 34.3750 (43.6806) lr 4.5098e-04 eta 0:06:11
epoch [139/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.034) loss 2.4316 (1.9114) acc 25.0000 (43.6875) lr 4.3792e-04 eta 0:05:59
epoch [140/200] batch [5/50] time 0.085 (0.310) data 0.000 (0.224) loss 1.4209 (1.7516) acc 53.1250 (48.7500) lr 4.3792e-04 eta 0:15:44
epoch [140/200] batch [10/50] time 0.087 (0.200) data 0.000 (0.113) loss 2.0469 (1.7860) acc 50.0000 (50.0000) lr 4.3792e-04 eta 0:10:08
epoch [140/200] batch [15/50] time 0.088 (0.170) data 0.001 (0.083) loss 1.6758 (1.8058) acc 50.0000 (47.9167) lr 4.3792e-04 eta 0:08:35
epoch [140/200] batch [20/50] time 0.085 (0.154) data 0.000 (0.067) loss 1.8877 (1.8225) acc 43.7500 (47.6562) lr 4.3792e-04 eta 0:07:45
epoch [140/200] batch [25/50] time 0.088 (0.140) data 0.000 (0.054) loss 1.7607 (1.8659) acc 53.1250 (46.5000) lr 4.3792e-04 eta 0:07:03
epoch [140/200] batch [30/50] time 0.085 (0.131) data 0.000 (0.045) loss 1.7402 (1.8993) acc 50.0000 (45.3125) lr 4.3792e-04 eta 0:06:36
epoch [140/200] batch [35/50] time 0.085 (0.125) data 0.001 (0.039) loss 1.8008 (1.8886) acc 46.8750 (45.3571) lr 4.3792e-04 eta 0:06:15
epoch [140/200] batch [40/50] time 0.085 (0.120) data 0.000 (0.034) loss 1.9443 (1.8737) acc 46.8750 (45.7812) lr 4.3792e-04 eta 0:05:59
epoch [140/200] batch [45/50] time 0.084 (0.117) data 0.000 (0.031) loss 1.6641 (1.8874) acc 59.3750 (45.4861) lr 4.3792e-04 eta 0:05:51
epoch [140/200] batch [50/50] time 0.084 (0.114) data 0.000 (0.028) loss 2.4121 (1.8971) acc 25.0000 (45.3750) lr 4.2499e-04 eta 0:05:40
epoch [141/200] batch [5/50] time 0.088 (0.328) data 0.000 (0.241) loss 1.6855 (1.8828) acc 62.5000 (48.1250) lr 4.2499e-04 eta 0:16:21
epoch [141/200] batch [10/50] time 0.141 (0.213) data 0.052 (0.126) loss 1.9062 (1.8823) acc 43.7500 (44.0625) lr 4.2499e-04 eta 0:10:36
epoch [141/200] batch [15/50] time 0.087 (0.171) data 0.001 (0.084) loss 1.7900 (1.8550) acc 46.8750 (45.0000) lr 4.2499e-04 eta 0:08:29
epoch [141/200] batch [20/50] time 0.086 (0.154) data 0.001 (0.067) loss 1.3135 (1.7929) acc 62.5000 (46.0938) lr 4.2499e-04 eta 0:07:38
epoch [141/200] batch [25/50] time 0.087 (0.141) data 0.000 (0.054) loss 1.9346 (1.8401) acc 46.8750 (45.2500) lr 4.2499e-04 eta 0:06:58
epoch [141/200] batch [30/50] time 0.085 (0.132) data 0.000 (0.045) loss 2.2637 (1.8511) acc 31.2500 (44.7917) lr 4.2499e-04 eta 0:06:31
epoch [141/200] batch [35/50] time 0.086 (0.126) data 0.001 (0.039) loss 1.7461 (1.8564) acc 59.3750 (45.0000) lr 4.2499e-04 eta 0:06:12
epoch [141/200] batch [40/50] time 0.085 (0.121) data 0.000 (0.034) loss 2.1250 (1.8529) acc 37.5000 (45.1562) lr 4.2499e-04 eta 0:05:57
epoch [141/200] batch [45/50] time 0.085 (0.117) data 0.000 (0.030) loss 1.7852 (1.8498) acc 50.0000 (45.4167) lr 4.2499e-04 eta 0:05:45
epoch [141/200] batch [50/50] time 0.083 (0.113) data 0.000 (0.027) loss 1.6328 (1.8592) acc 65.6250 (45.1250) lr 4.1221e-04 eta 0:05:34
epoch [142/200] batch [5/50] time 0.084 (0.314) data 0.000 (0.229) loss 2.1367 (1.8609) acc 34.3750 (45.0000) lr 4.1221e-04 eta 0:15:26
epoch [142/200] batch [10/50] time 0.273 (0.218) data 0.190 (0.133) loss 2.0137 (1.8322) acc 46.8750 (43.4375) lr 4.1221e-04 eta 0:10:41
epoch [142/200] batch [15/50] time 0.085 (0.174) data 0.000 (0.089) loss 2.1016 (1.8706) acc 46.8750 (45.2083) lr 4.1221e-04 eta 0:08:30
epoch [142/200] batch [20/50] time 0.086 (0.159) data 0.001 (0.074) loss 1.3096 (1.8769) acc 68.7500 (45.4688) lr 4.1221e-04 eta 0:07:46
epoch [142/200] batch [25/50] time 0.086 (0.145) data 0.000 (0.059) loss 2.2383 (1.8963) acc 37.5000 (45.0000) lr 4.1221e-04 eta 0:07:02
epoch [142/200] batch [30/50] time 0.085 (0.140) data 0.000 (0.055) loss 1.7764 (1.8831) acc 37.5000 (45.0000) lr 4.1221e-04 eta 0:06:48
epoch [142/200] batch [35/50] time 0.086 (0.137) data 0.001 (0.052) loss 2.2363 (1.8901) acc 37.5000 (44.7321) lr 4.1221e-04 eta 0:06:39
epoch [142/200] batch [40/50] time 0.082 (0.130) data 0.000 (0.045) loss 2.1621 (1.9049) acc 40.6250 (43.8281) lr 4.1221e-04 eta 0:06:19
epoch [142/200] batch [45/50] time 0.083 (0.128) data 0.000 (0.043) loss 2.0117 (1.9059) acc 37.5000 (43.4028) lr 4.1221e-04 eta 0:06:12
epoch [142/200] batch [50/50] time 0.083 (0.124) data 0.000 (0.039) loss 2.0117 (1.9157) acc 50.0000 (43.5000) lr 3.9958e-04 eta 0:05:58
epoch [143/200] batch [5/50] time 0.086 (0.305) data 0.000 (0.218) loss 1.9082 (1.8523) acc 40.6250 (41.2500) lr 3.9958e-04 eta 0:14:42
epoch [143/200] batch [10/50] time 0.204 (0.208) data 0.118 (0.121) loss 2.0703 (1.9249) acc 34.3750 (39.3750) lr 3.9958e-04 eta 0:10:00
epoch [143/200] batch [15/50] time 0.087 (0.167) data 0.000 (0.081) loss 2.0117 (1.9010) acc 31.2500 (41.4583) lr 3.9958e-04 eta 0:08:03
epoch [143/200] batch [20/50] time 0.084 (0.156) data 0.000 (0.069) loss 1.7451 (1.9098) acc 56.2500 (42.0312) lr 3.9958e-04 eta 0:07:28
epoch [143/200] batch [25/50] time 0.085 (0.142) data 0.000 (0.056) loss 2.1270 (1.9136) acc 37.5000 (42.2500) lr 3.9958e-04 eta 0:06:47
epoch [143/200] batch [30/50] time 0.086 (0.134) data 0.000 (0.048) loss 2.1582 (1.9211) acc 46.8750 (43.0208) lr 3.9958e-04 eta 0:06:24
epoch [143/200] batch [35/50] time 0.085 (0.127) data 0.001 (0.041) loss 1.8340 (1.9350) acc 50.0000 (43.3036) lr 3.9958e-04 eta 0:06:03
epoch [143/200] batch [40/50] time 0.083 (0.126) data 0.000 (0.041) loss 1.6885 (1.9275) acc 46.8750 (43.0469) lr 3.9958e-04 eta 0:06:01
epoch [143/200] batch [45/50] time 0.201 (0.124) data 0.118 (0.039) loss 2.0742 (1.9381) acc 37.5000 (42.5000) lr 3.9958e-04 eta 0:05:54
epoch [143/200] batch [50/50] time 0.084 (0.120) data 0.000 (0.035) loss 1.9404 (1.9312) acc 46.8750 (43.0625) lr 3.8709e-04 eta 0:05:42
epoch [144/200] batch [5/50] time 0.085 (0.347) data 0.000 (0.262) loss 1.9824 (1.8838) acc 53.1250 (49.3750) lr 3.8709e-04 eta 0:16:26
epoch [144/200] batch [10/50] time 0.085 (0.225) data 0.000 (0.140) loss 1.4111 (1.8960) acc 50.0000 (45.9375) lr 3.8709e-04 eta 0:10:37
epoch [144/200] batch [15/50] time 0.085 (0.178) data 0.000 (0.094) loss 1.8359 (1.8689) acc 56.2500 (45.6250) lr 3.8709e-04 eta 0:08:24
epoch [144/200] batch [20/50] time 0.087 (0.164) data 0.001 (0.079) loss 2.2930 (1.9167) acc 37.5000 (44.5312) lr 3.8709e-04 eta 0:07:44
epoch [144/200] batch [25/50] time 0.211 (0.153) data 0.125 (0.069) loss 1.5713 (1.9013) acc 53.1250 (44.0000) lr 3.8709e-04 eta 0:07:13
epoch [144/200] batch [30/50] time 0.085 (0.142) data 0.000 (0.057) loss 1.7178 (1.9149) acc 59.3750 (44.1667) lr 3.8709e-04 eta 0:06:40
epoch [144/200] batch [35/50] time 0.085 (0.140) data 0.000 (0.055) loss 1.7529 (1.9016) acc 53.1250 (44.5536) lr 3.8709e-04 eta 0:06:34
epoch [144/200] batch [40/50] time 0.084 (0.133) data 0.000 (0.049) loss 2.0605 (1.9060) acc 40.6250 (44.2188) lr 3.8709e-04 eta 0:06:14
epoch [144/200] batch [45/50] time 0.083 (0.131) data 0.000 (0.047) loss 1.6992 (1.8974) acc 40.6250 (43.8194) lr 3.8709e-04 eta 0:06:08
epoch [144/200] batch [50/50] time 0.084 (0.127) data 0.000 (0.042) loss 1.7012 (1.9022) acc 43.7500 (43.5000) lr 3.7476e-04 eta 0:05:54
epoch [145/200] batch [5/50] time 0.085 (0.319) data 0.000 (0.233) loss 1.9365 (1.9975) acc 53.1250 (41.8750) lr 3.7476e-04 eta 0:14:50
epoch [145/200] batch [10/50] time 0.085 (0.202) data 0.001 (0.117) loss 2.1699 (1.9743) acc 43.7500 (43.1250) lr 3.7476e-04 eta 0:09:23
epoch [145/200] batch [15/50] time 0.093 (0.164) data 0.001 (0.079) loss 2.2012 (1.9711) acc 34.3750 (43.1250) lr 3.7476e-04 eta 0:07:37
epoch [145/200] batch [20/50] time 0.087 (0.152) data 0.001 (0.066) loss 1.9551 (1.9759) acc 34.3750 (43.2812) lr 3.7476e-04 eta 0:07:01
epoch [145/200] batch [25/50] time 0.084 (0.138) data 0.000 (0.053) loss 1.8105 (1.9622) acc 43.7500 (44.0000) lr 3.7476e-04 eta 0:06:23
epoch [145/200] batch [30/50] time 0.084 (0.134) data 0.000 (0.050) loss 1.8955 (1.9511) acc 46.8750 (44.2708) lr 3.7476e-04 eta 0:06:12
epoch [145/200] batch [35/50] time 0.321 (0.134) data 0.239 (0.049) loss 2.1562 (1.9383) acc 21.8750 (44.1964) lr 3.7476e-04 eta 0:06:10
epoch [145/200] batch [40/50] time 0.084 (0.128) data 0.000 (0.043) loss 1.6963 (1.9202) acc 37.5000 (44.4531) lr 3.7476e-04 eta 0:05:52
epoch [145/200] batch [45/50] time 0.085 (0.126) data 0.000 (0.041) loss 2.0645 (1.9270) acc 37.5000 (44.2361) lr 3.7476e-04 eta 0:05:45
epoch [145/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.8193 (1.9200) acc 40.6250 (44.0625) lr 3.6258e-04 eta 0:05:33
epoch [146/200] batch [5/50] time 0.083 (0.321) data 0.000 (0.237) loss 1.8320 (1.9922) acc 43.7500 (41.2500) lr 3.6258e-04 eta 0:14:41
epoch [146/200] batch [10/50] time 0.194 (0.213) data 0.112 (0.130) loss 1.6484 (1.8907) acc 68.7500 (47.8125) lr 3.6258e-04 eta 0:09:44
epoch [146/200] batch [15/50] time 0.086 (0.170) data 0.001 (0.087) loss 1.7812 (1.8872) acc 43.7500 (45.8333) lr 3.6258e-04 eta 0:07:45
epoch [146/200] batch [20/50] time 0.084 (0.149) data 0.001 (0.065) loss 1.8838 (1.9368) acc 43.7500 (45.0000) lr 3.6258e-04 eta 0:06:45
epoch [146/200] batch [25/50] time 0.204 (0.141) data 0.119 (0.057) loss 2.6133 (1.9790) acc 21.8750 (42.7500) lr 3.6258e-04 eta 0:06:23
epoch [146/200] batch [30/50] time 0.085 (0.131) data 0.001 (0.048) loss 1.9375 (1.9491) acc 34.3750 (42.7083) lr 3.6258e-04 eta 0:05:57
epoch [146/200] batch [35/50] time 0.085 (0.133) data 0.000 (0.049) loss 1.9277 (1.9255) acc 37.5000 (42.7679) lr 3.6258e-04 eta 0:06:00
epoch [146/200] batch [40/50] time 0.085 (0.127) data 0.000 (0.043) loss 1.7578 (1.9487) acc 40.6250 (42.4219) lr 3.6258e-04 eta 0:05:43
epoch [146/200] batch [45/50] time 0.084 (0.124) data 0.000 (0.040) loss 1.4961 (1.9127) acc 56.2500 (42.8472) lr 3.6258e-04 eta 0:05:34
epoch [146/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.036) loss 1.7725 (1.9032) acc 56.2500 (43.1875) lr 3.5055e-04 eta 0:05:23
epoch [147/200] batch [5/50] time 0.130 (0.344) data 0.018 (0.253) loss 1.7383 (1.8256) acc 37.5000 (47.5000) lr 3.5055e-04 eta 0:15:26
epoch [147/200] batch [10/50] time 0.086 (0.231) data 0.001 (0.136) loss 1.6104 (1.7943) acc 53.1250 (49.0625) lr 3.5055e-04 eta 0:10:20
epoch [147/200] batch [15/50] time 0.085 (0.182) data 0.000 (0.091) loss 1.7031 (1.7877) acc 37.5000 (48.5417) lr 3.5055e-04 eta 0:08:09
epoch [147/200] batch [20/50] time 0.086 (0.166) data 0.001 (0.076) loss 1.5977 (1.8174) acc 59.3750 (46.8750) lr 3.5055e-04 eta 0:07:24
epoch [147/200] batch [25/50] time 0.083 (0.149) data 0.000 (0.061) loss 2.2188 (1.8702) acc 34.3750 (45.0000) lr 3.5055e-04 eta 0:06:39
epoch [147/200] batch [30/50] time 0.084 (0.139) data 0.000 (0.051) loss 2.4102 (1.9281) acc 31.2500 (43.6458) lr 3.5055e-04 eta 0:06:09
epoch [147/200] batch [35/50] time 0.085 (0.131) data 0.000 (0.044) loss 1.9648 (1.9324) acc 37.5000 (43.7500) lr 3.5055e-04 eta 0:05:48
epoch [147/200] batch [40/50] time 0.084 (0.125) data 0.000 (0.038) loss 1.9873 (1.9353) acc 40.6250 (44.2188) lr 3.5055e-04 eta 0:05:32
epoch [147/200] batch [45/50] time 0.085 (0.120) data 0.000 (0.034) loss 1.5312 (1.9124) acc 50.0000 (44.4444) lr 3.5055e-04 eta 0:05:19
epoch [147/200] batch [50/50] time 0.084 (0.117) data 0.000 (0.031) loss 1.7451 (1.8974) acc 43.7500 (44.8125) lr 3.3869e-04 eta 0:05:09
epoch [148/200] batch [5/50] time 0.085 (0.343) data 0.000 (0.258) loss 1.9229 (1.9545) acc 40.6250 (43.1250) lr 3.3869e-04 eta 0:15:07
epoch [148/200] batch [10/50] time 0.084 (0.228) data 0.000 (0.144) loss 2.2402 (1.9410) acc 37.5000 (42.1875) lr 3.3869e-04 eta 0:10:01
epoch [148/200] batch [15/50] time 0.086 (0.180) data 0.000 (0.096) loss 1.8906 (1.8864) acc 40.6250 (44.7917) lr 3.3869e-04 eta 0:07:54
epoch [148/200] batch [20/50] time 0.085 (0.161) data 0.000 (0.077) loss 1.2422 (1.8542) acc 65.6250 (45.9375) lr 3.3869e-04 eta 0:07:04
epoch [148/200] batch [25/50] time 0.085 (0.146) data 0.000 (0.062) loss 2.2266 (1.9140) acc 21.8750 (43.3750) lr 3.3869e-04 eta 0:06:23
epoch [148/200] batch [30/50] time 0.086 (0.142) data 0.001 (0.057) loss 2.0879 (1.9267) acc 40.6250 (43.2292) lr 3.3869e-04 eta 0:06:11
epoch [148/200] batch [35/50] time 0.085 (0.140) data 0.001 (0.055) loss 1.9980 (1.9531) acc 34.3750 (42.2321) lr 3.3869e-04 eta 0:06:06
epoch [148/200] batch [40/50] time 0.085 (0.133) data 0.000 (0.049) loss 1.9873 (1.9358) acc 46.8750 (42.1875) lr 3.3869e-04 eta 0:05:47
epoch [148/200] batch [45/50] time 0.084 (0.130) data 0.000 (0.046) loss 1.9121 (1.9382) acc 34.3750 (42.1528) lr 3.3869e-04 eta 0:05:39
epoch [148/200] batch [50/50] time 0.084 (0.126) data 0.000 (0.041) loss 1.7832 (1.9391) acc 50.0000 (42.3125) lr 3.2699e-04 eta 0:05:26
epoch [149/200] batch [5/50] time 0.085 (0.344) data 0.000 (0.259) loss 1.9951 (2.0732) acc 46.8750 (41.8750) lr 3.2699e-04 eta 0:14:53
epoch [149/200] batch [10/50] time 0.085 (0.227) data 0.000 (0.142) loss 1.6670 (2.0310) acc 40.6250 (41.2500) lr 3.2699e-04 eta 0:09:48
epoch [149/200] batch [15/50] time 0.084 (0.180) data 0.000 (0.095) loss 1.8213 (1.9944) acc 40.6250 (41.0417) lr 3.2699e-04 eta 0:07:44
epoch [149/200] batch [20/50] time 0.086 (0.162) data 0.001 (0.077) loss 1.6211 (2.0096) acc 50.0000 (40.4688) lr 3.2699e-04 eta 0:06:57
epoch [149/200] batch [25/50] time 0.202 (0.151) data 0.117 (0.066) loss 1.8252 (1.9978) acc 46.8750 (41.1250) lr 3.2699e-04 eta 0:06:29
epoch [149/200] batch [30/50] time 0.086 (0.140) data 0.001 (0.055) loss 1.3711 (1.9747) acc 59.3750 (42.0833) lr 3.2699e-04 eta 0:06:00
epoch [149/200] batch [35/50] time 0.087 (0.137) data 0.001 (0.052) loss 2.3398 (1.9928) acc 40.6250 (41.6964) lr 3.2699e-04 eta 0:05:51
epoch [149/200] batch [40/50] time 0.085 (0.130) data 0.000 (0.046) loss 1.7832 (1.9854) acc 65.6250 (42.1875) lr 3.2699e-04 eta 0:05:33
epoch [149/200] batch [45/50] time 0.083 (0.129) data 0.000 (0.045) loss 1.5332 (1.9601) acc 46.8750 (42.5694) lr 3.2699e-04 eta 0:05:30
epoch [149/200] batch [50/50] time 0.083 (0.125) data 0.000 (0.040) loss 1.7051 (1.9411) acc 53.1250 (43.4375) lr 3.1545e-04 eta 0:05:17
epoch [150/200] batch [5/50] time 0.100 (0.307) data 0.014 (0.220) loss 1.8516 (1.8465) acc 34.3750 (43.7500) lr 3.1545e-04 eta 0:13:00
epoch [150/200] batch [10/50] time 0.176 (0.206) data 0.091 (0.119) loss 2.1172 (1.9908) acc 37.5000 (42.1875) lr 3.1545e-04 eta 0:08:42
epoch [150/200] batch [15/50] time 0.087 (0.171) data 0.001 (0.084) loss 1.6934 (1.9115) acc 50.0000 (45.0000) lr 3.1545e-04 eta 0:07:12
epoch [150/200] batch [20/50] time 0.088 (0.150) data 0.000 (0.063) loss 1.8643 (1.9450) acc 50.0000 (44.5312) lr 3.1545e-04 eta 0:06:18
epoch [150/200] batch [25/50] time 0.086 (0.143) data 0.001 (0.057) loss 1.8848 (1.9221) acc 46.8750 (44.6250) lr 3.1545e-04 eta 0:05:59
epoch [150/200] batch [30/50] time 0.086 (0.139) data 0.000 (0.053) loss 1.4844 (1.9071) acc 56.2500 (45.4167) lr 3.1545e-04 eta 0:05:49
epoch [150/200] batch [35/50] time 0.085 (0.131) data 0.001 (0.046) loss 1.9082 (1.9082) acc 40.6250 (45.0000) lr 3.1545e-04 eta 0:05:29
epoch [150/200] batch [40/50] time 0.083 (0.130) data 0.000 (0.045) loss 2.0176 (1.8959) acc 40.6250 (45.3906) lr 3.1545e-04 eta 0:05:26
epoch [150/200] batch [45/50] time 0.276 (0.129) data 0.194 (0.044) loss 1.9814 (1.8950) acc 34.3750 (45.2778) lr 3.1545e-04 eta 0:05:23
epoch [150/200] batch [50/50] time 0.085 (0.125) data 0.000 (0.040) loss 2.1562 (1.9018) acc 40.6250 (45.3750) lr 3.0409e-04 eta 0:05:11
epoch [151/200] batch [5/50] time 0.084 (0.329) data 0.000 (0.245) loss 1.8096 (1.8752) acc 43.7500 (46.2500) lr 3.0409e-04 eta 0:13:41
epoch [151/200] batch [10/50] time 0.084 (0.223) data 0.000 (0.139) loss 1.4932 (1.8569) acc 56.2500 (45.9375) lr 3.0409e-04 eta 0:09:15
epoch [151/200] batch [15/50] time 0.083 (0.177) data 0.000 (0.093) loss 2.0332 (1.9064) acc 40.6250 (45.4167) lr 3.0409e-04 eta 0:07:19
epoch [151/200] batch [20/50] time 0.085 (0.154) data 0.000 (0.070) loss 1.6953 (1.9102) acc 40.6250 (44.3750) lr 3.0409e-04 eta 0:06:21
epoch [151/200] batch [25/50] time 0.085 (0.140) data 0.000 (0.056) loss 1.8672 (1.9066) acc 34.3750 (44.2500) lr 3.0409e-04 eta 0:05:46
epoch [151/200] batch [30/50] time 0.150 (0.133) data 0.066 (0.049) loss 1.6738 (1.8998) acc 40.6250 (44.0625) lr 3.0409e-04 eta 0:05:28
epoch [151/200] batch [35/50] time 0.084 (0.130) data 0.001 (0.046) loss 1.6436 (1.8910) acc 50.0000 (43.8393) lr 3.0409e-04 eta 0:05:20
epoch [151/200] batch [40/50] time 0.084 (0.125) data 0.000 (0.041) loss 1.9463 (1.8825) acc 37.5000 (44.0625) lr 3.0409e-04 eta 0:05:07
epoch [151/200] batch [45/50] time 0.083 (0.122) data 0.000 (0.038) loss 2.2715 (1.8760) acc 31.2500 (43.9583) lr 3.0409e-04 eta 0:05:00
epoch [151/200] batch [50/50] time 0.084 (0.118) data 0.000 (0.035) loss 1.6045 (1.8727) acc 50.0000 (43.8750) lr 2.9289e-04 eta 0:04:50
epoch [152/200] batch [5/50] time 0.209 (0.309) data 0.124 (0.222) loss 1.7041 (1.8135) acc 46.8750 (42.5000) lr 2.9289e-04 eta 0:12:35
epoch [152/200] batch [10/50] time 0.087 (0.198) data 0.001 (0.111) loss 1.8838 (1.8999) acc 28.1250 (39.0625) lr 2.9289e-04 eta 0:08:04
epoch [152/200] batch [15/50] time 0.087 (0.166) data 0.000 (0.078) loss 1.6465 (1.8958) acc 53.1250 (42.5000) lr 2.9289e-04 eta 0:06:43
epoch [152/200] batch [20/50] time 0.089 (0.146) data 0.001 (0.059) loss 1.7988 (1.8751) acc 50.0000 (44.3750) lr 2.9289e-04 eta 0:05:54
epoch [152/200] batch [25/50] time 0.085 (0.138) data 0.000 (0.051) loss 1.6953 (1.8763) acc 53.1250 (44.2500) lr 2.9289e-04 eta 0:05:33
epoch [152/200] batch [30/50] time 0.085 (0.136) data 0.000 (0.049) loss 2.0254 (1.8861) acc 28.1250 (43.2292) lr 2.9289e-04 eta 0:05:28
epoch [152/200] batch [35/50] time 0.085 (0.129) data 0.001 (0.042) loss 2.1543 (1.9223) acc 37.5000 (42.4107) lr 2.9289e-04 eta 0:05:10
epoch [152/200] batch [40/50] time 0.084 (0.127) data 0.000 (0.040) loss 1.5967 (1.9265) acc 43.7500 (42.7344) lr 2.9289e-04 eta 0:05:04
epoch [152/200] batch [45/50] time 0.334 (0.127) data 0.251 (0.041) loss 1.6133 (1.9237) acc 46.8750 (42.6389) lr 2.9289e-04 eta 0:05:06
epoch [152/200] batch [50/50] time 0.083 (0.123) data 0.000 (0.037) loss 1.7451 (1.9150) acc 43.7500 (42.9375) lr 2.8187e-04 eta 0:04:54
epoch [153/200] batch [5/50] time 0.087 (0.292) data 0.000 (0.204) loss 2.0137 (1.9594) acc 46.8750 (40.0000) lr 2.8187e-04 eta 0:11:40
epoch [153/200] batch [10/50] time 0.087 (0.190) data 0.001 (0.103) loss 1.7100 (1.9021) acc 59.3750 (43.4375) lr 2.8187e-04 eta 0:07:35
epoch [153/200] batch [15/50] time 0.089 (0.156) data 0.000 (0.069) loss 1.4990 (1.8915) acc 56.2500 (45.0000) lr 2.8187e-04 eta 0:06:12
epoch [153/200] batch [20/50] time 0.087 (0.140) data 0.000 (0.052) loss 2.0039 (1.8767) acc 46.8750 (45.3125) lr 2.8187e-04 eta 0:05:32
epoch [153/200] batch [25/50] time 0.089 (0.129) data 0.001 (0.042) loss 1.8047 (1.8768) acc 40.6250 (44.8750) lr 2.8187e-04 eta 0:05:06
epoch [153/200] batch [30/50] time 0.090 (0.122) data 0.002 (0.035) loss 2.3379 (1.9383) acc 31.2500 (43.3333) lr 2.8187e-04 eta 0:04:49
epoch [153/200] batch [35/50] time 0.088 (0.121) data 0.001 (0.034) loss 1.4238 (1.9090) acc 56.2500 (44.1071) lr 2.8187e-04 eta 0:04:45
epoch [153/200] batch [40/50] time 0.085 (0.116) data 0.000 (0.029) loss 1.8975 (1.9107) acc 50.0000 (44.2188) lr 2.8187e-04 eta 0:04:34
epoch [153/200] batch [45/50] time 0.084 (0.113) data 0.000 (0.026) loss 1.6250 (1.9080) acc 59.3750 (44.7222) lr 2.8187e-04 eta 0:04:25
epoch [153/200] batch [50/50] time 0.084 (0.110) data 0.000 (0.024) loss 1.6309 (1.9154) acc 53.1250 (44.5000) lr 2.7103e-04 eta 0:04:18
epoch [154/200] batch [5/50] time 0.087 (0.295) data 0.001 (0.208) loss 2.0215 (1.8293) acc 43.7500 (46.8750) lr 2.7103e-04 eta 0:11:31
epoch [154/200] batch [10/50] time 0.087 (0.191) data 0.000 (0.104) loss 2.1172 (1.8892) acc 28.1250 (42.8125) lr 2.7103e-04 eta 0:07:26
epoch [154/200] batch [15/50] time 0.086 (0.160) data 0.001 (0.073) loss 1.9795 (1.9296) acc 43.7500 (41.6667) lr 2.7103e-04 eta 0:06:13
epoch [154/200] batch [20/50] time 0.207 (0.148) data 0.118 (0.061) loss 1.9746 (1.9103) acc 40.6250 (43.7500) lr 2.7103e-04 eta 0:05:43
epoch [154/200] batch [25/50] time 0.086 (0.136) data 0.001 (0.049) loss 1.8076 (1.8980) acc 50.0000 (44.1250) lr 2.7103e-04 eta 0:05:15
epoch [154/200] batch [30/50] time 0.086 (0.132) data 0.001 (0.046) loss 1.9238 (1.8891) acc 53.1250 (44.8958) lr 2.7103e-04 eta 0:05:07
epoch [154/200] batch [35/50] time 0.088 (0.126) data 0.001 (0.039) loss 1.7812 (1.8686) acc 43.7500 (45.0000) lr 2.7103e-04 eta 0:04:51
epoch [154/200] batch [40/50] time 0.087 (0.124) data 0.000 (0.038) loss 2.1973 (1.8657) acc 37.5000 (44.6094) lr 2.7103e-04 eta 0:04:46
epoch [154/200] batch [45/50] time 0.103 (0.123) data 0.020 (0.037) loss 1.8721 (1.8748) acc 34.3750 (44.5833) lr 2.7103e-04 eta 0:04:43
epoch [154/200] batch [50/50] time 0.084 (0.119) data 0.000 (0.033) loss 1.4053 (1.8644) acc 59.3750 (45.0625) lr 2.6037e-04 eta 0:04:33
epoch [155/200] batch [5/50] time 0.084 (0.334) data 0.000 (0.249) loss 1.8467 (1.8797) acc 40.6250 (46.2500) lr 2.6037e-04 eta 0:12:45
epoch [155/200] batch [10/50] time 0.245 (0.225) data 0.164 (0.141) loss 1.8867 (1.9070) acc 40.6250 (45.9375) lr 2.6037e-04 eta 0:08:35
epoch [155/200] batch [15/50] time 0.084 (0.178) data 0.000 (0.094) loss 2.1113 (1.8835) acc 43.7500 (46.4583) lr 2.6037e-04 eta 0:06:46
epoch [155/200] batch [20/50] time 0.085 (0.157) data 0.000 (0.073) loss 1.8291 (1.8521) acc 56.2500 (47.1875) lr 2.6037e-04 eta 0:05:57
epoch [155/200] batch [25/50] time 0.084 (0.142) data 0.000 (0.058) loss 1.7803 (1.8522) acc 46.8750 (47.3750) lr 2.6037e-04 eta 0:05:23
epoch [155/200] batch [30/50] time 0.084 (0.133) data 0.000 (0.049) loss 2.3652 (1.8579) acc 28.1250 (46.5625) lr 2.6037e-04 eta 0:05:01
epoch [155/200] batch [35/50] time 0.085 (0.126) data 0.000 (0.042) loss 1.6289 (1.8722) acc 46.8750 (46.4286) lr 2.6037e-04 eta 0:04:44
epoch [155/200] batch [40/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.8760 (1.8608) acc 34.3750 (46.2500) lr 2.6037e-04 eta 0:04:32
epoch [155/200] batch [45/50] time 0.084 (0.116) data 0.000 (0.032) loss 1.9180 (1.8609) acc 40.6250 (45.9028) lr 2.6037e-04 eta 0:04:22
epoch [155/200] batch [50/50] time 0.085 (0.116) data 0.000 (0.032) loss 1.6768 (1.8815) acc 46.8750 (45.4375) lr 2.4989e-04 eta 0:04:20
epoch [156/200] batch [5/50] time 0.083 (0.340) data 0.000 (0.256) loss 2.0586 (1.8732) acc 40.6250 (46.2500) lr 2.4989e-04 eta 0:12:42
epoch [156/200] batch [10/50] time 0.131 (0.227) data 0.049 (0.144) loss 1.5752 (1.8686) acc 62.5000 (48.4375) lr 2.4989e-04 eta 0:08:28
epoch [156/200] batch [15/50] time 0.084 (0.179) data 0.000 (0.096) loss 1.7197 (1.8327) acc 50.0000 (48.1250) lr 2.4989e-04 eta 0:06:40
epoch [156/200] batch [20/50] time 0.085 (0.161) data 0.000 (0.077) loss 1.8242 (1.8319) acc 50.0000 (48.9062) lr 2.4989e-04 eta 0:05:58
epoch [156/200] batch [25/50] time 0.087 (0.146) data 0.001 (0.061) loss 1.7539 (1.8299) acc 37.5000 (49.0000) lr 2.4989e-04 eta 0:05:24
epoch [156/200] batch [30/50] time 0.086 (0.136) data 0.001 (0.051) loss 2.2188 (1.8435) acc 34.3750 (47.6042) lr 2.4989e-04 eta 0:05:01
epoch [156/200] batch [35/50] time 0.085 (0.128) data 0.001 (0.044) loss 1.9150 (1.8644) acc 43.7500 (46.5179) lr 2.4989e-04 eta 0:04:44
epoch [156/200] batch [40/50] time 0.086 (0.126) data 0.000 (0.042) loss 1.6377 (1.8508) acc 43.7500 (46.1719) lr 2.4989e-04 eta 0:04:39
epoch [156/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.040) loss 2.1230 (1.8717) acc 37.5000 (45.5556) lr 2.4989e-04 eta 0:04:34
epoch [156/200] batch [50/50] time 0.084 (0.120) data 0.000 (0.036) loss 1.9150 (1.8796) acc 50.0000 (44.8750) lr 2.3959e-04 eta 0:04:24
epoch [157/200] batch [5/50] time 0.085 (0.322) data 0.000 (0.236) loss 1.7969 (2.0223) acc 53.1250 (48.1250) lr 2.3959e-04 eta 0:11:45
epoch [157/200] batch [10/50] time 0.241 (0.219) data 0.157 (0.134) loss 1.9160 (1.8871) acc 40.6250 (47.8125) lr 2.3959e-04 eta 0:07:59
epoch [157/200] batch [15/50] time 0.085 (0.175) data 0.001 (0.090) loss 1.9600 (1.9314) acc 43.7500 (46.4583) lr 2.3959e-04 eta 0:06:21
epoch [157/200] batch [20/50] time 0.085 (0.160) data 0.001 (0.076) loss 1.7637 (1.9264) acc 56.2500 (45.7812) lr 2.3959e-04 eta 0:05:49
epoch [157/200] batch [25/50] time 0.085 (0.145) data 0.000 (0.061) loss 2.5547 (1.9586) acc 25.0000 (44.6250) lr 2.3959e-04 eta 0:05:16
epoch [157/200] batch [30/50] time 0.086 (0.140) data 0.000 (0.055) loss 2.0488 (1.9610) acc 50.0000 (44.1667) lr 2.3959e-04 eta 0:05:03
epoch [157/200] batch [35/50] time 0.085 (0.136) data 0.001 (0.051) loss 2.0234 (1.9593) acc 46.8750 (44.5536) lr 2.3959e-04 eta 0:04:54
epoch [157/200] batch [40/50] time 0.083 (0.130) data 0.000 (0.045) loss 2.1582 (1.9573) acc 46.8750 (44.5312) lr 2.3959e-04 eta 0:04:40
epoch [157/200] batch [45/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.5518 (1.9356) acc 59.3750 (44.9306) lr 2.3959e-04 eta 0:04:30
epoch [157/200] batch [50/50] time 0.083 (0.121) data 0.000 (0.037) loss 1.7217 (1.9236) acc 59.3750 (45.4375) lr 2.2949e-04 eta 0:04:20
epoch [158/200] batch [5/50] time 0.087 (0.298) data 0.001 (0.212) loss 1.6484 (1.8055) acc 53.1250 (42.5000) lr 2.2949e-04 eta 0:10:40
epoch [158/200] batch [10/50] time 0.085 (0.193) data 0.000 (0.106) loss 2.3711 (1.9716) acc 46.8750 (43.1250) lr 2.2949e-04 eta 0:06:52
epoch [158/200] batch [15/50] time 0.086 (0.160) data 0.000 (0.074) loss 1.8418 (1.9008) acc 43.7500 (44.5833) lr 2.2949e-04 eta 0:05:41
epoch [158/200] batch [20/50] time 0.088 (0.147) data 0.000 (0.061) loss 1.6572 (1.8673) acc 59.3750 (45.4688) lr 2.2949e-04 eta 0:05:12
epoch [158/200] batch [25/50] time 0.087 (0.138) data 0.001 (0.052) loss 1.6006 (1.8448) acc 53.1250 (45.7500) lr 2.2949e-04 eta 0:04:52
epoch [158/200] batch [30/50] time 0.086 (0.135) data 0.000 (0.050) loss 1.7529 (1.8425) acc 62.5000 (46.5625) lr 2.2949e-04 eta 0:04:47
epoch [158/200] batch [35/50] time 0.086 (0.128) data 0.001 (0.043) loss 2.0938 (1.8659) acc 37.5000 (45.8036) lr 2.2949e-04 eta 0:04:31
epoch [158/200] batch [40/50] time 0.084 (0.125) data 0.000 (0.040) loss 1.5654 (1.8711) acc 59.3750 (46.3281) lr 2.2949e-04 eta 0:04:24
epoch [158/200] batch [45/50] time 0.084 (0.121) data 0.000 (0.035) loss 1.5820 (1.8863) acc 53.1250 (44.8611) lr 2.2949e-04 eta 0:04:14
epoch [158/200] batch [50/50] time 0.084 (0.118) data 0.000 (0.033) loss 1.8203 (1.8753) acc 46.8750 (44.9375) lr 2.1957e-04 eta 0:04:08
epoch [159/200] batch [5/50] time 0.086 (0.369) data 0.000 (0.283) loss 2.3281 (2.0209) acc 31.2500 (41.8750) lr 2.1957e-04 eta 0:12:53
epoch [159/200] batch [10/50] time 0.089 (0.235) data 0.000 (0.148) loss 1.6162 (1.9615) acc 40.6250 (42.8125) lr 2.1957e-04 eta 0:08:10
epoch [159/200] batch [15/50] time 0.093 (0.186) data 0.001 (0.099) loss 1.8711 (1.9290) acc 40.6250 (42.9167) lr 2.1957e-04 eta 0:06:27
epoch [159/200] batch [20/50] time 0.087 (0.167) data 0.000 (0.080) loss 2.2090 (1.9283) acc 34.3750 (42.3438) lr 2.1957e-04 eta 0:05:46
epoch [159/200] batch [25/50] time 0.210 (0.156) data 0.125 (0.069) loss 1.8799 (1.9205) acc 43.7500 (43.1250) lr 2.1957e-04 eta 0:05:22
epoch [159/200] batch [30/50] time 0.086 (0.144) data 0.001 (0.057) loss 1.9570 (1.9055) acc 40.6250 (43.7500) lr 2.1957e-04 eta 0:04:58
epoch [159/200] batch [35/50] time 0.087 (0.142) data 0.001 (0.056) loss 1.6377 (1.8991) acc 56.2500 (44.2857) lr 2.1957e-04 eta 0:04:54
epoch [159/200] batch [40/50] time 0.085 (0.135) data 0.000 (0.049) loss 2.3301 (1.8918) acc 37.5000 (44.7656) lr 2.1957e-04 eta 0:04:38
epoch [159/200] batch [45/50] time 0.083 (0.133) data 0.000 (0.047) loss 1.7939 (1.8893) acc 40.6250 (44.6528) lr 2.1957e-04 eta 0:04:33
epoch [159/200] batch [50/50] time 0.083 (0.128) data 0.000 (0.042) loss 2.0898 (1.9007) acc 37.5000 (44.1875) lr 2.0984e-04 eta 0:04:22
epoch [160/200] batch [5/50] time 0.256 (0.333) data 0.168 (0.246) loss 2.4395 (2.0447) acc 34.3750 (41.2500) lr 2.0984e-04 eta 0:11:20
epoch [160/200] batch [10/50] time 0.087 (0.218) data 0.000 (0.131) loss 1.5029 (1.9991) acc 65.6250 (42.5000) lr 2.0984e-04 eta 0:07:24
epoch [160/200] batch [15/50] time 0.087 (0.182) data 0.000 (0.096) loss 1.6270 (1.9094) acc 59.3750 (44.3750) lr 2.0984e-04 eta 0:06:10
epoch [160/200] batch [20/50] time 0.087 (0.158) data 0.001 (0.072) loss 1.8604 (1.9122) acc 50.0000 (44.6875) lr 2.0984e-04 eta 0:05:21
epoch [160/200] batch [25/50] time 0.087 (0.149) data 0.000 (0.063) loss 1.8291 (1.9006) acc 46.8750 (44.7500) lr 2.0984e-04 eta 0:05:02
epoch [160/200] batch [30/50] time 0.199 (0.143) data 0.113 (0.056) loss 2.3965 (1.8930) acc 25.0000 (44.6875) lr 2.0984e-04 eta 0:04:48
epoch [160/200] batch [35/50] time 0.085 (0.135) data 0.001 (0.048) loss 1.8066 (1.9115) acc 46.8750 (43.7500) lr 2.0984e-04 eta 0:04:31
epoch [160/200] batch [40/50] time 0.086 (0.132) data 0.000 (0.046) loss 1.4561 (1.8986) acc 50.0000 (43.8281) lr 2.0984e-04 eta 0:04:26
epoch [160/200] batch [45/50] time 0.084 (0.127) data 0.000 (0.041) loss 1.9941 (1.9100) acc 46.8750 (44.0278) lr 2.0984e-04 eta 0:04:14
epoch [160/200] batch [50/50] time 0.084 (0.127) data 0.000 (0.041) loss 1.7764 (1.9078) acc 37.5000 (43.4375) lr 2.0032e-04 eta 0:04:13
epoch [161/200] batch [5/50] time 0.086 (0.335) data 0.000 (0.249) loss 1.6475 (1.8875) acc 46.8750 (43.1250) lr 2.0032e-04 eta 0:11:08
epoch [161/200] batch [10/50] time 0.087 (0.211) data 0.000 (0.125) loss 1.9541 (1.8430) acc 37.5000 (43.7500) lr 2.0032e-04 eta 0:06:59
epoch [161/200] batch [15/50] time 0.087 (0.172) data 0.001 (0.086) loss 1.8770 (1.8913) acc 46.8750 (41.4583) lr 2.0032e-04 eta 0:05:42
epoch [161/200] batch [20/50] time 0.088 (0.155) data 0.001 (0.069) loss 1.9170 (1.9228) acc 40.6250 (40.4688) lr 2.0032e-04 eta 0:05:07
epoch [161/200] batch [25/50] time 0.089 (0.142) data 0.001 (0.055) loss 1.7227 (1.9163) acc 50.0000 (41.7500) lr 2.0032e-04 eta 0:04:39
epoch [161/200] batch [30/50] time 0.087 (0.133) data 0.001 (0.047) loss 1.5615 (1.9085) acc 56.2500 (43.1250) lr 2.0032e-04 eta 0:04:22
epoch [161/200] batch [35/50] time 0.096 (0.127) data 0.012 (0.041) loss 1.9443 (1.9200) acc 43.7500 (42.6786) lr 2.0032e-04 eta 0:04:09
epoch [161/200] batch [40/50] time 0.106 (0.122) data 0.022 (0.036) loss 1.9707 (1.8992) acc 50.0000 (43.6719) lr 2.0032e-04 eta 0:03:59
epoch [161/200] batch [45/50] time 0.084 (0.120) data 0.000 (0.034) loss 1.4717 (1.8914) acc 53.1250 (44.0972) lr 2.0032e-04 eta 0:03:54
epoch [161/200] batch [50/50] time 0.086 (0.116) data 0.000 (0.031) loss 1.9736 (1.8877) acc 53.1250 (44.0000) lr 1.9098e-04 eta 0:03:46
epoch [162/200] batch [5/50] time 0.085 (0.336) data 0.000 (0.250) loss 1.6377 (1.7932) acc 50.0000 (51.2500) lr 1.9098e-04 eta 0:10:52
epoch [162/200] batch [10/50] time 0.085 (0.210) data 0.000 (0.125) loss 1.7246 (1.7769) acc 46.8750 (49.0625) lr 1.9098e-04 eta 0:06:47
epoch [162/200] batch [15/50] time 0.087 (0.169) data 0.000 (0.084) loss 2.4043 (1.7884) acc 21.8750 (47.9167) lr 1.9098e-04 eta 0:05:26
epoch [162/200] batch [20/50] time 0.087 (0.148) data 0.000 (0.063) loss 1.8408 (1.8083) acc 34.3750 (46.7188) lr 1.9098e-04 eta 0:04:46
epoch [162/200] batch [25/50] time 0.087 (0.136) data 0.000 (0.050) loss 2.0508 (1.8586) acc 37.5000 (44.8750) lr 1.9098e-04 eta 0:04:21
epoch [162/200] batch [30/50] time 0.086 (0.128) data 0.001 (0.042) loss 1.9883 (1.8608) acc 43.7500 (44.4792) lr 1.9098e-04 eta 0:04:05
epoch [162/200] batch [35/50] time 0.088 (0.122) data 0.001 (0.036) loss 1.9854 (1.8550) acc 37.5000 (44.6429) lr 1.9098e-04 eta 0:03:53
epoch [162/200] batch [40/50] time 0.085 (0.117) data 0.000 (0.032) loss 1.9121 (1.8655) acc 43.7500 (44.4531) lr 1.9098e-04 eta 0:03:43
epoch [162/200] batch [45/50] time 0.083 (0.114) data 0.000 (0.028) loss 1.8242 (1.8540) acc 50.0000 (44.9306) lr 1.9098e-04 eta 0:03:36
epoch [162/200] batch [50/50] time 0.083 (0.110) data 0.000 (0.025) loss 1.7373 (1.8533) acc 56.2500 (45.1250) lr 1.8185e-04 eta 0:03:29
epoch [163/200] batch [5/50] time 0.086 (0.335) data 0.000 (0.249) loss 1.3594 (1.7070) acc 65.6250 (48.7500) lr 1.8185e-04 eta 0:10:35
epoch [163/200] batch [10/50] time 0.085 (0.233) data 0.000 (0.147) loss 1.9346 (1.8851) acc 37.5000 (44.0625) lr 1.8185e-04 eta 0:07:19
epoch [163/200] batch [15/50] time 0.087 (0.184) data 0.000 (0.098) loss 1.5859 (1.8403) acc 50.0000 (44.5833) lr 1.8185e-04 eta 0:05:46
epoch [163/200] batch [20/50] time 0.087 (0.164) data 0.000 (0.078) loss 1.9141 (1.8730) acc 50.0000 (44.0625) lr 1.8185e-04 eta 0:05:08
epoch [163/200] batch [25/50] time 0.266 (0.155) data 0.179 (0.070) loss 1.2764 (1.8496) acc 62.5000 (44.7500) lr 1.8185e-04 eta 0:04:51
epoch [163/200] batch [30/50] time 0.088 (0.144) data 0.001 (0.058) loss 1.8047 (1.8538) acc 50.0000 (44.7917) lr 1.8185e-04 eta 0:04:29
epoch [163/200] batch [35/50] time 0.085 (0.136) data 0.001 (0.050) loss 2.0430 (1.8460) acc 43.7500 (44.9107) lr 1.8185e-04 eta 0:04:13
epoch [163/200] batch [40/50] time 0.085 (0.129) data 0.000 (0.044) loss 1.9619 (1.8733) acc 34.3750 (44.1406) lr 1.8185e-04 eta 0:04:00
epoch [163/200] batch [45/50] time 0.084 (0.124) data 0.000 (0.039) loss 1.9971 (1.8657) acc 37.5000 (44.3750) lr 1.8185e-04 eta 0:03:50
epoch [163/200] batch [50/50] time 0.084 (0.120) data 0.000 (0.035) loss 1.6680 (1.8776) acc 34.3750 (44.0000) lr 1.7292e-04 eta 0:03:42
epoch [164/200] batch [5/50] time 0.242 (0.329) data 0.157 (0.243) loss 2.0137 (2.0967) acc 40.6250 (37.5000) lr 1.7292e-04 eta 0:10:06
epoch [164/200] batch [10/50] time 0.085 (0.207) data 0.000 (0.122) loss 1.6162 (1.9480) acc 53.1250 (40.9375) lr 1.7292e-04 eta 0:06:21
epoch [164/200] batch [15/50] time 0.085 (0.174) data 0.000 (0.088) loss 1.9570 (1.8962) acc 31.2500 (44.1667) lr 1.7292e-04 eta 0:05:18
epoch [164/200] batch [20/50] time 0.086 (0.152) data 0.000 (0.066) loss 1.8730 (1.8599) acc 56.2500 (45.4688) lr 1.7292e-04 eta 0:04:37
epoch [164/200] batch [25/50] time 0.085 (0.145) data 0.000 (0.059) loss 2.0645 (1.8594) acc 43.7500 (45.1250) lr 1.7292e-04 eta 0:04:24
epoch [164/200] batch [30/50] time 0.085 (0.141) data 0.000 (0.056) loss 1.3809 (1.8817) acc 53.1250 (44.5833) lr 1.7292e-04 eta 0:04:17
epoch [164/200] batch [35/50] time 0.085 (0.133) data 0.001 (0.048) loss 1.7979 (1.9066) acc 43.7500 (43.7500) lr 1.7292e-04 eta 0:04:02
epoch [164/200] batch [40/50] time 0.084 (0.131) data 0.000 (0.046) loss 1.8691 (1.9078) acc 46.8750 (43.5938) lr 1.7292e-04 eta 0:03:57
epoch [164/200] batch [45/50] time 0.262 (0.130) data 0.179 (0.045) loss 2.0430 (1.8956) acc 34.3750 (43.7500) lr 1.7292e-04 eta 0:03:54
epoch [164/200] batch [50/50] time 0.084 (0.125) data 0.000 (0.041) loss 1.9062 (1.8912) acc 59.3750 (44.4375) lr 1.6419e-04 eta 0:03:45
epoch [165/200] batch [5/50] time 0.209 (0.382) data 0.124 (0.296) loss 1.7207 (1.7752) acc 56.2500 (49.3750) lr 1.6419e-04 eta 0:11:26
epoch [165/200] batch [10/50] time 0.087 (0.234) data 0.000 (0.148) loss 1.6553 (1.9187) acc 40.6250 (45.0000) lr 1.6419e-04 eta 0:06:59
epoch [165/200] batch [15/50] time 0.086 (0.189) data 0.000 (0.103) loss 1.7725 (1.8847) acc 50.0000 (45.0000) lr 1.6419e-04 eta 0:05:37
epoch [165/200] batch [20/50] time 0.254 (0.172) data 0.167 (0.086) loss 1.8564 (1.9439) acc 37.5000 (43.4375) lr 1.6419e-04 eta 0:05:05
epoch [165/200] batch [25/50] time 0.087 (0.155) data 0.000 (0.069) loss 1.8877 (1.9343) acc 46.8750 (43.6250) lr 1.6419e-04 eta 0:04:34
epoch [165/200] batch [30/50] time 0.086 (0.148) data 0.000 (0.062) loss 1.9893 (1.9327) acc 31.2500 (43.6458) lr 1.6419e-04 eta 0:04:22
epoch [165/200] batch [35/50] time 0.086 (0.139) data 0.001 (0.053) loss 1.8955 (1.9024) acc 56.2500 (44.8214) lr 1.6419e-04 eta 0:04:06
epoch [165/200] batch [40/50] time 0.085 (0.135) data 0.000 (0.049) loss 1.7598 (1.9012) acc 50.0000 (45.4688) lr 1.6419e-04 eta 0:03:57
epoch [165/200] batch [45/50] time 0.084 (0.133) data 0.000 (0.047) loss 1.9307 (1.9082) acc 40.6250 (45.1389) lr 1.6419e-04 eta 0:03:53
epoch [165/200] batch [50/50] time 0.084 (0.128) data 0.000 (0.043) loss 1.9229 (1.9105) acc 56.2500 (44.9375) lr 1.5567e-04 eta 0:03:44
epoch [166/200] batch [5/50] time 0.089 (0.333) data 0.001 (0.245) loss 1.6309 (1.8379) acc 46.8750 (46.8750) lr 1.5567e-04 eta 0:09:40
epoch [166/200] batch [10/50] time 0.166 (0.217) data 0.081 (0.131) loss 1.8467 (1.7796) acc 43.7500 (49.6875) lr 1.5567e-04 eta 0:06:17
epoch [166/200] batch [15/50] time 0.087 (0.174) data 0.001 (0.087) loss 1.5205 (1.7668) acc 50.0000 (50.4167) lr 1.5567e-04 eta 0:05:01
epoch [166/200] batch [20/50] time 0.085 (0.158) data 0.000 (0.072) loss 1.7500 (1.8005) acc 46.8750 (49.2188) lr 1.5567e-04 eta 0:04:33
epoch [166/200] batch [25/50] time 0.240 (0.150) data 0.154 (0.064) loss 1.7188 (1.7854) acc 50.0000 (49.1250) lr 1.5567e-04 eta 0:04:18
epoch [166/200] batch [30/50] time 0.087 (0.141) data 0.000 (0.055) loss 2.1973 (1.8401) acc 34.3750 (46.7708) lr 1.5567e-04 eta 0:04:01
epoch [166/200] batch [35/50] time 0.085 (0.137) data 0.001 (0.051) loss 2.0723 (1.8414) acc 34.3750 (46.3393) lr 1.5567e-04 eta 0:03:54
epoch [166/200] batch [40/50] time 0.084 (0.130) data 0.000 (0.045) loss 1.6357 (1.8211) acc 53.1250 (46.2500) lr 1.5567e-04 eta 0:03:42
epoch [166/200] batch [45/50] time 0.084 (0.129) data 0.000 (0.044) loss 2.1621 (1.8278) acc 34.3750 (46.1806) lr 1.5567e-04 eta 0:03:39
epoch [166/200] batch [50/50] time 0.083 (0.124) data 0.000 (0.039) loss 2.2656 (1.8474) acc 34.3750 (45.5625) lr 1.4736e-04 eta 0:03:31
epoch [167/200] batch [5/50] time 0.085 (0.346) data 0.000 (0.260) loss 1.7998 (1.7445) acc 50.0000 (54.3750) lr 1.4736e-04 eta 0:09:47
epoch [167/200] batch [10/50] time 0.085 (0.224) data 0.000 (0.138) loss 2.0820 (1.8440) acc 40.6250 (48.4375) lr 1.4736e-04 eta 0:06:17
epoch [167/200] batch [15/50] time 0.084 (0.177) data 0.000 (0.092) loss 2.0938 (1.8791) acc 34.3750 (46.8750) lr 1.4736e-04 eta 0:04:58
epoch [167/200] batch [20/50] time 0.085 (0.154) data 0.000 (0.069) loss 1.4980 (1.8794) acc 59.3750 (47.1875) lr 1.4736e-04 eta 0:04:19
epoch [167/200] batch [25/50] time 0.291 (0.149) data 0.206 (0.064) loss 1.9326 (1.8602) acc 34.3750 (47.0000) lr 1.4736e-04 eta 0:04:08
epoch [167/200] batch [30/50] time 0.085 (0.138) data 0.001 (0.053) loss 1.3926 (1.8494) acc 43.7500 (46.9792) lr 1.4736e-04 eta 0:03:50
epoch [167/200] batch [35/50] time 0.085 (0.132) data 0.001 (0.047) loss 1.8115 (1.8528) acc 46.8750 (46.6964) lr 1.4736e-04 eta 0:03:39
epoch [167/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.041) loss 1.6621 (1.8592) acc 43.7500 (46.1719) lr 1.4736e-04 eta 0:03:29
epoch [167/200] batch [45/50] time 0.084 (0.121) data 0.000 (0.037) loss 1.8135 (1.8664) acc 53.1250 (45.9028) lr 1.4736e-04 eta 0:03:20
epoch [167/200] batch [50/50] time 0.085 (0.118) data 0.000 (0.033) loss 1.6680 (1.8657) acc 65.6250 (46.3750) lr 1.3926e-04 eta 0:03:14
epoch [168/200] batch [5/50] time 0.087 (0.345) data 0.000 (0.258) loss 1.5938 (1.7229) acc 56.2500 (48.1250) lr 1.3926e-04 eta 0:09:27
epoch [168/200] batch [10/50] time 0.088 (0.222) data 0.000 (0.135) loss 1.8193 (1.8229) acc 37.5000 (45.3125) lr 1.3926e-04 eta 0:06:04
epoch [168/200] batch [15/50] time 0.086 (0.177) data 0.000 (0.090) loss 2.2363 (1.9033) acc 37.5000 (43.5417) lr 1.3926e-04 eta 0:04:49
epoch [168/200] batch [20/50] time 0.086 (0.163) data 0.000 (0.077) loss 2.4141 (1.9214) acc 25.0000 (43.1250) lr 1.3926e-04 eta 0:04:25
epoch [168/200] batch [25/50] time 0.086 (0.148) data 0.001 (0.062) loss 1.9287 (1.8943) acc 40.6250 (42.8750) lr 1.3926e-04 eta 0:04:00
epoch [168/200] batch [30/50] time 0.087 (0.138) data 0.001 (0.051) loss 1.9170 (1.8831) acc 43.7500 (43.9583) lr 1.3926e-04 eta 0:03:43
epoch [168/200] batch [35/50] time 0.087 (0.131) data 0.001 (0.044) loss 1.9150 (1.8880) acc 43.7500 (44.1964) lr 1.3926e-04 eta 0:03:30
epoch [168/200] batch [40/50] time 0.087 (0.125) data 0.000 (0.039) loss 1.6436 (1.8880) acc 46.8750 (43.7500) lr 1.3926e-04 eta 0:03:21
epoch [168/200] batch [45/50] time 0.084 (0.122) data 0.000 (0.036) loss 1.9971 (1.8971) acc 46.8750 (43.3333) lr 1.3926e-04 eta 0:03:15
epoch [168/200] batch [50/50] time 0.083 (0.118) data 0.000 (0.032) loss 2.0137 (1.9021) acc 46.8750 (43.6250) lr 1.3137e-04 eta 0:03:08
epoch [169/200] batch [5/50] time 0.087 (0.342) data 0.000 (0.255) loss 1.9238 (1.7117) acc 31.2500 (47.5000) lr 1.3137e-04 eta 0:09:05
epoch [169/200] batch [10/50] time 0.087 (0.218) data 0.000 (0.131) loss 2.0078 (1.8731) acc 37.5000 (47.5000) lr 1.3137e-04 eta 0:05:45
epoch [169/200] batch [15/50] time 0.086 (0.174) data 0.000 (0.087) loss 2.4336 (1.9010) acc 28.1250 (46.4583) lr 1.3137e-04 eta 0:04:35
epoch [169/200] batch [20/50] time 0.086 (0.155) data 0.000 (0.068) loss 1.9268 (1.8984) acc 46.8750 (46.0938) lr 1.3137e-04 eta 0:04:05
epoch [169/200] batch [25/50] time 0.202 (0.146) data 0.117 (0.059) loss 2.0195 (1.8934) acc 43.7500 (45.7500) lr 1.3137e-04 eta 0:03:49
epoch [169/200] batch [30/50] time 0.086 (0.136) data 0.000 (0.050) loss 1.7207 (1.8889) acc 46.8750 (45.5208) lr 1.3137e-04 eta 0:03:33
epoch [169/200] batch [35/50] time 0.088 (0.130) data 0.001 (0.043) loss 1.9297 (1.8968) acc 37.5000 (44.5536) lr 1.3137e-04 eta 0:03:23
epoch [169/200] batch [40/50] time 0.086 (0.124) data 0.000 (0.038) loss 1.8477 (1.8985) acc 53.1250 (44.5312) lr 1.3137e-04 eta 0:03:14
epoch [169/200] batch [45/50] time 0.085 (0.123) data 0.000 (0.037) loss 1.8291 (1.8852) acc 50.0000 (45.1389) lr 1.3137e-04 eta 0:03:11
epoch [169/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.033) loss 1.7686 (1.8774) acc 46.8750 (45.1875) lr 1.2369e-04 eta 0:03:04
epoch [170/200] batch [5/50] time 0.087 (0.335) data 0.001 (0.249) loss 1.9619 (1.7951) acc 43.7500 (48.7500) lr 1.2369e-04 eta 0:08:37
epoch [170/200] batch [10/50] time 0.195 (0.221) data 0.110 (0.136) loss 2.4531 (1.8109) acc 40.6250 (48.4375) lr 1.2369e-04 eta 0:05:40
epoch [170/200] batch [15/50] time 0.085 (0.176) data 0.000 (0.090) loss 1.9248 (1.8877) acc 50.0000 (45.6250) lr 1.2369e-04 eta 0:04:29
epoch [170/200] batch [20/50] time 0.085 (0.160) data 0.000 (0.075) loss 1.8867 (1.8808) acc 50.0000 (46.0938) lr 1.2369e-04 eta 0:04:04
epoch [170/200] batch [25/50] time 0.086 (0.145) data 0.001 (0.060) loss 2.0410 (1.9222) acc 37.5000 (44.6250) lr 1.2369e-04 eta 0:03:40
epoch [170/200] batch [30/50] time 0.084 (0.140) data 0.000 (0.055) loss 1.9219 (1.9264) acc 37.5000 (44.0625) lr 1.2369e-04 eta 0:03:33
epoch [170/200] batch [35/50] time 0.107 (0.138) data 0.023 (0.053) loss 1.6719 (1.8997) acc 46.8750 (44.8214) lr 1.2369e-04 eta 0:03:29
epoch [170/200] batch [40/50] time 0.083 (0.131) data 0.000 (0.047) loss 1.7090 (1.8770) acc 43.7500 (45.6250) lr 1.2369e-04 eta 0:03:18
epoch [170/200] batch [45/50] time 0.083 (0.129) data 0.000 (0.045) loss 2.0176 (1.8769) acc 40.6250 (45.8333) lr 1.2369e-04 eta 0:03:14
epoch [170/200] batch [50/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.8594 (1.8763) acc 46.8750 (45.8750) lr 1.1623e-04 eta 0:03:07
epoch [171/200] batch [5/50] time 0.088 (0.290) data 0.000 (0.203) loss 1.8506 (1.6791) acc 53.1250 (53.1250) lr 1.1623e-04 eta 0:07:13
epoch [171/200] batch [10/50] time 0.086 (0.188) data 0.001 (0.102) loss 2.1230 (1.8856) acc 37.5000 (45.3125) lr 1.1623e-04 eta 0:04:39
epoch [171/200] batch [15/50] time 0.173 (0.160) data 0.088 (0.074) loss 1.9170 (1.8943) acc 40.6250 (44.5833) lr 1.1623e-04 eta 0:03:56
epoch [171/200] batch [20/50] time 0.085 (0.141) data 0.000 (0.055) loss 2.1055 (1.9016) acc 37.5000 (43.9062) lr 1.1623e-04 eta 0:03:28
epoch [171/200] batch [25/50] time 0.085 (0.131) data 0.000 (0.046) loss 1.8438 (1.9288) acc 46.8750 (43.5000) lr 1.1623e-04 eta 0:03:13
epoch [171/200] batch [30/50] time 0.085 (0.124) data 0.001 (0.038) loss 1.9785 (1.9113) acc 43.7500 (44.6875) lr 1.1623e-04 eta 0:03:01
epoch [171/200] batch [35/50] time 0.085 (0.118) data 0.001 (0.033) loss 2.2305 (1.9155) acc 31.2500 (44.4643) lr 1.1623e-04 eta 0:02:52
epoch [171/200] batch [40/50] time 0.085 (0.114) data 0.000 (0.029) loss 2.0312 (1.9152) acc 37.5000 (44.2188) lr 1.1623e-04 eta 0:02:46
epoch [171/200] batch [45/50] time 0.086 (0.111) data 0.000 (0.026) loss 1.9941 (1.9034) acc 37.5000 (44.0972) lr 1.1623e-04 eta 0:02:41
epoch [171/200] batch [50/50] time 0.084 (0.108) data 0.000 (0.023) loss 1.9131 (1.9100) acc 46.8750 (44.0000) lr 1.0899e-04 eta 0:02:37
epoch [172/200] batch [5/50] time 0.085 (0.355) data 0.000 (0.270) loss 1.8291 (1.7297) acc 53.1250 (50.0000) lr 1.0899e-04 eta 0:08:33
epoch [172/200] batch [10/50] time 0.085 (0.221) data 0.000 (0.137) loss 1.8711 (1.8268) acc 50.0000 (47.1875) lr 1.0899e-04 eta 0:05:18
epoch [172/200] batch [15/50] time 0.083 (0.176) data 0.000 (0.091) loss 1.5439 (1.8137) acc 53.1250 (47.2917) lr 1.0899e-04 eta 0:04:12
epoch [172/200] batch [20/50] time 0.085 (0.158) data 0.001 (0.074) loss 2.0996 (1.8768) acc 34.3750 (45.3125) lr 1.0899e-04 eta 0:03:46
epoch [172/200] batch [25/50] time 0.085 (0.143) data 0.000 (0.059) loss 1.9258 (1.8717) acc 37.5000 (45.6250) lr 1.0899e-04 eta 0:03:24
epoch [172/200] batch [30/50] time 0.084 (0.140) data 0.000 (0.056) loss 1.6738 (1.8556) acc 53.1250 (45.8333) lr 1.0899e-04 eta 0:03:18
epoch [172/200] batch [35/50] time 0.085 (0.136) data 0.001 (0.051) loss 2.1074 (1.8782) acc 34.3750 (45.1786) lr 1.0899e-04 eta 0:03:11
epoch [172/200] batch [40/50] time 0.084 (0.129) data 0.000 (0.045) loss 1.8799 (1.8618) acc 43.7500 (45.0781) lr 1.0899e-04 eta 0:03:02
epoch [172/200] batch [45/50] time 0.085 (0.127) data 0.000 (0.043) loss 1.9072 (1.8732) acc 40.6250 (44.5833) lr 1.0899e-04 eta 0:02:58
epoch [172/200] batch [50/50] time 0.084 (0.123) data 0.000 (0.039) loss 2.0449 (1.8768) acc 46.8750 (44.3750) lr 1.0197e-04 eta 0:02:52
epoch [173/200] batch [5/50] time 0.088 (0.295) data 0.001 (0.208) loss 2.1094 (1.7635) acc 37.5000 (48.7500) lr 1.0197e-04 eta 0:06:51
epoch [173/200] batch [10/50] time 0.086 (0.191) data 0.000 (0.104) loss 2.1562 (1.7904) acc 40.6250 (47.5000) lr 1.0197e-04 eta 0:04:25
epoch [173/200] batch [15/50] time 0.085 (0.156) data 0.000 (0.070) loss 1.7695 (1.7611) acc 62.5000 (49.7917) lr 1.0197e-04 eta 0:03:35
epoch [173/200] batch [20/50] time 0.085 (0.138) data 0.000 (0.052) loss 1.5508 (1.7761) acc 53.1250 (48.7500) lr 1.0197e-04 eta 0:03:10
epoch [173/200] batch [25/50] time 0.085 (0.128) data 0.000 (0.042) loss 1.8389 (1.8016) acc 40.6250 (47.1250) lr 1.0197e-04 eta 0:02:55
epoch [173/200] batch [30/50] time 0.087 (0.121) data 0.001 (0.035) loss 1.6074 (1.7895) acc 50.0000 (47.5000) lr 1.0197e-04 eta 0:02:45
epoch [173/200] batch [35/50] time 0.088 (0.116) data 0.001 (0.031) loss 2.1699 (1.8344) acc 37.5000 (46.4286) lr 1.0197e-04 eta 0:02:38
epoch [173/200] batch [40/50] time 0.085 (0.114) data 0.000 (0.028) loss 1.8848 (1.8578) acc 50.0000 (45.6250) lr 1.0197e-04 eta 0:02:34
epoch [173/200] batch [45/50] time 0.084 (0.114) data 0.000 (0.029) loss 2.1133 (1.8790) acc 46.8750 (45.4167) lr 1.0197e-04 eta 0:02:34
epoch [173/200] batch [50/50] time 0.083 (0.111) data 0.000 (0.026) loss 1.5361 (1.8607) acc 53.1250 (45.6875) lr 9.5173e-05 eta 0:02:30
epoch [174/200] batch [5/50] time 0.085 (0.344) data 0.001 (0.258) loss 1.7695 (1.8189) acc 46.8750 (47.5000) lr 9.5173e-05 eta 0:07:42
epoch [174/200] batch [10/50] time 0.084 (0.226) data 0.001 (0.142) loss 2.1797 (1.8516) acc 31.2500 (43.4375) lr 9.5173e-05 eta 0:05:03
epoch [174/200] batch [15/50] time 0.084 (0.179) data 0.000 (0.094) loss 2.1016 (1.9109) acc 40.6250 (42.7083) lr 9.5173e-05 eta 0:03:58
epoch [174/200] batch [20/50] time 0.085 (0.166) data 0.000 (0.082) loss 2.0586 (1.8881) acc 37.5000 (42.8125) lr 9.5173e-05 eta 0:03:41
epoch [174/200] batch [25/50] time 0.141 (0.152) data 0.057 (0.068) loss 1.7959 (1.8906) acc 43.7500 (44.2500) lr 9.5173e-05 eta 0:03:21
epoch [174/200] batch [30/50] time 0.084 (0.141) data 0.000 (0.057) loss 1.5303 (1.8792) acc 46.8750 (44.6875) lr 9.5173e-05 eta 0:03:06
epoch [174/200] batch [35/50] time 0.085 (0.135) data 0.001 (0.051) loss 1.9258 (1.8384) acc 40.6250 (46.1607) lr 9.5173e-05 eta 0:02:57
epoch [174/200] batch [40/50] time 0.083 (0.129) data 0.000 (0.044) loss 1.8262 (1.8589) acc 43.7500 (45.8594) lr 9.5173e-05 eta 0:02:48
epoch [174/200] batch [45/50] time 0.083 (0.127) data 0.000 (0.042) loss 1.7773 (1.8408) acc 37.5000 (46.3889) lr 9.5173e-05 eta 0:02:45
epoch [174/200] batch [50/50] time 0.084 (0.122) data 0.000 (0.038) loss 2.0273 (1.8529) acc 37.5000 (45.9375) lr 8.8597e-05 eta 0:02:39
epoch [175/200] batch [5/50] time 0.086 (0.345) data 0.000 (0.259) loss 1.6885 (1.8717) acc 53.1250 (45.6250) lr 8.8597e-05 eta 0:07:26
epoch [175/200] batch [10/50] time 0.084 (0.221) data 0.000 (0.137) loss 1.8350 (1.9085) acc 46.8750 (45.6250) lr 8.8597e-05 eta 0:04:45
epoch [175/200] batch [15/50] time 0.086 (0.176) data 0.000 (0.091) loss 1.7598 (1.9299) acc 56.2500 (44.1667) lr 8.8597e-05 eta 0:03:46
epoch [175/200] batch [20/50] time 0.086 (0.154) data 0.000 (0.069) loss 1.4336 (1.8576) acc 50.0000 (45.9375) lr 8.8597e-05 eta 0:03:17
epoch [175/200] batch [25/50] time 0.085 (0.141) data 0.001 (0.056) loss 1.9717 (1.8764) acc 40.6250 (46.0000) lr 8.8597e-05 eta 0:02:59
epoch [175/200] batch [30/50] time 0.085 (0.132) data 0.000 (0.047) loss 2.2305 (1.9203) acc 31.2500 (44.3750) lr 8.8597e-05 eta 0:02:47
epoch [175/200] batch [35/50] time 0.085 (0.125) data 0.001 (0.041) loss 1.5342 (1.9215) acc 50.0000 (44.4643) lr 8.8597e-05 eta 0:02:38
epoch [175/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.1582 (1.8999) acc 40.6250 (44.4531) lr 8.8597e-05 eta 0:02:38
epoch [175/200] batch [45/50] time 0.084 (0.125) data 0.000 (0.040) loss 2.1699 (1.9000) acc 34.3750 (44.1667) lr 8.8597e-05 eta 0:02:37
epoch [175/200] batch [50/50] time 0.084 (0.121) data 0.000 (0.036) loss 1.9551 (1.8873) acc 46.8750 (45.0625) lr 8.2245e-05 eta 0:02:31
epoch [176/200] batch [5/50] time 0.087 (0.350) data 0.000 (0.263) loss 2.0508 (1.9320) acc 34.3750 (43.7500) lr 8.2245e-05 eta 0:07:16
epoch [176/200] batch [10/50] time 0.087 (0.225) data 0.001 (0.138) loss 1.7871 (1.8395) acc 53.1250 (50.0000) lr 8.2245e-05 eta 0:04:38
epoch [176/200] batch [15/50] time 0.088 (0.179) data 0.000 (0.092) loss 2.3398 (1.8456) acc 37.5000 (48.5417) lr 8.2245e-05 eta 0:03:40
epoch [176/200] batch [20/50] time 0.087 (0.156) data 0.001 (0.069) loss 2.2969 (1.8848) acc 43.7500 (46.2500) lr 8.2245e-05 eta 0:03:12
epoch [176/200] batch [25/50] time 0.088 (0.142) data 0.001 (0.056) loss 1.8076 (1.8809) acc 40.6250 (46.3750) lr 8.2245e-05 eta 0:02:54
epoch [176/200] batch [30/50] time 0.086 (0.135) data 0.000 (0.048) loss 1.9980 (1.9141) acc 53.1250 (45.3125) lr 8.2245e-05 eta 0:02:44
epoch [176/200] batch [35/50] time 0.084 (0.132) data 0.001 (0.045) loss 1.6426 (1.8743) acc 50.0000 (45.8929) lr 8.2245e-05 eta 0:02:40
epoch [176/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.040) loss 1.7041 (1.8607) acc 50.0000 (46.2500) lr 8.2245e-05 eta 0:02:32
epoch [176/200] batch [45/50] time 0.084 (0.124) data 0.000 (0.038) loss 2.3711 (1.8782) acc 40.6250 (45.7639) lr 8.2245e-05 eta 0:02:29
epoch [176/200] batch [50/50] time 0.083 (0.120) data 0.000 (0.034) loss 1.7100 (1.8872) acc 53.1250 (45.5625) lr 7.6120e-05 eta 0:02:24
epoch [177/200] batch [5/50] time 0.087 (0.311) data 0.000 (0.224) loss 1.5713 (1.7805) acc 53.1250 (46.8750) lr 7.6120e-05 eta 0:06:12
epoch [177/200] batch [10/50] time 0.090 (0.214) data 0.000 (0.127) loss 1.9980 (1.8220) acc 34.3750 (45.6250) lr 7.6120e-05 eta 0:04:14
epoch [177/200] batch [15/50] time 0.087 (0.173) data 0.000 (0.086) loss 1.8486 (1.7964) acc 37.5000 (46.2500) lr 7.6120e-05 eta 0:03:24
epoch [177/200] batch [20/50] time 0.086 (0.157) data 0.000 (0.071) loss 1.9580 (1.8155) acc 40.6250 (45.0000) lr 7.6120e-05 eta 0:03:05
epoch [177/200] batch [25/50] time 0.087 (0.143) data 0.001 (0.057) loss 1.8271 (1.8505) acc 50.0000 (45.1250) lr 7.6120e-05 eta 0:02:48
epoch [177/200] batch [30/50] time 0.087 (0.137) data 0.000 (0.050) loss 1.9814 (1.8295) acc 34.3750 (45.1042) lr 7.6120e-05 eta 0:02:39
epoch [177/200] batch [35/50] time 0.239 (0.134) data 0.153 (0.047) loss 1.9678 (1.8663) acc 37.5000 (44.1071) lr 7.6120e-05 eta 0:02:36
epoch [177/200] batch [40/50] time 0.086 (0.128) data 0.000 (0.041) loss 1.9121 (1.8781) acc 53.1250 (44.3750) lr 7.6120e-05 eta 0:02:28
epoch [177/200] batch [45/50] time 0.085 (0.123) data 0.000 (0.037) loss 1.8682 (1.8761) acc 34.3750 (44.0972) lr 7.6120e-05 eta 0:02:22
epoch [177/200] batch [50/50] time 0.084 (0.119) data 0.000 (0.033) loss 1.9248 (1.8870) acc 53.1250 (44.5000) lr 7.0224e-05 eta 0:02:17
epoch [178/200] batch [5/50] time 0.088 (0.334) data 0.000 (0.247) loss 1.9600 (2.0242) acc 40.6250 (38.7500) lr 7.0224e-05 eta 0:06:22
epoch [178/200] batch [10/50] time 0.088 (0.219) data 0.001 (0.132) loss 1.5664 (1.8977) acc 62.5000 (45.6250) lr 7.0224e-05 eta 0:04:10
epoch [178/200] batch [15/50] time 0.089 (0.175) data 0.001 (0.088) loss 2.0977 (1.8962) acc 40.6250 (43.9583) lr 7.0224e-05 eta 0:03:19
epoch [178/200] batch [20/50] time 0.090 (0.158) data 0.001 (0.071) loss 1.8809 (1.8790) acc 34.3750 (43.9062) lr 7.0224e-05 eta 0:02:58
epoch [178/200] batch [25/50] time 0.212 (0.149) data 0.125 (0.062) loss 1.5654 (1.8575) acc 53.1250 (45.2500) lr 7.0224e-05 eta 0:02:47
epoch [178/200] batch [30/50] time 0.086 (0.139) data 0.000 (0.051) loss 2.4707 (1.8950) acc 28.1250 (43.8542) lr 7.0224e-05 eta 0:02:35
epoch [178/200] batch [35/50] time 0.087 (0.133) data 0.001 (0.046) loss 2.0742 (1.9134) acc 28.1250 (42.9464) lr 7.0224e-05 eta 0:02:28
epoch [178/200] batch [40/50] time 0.085 (0.127) data 0.000 (0.041) loss 1.8223 (1.8888) acc 50.0000 (44.3750) lr 7.0224e-05 eta 0:02:21
epoch [178/200] batch [45/50] time 0.084 (0.124) data 0.000 (0.037) loss 1.7324 (1.8845) acc 46.8750 (44.9306) lr 7.0224e-05 eta 0:02:16
epoch [178/200] batch [50/50] time 0.084 (0.120) data 0.000 (0.034) loss 1.7422 (1.8822) acc 62.5000 (45.6875) lr 6.4556e-05 eta 0:02:11
epoch [179/200] batch [5/50] time 0.087 (0.354) data 0.001 (0.267) loss 2.4766 (1.9629) acc 37.5000 (42.5000) lr 6.4556e-05 eta 0:06:27
epoch [179/200] batch [10/50] time 0.089 (0.226) data 0.001 (0.139) loss 1.4434 (1.8898) acc 65.6250 (46.2500) lr 6.4556e-05 eta 0:04:06
epoch [179/200] batch [15/50] time 0.086 (0.179) data 0.000 (0.093) loss 1.6934 (1.8462) acc 53.1250 (46.2500) lr 6.4556e-05 eta 0:03:14
epoch [179/200] batch [20/50] time 0.084 (0.159) data 0.000 (0.073) loss 1.8711 (1.8307) acc 50.0000 (46.7188) lr 6.4556e-05 eta 0:02:51
epoch [179/200] batch [25/50] time 0.283 (0.152) data 0.199 (0.066) loss 2.4160 (1.8542) acc 37.5000 (46.5000) lr 6.4556e-05 eta 0:02:43
epoch [179/200] batch [30/50] time 0.086 (0.141) data 0.000 (0.055) loss 2.2500 (1.8611) acc 28.1250 (45.2083) lr 6.4556e-05 eta 0:02:30
epoch [179/200] batch [35/50] time 0.084 (0.139) data 0.000 (0.053) loss 1.7432 (1.8664) acc 46.8750 (44.6429) lr 6.4556e-05 eta 0:02:27
epoch [179/200] batch [40/50] time 0.085 (0.132) data 0.000 (0.047) loss 1.6992 (1.8642) acc 43.7500 (45.3906) lr 6.4556e-05 eta 0:02:19
epoch [179/200] batch [45/50] time 0.083 (0.130) data 0.000 (0.045) loss 2.1309 (1.8541) acc 28.1250 (45.1389) lr 6.4556e-05 eta 0:02:17
epoch [179/200] batch [50/50] time 0.084 (0.126) data 0.000 (0.041) loss 2.1406 (1.8580) acc 31.2500 (44.9375) lr 5.9119e-05 eta 0:02:11
epoch [180/200] batch [5/50] time 0.086 (0.320) data 0.000 (0.234) loss 1.9502 (2.0041) acc 50.0000 (45.0000) lr 5.9119e-05 eta 0:05:34
epoch [180/200] batch [10/50] time 0.116 (0.206) data 0.031 (0.120) loss 2.0820 (2.0006) acc 37.5000 (41.2500) lr 5.9119e-05 eta 0:03:34
epoch [180/200] batch [15/50] time 0.084 (0.169) data 0.000 (0.084) loss 2.0605 (1.9722) acc 46.8750 (41.4583) lr 5.9119e-05 eta 0:02:54
epoch [180/200] batch [20/50] time 0.087 (0.156) data 0.001 (0.070) loss 1.7666 (1.9414) acc 43.7500 (43.7500) lr 5.9119e-05 eta 0:02:40
epoch [180/200] batch [25/50] time 0.177 (0.146) data 0.093 (0.060) loss 2.2324 (1.9455) acc 37.5000 (43.8750) lr 5.9119e-05 eta 0:02:29
epoch [180/200] batch [30/50] time 0.084 (0.139) data 0.000 (0.053) loss 1.5957 (1.9451) acc 40.6250 (43.8542) lr 5.9119e-05 eta 0:02:21
epoch [180/200] batch [35/50] time 0.086 (0.133) data 0.001 (0.047) loss 1.5625 (1.9281) acc 50.0000 (44.4643) lr 5.9119e-05 eta 0:02:14
epoch [180/200] batch [40/50] time 0.083 (0.127) data 0.000 (0.041) loss 1.8076 (1.9238) acc 43.7500 (43.9844) lr 5.9119e-05 eta 0:02:07
epoch [180/200] batch [45/50] time 0.084 (0.126) data 0.000 (0.040) loss 1.9297 (1.9209) acc 43.7500 (43.9583) lr 5.9119e-05 eta 0:02:06
epoch [180/200] batch [50/50] time 0.084 (0.121) data 0.000 (0.036) loss 1.7803 (1.9096) acc 46.8750 (43.5625) lr 5.3915e-05 eta 0:02:01
epoch [181/200] batch [5/50] time 0.084 (0.395) data 0.000 (0.310) loss 1.9668 (1.9498) acc 43.7500 (43.7500) lr 5.3915e-05 eta 0:06:33
epoch [181/200] batch [10/50] time 0.086 (0.243) data 0.001 (0.158) loss 1.8428 (1.9499) acc 56.2500 (45.3125) lr 5.3915e-05 eta 0:04:00
epoch [181/200] batch [15/50] time 0.086 (0.190) data 0.000 (0.106) loss 2.0586 (1.9297) acc 37.5000 (43.5417) lr 5.3915e-05 eta 0:03:07
epoch [181/200] batch [20/50] time 0.086 (0.169) data 0.000 (0.084) loss 1.9512 (1.9432) acc 40.6250 (42.9688) lr 5.3915e-05 eta 0:02:45
epoch [181/200] batch [25/50] time 0.187 (0.156) data 0.103 (0.072) loss 1.7188 (1.9117) acc 40.6250 (43.0000) lr 5.3915e-05 eta 0:02:32
epoch [181/200] batch [30/50] time 0.086 (0.144) data 0.000 (0.060) loss 1.9365 (1.9296) acc 37.5000 (43.0208) lr 5.3915e-05 eta 0:02:20
epoch [181/200] batch [35/50] time 0.085 (0.144) data 0.001 (0.059) loss 1.7666 (1.9314) acc 56.2500 (43.3036) lr 5.3915e-05 eta 0:02:18
epoch [181/200] batch [40/50] time 0.083 (0.136) data 0.000 (0.052) loss 2.2109 (1.9291) acc 31.2500 (43.2812) lr 5.3915e-05 eta 0:02:10
epoch [181/200] batch [45/50] time 0.083 (0.134) data 0.000 (0.050) loss 2.3379 (1.9216) acc 25.0000 (43.4722) lr 5.3915e-05 eta 0:02:07
epoch [181/200] batch [50/50] time 0.083 (0.129) data 0.000 (0.045) loss 1.8223 (1.9019) acc 37.5000 (44.2500) lr 4.8943e-05 eta 0:02:02
epoch [182/200] batch [5/50] time 0.087 (0.332) data 0.000 (0.245) loss 2.0195 (1.9354) acc 59.3750 (43.1250) lr 4.8943e-05 eta 0:05:13
epoch [182/200] batch [10/50] time 0.159 (0.216) data 0.075 (0.131) loss 1.9668 (1.9296) acc 40.6250 (44.6875) lr 4.8943e-05 eta 0:03:22
epoch [182/200] batch [15/50] time 0.085 (0.173) data 0.000 (0.087) loss 1.5938 (1.8535) acc 43.7500 (46.0417) lr 4.8943e-05 eta 0:02:41
epoch [182/200] batch [20/50] time 0.086 (0.151) data 0.001 (0.066) loss 2.0801 (1.8373) acc 40.6250 (45.1562) lr 4.8943e-05 eta 0:02:20
epoch [182/200] batch [25/50] time 0.085 (0.138) data 0.001 (0.053) loss 2.0547 (1.8136) acc 37.5000 (46.1250) lr 4.8943e-05 eta 0:02:07
epoch [182/200] batch [30/50] time 0.085 (0.129) data 0.000 (0.044) loss 2.2871 (1.8429) acc 34.3750 (45.2083) lr 4.8943e-05 eta 0:01:58
epoch [182/200] batch [35/50] time 0.085 (0.127) data 0.001 (0.042) loss 2.1777 (1.8430) acc 28.1250 (45.5357) lr 4.8943e-05 eta 0:01:56
epoch [182/200] batch [40/50] time 0.085 (0.122) data 0.000 (0.037) loss 1.8350 (1.8364) acc 46.8750 (45.8594) lr 4.8943e-05 eta 0:01:51
epoch [182/200] batch [45/50] time 0.085 (0.118) data 0.000 (0.033) loss 1.4502 (1.8582) acc 56.2500 (45.4167) lr 4.8943e-05 eta 0:01:46
epoch [182/200] batch [50/50] time 0.084 (0.114) data 0.000 (0.030) loss 1.7812 (1.8643) acc 53.1250 (45.6250) lr 4.4207e-05 eta 0:01:43
epoch [183/200] batch [5/50] time 0.082 (0.328) data 0.000 (0.244) loss 2.0996 (1.8494) acc 40.6250 (48.1250) lr 4.4207e-05 eta 0:04:53
epoch [183/200] batch [10/50] time 0.359 (0.234) data 0.276 (0.150) loss 2.2031 (1.8895) acc 37.5000 (46.5625) lr 4.4207e-05 eta 0:03:28
epoch [183/200] batch [15/50] time 0.084 (0.184) data 0.000 (0.100) loss 1.9531 (1.8777) acc 43.7500 (47.0833) lr 4.4207e-05 eta 0:02:43
epoch [183/200] batch [20/50] time 0.084 (0.162) data 0.000 (0.077) loss 1.7285 (1.8800) acc 50.0000 (46.4062) lr 4.4207e-05 eta 0:02:22
epoch [183/200] batch [25/50] time 0.086 (0.147) data 0.000 (0.062) loss 1.8994 (1.8932) acc 40.6250 (45.5000) lr 4.4207e-05 eta 0:02:08
epoch [183/200] batch [30/50] time 0.084 (0.136) data 0.000 (0.052) loss 2.0566 (1.8975) acc 31.2500 (45.5208) lr 4.4207e-05 eta 0:01:58
epoch [183/200] batch [35/50] time 0.089 (0.129) data 0.001 (0.044) loss 2.1719 (1.8917) acc 37.5000 (45.0893) lr 4.4207e-05 eta 0:01:51
epoch [183/200] batch [40/50] time 0.085 (0.123) data 0.000 (0.039) loss 2.4414 (1.8952) acc 40.6250 (45.2344) lr 4.4207e-05 eta 0:01:46
epoch [183/200] batch [45/50] time 0.084 (0.119) data 0.000 (0.035) loss 2.0859 (1.9071) acc 43.7500 (44.7222) lr 4.4207e-05 eta 0:01:41
epoch [183/200] batch [50/50] time 0.085 (0.116) data 0.000 (0.031) loss 1.7803 (1.8900) acc 50.0000 (45.0625) lr 3.9706e-05 eta 0:01:38
epoch [184/200] batch [5/50] time 0.086 (0.295) data 0.000 (0.208) loss 1.5850 (1.8631) acc 59.3750 (50.0000) lr 3.9706e-05 eta 0:04:09
epoch [184/200] batch [10/50] time 0.086 (0.192) data 0.001 (0.104) loss 2.2441 (1.8556) acc 37.5000 (46.5625) lr 3.9706e-05 eta 0:02:41
epoch [184/200] batch [15/50] time 0.088 (0.162) data 0.000 (0.075) loss 1.8770 (1.8499) acc 50.0000 (48.1250) lr 3.9706e-05 eta 0:02:15
epoch [184/200] batch [20/50] time 0.255 (0.152) data 0.171 (0.065) loss 1.8301 (1.8651) acc 50.0000 (45.9375) lr 3.9706e-05 eta 0:02:05
epoch [184/200] batch [25/50] time 0.085 (0.139) data 0.000 (0.052) loss 2.0449 (1.8941) acc 46.8750 (44.6250) lr 3.9706e-05 eta 0:01:54
epoch [184/200] batch [30/50] time 0.084 (0.132) data 0.000 (0.046) loss 1.6484 (1.8940) acc 53.1250 (45.2083) lr 3.9706e-05 eta 0:01:48
epoch [184/200] batch [35/50] time 0.086 (0.126) data 0.001 (0.040) loss 1.6514 (1.8826) acc 46.8750 (44.7321) lr 3.9706e-05 eta 0:01:42
epoch [184/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.040) loss 2.0020 (1.8929) acc 40.6250 (44.0625) lr 3.9706e-05 eta 0:01:42
epoch [184/200] batch [45/50] time 0.082 (0.123) data 0.000 (0.038) loss 2.2090 (1.9084) acc 34.3750 (43.6806) lr 3.9706e-05 eta 0:01:39
epoch [184/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.034) loss 2.0059 (1.9189) acc 37.5000 (43.6250) lr 3.5443e-05 eta 0:01:35
epoch [185/200] batch [5/50] time 0.087 (0.295) data 0.000 (0.208) loss 1.8867 (1.8553) acc 40.6250 (45.6250) lr 3.5443e-05 eta 0:03:54
epoch [185/200] batch [10/50] time 0.086 (0.192) data 0.000 (0.105) loss 2.2422 (1.8944) acc 31.2500 (43.4375) lr 3.5443e-05 eta 0:02:31
epoch [185/200] batch [15/50] time 0.087 (0.157) data 0.000 (0.070) loss 1.7100 (1.8128) acc 53.1250 (46.4583) lr 3.5443e-05 eta 0:02:03
epoch [185/200] batch [20/50] time 0.086 (0.139) data 0.000 (0.053) loss 1.5586 (1.8095) acc 56.2500 (45.6250) lr 3.5443e-05 eta 0:01:48
epoch [185/200] batch [25/50] time 0.090 (0.133) data 0.001 (0.047) loss 1.8018 (1.8291) acc 43.7500 (44.5000) lr 3.5443e-05 eta 0:01:43
epoch [185/200] batch [30/50] time 0.258 (0.131) data 0.174 (0.045) loss 1.4160 (1.8129) acc 46.8750 (45.4167) lr 3.5443e-05 eta 0:01:41
epoch [185/200] batch [35/50] time 0.085 (0.126) data 0.000 (0.040) loss 1.9395 (1.8344) acc 40.6250 (45.5357) lr 3.5443e-05 eta 0:01:36
epoch [185/200] batch [40/50] time 0.085 (0.124) data 0.000 (0.038) loss 2.2109 (1.8504) acc 43.7500 (45.3125) lr 3.5443e-05 eta 0:01:33
epoch [185/200] batch [45/50] time 0.084 (0.121) data 0.000 (0.036) loss 1.8154 (1.8479) acc 37.5000 (45.0000) lr 3.5443e-05 eta 0:01:31
epoch [185/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.032) loss 2.0781 (1.8613) acc 53.1250 (44.8750) lr 3.1417e-05 eta 0:01:28
epoch [186/200] batch [5/50] time 0.085 (0.418) data 0.000 (0.331) loss 2.0605 (1.9395) acc 34.3750 (45.6250) lr 3.1417e-05 eta 0:05:11
epoch [186/200] batch [10/50] time 0.196 (0.263) data 0.110 (0.177) loss 1.7764 (1.9332) acc 50.0000 (45.3125) lr 3.1417e-05 eta 0:03:14
epoch [186/200] batch [15/50] time 0.087 (0.204) data 0.000 (0.118) loss 1.6064 (1.8712) acc 43.7500 (45.4167) lr 3.1417e-05 eta 0:02:30
epoch [186/200] batch [20/50] time 0.086 (0.181) data 0.000 (0.095) loss 1.5352 (1.8206) acc 50.0000 (47.1875) lr 3.1417e-05 eta 0:02:12
epoch [186/200] batch [25/50] time 0.087 (0.162) data 0.000 (0.076) loss 1.5459 (1.8060) acc 62.5000 (47.8750) lr 3.1417e-05 eta 0:01:57
epoch [186/200] batch [30/50] time 0.085 (0.153) data 0.000 (0.067) loss 2.2266 (1.8662) acc 37.5000 (45.7292) lr 3.1417e-05 eta 0:01:49
epoch [186/200] batch [35/50] time 0.085 (0.151) data 0.000 (0.065) loss 1.9463 (1.8690) acc 43.7500 (45.8929) lr 3.1417e-05 eta 0:01:48
epoch [186/200] batch [40/50] time 0.084 (0.143) data 0.000 (0.057) loss 1.7178 (1.8815) acc 50.0000 (45.3125) lr 3.1417e-05 eta 0:01:41
epoch [186/200] batch [45/50] time 0.084 (0.139) data 0.000 (0.053) loss 1.4707 (1.8719) acc 59.3750 (45.9722) lr 3.1417e-05 eta 0:01:37
epoch [186/200] batch [50/50] time 0.085 (0.133) data 0.000 (0.048) loss 2.0859 (1.8708) acc 34.3750 (45.8125) lr 2.7630e-05 eta 0:01:33
epoch [187/200] batch [5/50] time 0.085 (0.345) data 0.001 (0.259) loss 1.8135 (1.8576) acc 46.8750 (45.0000) lr 2.7630e-05 eta 0:03:59
epoch [187/200] batch [10/50] time 0.085 (0.226) data 0.000 (0.141) loss 1.5137 (1.7635) acc 53.1250 (47.8125) lr 2.7630e-05 eta 0:02:35
epoch [187/200] batch [15/50] time 0.085 (0.179) data 0.000 (0.094) loss 1.5430 (1.7408) acc 59.3750 (48.1250) lr 2.7630e-05 eta 0:02:02
epoch [187/200] batch [20/50] time 0.085 (0.164) data 0.000 (0.080) loss 1.7861 (1.7733) acc 43.7500 (47.0312) lr 2.7630e-05 eta 0:01:51
epoch [187/200] batch [25/50] time 0.214 (0.154) data 0.128 (0.069) loss 2.0137 (1.8114) acc 37.5000 (46.2500) lr 2.7630e-05 eta 0:01:43
epoch [187/200] batch [30/50] time 0.084 (0.142) data 0.000 (0.057) loss 1.9277 (1.7905) acc 46.8750 (47.5000) lr 2.7630e-05 eta 0:01:35
epoch [187/200] batch [35/50] time 0.085 (0.138) data 0.001 (0.054) loss 1.7891 (1.7853) acc 46.8750 (47.7679) lr 2.7630e-05 eta 0:01:32
epoch [187/200] batch [40/50] time 0.085 (0.132) data 0.000 (0.047) loss 2.1582 (1.8167) acc 31.2500 (46.6406) lr 2.7630e-05 eta 0:01:26
epoch [187/200] batch [45/50] time 0.083 (0.130) data 0.000 (0.045) loss 1.7803 (1.8152) acc 43.7500 (46.6667) lr 2.7630e-05 eta 0:01:24
epoch [187/200] batch [50/50] time 0.083 (0.125) data 0.000 (0.041) loss 1.7168 (1.8214) acc 43.7500 (46.3125) lr 2.4083e-05 eta 0:01:21
epoch [188/200] batch [5/50] time 0.087 (0.355) data 0.001 (0.270) loss 1.6113 (1.8131) acc 53.1250 (50.0000) lr 2.4083e-05 eta 0:03:49
epoch [188/200] batch [10/50] time 0.088 (0.221) data 0.001 (0.135) loss 1.8730 (1.7841) acc 53.1250 (50.9375) lr 2.4083e-05 eta 0:02:21
epoch [188/200] batch [15/50] time 0.087 (0.180) data 0.001 (0.094) loss 2.2168 (1.9205) acc 31.2500 (45.6250) lr 2.4083e-05 eta 0:01:54
epoch [188/200] batch [20/50] time 0.264 (0.165) data 0.179 (0.079) loss 1.5850 (1.8812) acc 53.1250 (46.4062) lr 2.4083e-05 eta 0:01:44
epoch [188/200] batch [25/50] time 0.088 (0.150) data 0.001 (0.064) loss 1.9658 (1.8964) acc 37.5000 (45.3750) lr 2.4083e-05 eta 0:01:33
epoch [188/200] batch [30/50] time 0.087 (0.143) data 0.000 (0.058) loss 2.3750 (1.9188) acc 37.5000 (44.8958) lr 2.4083e-05 eta 0:01:28
epoch [188/200] batch [35/50] time 0.087 (0.135) data 0.001 (0.049) loss 1.8730 (1.9059) acc 50.0000 (45.1786) lr 2.4083e-05 eta 0:01:23
epoch [188/200] batch [40/50] time 0.086 (0.133) data 0.000 (0.047) loss 1.9619 (1.9153) acc 50.0000 (45.3125) lr 2.4083e-05 eta 0:01:21
epoch [188/200] batch [45/50] time 0.083 (0.132) data 0.000 (0.046) loss 1.9307 (1.9095) acc 46.8750 (44.7222) lr 2.4083e-05 eta 0:01:19
epoch [188/200] batch [50/50] time 0.082 (0.127) data 0.000 (0.042) loss 1.6123 (1.8933) acc 43.7500 (45.0625) lr 2.0777e-05 eta 0:01:16
epoch [189/200] batch [5/50] time 0.084 (0.397) data 0.000 (0.313) loss 1.8486 (1.9879) acc 46.8750 (43.1250) lr 2.0777e-05 eta 0:03:56
epoch [189/200] batch [10/50] time 0.085 (0.241) data 0.000 (0.157) loss 2.1191 (2.0255) acc 43.7500 (43.1250) lr 2.0777e-05 eta 0:02:22
epoch [189/200] batch [15/50] time 0.084 (0.202) data 0.000 (0.117) loss 2.0137 (1.9501) acc 28.1250 (43.1250) lr 2.0777e-05 eta 0:01:57
epoch [189/200] batch [20/50] time 0.085 (0.181) data 0.000 (0.096) loss 1.7764 (1.8882) acc 43.7500 (43.2812) lr 2.0777e-05 eta 0:01:44
epoch [189/200] batch [25/50] time 0.084 (0.161) data 0.000 (0.077) loss 1.9941 (1.8789) acc 46.8750 (43.2500) lr 2.0777e-05 eta 0:01:32
epoch [189/200] batch [30/50] time 0.085 (0.155) data 0.000 (0.070) loss 1.7695 (1.8729) acc 50.0000 (44.1667) lr 2.0777e-05 eta 0:01:28
epoch [189/200] batch [35/50] time 0.197 (0.148) data 0.112 (0.063) loss 1.8760 (1.8819) acc 56.2500 (44.5536) lr 2.0777e-05 eta 0:01:23
epoch [189/200] batch [40/50] time 0.083 (0.140) data 0.000 (0.056) loss 1.7021 (1.9044) acc 50.0000 (43.5156) lr 2.0777e-05 eta 0:01:18
epoch [189/200] batch [45/50] time 0.083 (0.137) data 0.000 (0.053) loss 1.7900 (1.8981) acc 37.5000 (43.4028) lr 2.0777e-05 eta 0:01:16
epoch [189/200] batch [50/50] time 0.082 (0.132) data 0.000 (0.048) loss 2.0000 (1.8983) acc 53.1250 (43.3750) lr 1.7713e-05 eta 0:01:12
epoch [190/200] batch [5/50] time 0.086 (0.296) data 0.001 (0.209) loss 1.6904 (1.9053) acc 37.5000 (39.3750) lr 1.7713e-05 eta 0:02:41
epoch [190/200] batch [10/50] time 0.087 (0.208) data 0.001 (0.122) loss 1.7754 (1.8851) acc 46.8750 (40.9375) lr 1.7713e-05 eta 0:01:52
epoch [190/200] batch [15/50] time 0.085 (0.167) data 0.000 (0.081) loss 2.0195 (1.9281) acc 40.6250 (40.6250) lr 1.7713e-05 eta 0:01:29
epoch [190/200] batch [20/50] time 0.087 (0.156) data 0.000 (0.070) loss 1.7832 (1.9160) acc 50.0000 (41.8750) lr 1.7713e-05 eta 0:01:22
epoch [190/200] batch [25/50] time 0.225 (0.148) data 0.140 (0.062) loss 1.8691 (1.9215) acc 46.8750 (42.1250) lr 1.7713e-05 eta 0:01:17
epoch [190/200] batch [30/50] time 0.087 (0.138) data 0.000 (0.052) loss 1.8291 (1.9212) acc 53.1250 (43.6458) lr 1.7713e-05 eta 0:01:11
epoch [190/200] batch [35/50] time 0.086 (0.131) data 0.001 (0.044) loss 1.8887 (1.9084) acc 43.7500 (43.6607) lr 1.7713e-05 eta 0:01:07
epoch [190/200] batch [40/50] time 0.086 (0.125) data 0.001 (0.039) loss 1.6572 (1.8881) acc 62.5000 (44.0625) lr 1.7713e-05 eta 0:01:03
epoch [190/200] batch [45/50] time 0.085 (0.123) data 0.000 (0.037) loss 1.8203 (1.8620) acc 43.7500 (44.5139) lr 1.7713e-05 eta 0:01:01
epoch [190/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.033) loss 1.6465 (1.8687) acc 50.0000 (44.2500) lr 1.4891e-05 eta 0:00:59
epoch [191/200] batch [5/50] time 0.086 (0.342) data 0.000 (0.256) loss 1.7354 (1.8605) acc 46.8750 (41.8750) lr 1.4891e-05 eta 0:02:49
epoch [191/200] batch [10/50] time 0.088 (0.227) data 0.001 (0.140) loss 1.8750 (1.9035) acc 46.8750 (43.4375) lr 1.4891e-05 eta 0:01:51
epoch [191/200] batch [15/50] time 0.088 (0.180) data 0.000 (0.094) loss 1.6299 (1.8504) acc 59.3750 (45.6250) lr 1.4891e-05 eta 0:01:27
epoch [191/200] batch [20/50] time 0.087 (0.164) data 0.000 (0.077) loss 1.8535 (1.7944) acc 43.7500 (47.0312) lr 1.4891e-05 eta 0:01:18
epoch [191/200] batch [25/50] time 0.243 (0.155) data 0.157 (0.068) loss 1.5820 (1.8496) acc 50.0000 (45.5000) lr 1.4891e-05 eta 0:01:13
epoch [191/200] batch [30/50] time 0.087 (0.144) data 0.001 (0.058) loss 2.0430 (1.8531) acc 50.0000 (45.2083) lr 1.4891e-05 eta 0:01:07
epoch [191/200] batch [35/50] time 0.086 (0.141) data 0.001 (0.054) loss 1.4170 (1.8474) acc 62.5000 (45.0893) lr 1.4891e-05 eta 0:01:05
epoch [191/200] batch [40/50] time 0.084 (0.134) data 0.000 (0.047) loss 1.7363 (1.8657) acc 46.8750 (44.4531) lr 1.4891e-05 eta 0:01:01
epoch [191/200] batch [45/50] time 0.083 (0.131) data 0.000 (0.045) loss 1.9277 (1.8692) acc 37.5000 (44.7917) lr 1.4891e-05 eta 0:00:59
epoch [191/200] batch [50/50] time 0.084 (0.130) data 0.000 (0.045) loss 1.6826 (1.8521) acc 43.7500 (45.0000) lr 1.2312e-05 eta 0:00:58
epoch [192/200] batch [5/50] time 0.086 (0.313) data 0.000 (0.227) loss 2.4375 (2.0713) acc 34.3750 (45.0000) lr 1.2312e-05 eta 0:02:19
epoch [192/200] batch [10/50] time 0.085 (0.200) data 0.000 (0.114) loss 1.7314 (1.9350) acc 46.8750 (45.0000) lr 1.2312e-05 eta 0:01:27
epoch [192/200] batch [15/50] time 0.085 (0.167) data 0.000 (0.082) loss 2.0469 (1.9253) acc 37.5000 (44.7917) lr 1.2312e-05 eta 0:01:12
epoch [192/200] batch [20/50] time 0.085 (0.152) data 0.000 (0.066) loss 2.1992 (1.9271) acc 31.2500 (44.8438) lr 1.2312e-05 eta 0:01:05
epoch [192/200] batch [25/50] time 0.086 (0.139) data 0.000 (0.053) loss 2.0684 (1.9271) acc 28.1250 (43.7500) lr 1.2312e-05 eta 0:00:58
epoch [192/200] batch [30/50] time 0.086 (0.133) data 0.000 (0.047) loss 1.5654 (1.9378) acc 50.0000 (42.8125) lr 1.2312e-05 eta 0:00:55
epoch [192/200] batch [35/50] time 0.179 (0.129) data 0.093 (0.043) loss 2.0996 (1.9390) acc 46.8750 (43.1250) lr 1.2312e-05 eta 0:00:53
epoch [192/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.041) loss 1.7354 (1.9222) acc 56.2500 (43.9844) lr 1.2312e-05 eta 0:00:51
epoch [192/200] batch [45/50] time 0.083 (0.124) data 0.000 (0.039) loss 2.0098 (1.9146) acc 34.3750 (44.2361) lr 1.2312e-05 eta 0:00:50
epoch [192/200] batch [50/50] time 0.082 (0.120) data 0.000 (0.035) loss 1.8584 (1.8986) acc 46.8750 (44.3125) lr 9.9763e-06 eta 0:00:48
epoch [193/200] batch [5/50] time 0.087 (0.298) data 0.000 (0.210) loss 1.7295 (1.9719) acc 46.8750 (38.7500) lr 9.9763e-06 eta 0:01:57
epoch [193/200] batch [10/50] time 0.087 (0.193) data 0.001 (0.105) loss 1.7529 (1.9412) acc 43.7500 (41.2500) lr 9.9763e-06 eta 0:01:15
epoch [193/200] batch [15/50] time 0.086 (0.158) data 0.000 (0.070) loss 2.1426 (1.9581) acc 34.3750 (41.0417) lr 9.9763e-06 eta 0:01:00
epoch [193/200] batch [20/50] time 0.084 (0.139) data 0.000 (0.053) loss 1.9277 (1.9303) acc 40.6250 (42.9688) lr 9.9763e-06 eta 0:00:53
epoch [193/200] batch [25/50] time 0.086 (0.129) data 0.000 (0.042) loss 1.6357 (1.9128) acc 53.1250 (42.7500) lr 9.9763e-06 eta 0:00:48
epoch [193/200] batch [30/50] time 0.085 (0.122) data 0.000 (0.035) loss 1.9854 (1.8877) acc 40.6250 (43.6458) lr 9.9763e-06 eta 0:00:45
epoch [193/200] batch [35/50] time 0.085 (0.118) data 0.001 (0.030) loss 1.7021 (1.8926) acc 53.1250 (44.2857) lr 9.9763e-06 eta 0:00:42
epoch [193/200] batch [40/50] time 0.085 (0.114) data 0.000 (0.027) loss 1.9443 (1.8788) acc 46.8750 (44.6094) lr 9.9763e-06 eta 0:00:41
epoch [193/200] batch [45/50] time 0.084 (0.111) data 0.000 (0.024) loss 1.8086 (1.8685) acc 50.0000 (44.6528) lr 9.9763e-06 eta 0:00:39
epoch [193/200] batch [50/50] time 0.084 (0.108) data 0.000 (0.021) loss 2.0703 (1.8896) acc 50.0000 (44.3125) lr 7.8853e-06 eta 0:00:37
epoch [194/200] batch [5/50] time 0.084 (0.394) data 0.000 (0.307) loss 1.8018 (2.0109) acc 50.0000 (43.7500) lr 7.8853e-06 eta 0:02:15
epoch [194/200] batch [10/50] time 0.085 (0.239) data 0.001 (0.154) loss 1.9189 (2.0028) acc 46.8750 (43.1250) lr 7.8853e-06 eta 0:01:21
epoch [194/200] batch [15/50] time 0.087 (0.194) data 0.001 (0.108) loss 2.1953 (2.0379) acc 40.6250 (43.3333) lr 7.8853e-06 eta 0:01:04
epoch [194/200] batch [20/50] time 0.085 (0.172) data 0.000 (0.087) loss 2.1465 (2.0211) acc 40.6250 (43.7500) lr 7.8853e-06 eta 0:00:56
epoch [194/200] batch [25/50] time 0.088 (0.155) data 0.000 (0.069) loss 1.4736 (1.9587) acc 56.2500 (45.1250) lr 7.8853e-06 eta 0:00:50
epoch [194/200] batch [30/50] time 0.086 (0.147) data 0.000 (0.062) loss 1.7803 (1.9364) acc 56.2500 (45.8333) lr 7.8853e-06 eta 0:00:47
epoch [194/200] batch [35/50] time 0.207 (0.142) data 0.121 (0.057) loss 1.8154 (1.9198) acc 43.7500 (45.4464) lr 7.8853e-06 eta 0:00:44
epoch [194/200] batch [40/50] time 0.085 (0.135) data 0.000 (0.050) loss 1.8311 (1.9068) acc 40.6250 (45.2344) lr 7.8853e-06 eta 0:00:41
epoch [194/200] batch [45/50] time 0.083 (0.132) data 0.000 (0.047) loss 2.1582 (1.9276) acc 40.6250 (44.7222) lr 7.8853e-06 eta 0:00:40
epoch [194/200] batch [50/50] time 0.083 (0.127) data 0.000 (0.042) loss 2.2832 (1.9323) acc 34.3750 (44.7500) lr 6.0390e-06 eta 0:00:38
epoch [195/200] batch [5/50] time 0.087 (0.333) data 0.000 (0.247) loss 1.6494 (1.7719) acc 59.3750 (50.0000) lr 6.0390e-06 eta 0:01:38
epoch [195/200] batch [10/50] time 0.086 (0.210) data 0.000 (0.124) loss 1.7002 (1.8399) acc 46.8750 (46.5625) lr 6.0390e-06 eta 0:01:00
epoch [195/200] batch [15/50] time 0.085 (0.176) data 0.000 (0.089) loss 2.0508 (1.8380) acc 34.3750 (47.7083) lr 6.0390e-06 eta 0:00:50
epoch [195/200] batch [20/50] time 0.127 (0.155) data 0.043 (0.069) loss 1.6748 (1.8330) acc 40.6250 (46.5625) lr 6.0390e-06 eta 0:00:43
epoch [195/200] batch [25/50] time 0.088 (0.142) data 0.000 (0.055) loss 1.7998 (1.8183) acc 46.8750 (46.3750) lr 6.0390e-06 eta 0:00:38
epoch [195/200] batch [30/50] time 0.087 (0.132) data 0.000 (0.046) loss 1.8203 (1.8488) acc 46.8750 (45.7292) lr 6.0390e-06 eta 0:00:35
epoch [195/200] batch [35/50] time 0.086 (0.126) data 0.001 (0.040) loss 1.9961 (1.8328) acc 31.2500 (45.9821) lr 6.0390e-06 eta 0:00:33
epoch [195/200] batch [40/50] time 0.245 (0.126) data 0.162 (0.040) loss 2.2461 (1.8393) acc 37.5000 (46.3281) lr 6.0390e-06 eta 0:00:32
epoch [195/200] batch [45/50] time 0.083 (0.121) data 0.000 (0.035) loss 1.7783 (1.8374) acc 46.8750 (46.3889) lr 6.0390e-06 eta 0:00:30
epoch [195/200] batch [50/50] time 0.083 (0.119) data 0.000 (0.034) loss 2.2910 (1.8404) acc 37.5000 (46.1250) lr 4.4380e-06 eta 0:00:29
epoch [196/200] batch [5/50] time 0.086 (0.303) data 0.000 (0.217) loss 1.9414 (1.8703) acc 46.8750 (43.7500) lr 4.4380e-06 eta 0:01:14
epoch [196/200] batch [10/50] time 0.087 (0.195) data 0.000 (0.109) loss 1.7100 (1.8028) acc 53.1250 (45.3125) lr 4.4380e-06 eta 0:00:46
epoch [196/200] batch [15/50] time 0.087 (0.164) data 0.000 (0.078) loss 1.9111 (1.8552) acc 34.3750 (42.5000) lr 4.4380e-06 eta 0:00:38
epoch [196/200] batch [20/50] time 0.192 (0.150) data 0.106 (0.064) loss 1.9424 (1.8547) acc 46.8750 (44.2188) lr 4.4380e-06 eta 0:00:34
epoch [196/200] batch [25/50] time 0.088 (0.138) data 0.001 (0.051) loss 2.2246 (1.8554) acc 28.1250 (44.2500) lr 4.4380e-06 eta 0:00:30
epoch [196/200] batch [30/50] time 0.086 (0.134) data 0.001 (0.048) loss 1.9385 (1.8495) acc 31.2500 (44.1667) lr 4.4380e-06 eta 0:00:29
epoch [196/200] batch [35/50] time 0.085 (0.128) data 0.001 (0.041) loss 2.2969 (1.8485) acc 34.3750 (44.3750) lr 4.4380e-06 eta 0:00:27
epoch [196/200] batch [40/50] time 0.086 (0.125) data 0.000 (0.039) loss 1.9609 (1.8450) acc 53.1250 (44.4531) lr 4.4380e-06 eta 0:00:26
epoch [196/200] batch [45/50] time 0.085 (0.124) data 0.000 (0.038) loss 2.2168 (1.8635) acc 40.6250 (43.5417) lr 4.4380e-06 eta 0:00:25
epoch [196/200] batch [50/50] time 0.084 (0.120) data 0.000 (0.035) loss 1.6396 (1.8519) acc 59.3750 (44.3125) lr 3.0827e-06 eta 0:00:24
epoch [197/200] batch [5/50] time 0.087 (0.333) data 0.001 (0.246) loss 1.7354 (1.8422) acc 43.7500 (38.7500) lr 3.0827e-06 eta 0:01:04
epoch [197/200] batch [10/50] time 0.087 (0.222) data 0.001 (0.135) loss 1.4541 (1.8220) acc 59.3750 (46.8750) lr 3.0827e-06 eta 0:00:42
epoch [197/200] batch [15/50] time 0.087 (0.177) data 0.000 (0.090) loss 1.8252 (1.8085) acc 43.7500 (47.0833) lr 3.0827e-06 eta 0:00:32
epoch [197/200] batch [20/50] time 0.087 (0.155) data 0.000 (0.068) loss 1.6357 (1.8054) acc 56.2500 (47.5000) lr 3.0827e-06 eta 0:00:27
epoch [197/200] batch [25/50] time 0.084 (0.141) data 0.001 (0.054) loss 1.9316 (1.7981) acc 37.5000 (47.0000) lr 3.0827e-06 eta 0:00:24
epoch [197/200] batch [30/50] time 0.084 (0.139) data 0.000 (0.052) loss 1.7793 (1.7825) acc 40.6250 (47.6042) lr 3.0827e-06 eta 0:00:23
epoch [197/200] batch [35/50] time 0.085 (0.136) data 0.001 (0.050) loss 2.1230 (1.7979) acc 28.1250 (46.5179) lr 3.0827e-06 eta 0:00:22
epoch [197/200] batch [40/50] time 0.084 (0.129) data 0.000 (0.043) loss 2.0488 (1.7950) acc 43.7500 (46.7969) lr 3.0827e-06 eta 0:00:20
epoch [197/200] batch [45/50] time 0.083 (0.126) data 0.000 (0.040) loss 2.5176 (1.8259) acc 28.1250 (45.8333) lr 3.0827e-06 eta 0:00:19
epoch [197/200] batch [50/50] time 0.085 (0.121) data 0.000 (0.036) loss 2.1855 (1.8570) acc 46.8750 (45.2500) lr 1.9733e-06 eta 0:00:18
epoch [198/200] batch [5/50] time 0.084 (0.352) data 0.000 (0.268) loss 1.5264 (1.8982) acc 68.7500 (44.3750) lr 1.9733e-06 eta 0:00:51
epoch [198/200] batch [10/50] time 0.083 (0.218) data 0.000 (0.134) loss 1.8760 (1.8850) acc 43.7500 (45.3125) lr 1.9733e-06 eta 0:00:30
epoch [198/200] batch [15/50] time 0.084 (0.173) data 0.000 (0.090) loss 1.6484 (1.8441) acc 43.7500 (45.8333) lr 1.9733e-06 eta 0:00:23
epoch [198/200] batch [20/50] time 0.085 (0.155) data 0.000 (0.071) loss 2.4121 (1.8264) acc 40.6250 (46.2500) lr 1.9733e-06 eta 0:00:20
epoch [198/200] batch [25/50] time 0.086 (0.141) data 0.000 (0.057) loss 1.9268 (1.8756) acc 40.6250 (44.3750) lr 1.9733e-06 eta 0:00:17
epoch [198/200] batch [30/50] time 0.085 (0.135) data 0.000 (0.051) loss 1.8457 (1.8660) acc 43.7500 (44.3750) lr 1.9733e-06 eta 0:00:16
epoch [198/200] batch [35/50] time 0.084 (0.131) data 0.001 (0.047) loss 2.0391 (1.8639) acc 40.6250 (44.8214) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [40/50] time 0.085 (0.125) data 0.000 (0.041) loss 2.4023 (1.8622) acc 28.1250 (45.0000) lr 1.9733e-06 eta 0:00:13
epoch [198/200] batch [45/50] time 0.085 (0.121) data 0.000 (0.037) loss 1.5273 (1.8710) acc 50.0000 (44.3750) lr 1.9733e-06 eta 0:00:12
epoch [198/200] batch [50/50] time 0.083 (0.117) data 0.000 (0.033) loss 1.5293 (1.8795) acc 59.3750 (44.6875) lr 1.1101e-06 eta 0:00:11
epoch [199/200] batch [5/50] time 0.084 (0.331) data 0.000 (0.246) loss 1.8223 (1.9930) acc 43.7500 (39.3750) lr 1.1101e-06 eta 0:00:31
epoch [199/200] batch [10/50] time 0.265 (0.229) data 0.181 (0.144) loss 1.5732 (1.9093) acc 53.1250 (42.5000) lr 1.1101e-06 eta 0:00:20
epoch [199/200] batch [15/50] time 0.086 (0.181) data 0.000 (0.096) loss 1.8662 (1.8842) acc 40.6250 (42.5000) lr 1.1101e-06 eta 0:00:15
epoch [199/200] batch [20/50] time 0.096 (0.165) data 0.010 (0.075) loss 1.9541 (1.8709) acc 37.5000 (43.2812) lr 1.1101e-06 eta 0:00:13
epoch [199/200] batch [25/50] time 0.208 (0.155) data 0.119 (0.065) loss 1.9717 (1.8662) acc 46.8750 (43.1250) lr 1.1101e-06 eta 0:00:11
epoch [199/200] batch [30/50] time 0.085 (0.143) data 0.000 (0.054) loss 1.7227 (1.8537) acc 40.6250 (43.7500) lr 1.1101e-06 eta 0:00:10
epoch [199/200] batch [35/50] time 0.086 (0.138) data 0.001 (0.050) loss 1.5391 (1.8468) acc 68.7500 (44.7321) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [40/50] time 0.084 (0.132) data 0.000 (0.044) loss 2.1348 (1.8385) acc 37.5000 (45.3125) lr 1.1101e-06 eta 0:00:07
epoch [199/200] batch [45/50] time 0.083 (0.129) data 0.000 (0.041) loss 1.7979 (1.8421) acc 50.0000 (45.9028) lr 1.1101e-06 eta 0:00:07
epoch [199/200] batch [50/50] time 0.083 (0.124) data 0.000 (0.037) loss 1.7529 (1.8534) acc 50.0000 (44.7500) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [5/50] time 0.089 (0.342) data 0.001 (0.255) loss 1.9590 (1.8963) acc 40.6250 (46.8750) lr 4.9344e-07 eta 0:00:15
epoch [200/200] batch [10/50] time 0.154 (0.221) data 0.067 (0.134) loss 1.9053 (1.8527) acc 43.7500 (45.6250) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [15/50] time 0.085 (0.176) data 0.000 (0.090) loss 2.0488 (1.8755) acc 46.8750 (44.7917) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [20/50] time 0.087 (0.162) data 0.000 (0.076) loss 2.0469 (1.8979) acc 31.2500 (44.0625) lr 4.9344e-07 eta 0:00:04
epoch [200/200] batch [25/50] time 0.087 (0.147) data 0.000 (0.061) loss 1.7275 (1.9081) acc 40.6250 (43.8750) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [30/50] time 0.087 (0.137) data 0.000 (0.051) loss 1.7383 (1.8981) acc 53.1250 (43.8542) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [35/50] time 0.085 (0.130) data 0.001 (0.043) loss 2.0156 (1.9057) acc 50.0000 (44.4643) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [40/50] time 0.084 (0.126) data 0.000 (0.040) loss 1.8584 (1.8999) acc 53.1250 (44.7656) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [45/50] time 0.261 (0.125) data 0.178 (0.039) loss 1.9434 (1.8838) acc 46.8750 (45.2083) lr 4.9344e-07 eta 0:00:00
epoch [200/200] batch [50/50] time 0.084 (0.121) data 0.000 (0.035) loss 1.9629 (1.8769) acc 31.2500 (45.6875) lr 1.2337e-07 eta 0:00:00
Checkpoint saved to output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3/prompt_learner/model.pth.tar-200
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/34 [00:00<?, ?it/s]  3%|▎         | 1/34 [00:02<01:29,  2.71s/it]  6%|▌         | 2/34 [00:03<00:58,  1.82s/it]  9%|▉         | 3/34 [00:04<00:32,  1.04s/it] 12%|█▏        | 4/34 [00:04<00:20,  1.47it/s] 15%|█▍        | 5/34 [00:04<00:13,  2.09it/s] 18%|█▊        | 6/34 [00:04<00:10,  2.80it/s] 21%|██        | 7/34 [00:04<00:07,  3.57it/s] 24%|██▎       | 8/34 [00:04<00:05,  4.35it/s] 26%|██▋       | 9/34 [00:05<00:10,  2.47it/s] 29%|██▉       | 10/34 [00:05<00:10,  2.25it/s] 32%|███▏      | 11/34 [00:07<00:16,  1.41it/s] 35%|███▌      | 12/34 [00:07<00:11,  1.89it/s] 38%|███▊      | 13/34 [00:07<00:08,  2.46it/s] 41%|████      | 14/34 [00:07<00:06,  3.13it/s] 44%|████▍     | 15/34 [00:07<00:04,  3.85it/s] 47%|████▋     | 16/34 [00:07<00:03,  4.58it/s] 50%|█████     | 17/34 [00:08<00:07,  2.22it/s] 53%|█████▎    | 18/34 [00:08<00:05,  2.84it/s] 56%|█████▌    | 19/34 [00:10<00:09,  1.51it/s] 59%|█████▉    | 20/34 [00:10<00:07,  2.00it/s] 62%|██████▏   | 21/34 [00:10<00:05,  2.59it/s] 65%|██████▍   | 22/34 [00:10<00:03,  3.25it/s] 68%|██████▊   | 23/34 [00:10<00:02,  3.98it/s] 71%|███████   | 24/34 [00:10<00:02,  4.71it/s] 74%|███████▎  | 25/34 [00:12<00:05,  1.70it/s] 76%|███████▋  | 26/34 [00:12<00:03,  2.23it/s] 79%|███████▉  | 27/34 [00:13<00:04,  1.71it/s] 82%|████████▏ | 28/34 [00:13<00:02,  2.25it/s] 85%|████████▌ | 29/34 [00:13<00:01,  2.87it/s] 88%|████████▊ | 30/34 [00:13<00:01,  3.57it/s] 91%|█████████ | 31/34 [00:13<00:00,  4.31it/s] 94%|█████████▍| 32/34 [00:14<00:00,  5.03it/s] 97%|█████████▋| 33/34 [00:14<00:00,  3.25it/s]100%|██████████| 34/34 [00:14<00:00,  2.30it/s]
=> result
* total: 3,333
* correct: 1,301
* accuracy: 39.0%
* error: 61.0%
* macro_f1: 38.3%
Elapsed: 0:20:35

for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet  
do
    for seed in 1 2 3
    do
        # evaluation
        sh scripts/coop/crossdataset_test.sh fgvc_aircraft ${dataset} ${seed} ${GPU} ${cfg} ${SHOT} ${EPOCH} ${TRAINER}
    done
done
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft eurosat 1 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/eurosat/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/eurosat/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_1.pkl
160 5400 8100
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      5,400
# test     8,100
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:03<04:26,  3.33s/it]  2%|▏         | 2/81 [00:03<01:53,  1.44s/it]  4%|▎         | 3/81 [00:03<01:04,  1.20it/s]  5%|▍         | 4/81 [00:03<00:41,  1.84it/s]  6%|▌         | 5/81 [00:03<00:29,  2.59it/s]  7%|▋         | 6/81 [00:03<00:21,  3.43it/s]  9%|▊         | 7/81 [00:03<00:17,  4.34it/s] 10%|▉         | 8/81 [00:04<00:13,  5.26it/s] 11%|█         | 9/81 [00:04<00:11,  6.09it/s] 12%|█▏        | 10/81 [00:04<00:10,  6.82it/s] 14%|█▎        | 11/81 [00:04<00:09,  7.44it/s] 15%|█▍        | 12/81 [00:04<00:08,  7.98it/s] 16%|█▌        | 13/81 [00:04<00:08,  8.36it/s] 17%|█▋        | 14/81 [00:04<00:07,  8.65it/s] 19%|█▊        | 15/81 [00:04<00:07,  8.84it/s] 20%|█▉        | 16/81 [00:04<00:07,  9.04it/s] 21%|██        | 17/81 [00:05<00:07,  9.13it/s] 22%|██▏       | 18/81 [00:05<00:06,  9.22it/s] 23%|██▎       | 19/81 [00:05<00:06,  9.28it/s] 25%|██▍       | 20/81 [00:05<00:06,  9.37it/s] 26%|██▌       | 21/81 [00:05<00:06,  9.32it/s] 27%|██▋       | 22/81 [00:05<00:06,  9.31it/s] 28%|██▊       | 23/81 [00:05<00:06,  9.36it/s] 30%|██▉       | 24/81 [00:05<00:06,  9.44it/s] 31%|███       | 25/81 [00:05<00:06,  9.28it/s] 32%|███▏      | 26/81 [00:05<00:05,  9.29it/s] 33%|███▎      | 27/81 [00:06<00:05,  9.35it/s] 35%|███▍      | 28/81 [00:06<00:05,  9.42it/s] 36%|███▌      | 29/81 [00:06<00:05,  9.41it/s] 37%|███▋      | 30/81 [00:06<00:05,  9.35it/s] 38%|███▊      | 31/81 [00:06<00:05,  9.37it/s] 40%|███▉      | 32/81 [00:06<00:05,  9.44it/s] 41%|████      | 33/81 [00:06<00:05,  9.38it/s] 42%|████▏     | 34/81 [00:06<00:05,  9.36it/s] 43%|████▎     | 35/81 [00:06<00:04,  9.31it/s] 44%|████▍     | 36/81 [00:07<00:04,  9.39it/s] 46%|████▌     | 37/81 [00:07<00:04,  9.39it/s] 47%|████▋     | 38/81 [00:07<00:04,  9.37it/s] 48%|████▊     | 39/81 [00:07<00:04,  9.41it/s] 49%|████▉     | 40/81 [00:07<00:04,  9.45it/s] 51%|█████     | 41/81 [00:07<00:04,  9.36it/s] 52%|█████▏    | 42/81 [00:07<00:04,  9.34it/s] 53%|█████▎    | 43/81 [00:07<00:04,  9.34it/s] 54%|█████▍    | 44/81 [00:07<00:03,  9.33it/s] 56%|█████▌    | 45/81 [00:08<00:03,  9.32it/s] 57%|█████▋    | 46/81 [00:08<00:03,  9.32it/s] 58%|█████▊    | 47/81 [00:08<00:03,  9.36it/s] 59%|█████▉    | 48/81 [00:08<00:03,  9.42it/s] 60%|██████    | 49/81 [00:08<00:03,  9.40it/s] 62%|██████▏   | 50/81 [00:08<00:03,  9.38it/s] 63%|██████▎   | 51/81 [00:08<00:03,  9.39it/s] 64%|██████▍   | 52/81 [00:08<00:03,  9.43it/s] 65%|██████▌   | 53/81 [00:08<00:02,  9.36it/s] 67%|██████▋   | 54/81 [00:08<00:02,  9.35it/s] 68%|██████▊   | 55/81 [00:09<00:02,  9.36it/s] 69%|██████▉   | 56/81 [00:09<00:02,  9.42it/s] 70%|███████   | 57/81 [00:09<00:02,  9.41it/s] 72%|███████▏  | 58/81 [00:09<00:02,  9.38it/s] 73%|███████▎  | 59/81 [00:09<00:02,  9.36it/s] 74%|███████▍  | 60/81 [00:09<00:02,  9.41it/s] 75%|███████▌  | 61/81 [00:09<00:02,  9.39it/s] 77%|███████▋  | 62/81 [00:09<00:02,  9.35it/s] 78%|███████▊  | 63/81 [00:09<00:01,  9.38it/s] 79%|███████▉  | 64/81 [00:10<00:01,  9.43it/s] 80%|████████  | 65/81 [00:10<00:01,  9.47it/s] 81%|████████▏ | 66/81 [00:10<00:01,  9.51it/s] 83%|████████▎ | 67/81 [00:10<00:01,  9.56it/s] 84%|████████▍ | 68/81 [00:10<00:01,  9.59it/s] 85%|████████▌ | 69/81 [00:10<00:01,  9.59it/s] 86%|████████▋ | 70/81 [00:10<00:01,  9.58it/s] 88%|████████▊ | 71/81 [00:10<00:01,  9.57it/s] 89%|████████▉ | 72/81 [00:10<00:00,  9.57it/s] 90%|█████████ | 73/81 [00:10<00:00,  9.57it/s] 91%|█████████▏| 74/81 [00:11<00:00,  9.59it/s] 93%|█████████▎| 75/81 [00:11<00:00,  9.58it/s] 94%|█████████▍| 76/81 [00:11<00:00,  9.57it/s] 95%|█████████▌| 77/81 [00:11<00:00,  9.58it/s] 96%|█████████▋| 78/81 [00:11<00:00,  9.57it/s] 98%|█████████▊| 79/81 [00:11<00:00,  9.58it/s] 99%|█████████▉| 80/81 [00:11<00:00,  9.58it/s]100%|██████████| 81/81 [00:11<00:00,  9.59it/s]100%|██████████| 81/81 [00:11<00:00,  6.81it/s]
=> result
* total: 8,100
* correct: 2,968
* accuracy: 36.6%
* error: 63.4%
* macro_f1: 28.5%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft eurosat 2 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/eurosat/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/eurosat/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_2.pkl
160 5400 8100
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      5,400
# test     8,100
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:03<04:22,  3.28s/it]  2%|▏         | 2/81 [00:03<01:51,  1.41s/it]  4%|▎         | 3/81 [00:03<01:03,  1.22it/s]  5%|▍         | 4/81 [00:03<00:41,  1.87it/s]  6%|▌         | 5/81 [00:03<00:28,  2.63it/s]  7%|▋         | 6/81 [00:03<00:21,  3.50it/s]  9%|▊         | 7/81 [00:03<00:16,  4.42it/s] 10%|▉         | 8/81 [00:04<00:13,  5.31it/s] 11%|█         | 9/81 [00:04<00:11,  6.14it/s] 12%|█▏        | 10/81 [00:04<00:10,  6.87it/s] 14%|█▎        | 11/81 [00:04<00:09,  7.52it/s] 15%|█▍        | 12/81 [00:04<00:08,  8.05it/s] 16%|█▌        | 13/81 [00:04<00:08,  8.48it/s] 17%|█▋        | 14/81 [00:04<00:07,  8.80it/s] 19%|█▊        | 15/81 [00:04<00:07,  9.01it/s] 20%|█▉        | 16/81 [00:04<00:07,  9.10it/s] 21%|██        | 17/81 [00:04<00:06,  9.15it/s] 22%|██▏       | 18/81 [00:05<00:06,  9.23it/s] 23%|██▎       | 19/81 [00:05<00:06,  9.34it/s] 25%|██▍       | 20/81 [00:05<00:06,  9.41it/s] 26%|██▌       | 21/81 [00:05<00:06,  9.48it/s] 27%|██▋       | 22/81 [00:05<00:06,  9.50it/s] 28%|██▊       | 23/81 [00:05<00:06,  9.52it/s] 30%|██▉       | 24/81 [00:05<00:06,  9.46it/s] 31%|███       | 25/81 [00:05<00:05,  9.40it/s] 32%|███▏      | 26/81 [00:05<00:05,  9.40it/s] 33%|███▎      | 27/81 [00:06<00:05,  9.46it/s] 35%|███▍      | 28/81 [00:06<00:05,  9.51it/s] 36%|███▌      | 29/81 [00:06<00:05,  9.55it/s] 37%|███▋      | 30/81 [00:06<00:05,  9.56it/s] 38%|███▊      | 31/81 [00:06<00:05,  9.58it/s] 40%|███▉      | 32/81 [00:06<00:05,  9.51it/s] 41%|████      | 33/81 [00:06<00:05,  9.44it/s] 42%|████▏     | 34/81 [00:06<00:05,  9.38it/s] 43%|████▎     | 35/81 [00:06<00:04,  9.44it/s] 44%|████▍     | 36/81 [00:06<00:04,  9.49it/s] 46%|████▌     | 37/81 [00:07<00:04,  9.53it/s] 47%|████▋     | 38/81 [00:07<00:04,  9.55it/s] 48%|████▊     | 39/81 [00:07<00:04,  9.57it/s] 49%|████▉     | 40/81 [00:07<00:04,  9.48it/s] 51%|█████     | 41/81 [00:07<00:04,  9.40it/s] 52%|█████▏    | 42/81 [00:07<00:04,  9.36it/s] 53%|█████▎    | 43/81 [00:07<00:04,  9.42it/s] 54%|█████▍    | 44/81 [00:07<00:03,  9.48it/s] 56%|█████▌    | 45/81 [00:07<00:03,  9.51it/s] 57%|█████▋    | 46/81 [00:08<00:03,  9.54it/s] 58%|█████▊    | 47/81 [00:08<00:03,  9.56it/s] 59%|█████▉    | 48/81 [00:08<00:03,  9.23it/s] 60%|██████    | 49/81 [00:08<00:03,  9.25it/s] 62%|██████▏   | 50/81 [00:08<00:03,  9.29it/s] 63%|██████▎   | 51/81 [00:08<00:03,  9.37it/s] 64%|██████▍   | 52/81 [00:08<00:03,  9.45it/s] 65%|██████▌   | 53/81 [00:08<00:02,  9.50it/s] 67%|██████▋   | 54/81 [00:08<00:02,  9.53it/s] 68%|██████▊   | 55/81 [00:08<00:02,  9.55it/s] 69%|██████▉   | 56/81 [00:09<00:02,  9.47it/s] 70%|███████   | 57/81 [00:09<00:02,  9.40it/s] 72%|███████▏  | 58/81 [00:09<00:02,  9.42it/s] 73%|███████▎  | 59/81 [00:09<00:02,  9.47it/s] 74%|███████▍  | 60/81 [00:09<00:02,  9.51it/s] 75%|███████▌  | 61/81 [00:09<00:02,  9.55it/s] 77%|███████▋  | 62/81 [00:09<00:01,  9.57it/s] 78%|███████▊  | 63/81 [00:09<00:01,  9.57it/s] 79%|███████▉  | 64/81 [00:09<00:01,  9.48it/s] 80%|████████  | 65/81 [00:10<00:01,  9.41it/s] 81%|████████▏ | 66/81 [00:10<00:01,  9.43it/s] 83%|████████▎ | 67/81 [00:10<00:01,  9.48it/s] 84%|████████▍ | 68/81 [00:10<00:01,  9.49it/s] 85%|████████▌ | 69/81 [00:10<00:01,  9.53it/s] 86%|████████▋ | 70/81 [00:10<00:01,  9.54it/s] 88%|████████▊ | 71/81 [00:10<00:01,  9.55it/s] 89%|████████▉ | 72/81 [00:10<00:00,  9.55it/s] 90%|█████████ | 73/81 [00:10<00:00,  9.54it/s] 91%|█████████▏| 74/81 [00:10<00:00,  9.55it/s] 93%|█████████▎| 75/81 [00:11<00:00,  9.58it/s] 94%|█████████▍| 76/81 [00:11<00:00,  9.58it/s] 95%|█████████▌| 77/81 [00:11<00:00,  9.57it/s] 96%|█████████▋| 78/81 [00:11<00:00,  9.55it/s] 98%|█████████▊| 79/81 [00:11<00:00,  9.57it/s] 99%|█████████▉| 80/81 [00:11<00:00,  9.58it/s]100%|██████████| 81/81 [00:11<00:00,  9.59it/s]100%|██████████| 81/81 [00:11<00:00,  6.87it/s]
=> result
* total: 8,100
* correct: 2,449
* accuracy: 30.2%
* error: 69.8%
* macro_f1: 22.2%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft eurosat 3 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/eurosat/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/eurosat/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_3.pkl
160 5400 8100
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  10
# train_x  160
# val      5,400
# test     8,100
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:03<04:27,  3.34s/it]  2%|▏         | 2/81 [00:03<01:53,  1.44s/it]  4%|▎         | 3/81 [00:03<01:04,  1.20it/s]  5%|▍         | 4/81 [00:03<00:41,  1.84it/s]  6%|▌         | 5/81 [00:03<00:29,  2.58it/s]  7%|▋         | 6/81 [00:03<00:21,  3.44it/s]  9%|▊         | 7/81 [00:03<00:17,  4.35it/s] 10%|▉         | 8/81 [00:04<00:13,  5.27it/s] 11%|█         | 9/81 [00:04<00:11,  6.09it/s] 12%|█▏        | 10/81 [00:04<00:10,  6.86it/s] 14%|█▎        | 11/81 [00:04<00:09,  7.53it/s] 15%|█▍        | 12/81 [00:04<00:08,  8.05it/s] 16%|█▌        | 13/81 [00:04<00:08,  8.47it/s] 17%|█▋        | 14/81 [00:04<00:07,  8.80it/s] 19%|█▊        | 15/81 [00:04<00:07,  9.04it/s] 20%|█▉        | 16/81 [00:04<00:07,  9.21it/s] 21%|██        | 17/81 [00:05<00:06,  9.28it/s] 22%|██▏       | 18/81 [00:05<00:06,  9.31it/s] 23%|██▎       | 19/81 [00:05<00:06,  9.42it/s] 25%|██▍       | 20/81 [00:05<00:06,  9.40it/s] 26%|██▌       | 21/81 [00:05<00:06,  9.34it/s] 27%|██▋       | 22/81 [00:05<00:06,  9.40it/s] 28%|██▊       | 23/81 [00:05<00:06,  9.46it/s] 30%|██▉       | 24/81 [00:05<00:05,  9.51it/s] 31%|███       | 25/81 [00:05<00:05,  9.55it/s] 32%|███▏      | 26/81 [00:05<00:05,  9.47it/s] 33%|███▎      | 27/81 [00:06<00:05,  9.40it/s] 35%|███▍      | 28/81 [00:06<00:05,  9.34it/s] 36%|███▌      | 29/81 [00:06<00:05,  9.30it/s] 37%|███▋      | 30/81 [00:06<00:05,  9.35it/s] 38%|███▊      | 31/81 [00:06<00:05,  9.42it/s] 40%|███▉      | 32/81 [00:06<00:05,  9.38it/s] 41%|████      | 33/81 [00:06<00:05,  9.34it/s] 42%|████▏     | 34/81 [00:06<00:05,  9.39it/s] 43%|████▎     | 35/81 [00:06<00:04,  9.45it/s] 44%|████▍     | 36/81 [00:07<00:04,  9.41it/s] 46%|████▌     | 37/81 [00:07<00:04,  9.36it/s] 47%|████▋     | 38/81 [00:07<00:04,  9.39it/s] 48%|████▊     | 39/81 [00:07<00:04,  9.45it/s] 49%|████▉     | 40/81 [00:07<00:04,  9.41it/s] 51%|█████     | 41/81 [00:07<00:04,  9.36it/s] 52%|█████▏    | 42/81 [00:07<00:04,  9.41it/s] 53%|█████▎    | 43/81 [00:07<00:04,  9.41it/s] 54%|█████▍    | 44/81 [00:07<00:03,  9.36it/s] 56%|█████▌    | 45/81 [00:08<00:03,  9.31it/s] 57%|█████▋    | 46/81 [00:08<00:03,  9.35it/s] 58%|█████▊    | 47/81 [00:08<00:03,  9.41it/s] 59%|█████▉    | 48/81 [00:08<00:03,  9.38it/s] 60%|██████    | 49/81 [00:08<00:03,  9.36it/s] 62%|██████▏   | 50/81 [00:08<00:03,  9.39it/s] 63%|██████▎   | 51/81 [00:08<00:03,  9.43it/s] 64%|██████▍   | 52/81 [00:08<00:03,  9.39it/s] 65%|██████▌   | 53/81 [00:08<00:02,  9.33it/s] 67%|██████▋   | 54/81 [00:08<00:02,  9.39it/s] 68%|██████▊   | 55/81 [00:09<00:02,  9.45it/s] 69%|██████▉   | 56/81 [00:09<00:02,  9.41it/s] 70%|███████   | 57/81 [00:09<00:02,  9.37it/s] 72%|███████▏  | 58/81 [00:09<00:02,  9.41it/s] 73%|███████▎  | 59/81 [00:09<00:02,  9.45it/s] 74%|███████▍  | 60/81 [00:09<00:02,  9.38it/s] 75%|███████▌  | 61/81 [00:09<00:02,  9.40it/s] 77%|███████▋  | 62/81 [00:09<00:02,  9.45it/s] 78%|███████▊  | 63/81 [00:09<00:01,  9.50it/s] 79%|███████▉  | 64/81 [00:10<00:01,  9.44it/s] 80%|████████  | 65/81 [00:10<00:01,  9.38it/s] 81%|████████▏ | 66/81 [00:10<00:01,  9.43it/s] 83%|████████▎ | 67/81 [00:10<00:01,  9.49it/s] 84%|████████▍ | 68/81 [00:10<00:01,  9.52it/s] 85%|████████▌ | 69/81 [00:10<00:01,  9.52it/s] 86%|████████▋ | 70/81 [00:10<00:01,  9.54it/s] 88%|████████▊ | 71/81 [00:10<00:01,  9.55it/s] 89%|████████▉ | 72/81 [00:10<00:00,  9.57it/s] 90%|█████████ | 73/81 [00:10<00:00,  9.59it/s] 91%|█████████▏| 74/81 [00:11<00:00,  9.59it/s] 93%|█████████▎| 75/81 [00:11<00:00,  9.57it/s] 94%|█████████▍| 76/81 [00:11<00:00,  9.57it/s] 95%|█████████▌| 77/81 [00:11<00:00,  9.57it/s] 96%|█████████▋| 78/81 [00:11<00:00,  9.57it/s] 98%|█████████▊| 79/81 [00:11<00:00,  9.56it/s] 99%|█████████▉| 80/81 [00:11<00:00,  9.56it/s]100%|██████████| 81/81 [00:11<00:00,  9.56it/s]100%|██████████| 81/81 [00:11<00:00,  6.81it/s]
=> result
* total: 8,100
* correct: 2,702
* accuracy: 33.4%
* error: 66.6%
* macro_f1: 24.0%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft dtd 1 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/dtd/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/dtd/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_1.pkl
752 1128 1692
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      1,128
# test     1,692
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:04<01:04,  4.05s/it] 12%|█▏        | 2/17 [00:04<00:26,  1.73s/it] 18%|█▊        | 3/17 [00:04<00:13,  1.01it/s] 24%|██▎       | 4/17 [00:04<00:08,  1.55it/s] 29%|██▉       | 5/17 [00:04<00:05,  2.22it/s] 35%|███▌      | 6/17 [00:04<00:03,  2.99it/s] 41%|████      | 7/17 [00:04<00:02,  3.83it/s] 47%|████▋     | 8/17 [00:04<00:01,  4.71it/s] 53%|█████▎    | 9/17 [00:04<00:01,  5.55it/s] 59%|█████▉    | 10/17 [00:05<00:01,  6.32it/s] 65%|██████▍   | 11/17 [00:05<00:00,  6.98it/s] 71%|███████   | 12/17 [00:05<00:00,  7.52it/s] 76%|███████▋  | 13/17 [00:05<00:00,  7.95it/s] 82%|████████▏ | 14/17 [00:05<00:00,  8.29it/s] 88%|████████▊ | 15/17 [00:05<00:00,  8.53it/s] 94%|█████████▍| 16/17 [00:05<00:00,  8.71it/s]100%|██████████| 17/17 [00:05<00:00,  9.01it/s]100%|██████████| 17/17 [00:05<00:00,  2.89it/s]
=> result
* total: 1,692
* correct: 327
* accuracy: 19.3%
* error: 80.7%
* macro_f1: 14.9%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft dtd 2 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/dtd/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/dtd/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_2.pkl
752 1128 1692
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      1,128
# test     1,692
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:04<01:04,  4.03s/it] 12%|█▏        | 2/17 [00:04<00:25,  1.73s/it] 18%|█▊        | 3/17 [00:04<00:13,  1.01it/s] 24%|██▎       | 4/17 [00:04<00:08,  1.56it/s] 29%|██▉       | 5/17 [00:04<00:05,  2.23it/s] 35%|███▌      | 6/17 [00:04<00:03,  3.00it/s] 41%|████      | 7/17 [00:04<00:02,  3.84it/s] 47%|████▋     | 8/17 [00:04<00:01,  4.72it/s] 53%|█████▎    | 9/17 [00:04<00:01,  5.54it/s] 59%|█████▉    | 10/17 [00:05<00:01,  6.31it/s] 65%|██████▍   | 11/17 [00:05<00:00,  6.98it/s] 71%|███████   | 12/17 [00:05<00:00,  7.52it/s] 76%|███████▋  | 13/17 [00:05<00:00,  7.96it/s] 82%|████████▏ | 14/17 [00:05<00:00,  8.29it/s] 88%|████████▊ | 15/17 [00:05<00:00,  8.54it/s] 94%|█████████▍| 16/17 [00:05<00:00,  8.71it/s]100%|██████████| 17/17 [00:05<00:00,  9.02it/s]100%|██████████| 17/17 [00:05<00:00,  2.91it/s]
=> result
* total: 1,692
* correct: 610
* accuracy: 36.1%
* error: 63.9%
* macro_f1: 30.7%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft dtd 3 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/dtd/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/dtd/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_3.pkl
752 1128 1692
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  47
# train_x  752
# val      1,128
# test     1,692
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:03<01:01,  3.87s/it] 12%|█▏        | 2/17 [00:03<00:24,  1.66s/it] 18%|█▊        | 3/17 [00:04<00:13,  1.05it/s] 24%|██▎       | 4/17 [00:04<00:08,  1.61it/s] 29%|██▉       | 5/17 [00:04<00:05,  2.30it/s] 35%|███▌      | 6/17 [00:04<00:03,  3.08it/s] 41%|████      | 7/17 [00:04<00:02,  3.94it/s] 47%|████▋     | 8/17 [00:04<00:01,  4.82it/s] 53%|█████▎    | 9/17 [00:04<00:01,  5.66it/s] 59%|█████▉    | 10/17 [00:04<00:01,  6.43it/s] 65%|██████▍   | 11/17 [00:04<00:00,  7.09it/s] 71%|███████   | 12/17 [00:05<00:00,  7.62it/s] 76%|███████▋  | 13/17 [00:05<00:00,  7.87it/s] 82%|████████▏ | 14/17 [00:05<00:00,  8.24it/s] 88%|████████▊ | 15/17 [00:05<00:00,  8.50it/s] 94%|█████████▍| 16/17 [00:05<00:00,  8.70it/s]100%|██████████| 17/17 [00:05<00:00,  9.02it/s]100%|██████████| 17/17 [00:05<00:00,  2.99it/s]
=> result
* total: 1,692
* correct: 366
* accuracy: 21.6%
* error: 78.4%
* macro_f1: 17.5%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft fgvc_aircraft 1 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/fgvc_aircraft/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/fgvc_aircraft/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_1.pkl
1600 3333 3333
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  100
# train_x  1,600
# val      3,333
# test     3,333
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/34 [00:00<?, ?it/s]  3%|▎         | 1/34 [00:05<02:52,  5.23s/it]  6%|▌         | 2/34 [00:05<01:11,  2.23s/it]  9%|▉         | 3/34 [00:05<00:39,  1.27s/it] 12%|█▏        | 4/34 [00:05<00:24,  1.23it/s] 15%|█▍        | 5/34 [00:05<00:16,  1.78it/s] 18%|█▊        | 6/34 [00:05<00:11,  2.42it/s] 21%|██        | 7/34 [00:05<00:08,  3.16it/s] 24%|██▎       | 8/34 [00:06<00:06,  3.93it/s] 26%|██▋       | 9/34 [00:06<00:05,  4.71it/s] 29%|██▉       | 10/34 [00:06<00:04,  5.41it/s] 32%|███▏      | 11/34 [00:07<00:08,  2.78it/s] 35%|███▌      | 12/34 [00:07<00:06,  3.49it/s] 38%|███▊      | 13/34 [00:07<00:04,  4.23it/s] 41%|████      | 14/34 [00:07<00:04,  4.97it/s] 44%|████▍     | 15/34 [00:07<00:03,  5.65it/s] 47%|████▋     | 16/34 [00:07<00:02,  6.25it/s] 50%|█████     | 17/34 [00:08<00:05,  3.31it/s] 53%|█████▎    | 18/34 [00:08<00:03,  4.04it/s] 56%|█████▌    | 19/34 [00:10<00:10,  1.43it/s] 59%|█████▉    | 20/34 [00:10<00:07,  1.91it/s] 62%|██████▏   | 21/34 [00:10<00:05,  2.48it/s] 65%|██████▍   | 22/34 [00:10<00:03,  3.14it/s] 68%|██████▊   | 23/34 [00:10<00:02,  3.86it/s] 71%|███████   | 24/34 [00:10<00:02,  4.60it/s] 74%|███████▎  | 25/34 [00:12<00:04,  1.86it/s] 76%|███████▋  | 26/34 [00:12<00:03,  2.42it/s] 79%|███████▉  | 27/34 [00:13<00:03,  1.80it/s] 82%|████████▏ | 28/34 [00:13<00:02,  2.36it/s] 85%|████████▌ | 29/34 [00:13<00:01,  3.01it/s] 88%|████████▊ | 30/34 [00:13<00:01,  3.72it/s] 91%|█████████ | 31/34 [00:13<00:00,  3.97it/s] 94%|█████████▍| 32/34 [00:13<00:00,  4.71it/s] 97%|█████████▋| 33/34 [00:14<00:00,  2.26it/s]100%|██████████| 34/34 [00:14<00:00,  2.29it/s]
=> result
* total: 3,333
* correct: 1,336
* accuracy: 40.1%
* error: 59.9%
* macro_f1: 39.2%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft fgvc_aircraft 2 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/fgvc_aircraft/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/fgvc_aircraft/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_2.pkl
1600 3333 3333
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  100
# train_x  1,600
# val      3,333
# test     3,333
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/34 [00:00<?, ?it/s]  3%|▎         | 1/34 [00:05<02:59,  5.44s/it]  6%|▌         | 2/34 [00:05<01:14,  2.31s/it]  9%|▉         | 3/34 [00:05<00:40,  1.31s/it] 12%|█▏        | 4/34 [00:05<00:25,  1.19it/s] 15%|█▍        | 5/34 [00:05<00:16,  1.72it/s] 18%|█▊        | 6/34 [00:06<00:11,  2.35it/s] 21%|██        | 7/34 [00:06<00:08,  3.07it/s] 24%|██▎       | 8/34 [00:06<00:06,  3.85it/s] 26%|██▋       | 9/34 [00:06<00:05,  4.62it/s] 29%|██▉       | 10/34 [00:07<00:09,  2.46it/s] 32%|███▏      | 11/34 [00:07<00:07,  3.14it/s] 35%|███▌      | 12/34 [00:07<00:05,  3.87it/s] 38%|███▊      | 13/34 [00:07<00:04,  4.62it/s] 41%|████      | 14/34 [00:07<00:03,  5.34it/s] 44%|████▍     | 15/34 [00:07<00:03,  5.99it/s] 47%|████▋     | 16/34 [00:07<00:02,  6.54it/s] 50%|█████     | 17/34 [00:08<00:05,  3.08it/s] 53%|█████▎    | 18/34 [00:10<00:11,  1.41it/s] 56%|█████▌    | 19/34 [00:10<00:07,  1.88it/s] 59%|█████▉    | 20/34 [00:10<00:05,  2.45it/s] 62%|██████▏   | 21/34 [00:10<00:04,  3.11it/s] 65%|██████▍   | 22/34 [00:10<00:03,  3.80it/s] 68%|██████▊   | 23/34 [00:10<00:02,  4.55it/s] 71%|███████   | 24/34 [00:11<00:01,  5.24it/s] 74%|███████▎  | 25/34 [00:11<00:01,  5.88it/s] 76%|███████▋  | 26/34 [00:13<00:06,  1.17it/s] 79%|███████▉  | 27/34 [00:13<00:04,  1.57it/s] 82%|████████▏ | 28/34 [00:13<00:02,  2.08it/s] 85%|████████▌ | 29/34 [00:13<00:01,  2.69it/s] 88%|████████▊ | 30/34 [00:14<00:01,  3.38it/s] 91%|█████████ | 31/34 [00:14<00:00,  4.11it/s] 94%|█████████▍| 32/34 [00:14<00:00,  4.86it/s] 97%|█████████▋| 33/34 [00:14<00:00,  5.56it/s]100%|██████████| 34/34 [00:14<00:00,  2.33it/s]
=> result
* total: 3,333
* correct: 1,366
* accuracy: 41.0%
* error: 59.0%
* macro_f1: 39.7%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft fgvc_aircraft 3 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/fgvc_aircraft/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/fgvc_aircraft/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_3.pkl
1600 3333 3333
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  100
# train_x  1,600
# val      3,333
# test     3,333
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/34 [00:00<?, ?it/s]  3%|▎         | 1/34 [00:04<02:44,  4.98s/it]  6%|▌         | 2/34 [00:05<01:07,  2.12s/it]  9%|▉         | 3/34 [00:05<00:37,  1.22s/it] 12%|█▏        | 4/34 [00:05<00:23,  1.27it/s] 15%|█▍        | 5/34 [00:05<00:15,  1.83it/s] 18%|█▊        | 6/34 [00:05<00:11,  2.49it/s] 21%|██        | 7/34 [00:05<00:08,  3.24it/s] 24%|██▎       | 8/34 [00:05<00:06,  4.02it/s] 26%|██▋       | 9/34 [00:05<00:05,  4.80it/s] 29%|██▉       | 10/34 [00:06<00:04,  5.52it/s] 32%|███▏      | 11/34 [00:07<00:12,  1.91it/s] 35%|███▌      | 12/34 [00:07<00:08,  2.49it/s] 38%|███▊      | 13/34 [00:07<00:06,  3.17it/s] 41%|████      | 14/34 [00:07<00:05,  3.90it/s] 44%|████▍     | 15/34 [00:07<00:04,  4.64it/s] 47%|████▋     | 16/34 [00:07<00:03,  5.35it/s] 50%|█████     | 17/34 [00:08<00:02,  5.97it/s] 53%|█████▎    | 18/34 [00:08<00:02,  6.53it/s] 56%|█████▌    | 19/34 [00:10<00:11,  1.25it/s] 59%|█████▉    | 20/34 [00:10<00:08,  1.68it/s] 62%|██████▏   | 21/34 [00:10<00:05,  2.20it/s] 65%|██████▍   | 22/34 [00:10<00:04,  2.83it/s] 68%|██████▊   | 23/34 [00:11<00:03,  3.53it/s] 71%|███████   | 24/34 [00:11<00:02,  4.26it/s] 74%|███████▎  | 25/34 [00:11<00:01,  5.00it/s] 76%|███████▋  | 26/34 [00:11<00:01,  5.68it/s] 79%|███████▉  | 27/34 [00:12<00:03,  1.93it/s] 82%|████████▏ | 28/34 [00:12<00:02,  2.50it/s] 85%|████████▌ | 29/34 [00:12<00:01,  3.17it/s] 88%|████████▊ | 30/34 [00:13<00:01,  3.89it/s] 91%|█████████ | 31/34 [00:13<00:00,  4.64it/s] 94%|█████████▍| 32/34 [00:13<00:00,  5.36it/s] 97%|█████████▋| 33/34 [00:13<00:00,  6.01it/s]100%|██████████| 34/34 [00:13<00:00,  2.51it/s]
=> result
* total: 3,333
* correct: 1,301
* accuracy: 39.0%
* error: 61.0%
* macro_f1: 38.3%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft oxford_flowers 1 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/oxford_flowers/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/oxford_flowers/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_1.pkl
1632 1633 2463
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      1,633
# test     2,463
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:04<01:48,  4.54s/it]  8%|▊         | 2/25 [00:04<00:44,  1.94s/it] 12%|█▏        | 3/25 [00:04<00:24,  1.11s/it] 16%|█▌        | 4/25 [00:04<00:15,  1.39it/s] 20%|██        | 5/25 [00:05<00:10,  1.99it/s] 24%|██▍       | 6/25 [00:05<00:07,  2.68it/s] 28%|██▊       | 7/25 [00:05<00:05,  3.44it/s] 32%|███▏      | 8/25 [00:05<00:04,  4.23it/s] 36%|███▌      | 9/25 [00:05<00:03,  4.99it/s] 40%|████      | 10/25 [00:05<00:02,  5.68it/s] 44%|████▍     | 11/25 [00:05<00:02,  6.28it/s] 48%|████▊     | 12/25 [00:05<00:01,  6.77it/s] 52%|█████▏    | 13/25 [00:05<00:01,  7.16it/s] 56%|█████▌    | 14/25 [00:06<00:01,  7.47it/s] 60%|██████    | 15/25 [00:06<00:01,  7.70it/s] 64%|██████▍   | 16/25 [00:06<00:01,  7.88it/s] 68%|██████▊   | 17/25 [00:06<00:00,  8.00it/s] 72%|███████▏  | 18/25 [00:06<00:00,  8.10it/s] 76%|███████▌  | 19/25 [00:06<00:00,  8.17it/s] 80%|████████  | 20/25 [00:06<00:00,  8.22it/s] 84%|████████▍ | 21/25 [00:06<00:00,  8.26it/s] 88%|████████▊ | 22/25 [00:07<00:00,  8.28it/s] 92%|█████████▏| 23/25 [00:07<00:00,  8.29it/s] 96%|█████████▌| 24/25 [00:07<00:00,  8.31it/s]100%|██████████| 25/25 [00:07<00:00,  3.34it/s]
=> result
* total: 2,463
* correct: 1,054
* accuracy: 42.8%
* error: 57.2%
* macro_f1: 34.4%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft oxford_flowers 2 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/oxford_flowers/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/oxford_flowers/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_2.pkl
1632 1633 2463
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      1,633
# test     2,463
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:04<01:43,  4.30s/it]  8%|▊         | 2/25 [00:04<00:42,  1.84s/it] 12%|█▏        | 3/25 [00:04<00:23,  1.06s/it] 16%|█▌        | 4/25 [00:04<00:14,  1.45it/s] 20%|██        | 5/25 [00:04<00:09,  2.07it/s] 24%|██▍       | 6/25 [00:04<00:06,  2.78it/s] 28%|██▊       | 7/25 [00:05<00:05,  3.55it/s] 32%|███▏      | 8/25 [00:05<00:03,  4.35it/s] 36%|███▌      | 9/25 [00:05<00:03,  5.11it/s] 40%|████      | 10/25 [00:05<00:02,  5.78it/s] 44%|████▍     | 11/25 [00:05<00:02,  6.38it/s] 48%|████▊     | 12/25 [00:05<00:01,  6.87it/s] 52%|█████▏    | 13/25 [00:05<00:01,  7.25it/s] 56%|█████▌    | 14/25 [00:05<00:01,  7.55it/s] 60%|██████    | 15/25 [00:05<00:01,  7.77it/s] 64%|██████▍   | 16/25 [00:06<00:01,  7.93it/s] 68%|██████▊   | 17/25 [00:06<00:00,  8.06it/s] 72%|███████▏  | 18/25 [00:06<00:00,  8.14it/s] 76%|███████▌  | 19/25 [00:06<00:00,  8.21it/s] 80%|████████  | 20/25 [00:06<00:00,  8.25it/s] 84%|████████▍ | 21/25 [00:06<00:00,  8.28it/s] 88%|████████▊ | 22/25 [00:06<00:00,  8.31it/s] 92%|█████████▏| 23/25 [00:06<00:00,  8.33it/s] 96%|█████████▌| 24/25 [00:07<00:00,  8.33it/s]100%|██████████| 25/25 [00:07<00:00,  3.46it/s]
=> result
* total: 2,463
* correct: 1,275
* accuracy: 51.8%
* error: 48.2%
* macro_f1: 43.0%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft oxford_flowers 3 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/oxford_flowers/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/oxford_flowers/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_3.pkl
1632 1633 2463
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  102
# train_x  1,632
# val      1,633
# test     2,463
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:04<01:38,  4.10s/it]  8%|▊         | 2/25 [00:04<00:40,  1.76s/it] 12%|█▏        | 3/25 [00:04<00:22,  1.01s/it] 16%|█▌        | 4/25 [00:04<00:13,  1.52it/s] 20%|██        | 5/25 [00:04<00:09,  2.15it/s] 24%|██▍       | 6/25 [00:04<00:06,  2.88it/s] 28%|██▊       | 7/25 [00:04<00:04,  3.67it/s] 32%|███▏      | 8/25 [00:04<00:03,  4.46it/s] 36%|███▌      | 9/25 [00:05<00:03,  5.23it/s] 40%|████      | 10/25 [00:05<00:02,  5.92it/s] 44%|████▍     | 11/25 [00:05<00:02,  6.51it/s] 48%|████▊     | 12/25 [00:05<00:01,  6.97it/s] 52%|█████▏    | 13/25 [00:05<00:01,  7.33it/s] 56%|█████▌    | 14/25 [00:05<00:01,  7.61it/s] 60%|██████    | 15/25 [00:05<00:01,  7.81it/s] 64%|██████▍   | 16/25 [00:05<00:01,  7.97it/s] 68%|██████▊   | 17/25 [00:06<00:00,  8.08it/s] 72%|███████▏  | 18/25 [00:06<00:00,  8.16it/s] 76%|███████▌  | 19/25 [00:06<00:00,  8.17it/s] 80%|████████  | 20/25 [00:06<00:00,  8.21it/s] 84%|████████▍ | 21/25 [00:06<00:00,  8.25it/s] 88%|████████▊ | 22/25 [00:06<00:00,  8.28it/s] 92%|█████████▏| 23/25 [00:06<00:00,  8.29it/s] 96%|█████████▌| 24/25 [00:06<00:00,  8.31it/s]100%|██████████| 25/25 [00:07<00:00,  3.55it/s]
=> result
* total: 2,463
* correct: 1,038
* accuracy: 42.1%
* error: 57.9%
* macro_f1: 34.8%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft stanford_cars 1 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/stanford_cars/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/stanford_cars/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:StanfordCars
Loading dataset: StanfordCars
Reading split from /shared/s2/lab01/dataset/clip/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/stanford_cars/split_fewshot_taesup/shot_16-seed_1.pkl
3136 1635 8041
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      1,635
# test     8,041
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:04<06:13,  4.67s/it]  2%|▏         | 2/81 [00:04<02:38,  2.01s/it]  4%|▎         | 3/81 [00:04<01:29,  1.15s/it]  5%|▍         | 4/81 [00:05<00:57,  1.33it/s]  6%|▌         | 5/81 [00:05<00:40,  1.88it/s]  7%|▋         | 6/81 [00:05<00:29,  2.52it/s]  9%|▊         | 7/81 [00:05<00:23,  3.20it/s] 10%|▉         | 8/81 [00:05<00:18,  3.89it/s] 11%|█         | 9/81 [00:05<00:15,  4.55it/s] 12%|█▏        | 10/81 [00:05<00:13,  5.14it/s] 14%|█▎        | 11/81 [00:06<00:12,  5.62it/s] 15%|█▍        | 12/81 [00:06<00:11,  6.02it/s] 16%|█▌        | 13/81 [00:06<00:10,  6.34it/s] 17%|█▋        | 14/81 [00:06<00:10,  6.59it/s] 19%|█▊        | 15/81 [00:06<00:09,  6.77it/s] 20%|█▉        | 16/81 [00:06<00:09,  6.90it/s] 21%|██        | 17/81 [00:06<00:09,  6.99it/s] 22%|██▏       | 18/81 [00:07<00:08,  7.06it/s] 23%|██▎       | 19/81 [00:07<00:08,  7.11it/s] 25%|██▍       | 20/81 [00:07<00:08,  7.13it/s] 26%|██▌       | 21/81 [00:07<00:08,  7.16it/s] 27%|██▋       | 22/81 [00:07<00:08,  7.17it/s] 28%|██▊       | 23/81 [00:07<00:08,  7.18it/s] 30%|██▉       | 24/81 [00:07<00:07,  7.19it/s] 31%|███       | 25/81 [00:08<00:11,  4.74it/s] 32%|███▏      | 26/81 [00:08<00:12,  4.47it/s] 33%|███▎      | 27/81 [00:08<00:15,  3.50it/s] 35%|███▍      | 28/81 [00:09<00:12,  4.14it/s] 36%|███▌      | 29/81 [00:09<00:10,  4.74it/s] 37%|███▋      | 30/81 [00:09<00:13,  3.87it/s] 38%|███▊      | 31/81 [00:09<00:11,  4.45it/s] 40%|███▉      | 32/81 [00:09<00:10,  4.76it/s] 41%|████      | 33/81 [00:11<00:26,  1.79it/s] 42%|████▏     | 34/81 [00:11<00:20,  2.31it/s] 43%|████▎     | 35/81 [00:11<00:15,  2.90it/s] 44%|████▍     | 36/81 [00:11<00:12,  3.53it/s] 46%|████▌     | 37/81 [00:11<00:10,  4.17it/s] 47%|████▋     | 38/81 [00:11<00:09,  4.78it/s] 48%|████▊     | 39/81 [00:12<00:07,  5.31it/s] 49%|████▉     | 40/81 [00:12<00:07,  5.76it/s] 51%|█████     | 41/81 [00:13<00:19,  2.05it/s] 52%|█████▏    | 42/81 [00:13<00:14,  2.61it/s] 53%|█████▎    | 43/81 [00:13<00:11,  3.23it/s] 54%|█████▍    | 44/81 [00:13<00:09,  3.88it/s] 56%|█████▌    | 45/81 [00:14<00:07,  4.50it/s] 57%|█████▋    | 46/81 [00:14<00:06,  5.07it/s] 58%|█████▊    | 47/81 [00:14<00:06,  5.56it/s] 59%|█████▉    | 48/81 [00:14<00:07,  4.14it/s] 60%|██████    | 49/81 [00:15<00:12,  2.65it/s] 62%|██████▏   | 50/81 [00:15<00:09,  3.26it/s] 63%|██████▎   | 51/81 [00:15<00:07,  3.91it/s] 64%|██████▍   | 52/81 [00:15<00:06,  4.53it/s] 65%|██████▌   | 53/81 [00:15<00:05,  5.10it/s] 67%|██████▋   | 54/81 [00:16<00:04,  5.59it/s] 68%|██████▊   | 55/81 [00:16<00:04,  5.99it/s] 69%|██████▉   | 56/81 [00:16<00:04,  6.21it/s] 70%|███████   | 57/81 [00:17<00:08,  2.77it/s] 72%|███████▏  | 58/81 [00:17<00:06,  3.40it/s] 73%|███████▎  | 59/81 [00:17<00:05,  4.04it/s] 74%|███████▍  | 60/81 [00:17<00:04,  4.65it/s] 75%|███████▌  | 61/81 [00:17<00:03,  5.20it/s] 77%|███████▋  | 62/81 [00:17<00:03,  5.67it/s] 78%|███████▊  | 63/81 [00:18<00:02,  6.06it/s] 79%|███████▉  | 64/81 [00:18<00:02,  6.35it/s] 80%|████████  | 65/81 [00:19<00:08,  1.90it/s] 81%|████████▏ | 66/81 [00:19<00:06,  2.44it/s] 83%|████████▎ | 67/81 [00:19<00:04,  3.04it/s] 84%|████████▍ | 68/81 [00:19<00:03,  3.68it/s] 85%|████████▌ | 69/81 [00:20<00:02,  4.32it/s] 86%|████████▋ | 70/81 [00:20<00:02,  4.91it/s] 88%|████████▊ | 71/81 [00:20<00:01,  5.43it/s] 89%|████████▉ | 72/81 [00:20<00:01,  5.87it/s] 90%|█████████ | 73/81 [00:21<00:02,  3.71it/s] 91%|█████████▏| 74/81 [00:21<00:01,  4.34it/s] 93%|█████████▎| 75/81 [00:21<00:01,  4.93it/s] 94%|█████████▍| 76/81 [00:21<00:00,  5.44it/s] 95%|█████████▌| 77/81 [00:21<00:00,  5.88it/s] 96%|█████████▋| 78/81 [00:21<00:00,  6.22it/s] 98%|█████████▊| 79/81 [00:21<00:00,  6.49it/s] 99%|█████████▉| 80/81 [00:21<00:00,  6.69it/s]100%|██████████| 81/81 [00:22<00:00,  3.66it/s]
=> result
* total: 8,041
* correct: 4,605
* accuracy: 57.3%
* error: 42.7%
* macro_f1: 55.9%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft stanford_cars 2 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/stanford_cars/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/stanford_cars/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:StanfordCars
Loading dataset: StanfordCars
Reading split from /shared/s2/lab01/dataset/clip/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/stanford_cars/split_fewshot_taesup/shot_16-seed_2.pkl
3136 1635 8041
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      1,635
# test     8,041
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:04<06:07,  4.60s/it]  2%|▏         | 2/81 [00:04<02:36,  1.98s/it]  4%|▎         | 3/81 [00:04<01:28,  1.14s/it]  5%|▍         | 4/81 [00:05<00:57,  1.35it/s]  6%|▌         | 5/81 [00:05<00:39,  1.91it/s]  7%|▋         | 6/81 [00:05<00:29,  2.54it/s]  9%|▊         | 7/81 [00:05<00:22,  3.23it/s] 10%|▉         | 8/81 [00:05<00:18,  3.92it/s] 11%|█         | 9/81 [00:05<00:15,  4.57it/s] 12%|█▏        | 10/81 [00:05<00:13,  5.15it/s] 14%|█▎        | 11/81 [00:05<00:12,  5.62it/s] 15%|█▍        | 12/81 [00:06<00:11,  6.01it/s] 16%|█▌        | 13/81 [00:06<00:10,  6.31it/s] 17%|█▋        | 14/81 [00:06<00:10,  6.54it/s] 19%|█▊        | 15/81 [00:06<00:09,  6.71it/s] 20%|█▉        | 16/81 [00:06<00:09,  6.85it/s] 21%|██        | 17/81 [00:06<00:09,  6.95it/s] 22%|██▏       | 18/81 [00:06<00:08,  7.02it/s] 23%|██▎       | 19/81 [00:07<00:08,  7.05it/s] 25%|██▍       | 20/81 [00:07<00:08,  7.10it/s] 26%|██▌       | 21/81 [00:07<00:08,  7.10it/s] 27%|██▋       | 22/81 [00:07<00:08,  7.11it/s] 28%|██▊       | 23/81 [00:07<00:08,  7.12it/s] 30%|██▉       | 24/81 [00:07<00:08,  7.12it/s] 31%|███       | 25/81 [00:07<00:07,  7.14it/s] 32%|███▏      | 26/81 [00:08<00:15,  3.58it/s] 33%|███▎      | 27/81 [00:08<00:12,  4.21it/s] 35%|███▍      | 28/81 [00:08<00:11,  4.81it/s] 36%|███▌      | 29/81 [00:08<00:09,  5.34it/s] 37%|███▋      | 30/81 [00:09<00:08,  5.79it/s] 38%|███▊      | 31/81 [00:09<00:08,  6.15it/s] 40%|███▉      | 32/81 [00:09<00:07,  6.42it/s] 41%|████      | 33/81 [00:09<00:07,  6.63it/s] 42%|████▏     | 34/81 [00:09<00:10,  4.50it/s] 43%|████▎     | 35/81 [00:10<00:09,  5.07it/s] 44%|████▍     | 36/81 [00:10<00:08,  5.56it/s] 46%|████▌     | 37/81 [00:10<00:07,  5.97it/s] 47%|████▋     | 38/81 [00:10<00:06,  6.29it/s] 48%|████▊     | 39/81 [00:10<00:06,  6.52it/s] 49%|████▉     | 40/81 [00:10<00:08,  4.83it/s] 51%|█████     | 41/81 [00:11<00:07,  5.35it/s] 52%|█████▏    | 42/81 [00:11<00:10,  3.69it/s] 53%|█████▎    | 43/81 [00:11<00:08,  4.33it/s] 54%|█████▍    | 44/81 [00:11<00:07,  4.90it/s] 56%|█████▌    | 45/81 [00:11<00:06,  5.41it/s] 57%|█████▋    | 46/81 [00:12<00:06,  5.83it/s] 58%|█████▊    | 47/81 [00:12<00:05,  6.18it/s] 59%|█████▉    | 48/81 [00:13<00:19,  1.72it/s] 60%|██████    | 49/81 [00:13<00:14,  2.23it/s] 62%|██████▏   | 50/81 [00:14<00:11,  2.82it/s] 63%|██████▎   | 51/81 [00:14<00:08,  3.45it/s] 64%|██████▍   | 52/81 [00:14<00:07,  4.09it/s] 65%|██████▌   | 53/81 [00:14<00:05,  4.69it/s] 67%|██████▋   | 54/81 [00:14<00:05,  5.23it/s] 68%|██████▊   | 55/81 [00:14<00:04,  5.68it/s] 69%|██████▉   | 56/81 [00:15<00:09,  2.69it/s] 70%|███████   | 57/81 [00:15<00:07,  3.31it/s] 72%|███████▏  | 58/81 [00:15<00:05,  3.95it/s] 73%|███████▎  | 59/81 [00:16<00:04,  4.57it/s] 74%|███████▍  | 60/81 [00:16<00:04,  5.12it/s] 75%|███████▌  | 61/81 [00:16<00:03,  5.60it/s] 77%|███████▋  | 62/81 [00:16<00:03,  5.99it/s] 78%|███████▊  | 63/81 [00:16<00:02,  6.30it/s] 79%|███████▉  | 64/81 [00:17<00:06,  2.46it/s] 80%|████████  | 65/81 [00:17<00:05,  3.07it/s] 81%|████████▏ | 66/81 [00:17<00:04,  3.71it/s] 83%|████████▎ | 67/81 [00:17<00:03,  4.34it/s] 84%|████████▍ | 68/81 [00:18<00:02,  4.93it/s] 85%|████████▌ | 69/81 [00:18<00:02,  5.45it/s] 86%|████████▋ | 70/81 [00:18<00:01,  5.89it/s] 88%|████████▊ | 71/81 [00:18<00:01,  6.24it/s] 89%|████████▉ | 72/81 [00:19<00:02,  3.11it/s] 90%|█████████ | 73/81 [00:19<00:02,  3.75it/s] 91%|█████████▏| 74/81 [00:19<00:01,  4.38it/s] 93%|█████████▎| 75/81 [00:19<00:01,  4.97it/s] 94%|█████████▍| 76/81 [00:19<00:00,  5.49it/s] 95%|█████████▌| 77/81 [00:19<00:00,  5.92it/s] 96%|█████████▋| 78/81 [00:20<00:00,  6.26it/s] 98%|█████████▊| 79/81 [00:20<00:00,  6.52it/s] 99%|█████████▉| 80/81 [00:20<00:00,  6.71it/s]100%|██████████| 81/81 [00:20<00:00,  3.95it/s]
=> result
* total: 8,041
* correct: 4,732
* accuracy: 58.8%
* error: 41.2%
* macro_f1: 57.3%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft stanford_cars 3 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/stanford_cars/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/stanford_cars/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:StanfordCars
Loading dataset: StanfordCars
Reading split from /shared/s2/lab01/dataset/clip/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/stanford_cars/split_fewshot_taesup/shot_16-seed_3.pkl
3136 1635 8041
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      1,635
# test     8,041
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/81 [00:00<?, ?it/s]  1%|          | 1/81 [00:04<06:19,  4.75s/it]  2%|▏         | 2/81 [00:04<02:40,  2.04s/it]  4%|▎         | 3/81 [00:05<01:31,  1.17s/it]  5%|▍         | 4/81 [00:05<00:58,  1.31it/s]  6%|▌         | 5/81 [00:05<00:40,  1.86it/s]  7%|▋         | 6/81 [00:05<00:30,  2.49it/s]  9%|▊         | 7/81 [00:05<00:23,  3.16it/s] 10%|▉         | 8/81 [00:05<00:18,  3.85it/s] 11%|█         | 9/81 [00:05<00:15,  4.50it/s] 12%|█▏        | 10/81 [00:05<00:13,  5.08it/s] 14%|█▎        | 11/81 [00:06<00:12,  5.58it/s] 15%|█▍        | 12/81 [00:06<00:11,  5.97it/s] 16%|█▌        | 13/81 [00:06<00:10,  6.28it/s] 17%|█▋        | 14/81 [00:06<00:10,  6.51it/s] 19%|█▊        | 15/81 [00:06<00:10,  6.58it/s] 20%|█▉        | 16/81 [00:06<00:09,  6.76it/s] 21%|██        | 17/81 [00:06<00:09,  6.86it/s] 22%|██▏       | 18/81 [00:07<00:09,  6.94it/s] 23%|██▎       | 19/81 [00:07<00:08,  7.00it/s] 25%|██▍       | 20/81 [00:07<00:08,  7.03it/s] 26%|██▌       | 21/81 [00:07<00:08,  7.06it/s] 27%|██▋       | 22/81 [00:07<00:08,  7.07it/s] 28%|██▊       | 23/81 [00:07<00:08,  7.10it/s] 30%|██▉       | 24/81 [00:07<00:08,  7.12it/s] 31%|███       | 25/81 [00:08<00:07,  7.15it/s] 32%|███▏      | 26/81 [00:08<00:07,  7.17it/s] 33%|███▎      | 27/81 [00:08<00:10,  5.35it/s] 35%|███▍      | 28/81 [00:08<00:09,  5.79it/s] 36%|███▌      | 29/81 [00:08<00:08,  6.14it/s] 37%|███▋      | 30/81 [00:08<00:07,  6.41it/s] 38%|███▊      | 31/81 [00:09<00:07,  6.62it/s] 40%|███▉      | 32/81 [00:09<00:07,  6.78it/s] 41%|████      | 33/81 [00:10<00:21,  2.24it/s] 42%|████▏     | 34/81 [00:10<00:16,  2.82it/s] 43%|████▎     | 35/81 [00:10<00:13,  3.42it/s] 44%|████▍     | 36/81 [00:10<00:11,  4.06it/s] 46%|████▌     | 37/81 [00:10<00:09,  4.66it/s] 47%|████▋     | 38/81 [00:11<00:08,  5.21it/s] 48%|████▊     | 39/81 [00:11<00:07,  5.64it/s] 49%|████▉     | 40/81 [00:11<00:06,  6.01it/s] 51%|█████     | 41/81 [00:11<00:07,  5.33it/s] 52%|█████▏    | 42/81 [00:11<00:06,  5.77it/s] 53%|█████▎    | 43/81 [00:11<00:06,  6.11it/s] 54%|█████▍    | 44/81 [00:12<00:05,  6.38it/s] 56%|█████▌    | 45/81 [00:12<00:05,  6.59it/s] 57%|█████▋    | 46/81 [00:12<00:05,  6.71it/s] 58%|█████▊    | 47/81 [00:12<00:04,  6.81it/s] 59%|█████▉    | 48/81 [00:13<00:18,  1.78it/s] 60%|██████    | 49/81 [00:14<00:13,  2.30it/s] 62%|██████▏   | 50/81 [00:14<00:10,  2.89it/s] 63%|██████▎   | 51/81 [00:14<00:08,  3.52it/s] 64%|██████▍   | 52/81 [00:14<00:06,  4.15it/s] 65%|██████▌   | 53/81 [00:14<00:05,  4.76it/s] 67%|██████▋   | 54/81 [00:14<00:05,  5.28it/s] 68%|██████▊   | 55/81 [00:14<00:04,  5.65it/s] 69%|██████▉   | 56/81 [00:15<00:08,  2.96it/s] 70%|███████   | 57/81 [00:15<00:06,  3.60it/s] 72%|███████▏  | 58/81 [00:15<00:05,  4.23it/s] 73%|███████▎  | 59/81 [00:16<00:04,  4.83it/s] 74%|███████▍  | 60/81 [00:16<00:03,  5.35it/s] 75%|███████▌  | 61/81 [00:16<00:03,  5.78it/s] 77%|███████▋  | 62/81 [00:16<00:03,  6.10it/s] 78%|███████▊  | 63/81 [00:16<00:02,  6.37it/s] 79%|███████▉  | 64/81 [00:17<00:06,  2.44it/s] 80%|████████  | 65/81 [00:17<00:05,  3.04it/s] 81%|████████▏ | 66/81 [00:17<00:04,  3.67it/s] 83%|████████▎ | 67/81 [00:18<00:03,  4.30it/s] 84%|████████▍ | 68/81 [00:18<00:02,  4.90it/s] 85%|████████▌ | 69/81 [00:18<00:02,  5.42it/s] 86%|████████▋ | 70/81 [00:18<00:01,  5.85it/s] 88%|████████▊ | 71/81 [00:18<00:01,  6.21it/s] 89%|████████▉ | 72/81 [00:19<00:02,  3.30it/s] 90%|█████████ | 73/81 [00:19<00:02,  3.94it/s] 91%|█████████▏| 74/81 [00:19<00:01,  4.56it/s] 93%|█████████▎| 75/81 [00:19<00:01,  5.11it/s] 94%|█████████▍| 76/81 [00:19<00:00,  5.60it/s] 95%|█████████▌| 77/81 [00:19<00:00,  6.02it/s] 96%|█████████▋| 78/81 [00:20<00:00,  6.34it/s] 98%|█████████▊| 79/81 [00:20<00:00,  6.57it/s] 99%|█████████▉| 80/81 [00:20<00:00,  6.75it/s]100%|██████████| 81/81 [00:20<00:00,  3.94it/s]
=> result
* total: 8,041
* correct: 4,726
* accuracy: 58.8%
* error: 41.2%
* macro_f1: 56.9%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft oxford_pets 1 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/oxford_pets/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/oxford_pets/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:OxfordPets
Loading dataset: OxfordPets
Reading split from /shared/s2/lab01/dataset/clip/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_pets/split_fewshot_taesup/shot_16-seed_1.pkl
592 736 3669
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  37
# train_x  592
# val      736
# test     3,669
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/37 [00:00<?, ?it/s]  3%|▎         | 1/37 [00:04<02:30,  4.19s/it]  5%|▌         | 2/37 [00:04<01:02,  1.79s/it]  8%|▊         | 3/37 [00:04<00:34,  1.02s/it] 11%|█         | 4/37 [00:04<00:21,  1.52it/s] 14%|█▎        | 5/37 [00:04<00:14,  2.15it/s] 16%|█▌        | 6/37 [00:04<00:10,  2.91it/s] 19%|█▉        | 7/37 [00:04<00:07,  3.75it/s] 22%|██▏       | 8/37 [00:04<00:06,  4.62it/s] 24%|██▍       | 9/37 [00:05<00:05,  5.48it/s] 27%|██▋       | 10/37 [00:05<00:04,  6.26it/s] 30%|██▉       | 11/37 [00:05<00:03,  6.94it/s] 32%|███▏      | 12/37 [00:05<00:03,  7.43it/s] 35%|███▌      | 13/37 [00:05<00:03,  7.88it/s] 38%|███▊      | 14/37 [00:05<00:02,  8.24it/s] 41%|████      | 15/37 [00:05<00:02,  8.50it/s] 43%|████▎     | 16/37 [00:05<00:02,  8.69it/s] 46%|████▌     | 17/37 [00:05<00:02,  8.84it/s] 49%|████▊     | 18/37 [00:06<00:02,  8.91it/s] 51%|█████▏    | 19/37 [00:06<00:02,  8.99it/s] 54%|█████▍    | 20/37 [00:06<00:01,  9.03it/s] 57%|█████▋    | 21/37 [00:06<00:01,  9.00it/s] 59%|█████▉    | 22/37 [00:06<00:01,  9.00it/s] 62%|██████▏   | 23/37 [00:06<00:01,  9.06it/s] 65%|██████▍   | 24/37 [00:06<00:01,  9.09it/s] 68%|██████▊   | 25/37 [00:06<00:01,  9.13it/s] 70%|███████   | 26/37 [00:06<00:01,  9.14it/s] 73%|███████▎  | 27/37 [00:07<00:01,  9.15it/s] 76%|███████▌  | 28/37 [00:07<00:00,  9.17it/s] 78%|███████▊  | 29/37 [00:07<00:00,  9.16it/s] 81%|████████  | 30/37 [00:07<00:00,  9.17it/s] 84%|████████▍ | 31/37 [00:07<00:00,  9.16it/s] 86%|████████▋ | 32/37 [00:07<00:00,  9.17it/s] 89%|████████▉ | 33/37 [00:07<00:00,  9.20it/s] 92%|█████████▏| 34/37 [00:07<00:00,  9.23it/s] 95%|█████████▍| 35/37 [00:07<00:00,  9.24it/s] 97%|█████████▋| 36/37 [00:08<00:00,  9.27it/s]100%|██████████| 37/37 [00:08<00:00,  4.52it/s]
=> result
* total: 3,669
* correct: 1,955
* accuracy: 53.3%
* error: 46.7%
* macro_f1: 45.0%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft oxford_pets 2 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/oxford_pets/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/oxford_pets/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:OxfordPets
Loading dataset: OxfordPets
Reading split from /shared/s2/lab01/dataset/clip/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_pets/split_fewshot_taesup/shot_16-seed_2.pkl
592 736 3669
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  37
# train_x  592
# val      736
# test     3,669
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/37 [00:00<?, ?it/s]  3%|▎         | 1/37 [00:03<02:19,  3.87s/it]  5%|▌         | 2/37 [00:03<00:57,  1.66s/it]  8%|▊         | 3/37 [00:04<00:32,  1.05it/s] 11%|█         | 4/37 [00:04<00:20,  1.62it/s] 14%|█▎        | 5/37 [00:04<00:13,  2.30it/s] 16%|█▌        | 6/37 [00:04<00:10,  3.10it/s] 19%|█▉        | 7/37 [00:04<00:07,  3.96it/s] 22%|██▏       | 8/37 [00:04<00:05,  4.84it/s] 24%|██▍       | 9/37 [00:04<00:04,  5.69it/s] 27%|██▋       | 10/37 [00:04<00:04,  6.45it/s] 30%|██▉       | 11/37 [00:04<00:03,  7.11it/s] 32%|███▏      | 12/37 [00:05<00:03,  7.64it/s] 35%|███▌      | 13/37 [00:05<00:02,  8.05it/s] 38%|███▊      | 14/37 [00:05<00:02,  8.34it/s] 41%|████      | 15/37 [00:05<00:02,  8.55it/s] 43%|████▎     | 16/37 [00:05<00:02,  8.71it/s] 46%|████▌     | 17/37 [00:05<00:02,  8.84it/s] 49%|████▊     | 18/37 [00:05<00:02,  8.95it/s] 51%|█████▏    | 19/37 [00:05<00:01,  9.04it/s] 54%|█████▍    | 20/37 [00:05<00:01,  9.09it/s] 57%|█████▋    | 21/37 [00:06<00:01,  9.06it/s] 59%|█████▉    | 22/37 [00:06<00:01,  9.08it/s] 62%|██████▏   | 23/37 [00:06<00:01,  9.09it/s] 65%|██████▍   | 24/37 [00:06<00:01,  9.12it/s] 68%|██████▊   | 25/37 [00:06<00:01,  9.14it/s] 70%|███████   | 26/37 [00:06<00:01,  9.16it/s] 73%|███████▎  | 27/37 [00:06<00:01,  9.18it/s] 76%|███████▌  | 28/37 [00:06<00:00,  9.20it/s] 78%|███████▊  | 29/37 [00:06<00:00,  9.21it/s] 81%|████████  | 30/37 [00:07<00:00,  9.23it/s] 84%|████████▍ | 31/37 [00:07<00:00,  9.25it/s] 86%|████████▋ | 32/37 [00:07<00:00,  9.28it/s] 89%|████████▉ | 33/37 [00:07<00:00,  9.29it/s] 92%|█████████▏| 34/37 [00:07<00:00,  9.30it/s] 95%|█████████▍| 35/37 [00:07<00:00,  9.32it/s] 97%|█████████▋| 36/37 [00:07<00:00,  9.30it/s]100%|██████████| 37/37 [00:07<00:00,  4.72it/s]
=> result
* total: 3,669
* correct: 2,746
* accuracy: 74.8%
* error: 25.2%
* macro_f1: 70.9%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft oxford_pets 3 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/oxford_pets/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/oxford_pets/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:OxfordPets
Loading dataset: OxfordPets
Reading split from /shared/s2/lab01/dataset/clip/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_pets/split_fewshot_taesup/shot_16-seed_3.pkl
592 736 3669
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  37
# train_x  592
# val      736
# test     3,669
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/37 [00:00<?, ?it/s]  3%|▎         | 1/37 [00:04<02:29,  4.15s/it]  5%|▌         | 2/37 [00:04<01:02,  1.77s/it]  8%|▊         | 3/37 [00:04<00:34,  1.01s/it] 11%|█         | 4/37 [00:04<00:21,  1.52it/s] 14%|█▎        | 5/37 [00:04<00:14,  2.18it/s] 16%|█▌        | 6/37 [00:04<00:10,  2.94it/s] 19%|█▉        | 7/37 [00:04<00:07,  3.79it/s] 22%|██▏       | 8/37 [00:04<00:06,  4.67it/s] 24%|██▍       | 9/37 [00:05<00:05,  5.51it/s] 27%|██▋       | 10/37 [00:05<00:04,  6.28it/s] 30%|██▉       | 11/37 [00:05<00:03,  6.93it/s] 32%|███▏      | 12/37 [00:05<00:03,  7.46it/s] 35%|███▌      | 13/37 [00:05<00:03,  7.90it/s] 38%|███▊      | 14/37 [00:05<00:02,  8.23it/s] 41%|████      | 15/37 [00:05<00:02,  8.45it/s] 43%|████▎     | 16/37 [00:05<00:02,  8.63it/s] 46%|████▌     | 17/37 [00:05<00:02,  8.73it/s] 49%|████▊     | 18/37 [00:06<00:02,  8.85it/s] 51%|█████▏    | 19/37 [00:06<00:02,  8.94it/s] 54%|█████▍    | 20/37 [00:06<00:01,  8.99it/s] 57%|█████▋    | 21/37 [00:06<00:01,  9.04it/s] 59%|█████▉    | 22/37 [00:06<00:01,  9.06it/s] 62%|██████▏   | 23/37 [00:06<00:01,  9.09it/s] 65%|██████▍   | 24/37 [00:06<00:01,  9.10it/s] 68%|██████▊   | 25/37 [00:06<00:01,  9.11it/s] 70%|███████   | 26/37 [00:06<00:01,  9.13it/s] 73%|███████▎  | 27/37 [00:06<00:01,  9.17it/s] 76%|███████▌  | 28/37 [00:07<00:00,  9.20it/s] 78%|███████▊  | 29/37 [00:07<00:00,  9.24it/s] 81%|████████  | 30/37 [00:07<00:00,  9.27it/s] 84%|████████▍ | 31/37 [00:07<00:00,  9.27it/s] 86%|████████▋ | 32/37 [00:07<00:00,  9.29it/s] 89%|████████▉ | 33/37 [00:07<00:00,  9.30it/s] 92%|█████████▏| 34/37 [00:07<00:00,  9.31it/s] 95%|█████████▍| 35/37 [00:07<00:00,  9.31it/s] 97%|█████████▋| 36/37 [00:07<00:00,  9.13it/s]100%|██████████| 37/37 [00:08<00:00,  4.55it/s]
=> result
* total: 3,669
* correct: 2,468
* accuracy: 67.3%
* error: 32.7%
* macro_f1: 61.8%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft food101 1 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/food101/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/food101/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:Food101
Loading dataset: Food101
Reading split from /shared/s2/lab01/dataset/clip/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/food-101/split_fewshot_taesup/shot_16-seed_1.pkl
1616 20200 30300
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  101
# train_x  1,616
# val      20,200
# test     30,300
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/303 [00:00<?, ?it/s]  0%|          | 1/303 [00:04<21:23,  4.25s/it]  1%|          | 2/303 [00:04<09:08,  1.82s/it]  1%|          | 3/303 [00:04<05:13,  1.05s/it]  1%|▏         | 4/303 [00:04<03:23,  1.47it/s]  2%|▏         | 5/303 [00:04<02:22,  2.09it/s]  2%|▏         | 6/303 [00:04<01:45,  2.81it/s]  2%|▏         | 7/303 [00:04<01:22,  3.59it/s]  3%|▎         | 8/303 [00:05<01:07,  4.38it/s]  3%|▎         | 9/303 [00:05<00:57,  5.15it/s]  3%|▎         | 10/303 [00:05<00:50,  5.83it/s]  4%|▎         | 11/303 [00:05<00:45,  6.43it/s]  4%|▍         | 12/303 [00:05<00:42,  6.90it/s]  4%|▍         | 13/303 [00:05<00:39,  7.26it/s]  5%|▍         | 14/303 [00:05<00:38,  7.54it/s]  5%|▍         | 15/303 [00:05<00:37,  7.75it/s]  5%|▌         | 16/303 [00:06<00:36,  7.82it/s]  6%|▌         | 17/303 [00:06<00:35,  7.97it/s]  6%|▌         | 18/303 [00:06<00:35,  8.07it/s]  6%|▋         | 19/303 [00:06<00:34,  8.14it/s]  7%|▋         | 20/303 [00:06<00:34,  8.20it/s]  7%|▋         | 21/303 [00:06<00:34,  8.22it/s]  7%|▋         | 22/303 [00:06<00:34,  8.24it/s]  8%|▊         | 23/303 [00:06<00:33,  8.27it/s]  8%|▊         | 24/303 [00:07<00:33,  8.27it/s]  8%|▊         | 25/303 [00:07<00:33,  8.28it/s]  9%|▊         | 26/303 [00:07<00:33,  8.24it/s]  9%|▉         | 27/303 [00:07<00:33,  8.26it/s]  9%|▉         | 28/303 [00:07<00:33,  8.26it/s] 10%|▉         | 29/303 [00:07<00:33,  8.26it/s] 10%|▉         | 30/303 [00:07<00:32,  8.27it/s] 10%|█         | 31/303 [00:07<00:32,  8.28it/s] 11%|█         | 32/303 [00:07<00:32,  8.29it/s] 11%|█         | 33/303 [00:08<00:32,  8.30it/s] 11%|█         | 34/303 [00:08<00:32,  8.28it/s] 12%|█▏        | 35/303 [00:08<00:32,  8.29it/s] 12%|█▏        | 36/303 [00:08<00:32,  8.30it/s] 12%|█▏        | 37/303 [00:08<00:32,  8.31it/s] 13%|█▎        | 38/303 [00:08<00:31,  8.30it/s] 13%|█▎        | 39/303 [00:08<00:31,  8.30it/s] 13%|█▎        | 40/303 [00:08<00:31,  8.30it/s] 14%|█▎        | 41/303 [00:09<00:31,  8.29it/s] 14%|█▍        | 42/303 [00:09<00:31,  8.29it/s] 14%|█▍        | 43/303 [00:09<00:31,  8.29it/s] 15%|█▍        | 44/303 [00:09<00:31,  8.28it/s] 15%|█▍        | 45/303 [00:09<00:31,  8.28it/s] 15%|█▌        | 46/303 [00:09<00:31,  8.28it/s] 16%|█▌        | 47/303 [00:09<00:30,  8.28it/s] 16%|█▌        | 48/303 [00:09<00:30,  8.25it/s] 16%|█▌        | 49/303 [00:10<00:30,  8.21it/s] 17%|█▋        | 50/303 [00:10<00:30,  8.20it/s] 17%|█▋        | 51/303 [00:10<00:30,  8.22it/s] 17%|█▋        | 52/303 [00:10<00:30,  8.20it/s] 17%|█▋        | 53/303 [00:10<00:30,  8.20it/s] 18%|█▊        | 54/303 [00:10<00:30,  8.21it/s] 18%|█▊        | 55/303 [00:10<00:30,  8.21it/s] 18%|█▊        | 56/303 [00:10<00:30,  8.21it/s] 19%|█▉        | 57/303 [00:11<00:29,  8.22it/s] 19%|█▉        | 58/303 [00:11<00:29,  8.20it/s] 19%|█▉        | 59/303 [00:11<00:29,  8.21it/s] 20%|█▉        | 60/303 [00:11<00:29,  8.21it/s] 20%|██        | 61/303 [00:11<00:29,  8.21it/s] 20%|██        | 62/303 [00:11<00:29,  8.23it/s] 21%|██        | 63/303 [00:11<00:29,  8.25it/s] 21%|██        | 64/303 [00:11<00:29,  8.24it/s] 21%|██▏       | 65/303 [00:11<00:28,  8.23it/s] 22%|██▏       | 66/303 [00:12<00:28,  8.24it/s] 22%|██▏       | 67/303 [00:12<00:28,  8.25it/s] 22%|██▏       | 68/303 [00:12<00:28,  8.25it/s] 23%|██▎       | 69/303 [00:12<00:28,  8.25it/s] 23%|██▎       | 70/303 [00:12<00:28,  8.24it/s] 23%|██▎       | 71/303 [00:12<00:28,  8.25it/s] 24%|██▍       | 72/303 [00:12<00:27,  8.26it/s] 24%|██▍       | 73/303 [00:12<00:27,  8.27it/s] 24%|██▍       | 74/303 [00:13<00:27,  8.25it/s] 25%|██▍       | 75/303 [00:13<00:27,  8.26it/s] 25%|██▌       | 76/303 [00:13<00:27,  8.26it/s] 25%|██▌       | 77/303 [00:13<00:27,  8.26it/s] 26%|██▌       | 78/303 [00:13<00:27,  8.26it/s] 26%|██▌       | 79/303 [00:13<00:27,  8.26it/s] 26%|██▋       | 80/303 [00:13<00:26,  8.26it/s] 27%|██▋       | 81/303 [00:13<00:26,  8.27it/s] 27%|██▋       | 82/303 [00:14<00:26,  8.26it/s] 27%|██▋       | 83/303 [00:14<00:26,  8.26it/s] 28%|██▊       | 84/303 [00:14<00:26,  8.26it/s] 28%|██▊       | 85/303 [00:14<00:26,  8.26it/s] 28%|██▊       | 86/303 [00:14<00:34,  6.33it/s] 29%|██▊       | 87/303 [00:14<00:31,  6.81it/s] 29%|██▉       | 88/303 [00:14<00:29,  7.19it/s] 29%|██▉       | 89/303 [00:15<00:28,  7.47it/s] 30%|██▉       | 90/303 [00:15<00:30,  6.96it/s] 30%|███       | 91/303 [00:15<00:29,  7.29it/s] 30%|███       | 92/303 [00:15<00:28,  7.52it/s] 31%|███       | 93/303 [00:15<00:27,  7.71it/s] 31%|███       | 94/303 [00:15<00:29,  7.02it/s] 31%|███▏      | 95/303 [00:15<00:28,  7.35it/s] 32%|███▏      | 96/303 [00:15<00:27,  7.59it/s] 32%|███▏      | 97/303 [00:16<00:26,  7.77it/s] 32%|███▏      | 98/303 [00:16<00:25,  7.90it/s] 33%|███▎      | 99/303 [00:16<00:25,  8.00it/s] 33%|███▎      | 100/303 [00:16<00:25,  8.08it/s] 33%|███▎      | 101/303 [00:16<00:24,  8.13it/s] 34%|███▎      | 102/303 [00:16<00:24,  8.17it/s] 34%|███▍      | 103/303 [00:16<00:24,  8.20it/s] 34%|███▍      | 104/303 [00:16<00:24,  8.21it/s] 35%|███▍      | 105/303 [00:17<00:24,  8.21it/s] 35%|███▍      | 106/303 [00:17<00:24,  8.21it/s] 35%|███▌      | 107/303 [00:17<00:23,  8.21it/s] 36%|███▌      | 108/303 [00:17<00:23,  8.21it/s] 36%|███▌      | 109/303 [00:17<00:23,  8.23it/s] 36%|███▋      | 110/303 [00:17<00:23,  8.23it/s] 37%|███▋      | 111/303 [00:17<00:23,  8.23it/s] 37%|███▋      | 112/303 [00:17<00:23,  8.23it/s] 37%|███▋      | 113/303 [00:18<00:23,  8.23it/s] 38%|███▊      | 114/303 [00:18<00:22,  8.23it/s] 38%|███▊      | 115/303 [00:18<00:22,  8.23it/s] 38%|███▊      | 116/303 [00:18<00:22,  8.23it/s] 39%|███▊      | 117/303 [00:18<00:22,  8.23it/s] 39%|███▉      | 118/303 [00:18<00:22,  8.24it/s] 39%|███▉      | 119/303 [00:18<00:22,  8.25it/s] 40%|███▉      | 120/303 [00:18<00:22,  8.26it/s] 40%|███▉      | 121/303 [00:19<00:22,  8.25it/s] 40%|████      | 122/303 [00:19<00:21,  8.24it/s] 41%|████      | 123/303 [00:19<00:22,  8.16it/s] 41%|████      | 124/303 [00:19<00:22,  8.09it/s] 41%|████▏     | 125/303 [00:19<00:21,  8.14it/s] 42%|████▏     | 126/303 [00:19<00:21,  8.13it/s] 42%|████▏     | 127/303 [00:19<00:21,  8.17it/s] 42%|████▏     | 128/303 [00:19<00:21,  8.20it/s] 43%|████▎     | 129/303 [00:19<00:21,  8.22it/s] 43%|████▎     | 130/303 [00:20<00:21,  8.23it/s] 43%|████▎     | 131/303 [00:20<00:20,  8.23it/s] 44%|████▎     | 132/303 [00:20<00:21,  8.10it/s] 44%|████▍     | 133/303 [00:20<00:20,  8.15it/s] 44%|████▍     | 134/303 [00:20<00:20,  8.18it/s] 45%|████▍     | 135/303 [00:20<00:20,  8.18it/s] 45%|████▍     | 136/303 [00:20<00:20,  8.17it/s] 45%|████▌     | 137/303 [00:20<00:20,  8.20it/s] 46%|████▌     | 138/303 [00:21<00:20,  8.22it/s] 46%|████▌     | 139/303 [00:21<00:19,  8.23it/s] 46%|████▌     | 140/303 [00:21<00:30,  5.37it/s] 47%|████▋     | 141/303 [00:21<00:27,  6.00it/s] 47%|████▋     | 142/303 [00:21<00:24,  6.52it/s] 47%|████▋     | 143/303 [00:21<00:23,  6.95it/s] 48%|████▊     | 144/303 [00:22<00:21,  7.28it/s] 48%|████▊     | 145/303 [00:22<00:20,  7.55it/s] 48%|████▊     | 146/303 [00:22<00:20,  7.75it/s] 49%|████▊     | 147/303 [00:22<00:19,  7.89it/s] 49%|████▉     | 148/303 [00:22<00:25,  6.08it/s] 49%|████▉     | 149/303 [00:22<00:23,  6.60it/s] 50%|████▉     | 150/303 [00:22<00:21,  7.02it/s] 50%|████▉     | 151/303 [00:23<00:20,  7.34it/s] 50%|█████     | 152/303 [00:23<00:19,  7.58it/s] 50%|█████     | 153/303 [00:23<00:19,  7.75it/s] 51%|█████     | 154/303 [00:23<00:18,  7.88it/s] 51%|█████     | 155/303 [00:23<00:18,  7.94it/s] 51%|█████▏    | 156/303 [00:23<00:23,  6.34it/s] 52%|█████▏    | 157/303 [00:23<00:21,  6.79it/s] 52%|█████▏    | 158/303 [00:23<00:20,  7.17it/s] 52%|█████▏    | 159/303 [00:24<00:19,  7.47it/s] 53%|█████▎    | 160/303 [00:24<00:18,  7.68it/s] 53%|█████▎    | 161/303 [00:24<00:18,  7.83it/s] 53%|█████▎    | 162/303 [00:24<00:17,  7.93it/s] 54%|█████▍    | 163/303 [00:24<00:17,  8.02it/s] 54%|█████▍    | 164/303 [00:24<00:17,  8.07it/s] 54%|█████▍    | 165/303 [00:24<00:17,  8.11it/s] 55%|█████▍    | 166/303 [00:24<00:16,  8.14it/s] 55%|█████▌    | 167/303 [00:25<00:16,  8.17it/s] 55%|█████▌    | 168/303 [00:25<00:16,  8.19it/s] 56%|█████▌    | 169/303 [00:25<00:16,  8.20it/s] 56%|█████▌    | 170/303 [00:25<00:16,  8.20it/s] 56%|█████▋    | 171/303 [00:25<00:16,  8.20it/s] 57%|█████▋    | 172/303 [00:25<00:16,  8.19it/s] 57%|█████▋    | 173/303 [00:25<00:15,  8.19it/s] 57%|█████▋    | 174/303 [00:25<00:15,  8.18it/s] 58%|█████▊    | 175/303 [00:26<00:15,  8.16it/s] 58%|█████▊    | 176/303 [00:26<00:15,  8.17it/s] 58%|█████▊    | 177/303 [00:26<00:15,  8.19it/s] 59%|█████▊    | 178/303 [00:26<00:15,  8.20it/s] 59%|█████▉    | 179/303 [00:26<00:15,  8.22it/s] 59%|█████▉    | 180/303 [00:26<00:14,  8.22it/s] 60%|█████▉    | 181/303 [00:26<00:14,  8.22it/s] 60%|██████    | 182/303 [00:26<00:14,  8.22it/s] 60%|██████    | 183/303 [00:27<00:14,  8.22it/s] 61%|██████    | 184/303 [00:27<00:14,  8.22it/s] 61%|██████    | 185/303 [00:27<00:14,  8.22it/s] 61%|██████▏   | 186/303 [00:27<00:14,  8.22it/s] 62%|██████▏   | 187/303 [00:27<00:14,  8.23it/s] 62%|██████▏   | 188/303 [00:27<00:13,  8.24it/s] 62%|██████▏   | 189/303 [00:27<00:13,  8.23it/s] 63%|██████▎   | 190/303 [00:27<00:13,  8.22it/s] 63%|██████▎   | 191/303 [00:27<00:13,  8.23it/s] 63%|██████▎   | 192/303 [00:28<00:13,  8.24it/s] 64%|██████▎   | 193/303 [00:28<00:13,  8.13it/s] 64%|██████▍   | 194/303 [00:28<00:13,  8.17it/s] 64%|██████▍   | 195/303 [00:28<00:13,  8.20it/s] 65%|██████▍   | 196/303 [00:28<00:12,  8.24it/s] 65%|██████▌   | 197/303 [00:28<00:12,  8.25it/s] 65%|██████▌   | 198/303 [00:28<00:12,  8.28it/s] 66%|██████▌   | 199/303 [00:28<00:12,  8.28it/s] 66%|██████▌   | 200/303 [00:29<00:12,  8.28it/s] 66%|██████▋   | 201/303 [00:29<00:12,  8.27it/s] 67%|██████▋   | 202/303 [00:29<00:12,  8.26it/s] 67%|██████▋   | 203/303 [00:29<00:12,  8.26it/s] 67%|██████▋   | 204/303 [00:29<00:12,  8.24it/s] 68%|██████▊   | 205/303 [00:29<00:11,  8.23it/s] 68%|██████▊   | 206/303 [00:29<00:11,  8.21it/s] 68%|██████▊   | 207/303 [00:29<00:11,  8.20it/s] 69%|██████▊   | 208/303 [00:30<00:11,  8.20it/s] 69%|██████▉   | 209/303 [00:30<00:11,  8.21it/s] 69%|██████▉   | 210/303 [00:30<00:11,  8.21it/s] 70%|██████▉   | 211/303 [00:30<00:11,  8.22it/s] 70%|██████▉   | 212/303 [00:30<00:11,  8.21it/s] 70%|███████   | 213/303 [00:30<00:10,  8.21it/s] 71%|███████   | 214/303 [00:30<00:10,  8.22it/s] 71%|███████   | 215/303 [00:30<00:10,  8.21it/s] 71%|███████▏  | 216/303 [00:31<00:10,  8.20it/s] 72%|███████▏  | 217/303 [00:31<00:10,  8.21it/s] 72%|███████▏  | 218/303 [00:31<00:10,  8.21it/s] 72%|███████▏  | 219/303 [00:31<00:10,  8.21it/s] 73%|███████▎  | 220/303 [00:31<00:10,  8.10it/s] 73%|███████▎  | 221/303 [00:31<00:10,  8.13it/s] 73%|███████▎  | 222/303 [00:31<00:09,  8.16it/s] 74%|███████▎  | 223/303 [00:31<00:09,  8.17it/s] 74%|███████▍  | 224/303 [00:32<00:09,  8.19it/s] 74%|███████▍  | 225/303 [00:32<00:09,  8.18it/s] 75%|███████▍  | 226/303 [00:32<00:09,  8.20it/s] 75%|███████▍  | 227/303 [00:32<00:09,  8.20it/s] 75%|███████▌  | 228/303 [00:32<00:09,  8.21it/s] 76%|███████▌  | 229/303 [00:32<00:09,  8.20it/s] 76%|███████▌  | 230/303 [00:32<00:08,  8.21it/s] 76%|███████▌  | 231/303 [00:32<00:08,  8.21it/s] 77%|███████▋  | 232/303 [00:32<00:08,  8.20it/s] 77%|███████▋  | 233/303 [00:33<00:08,  8.20it/s] 77%|███████▋  | 234/303 [00:33<00:08,  8.19it/s] 78%|███████▊  | 235/303 [00:33<00:08,  8.18it/s] 78%|███████▊  | 236/303 [00:33<00:08,  8.20it/s] 78%|███████▊  | 237/303 [00:33<00:08,  8.21it/s] 79%|███████▊  | 238/303 [00:33<00:07,  8.22it/s] 79%|███████▉  | 239/303 [00:33<00:07,  8.22it/s] 79%|███████▉  | 240/303 [00:33<00:07,  8.21it/s] 80%|███████▉  | 241/303 [00:34<00:07,  8.21it/s] 80%|███████▉  | 242/303 [00:34<00:07,  8.21it/s] 80%|████████  | 243/303 [00:34<00:07,  8.19it/s] 81%|████████  | 244/303 [00:34<00:07,  8.20it/s] 81%|████████  | 245/303 [00:34<00:07,  8.20it/s] 81%|████████  | 246/303 [00:34<00:06,  8.20it/s] 82%|████████▏ | 247/303 [00:34<00:06,  8.17it/s] 82%|████████▏ | 248/303 [00:34<00:06,  8.17it/s] 82%|████████▏ | 249/303 [00:35<00:06,  8.16it/s] 83%|████████▎ | 250/303 [00:35<00:06,  8.15it/s] 83%|████████▎ | 251/303 [00:35<00:06,  8.14it/s] 83%|████████▎ | 252/303 [00:35<00:06,  7.37it/s] 83%|████████▎ | 253/303 [00:35<00:06,  7.55it/s] 84%|████████▍ | 254/303 [00:35<00:06,  7.73it/s] 84%|████████▍ | 255/303 [00:35<00:06,  7.86it/s] 84%|████████▍ | 256/303 [00:35<00:05,  7.93it/s] 85%|████████▍ | 257/303 [00:36<00:05,  8.01it/s] 85%|████████▌ | 258/303 [00:36<00:05,  8.07it/s] 85%|████████▌ | 259/303 [00:36<00:05,  8.11it/s] 86%|████████▌ | 260/303 [00:36<00:06,  6.76it/s] 86%|████████▌ | 261/303 [00:36<00:05,  7.14it/s] 86%|████████▋ | 262/303 [00:36<00:05,  7.43it/s] 87%|████████▋ | 263/303 [00:36<00:05,  7.65it/s] 87%|████████▋ | 264/303 [00:37<00:05,  7.80it/s] 87%|████████▋ | 265/303 [00:37<00:04,  7.92it/s] 88%|████████▊ | 266/303 [00:37<00:04,  8.01it/s] 88%|████████▊ | 267/303 [00:37<00:04,  8.07it/s] 88%|████████▊ | 268/303 [00:37<00:05,  6.22it/s] 89%|████████▉ | 269/303 [00:37<00:05,  6.70it/s] 89%|████████▉ | 270/303 [00:37<00:04,  7.10it/s] 89%|████████▉ | 271/303 [00:38<00:04,  7.38it/s] 90%|████████▉ | 272/303 [00:38<00:04,  7.60it/s] 90%|█████████ | 273/303 [00:38<00:03,  7.75it/s] 90%|█████████ | 274/303 [00:38<00:03,  7.88it/s] 91%|█████████ | 275/303 [00:38<00:03,  7.96it/s] 91%|█████████ | 276/303 [00:38<00:04,  5.99it/s] 91%|█████████▏| 277/303 [00:38<00:03,  6.52it/s] 92%|█████████▏| 278/303 [00:39<00:03,  6.95it/s] 92%|█████████▏| 279/303 [00:39<00:03,  7.29it/s] 92%|█████████▏| 280/303 [00:39<00:03,  7.53it/s] 93%|█████████▎| 281/303 [00:39<00:02,  7.70it/s] 93%|█████████▎| 282/303 [00:39<00:02,  7.85it/s] 93%|█████████▎| 283/303 [00:39<00:02,  7.95it/s] 94%|█████████▎| 284/303 [00:39<00:02,  7.34it/s] 94%|█████████▍| 285/303 [00:39<00:02,  7.58it/s] 94%|█████████▍| 286/303 [00:40<00:02,  7.76it/s] 95%|█████████▍| 287/303 [00:40<00:02,  7.89it/s] 95%|█████████▌| 288/303 [00:40<00:01,  7.99it/s] 95%|█████████▌| 289/303 [00:40<00:01,  8.06it/s] 96%|█████████▌| 290/303 [00:40<00:01,  8.10it/s] 96%|█████████▌| 291/303 [00:40<00:01,  8.12it/s] 96%|█████████▋| 292/303 [00:40<00:01,  8.14it/s] 97%|█████████▋| 293/303 [00:40<00:01,  8.18it/s] 97%|█████████▋| 294/303 [00:40<00:01,  8.20it/s] 97%|█████████▋| 295/303 [00:41<00:00,  8.20it/s] 98%|█████████▊| 296/303 [00:41<00:00,  8.19it/s] 98%|█████████▊| 297/303 [00:41<00:00,  8.20it/s] 98%|█████████▊| 298/303 [00:41<00:00,  8.22it/s] 99%|█████████▊| 299/303 [00:41<00:00,  8.23it/s] 99%|█████████▉| 300/303 [00:41<00:00,  8.24it/s] 99%|█████████▉| 301/303 [00:41<00:00,  8.24it/s]100%|█████████▉| 302/303 [00:41<00:00,  8.25it/s]100%|██████████| 303/303 [00:42<00:00,  8.25it/s]100%|██████████| 303/303 [00:42<00:00,  7.18it/s]
=> result
* total: 30,300
* correct: 16,954
* accuracy: 56.0%
* error: 44.0%
* macro_f1: 50.4%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft food101 2 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/food101/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/food101/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:Food101
Loading dataset: Food101
Reading split from /shared/s2/lab01/dataset/clip/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/food-101/split_fewshot_taesup/shot_16-seed_2.pkl
1616 20200 30300
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  101
# train_x  1,616
# val      20,200
# test     30,300
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/303 [00:00<?, ?it/s]  0%|          | 1/303 [00:03<19:42,  3.92s/it]  1%|          | 2/303 [00:04<08:27,  1.69s/it]  1%|          | 3/303 [00:04<04:51,  1.03it/s]  1%|▏         | 4/303 [00:04<03:09,  1.57it/s]  2%|▏         | 5/303 [00:04<02:13,  2.23it/s]  2%|▏         | 6/303 [00:04<01:40,  2.97it/s]  2%|▏         | 7/303 [00:04<01:18,  3.76it/s]  3%|▎         | 8/303 [00:04<01:04,  4.54it/s]  3%|▎         | 9/303 [00:04<00:55,  5.29it/s]  3%|▎         | 10/303 [00:05<00:49,  5.96it/s]  4%|▎         | 11/303 [00:05<00:44,  6.52it/s]  4%|▍         | 12/303 [00:05<00:41,  6.97it/s]  4%|▍         | 13/303 [00:05<00:39,  7.32it/s]  5%|▍         | 14/303 [00:05<00:38,  7.57it/s]  5%|▍         | 15/303 [00:05<00:37,  7.75it/s]  5%|▌         | 16/303 [00:05<00:36,  7.90it/s]  6%|▌         | 17/303 [00:05<00:35,  8.00it/s]  6%|▌         | 18/303 [00:05<00:35,  8.08it/s]  6%|▋         | 19/303 [00:06<00:34,  8.14it/s]  7%|▋         | 20/303 [00:06<00:34,  8.18it/s]  7%|▋         | 21/303 [00:06<00:34,  8.21it/s]  7%|▋         | 22/303 [00:06<00:34,  8.24it/s]  8%|▊         | 23/303 [00:06<00:33,  8.25it/s]  8%|▊         | 24/303 [00:06<00:33,  8.25it/s]  8%|▊         | 25/303 [00:06<00:33,  8.25it/s]  9%|▊         | 26/303 [00:06<00:33,  8.26it/s]  9%|▉         | 27/303 [00:07<00:33,  8.26it/s]  9%|▉         | 28/303 [00:07<00:33,  8.27it/s] 10%|▉         | 29/303 [00:07<00:33,  8.26it/s] 10%|▉         | 30/303 [00:07<00:33,  8.26it/s] 10%|█         | 31/303 [00:07<00:33,  8.24it/s] 11%|█         | 32/303 [00:07<00:32,  8.25it/s] 11%|█         | 33/303 [00:07<00:32,  8.26it/s] 11%|█         | 34/303 [00:07<00:32,  8.26it/s] 12%|█▏        | 35/303 [00:08<00:32,  8.26it/s] 12%|█▏        | 36/303 [00:08<00:32,  8.27it/s] 12%|█▏        | 37/303 [00:08<00:32,  8.25it/s] 13%|█▎        | 38/303 [00:08<00:32,  8.25it/s] 13%|█▎        | 39/303 [00:08<00:32,  8.23it/s] 13%|█▎        | 40/303 [00:08<00:31,  8.24it/s] 14%|█▎        | 41/303 [00:08<00:31,  8.25it/s] 14%|█▍        | 42/303 [00:08<00:31,  8.24it/s] 14%|█▍        | 43/303 [00:09<00:31,  8.23it/s] 15%|█▍        | 44/303 [00:09<00:31,  8.25it/s] 15%|█▍        | 45/303 [00:09<00:31,  8.25it/s] 15%|█▌        | 46/303 [00:09<00:31,  8.26it/s] 16%|█▌        | 47/303 [00:09<00:30,  8.26it/s] 16%|█▌        | 48/303 [00:09<00:30,  8.25it/s] 16%|█▌        | 49/303 [00:09<00:30,  8.25it/s] 17%|█▋        | 50/303 [00:09<00:30,  8.25it/s] 17%|█▋        | 51/303 [00:09<00:30,  8.26it/s] 17%|█▋        | 52/303 [00:10<00:30,  8.26it/s] 17%|█▋        | 53/303 [00:10<00:30,  8.23it/s] 18%|█▊        | 54/303 [00:10<00:30,  8.23it/s] 18%|█▊        | 55/303 [00:10<00:30,  8.23it/s] 18%|█▊        | 56/303 [00:10<00:30,  8.21it/s] 19%|█▉        | 57/303 [00:10<00:29,  8.21it/s] 19%|█▉        | 58/303 [00:10<00:29,  8.23it/s] 19%|█▉        | 59/303 [00:10<00:29,  8.23it/s] 20%|█▉        | 60/303 [00:11<00:29,  8.24it/s] 20%|██        | 61/303 [00:11<00:29,  8.24it/s] 20%|██        | 62/303 [00:11<00:29,  8.26it/s] 21%|██        | 63/303 [00:11<00:29,  8.27it/s] 21%|██        | 64/303 [00:11<00:28,  8.26it/s] 21%|██▏       | 65/303 [00:11<00:28,  8.26it/s] 22%|██▏       | 66/303 [00:11<00:28,  8.26it/s] 22%|██▏       | 67/303 [00:11<00:28,  8.25it/s] 22%|██▏       | 68/303 [00:12<00:28,  8.25it/s] 23%|██▎       | 69/303 [00:12<00:28,  8.26it/s] 23%|██▎       | 70/303 [00:12<00:28,  8.26it/s] 23%|██▎       | 71/303 [00:12<00:28,  8.25it/s] 24%|██▍       | 72/303 [00:12<00:27,  8.26it/s] 24%|██▍       | 73/303 [00:12<00:27,  8.26it/s] 24%|██▍       | 74/303 [00:12<00:27,  8.25it/s] 25%|██▍       | 75/303 [00:12<00:27,  8.19it/s] 25%|██▌       | 76/303 [00:13<00:27,  8.19it/s] 25%|██▌       | 77/303 [00:13<00:27,  8.22it/s] 26%|██▌       | 78/303 [00:13<00:27,  8.24it/s] 26%|██▌       | 79/303 [00:13<00:27,  8.24it/s] 26%|██▋       | 80/303 [00:13<00:27,  8.24it/s] 27%|██▋       | 81/303 [00:13<00:26,  8.23it/s] 27%|██▋       | 82/303 [00:13<00:26,  8.23it/s] 27%|██▋       | 83/303 [00:13<00:26,  8.23it/s] 28%|██▊       | 84/303 [00:13<00:26,  8.24it/s] 28%|██▊       | 85/303 [00:14<00:26,  8.25it/s] 28%|██▊       | 86/303 [00:14<00:26,  8.24it/s] 29%|██▊       | 87/303 [00:14<00:26,  8.22it/s] 29%|██▉       | 88/303 [00:14<00:26,  8.23it/s] 29%|██▉       | 89/303 [00:14<00:26,  8.23it/s] 30%|██▉       | 90/303 [00:14<00:25,  8.24it/s] 30%|███       | 91/303 [00:14<00:25,  8.21it/s] 30%|███       | 92/303 [00:14<00:25,  8.21it/s] 31%|███       | 93/303 [00:15<00:25,  8.23it/s] 31%|███       | 94/303 [00:15<00:25,  8.23it/s] 31%|███▏      | 95/303 [00:15<00:25,  8.23it/s] 32%|███▏      | 96/303 [00:15<00:25,  8.21it/s] 32%|███▏      | 97/303 [00:15<00:25,  8.22it/s] 32%|███▏      | 98/303 [00:15<00:24,  8.22it/s] 33%|███▎      | 99/303 [00:15<00:24,  8.22it/s] 33%|███▎      | 100/303 [00:15<00:24,  8.22it/s] 33%|███▎      | 101/303 [00:16<00:24,  8.22it/s] 34%|███▎      | 102/303 [00:16<00:24,  8.22it/s] 34%|███▍      | 103/303 [00:16<00:24,  8.21it/s] 34%|███▍      | 104/303 [00:16<00:24,  8.22it/s] 35%|███▍      | 105/303 [00:16<00:24,  8.23it/s] 35%|███▍      | 106/303 [00:16<00:23,  8.22it/s] 35%|███▌      | 107/303 [00:16<00:23,  8.23it/s] 36%|███▌      | 108/303 [00:16<00:23,  8.22it/s] 36%|███▌      | 109/303 [00:17<00:23,  8.23it/s] 36%|███▋      | 110/303 [00:17<00:23,  8.23it/s] 37%|███▋      | 111/303 [00:17<00:23,  8.21it/s] 37%|███▋      | 112/303 [00:17<00:23,  8.22it/s] 37%|███▋      | 113/303 [00:17<00:23,  8.23it/s] 38%|███▊      | 114/303 [00:17<00:23,  8.21it/s] 38%|███▊      | 115/303 [00:17<00:22,  8.22it/s] 38%|███▊      | 116/303 [00:17<00:22,  8.20it/s] 39%|███▊      | 117/303 [00:17<00:22,  8.21it/s] 39%|███▉      | 118/303 [00:18<00:22,  8.22it/s] 39%|███▉      | 119/303 [00:18<00:22,  8.21it/s] 40%|███▉      | 120/303 [00:18<00:22,  8.23it/s] 40%|███▉      | 121/303 [00:18<00:22,  8.22it/s] 40%|████      | 122/303 [00:18<00:21,  8.23it/s] 41%|████      | 123/303 [00:18<00:21,  8.23it/s] 41%|████      | 124/303 [00:18<00:21,  8.23it/s] 41%|████▏     | 125/303 [00:18<00:21,  8.22it/s] 42%|████▏     | 126/303 [00:19<00:21,  8.23it/s] 42%|████▏     | 127/303 [00:19<00:21,  8.23it/s] 42%|████▏     | 128/303 [00:19<00:21,  8.24it/s] 43%|████▎     | 129/303 [00:19<00:21,  8.23it/s] 43%|████▎     | 130/303 [00:19<00:21,  8.21it/s] 43%|████▎     | 131/303 [00:19<00:20,  8.23it/s] 44%|████▎     | 132/303 [00:19<00:20,  8.23it/s] 44%|████▍     | 133/303 [00:19<00:20,  8.23it/s] 44%|████▍     | 134/303 [00:20<00:20,  8.23it/s] 45%|████▍     | 135/303 [00:20<00:20,  8.22it/s] 45%|████▍     | 136/303 [00:20<00:20,  8.23it/s] 45%|████▌     | 137/303 [00:20<00:20,  8.23it/s] 46%|████▌     | 138/303 [00:20<00:20,  8.21it/s] 46%|████▌     | 139/303 [00:20<00:20,  8.19it/s] 46%|████▌     | 140/303 [00:20<00:19,  8.19it/s] 47%|████▋     | 141/303 [00:20<00:19,  8.20it/s] 47%|████▋     | 142/303 [00:21<00:19,  8.22it/s] 47%|████▋     | 143/303 [00:21<00:19,  8.21it/s] 48%|████▊     | 144/303 [00:21<00:19,  8.20it/s] 48%|████▊     | 145/303 [00:21<00:19,  8.20it/s] 48%|████▊     | 146/303 [00:21<00:19,  8.21it/s] 49%|████▊     | 147/303 [00:21<00:19,  8.21it/s] 49%|████▉     | 148/303 [00:21<00:18,  8.21it/s] 49%|████▉     | 149/303 [00:21<00:18,  8.22it/s] 50%|████▉     | 150/303 [00:22<00:18,  8.22it/s] 50%|████▉     | 151/303 [00:22<00:18,  8.19it/s] 50%|█████     | 152/303 [00:22<00:18,  8.21it/s] 50%|█████     | 153/303 [00:22<00:18,  8.20it/s] 51%|█████     | 154/303 [00:22<00:18,  8.18it/s] 51%|█████     | 155/303 [00:22<00:18,  8.21it/s] 51%|█████▏    | 156/303 [00:22<00:17,  8.21it/s] 52%|█████▏    | 157/303 [00:22<00:17,  8.20it/s] 52%|█████▏    | 158/303 [00:22<00:17,  8.20it/s] 52%|█████▏    | 159/303 [00:23<00:17,  8.21it/s] 53%|█████▎    | 160/303 [00:23<00:17,  8.20it/s] 53%|█████▎    | 161/303 [00:23<00:17,  8.17it/s] 53%|█████▎    | 162/303 [00:23<00:17,  8.18it/s] 54%|█████▍    | 163/303 [00:23<00:17,  8.19it/s] 54%|█████▍    | 164/303 [00:23<00:17,  8.17it/s] 54%|█████▍    | 165/303 [00:23<00:16,  8.16it/s] 55%|█████▍    | 166/303 [00:23<00:16,  8.17it/s] 55%|█████▌    | 167/303 [00:24<00:16,  8.19it/s] 55%|█████▌    | 168/303 [00:24<00:16,  8.20it/s] 56%|█████▌    | 169/303 [00:24<00:16,  8.21it/s] 56%|█████▌    | 170/303 [00:24<00:16,  8.20it/s] 56%|█████▋    | 171/303 [00:24<00:16,  8.19it/s] 57%|█████▋    | 172/303 [00:24<00:15,  8.20it/s] 57%|█████▋    | 173/303 [00:24<00:15,  8.20it/s] 57%|█████▋    | 174/303 [00:24<00:15,  8.18it/s] 58%|█████▊    | 175/303 [00:25<00:15,  8.19it/s] 58%|█████▊    | 176/303 [00:25<00:15,  8.20it/s] 58%|█████▊    | 177/303 [00:25<00:15,  8.20it/s] 59%|█████▊    | 178/303 [00:25<00:15,  8.20it/s] 59%|█████▉    | 179/303 [00:25<00:15,  8.19it/s] 59%|█████▉    | 180/303 [00:25<00:15,  8.12it/s] 60%|█████▉    | 181/303 [00:25<00:14,  8.14it/s] 60%|██████    | 182/303 [00:25<00:14,  8.15it/s] 60%|██████    | 183/303 [00:26<00:14,  8.16it/s] 61%|██████    | 184/303 [00:26<00:14,  8.18it/s] 61%|██████    | 185/303 [00:26<00:14,  8.20it/s] 61%|██████▏   | 186/303 [00:26<00:14,  8.18it/s] 62%|██████▏   | 187/303 [00:26<00:14,  8.17it/s] 62%|██████▏   | 188/303 [00:26<00:14,  8.18it/s] 62%|██████▏   | 189/303 [00:26<00:13,  8.20it/s] 63%|██████▎   | 190/303 [00:26<00:13,  8.19it/s] 63%|██████▎   | 191/303 [00:27<00:13,  8.20it/s] 63%|██████▎   | 192/303 [00:27<00:13,  8.20it/s] 64%|██████▎   | 193/303 [00:27<00:13,  8.19it/s] 64%|██████▍   | 194/303 [00:27<00:13,  8.17it/s] 64%|██████▍   | 195/303 [00:27<00:13,  8.18it/s] 65%|██████▍   | 196/303 [00:27<00:13,  8.17it/s] 65%|██████▌   | 197/303 [00:27<00:12,  8.17it/s] 65%|██████▌   | 198/303 [00:27<00:12,  8.18it/s] 66%|██████▌   | 199/303 [00:27<00:12,  8.18it/s] 66%|██████▌   | 200/303 [00:28<00:12,  8.19it/s] 66%|██████▋   | 201/303 [00:28<00:12,  8.18it/s] 67%|██████▋   | 202/303 [00:28<00:12,  8.20it/s] 67%|██████▋   | 203/303 [00:28<00:12,  8.19it/s] 67%|██████▋   | 204/303 [00:28<00:12,  8.18it/s] 68%|██████▊   | 205/303 [00:28<00:11,  8.17it/s] 68%|██████▊   | 206/303 [00:28<00:11,  8.17it/s] 68%|██████▊   | 207/303 [00:28<00:11,  8.19it/s] 69%|██████▊   | 208/303 [00:29<00:11,  8.18it/s] 69%|██████▉   | 209/303 [00:29<00:11,  8.17it/s] 69%|██████▉   | 210/303 [00:29<00:11,  8.18it/s] 70%|██████▉   | 211/303 [00:29<00:11,  8.14it/s] 70%|██████▉   | 212/303 [00:29<00:11,  8.14it/s] 70%|███████   | 213/303 [00:29<00:11,  8.15it/s] 71%|███████   | 214/303 [00:29<00:10,  8.16it/s] 71%|███████   | 215/303 [00:29<00:10,  8.17it/s] 71%|███████▏  | 216/303 [00:30<00:10,  8.17it/s] 72%|███████▏  | 217/303 [00:30<00:10,  8.17it/s] 72%|███████▏  | 218/303 [00:30<00:10,  8.19it/s] 72%|███████▏  | 219/303 [00:30<00:10,  8.18it/s] 73%|███████▎  | 220/303 [00:30<00:10,  8.17it/s] 73%|███████▎  | 221/303 [00:30<00:10,  8.16it/s] 73%|███████▎  | 222/303 [00:30<00:10,  8.09it/s] 74%|███████▎  | 223/303 [00:30<00:09,  8.12it/s] 74%|███████▍  | 224/303 [00:31<00:09,  8.14it/s] 74%|███████▍  | 225/303 [00:31<00:09,  8.15it/s] 75%|███████▍  | 226/303 [00:31<00:09,  8.16it/s] 75%|███████▍  | 227/303 [00:31<00:09,  8.16it/s] 75%|███████▌  | 228/303 [00:31<00:09,  8.16it/s] 76%|███████▌  | 229/303 [00:31<00:09,  8.18it/s] 76%|███████▌  | 230/303 [00:31<00:08,  8.17it/s] 76%|███████▌  | 231/303 [00:31<00:08,  8.17it/s] 77%|███████▋  | 232/303 [00:32<00:08,  8.15it/s] 77%|███████▋  | 233/303 [00:32<00:08,  8.15it/s] 77%|███████▋  | 234/303 [00:32<00:08,  8.14it/s] 78%|███████▊  | 235/303 [00:32<00:08,  8.15it/s] 78%|███████▊  | 236/303 [00:32<00:08,  8.14it/s] 78%|███████▊  | 237/303 [00:32<00:08,  8.14it/s] 79%|███████▊  | 238/303 [00:32<00:07,  8.15it/s] 79%|███████▉  | 239/303 [00:32<00:07,  8.17it/s] 79%|███████▉  | 240/303 [00:33<00:07,  8.18it/s] 80%|███████▉  | 241/303 [00:33<00:07,  8.18it/s] 80%|███████▉  | 242/303 [00:33<00:07,  8.19it/s] 80%|████████  | 243/303 [00:33<00:07,  8.16it/s] 81%|████████  | 244/303 [00:33<00:07,  8.11it/s] 81%|████████  | 245/303 [00:33<00:07,  8.13it/s] 81%|████████  | 246/303 [00:33<00:07,  8.14it/s] 82%|████████▏ | 247/303 [00:33<00:06,  8.15it/s] 82%|████████▏ | 248/303 [00:33<00:06,  8.18it/s] 82%|████████▏ | 249/303 [00:34<00:06,  8.16it/s] 83%|████████▎ | 250/303 [00:34<00:06,  8.17it/s] 83%|████████▎ | 251/303 [00:34<00:06,  8.18it/s] 83%|████████▎ | 252/303 [00:34<00:06,  8.19it/s] 83%|████████▎ | 253/303 [00:34<00:06,  8.18it/s] 84%|████████▍ | 254/303 [00:34<00:05,  8.17it/s] 84%|████████▍ | 255/303 [00:34<00:05,  8.15it/s] 84%|████████▍ | 256/303 [00:34<00:05,  8.17it/s] 85%|████████▍ | 257/303 [00:35<00:05,  8.18it/s] 85%|████████▌ | 258/303 [00:35<00:05,  8.18it/s] 85%|████████▌ | 259/303 [00:35<00:05,  8.08it/s] 86%|████████▌ | 260/303 [00:35<00:05,  8.12it/s] 86%|████████▌ | 261/303 [00:35<00:05,  8.13it/s] 86%|████████▋ | 262/303 [00:35<00:05,  8.15it/s] 87%|████████▋ | 263/303 [00:35<00:04,  8.17it/s] 87%|████████▋ | 264/303 [00:35<00:04,  8.18it/s] 87%|████████▋ | 265/303 [00:36<00:04,  8.17it/s] 88%|████████▊ | 266/303 [00:36<00:04,  8.16it/s] 88%|████████▊ | 267/303 [00:36<00:04,  8.16it/s] 88%|████████▊ | 268/303 [00:36<00:04,  8.16it/s] 89%|████████▉ | 269/303 [00:36<00:04,  8.17it/s] 89%|████████▉ | 270/303 [00:36<00:04,  8.16it/s] 89%|████████▉ | 271/303 [00:36<00:03,  8.17it/s] 90%|████████▉ | 272/303 [00:36<00:03,  8.17it/s] 90%|█████████ | 273/303 [00:37<00:03,  8.18it/s] 90%|█████████ | 274/303 [00:37<00:03,  8.18it/s] 91%|█████████ | 275/303 [00:37<00:03,  8.17it/s] 91%|█████████ | 276/303 [00:37<00:03,  8.18it/s] 91%|█████████▏| 277/303 [00:37<00:03,  8.17it/s] 92%|█████████▏| 278/303 [00:37<00:03,  8.17it/s] 92%|█████████▏| 279/303 [00:37<00:02,  8.17it/s] 92%|█████████▏| 280/303 [00:37<00:02,  8.17it/s] 93%|█████████▎| 281/303 [00:38<00:02,  8.16it/s] 93%|█████████▎| 282/303 [00:38<00:02,  8.18it/s] 93%|█████████▎| 283/303 [00:38<00:02,  8.19it/s] 94%|█████████▎| 284/303 [00:38<00:02,  8.19it/s] 94%|█████████▍| 285/303 [00:38<00:02,  8.17it/s] 94%|█████████▍| 286/303 [00:38<00:02,  8.18it/s] 95%|█████████▍| 287/303 [00:38<00:01,  8.18it/s] 95%|█████████▌| 288/303 [00:38<00:01,  8.19it/s] 95%|█████████▌| 289/303 [00:39<00:01,  8.18it/s] 96%|█████████▌| 290/303 [00:39<00:01,  8.19it/s] 96%|█████████▌| 291/303 [00:39<00:01,  8.18it/s] 96%|█████████▋| 292/303 [00:39<00:01,  8.20it/s] 97%|█████████▋| 293/303 [00:39<00:01,  8.21it/s] 97%|█████████▋| 294/303 [00:39<00:01,  8.20it/s] 97%|█████████▋| 295/303 [00:39<00:00,  8.19it/s] 98%|█████████▊| 296/303 [00:39<00:00,  8.19it/s] 98%|█████████▊| 297/303 [00:39<00:00,  8.17it/s] 98%|█████████▊| 298/303 [00:40<00:00,  8.20it/s] 99%|█████████▊| 299/303 [00:40<00:00,  8.21it/s] 99%|█████████▉| 300/303 [00:40<00:00,  8.21it/s] 99%|█████████▉| 301/303 [00:40<00:00,  8.22it/s]100%|█████████▉| 302/303 [00:40<00:00,  8.21it/s]100%|██████████| 303/303 [00:40<00:00,  8.21it/s]100%|██████████| 303/303 [00:40<00:00,  7.42it/s]
=> result
* total: 30,300
* correct: 22,919
* accuracy: 75.6%
* error: 24.4%
* macro_f1: 74.6%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft food101 3 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/food101/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/food101/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:Food101
Loading dataset: Food101
Reading split from /shared/s2/lab01/dataset/clip/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/food-101/split_fewshot_taesup/shot_16-seed_3.pkl
1616 20200 30300
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  101
# train_x  1,616
# val      20,200
# test     30,300
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/303 [00:00<?, ?it/s]  0%|          | 1/303 [00:03<18:53,  3.75s/it]  1%|          | 2/303 [00:03<08:07,  1.62s/it]  1%|          | 3/303 [00:03<04:40,  1.07it/s]  1%|▏         | 4/303 [00:04<03:03,  1.63it/s]  2%|▏         | 5/303 [00:04<02:09,  2.30it/s]  2%|▏         | 6/303 [00:04<01:37,  3.05it/s]  2%|▏         | 7/303 [00:04<01:16,  3.85it/s]  3%|▎         | 8/303 [00:04<01:03,  4.64it/s]  3%|▎         | 9/303 [00:04<00:54,  5.38it/s]  3%|▎         | 10/303 [00:04<00:48,  6.03it/s]  4%|▎         | 11/303 [00:04<00:44,  6.57it/s]  4%|▍         | 12/303 [00:05<00:41,  7.01it/s]  4%|▍         | 13/303 [00:05<00:39,  7.36it/s]  5%|▍         | 14/303 [00:05<00:37,  7.61it/s]  5%|▍         | 15/303 [00:05<00:36,  7.80it/s]  5%|▌         | 16/303 [00:05<00:36,  7.94it/s]  6%|▌         | 17/303 [00:05<00:35,  8.05it/s]  6%|▌         | 18/303 [00:05<00:35,  8.12it/s]  6%|▋         | 19/303 [00:05<00:34,  8.17it/s]  7%|▋         | 20/303 [00:06<00:34,  8.21it/s]  7%|▋         | 21/303 [00:06<00:34,  8.22it/s]  7%|▋         | 22/303 [00:06<00:34,  8.23it/s]  8%|▊         | 23/303 [00:06<00:33,  8.25it/s]  8%|▊         | 24/303 [00:06<00:33,  8.25it/s]  8%|▊         | 25/303 [00:06<00:33,  8.25it/s]  9%|▊         | 26/303 [00:06<00:33,  8.24it/s]  9%|▉         | 27/303 [00:06<00:33,  8.25it/s]  9%|▉         | 28/303 [00:07<00:33,  8.26it/s] 10%|▉         | 29/303 [00:07<00:33,  8.23it/s] 10%|▉         | 30/303 [00:07<00:33,  8.23it/s] 10%|█         | 31/303 [00:07<00:32,  8.24it/s] 11%|█         | 32/303 [00:07<00:32,  8.27it/s] 11%|█         | 33/303 [00:07<00:32,  8.27it/s] 11%|█         | 34/303 [00:07<00:32,  8.27it/s] 12%|█▏        | 35/303 [00:07<00:32,  8.28it/s] 12%|█▏        | 36/303 [00:07<00:32,  8.27it/s] 12%|█▏        | 37/303 [00:08<00:32,  8.26it/s] 13%|█▎        | 38/303 [00:08<00:32,  8.26it/s] 13%|█▎        | 39/303 [00:08<00:31,  8.27it/s] 13%|█▎        | 40/303 [00:08<00:31,  8.26it/s] 14%|█▎        | 41/303 [00:08<00:31,  8.26it/s] 14%|█▍        | 42/303 [00:08<00:32,  8.13it/s] 14%|█▍        | 43/303 [00:08<00:31,  8.19it/s] 15%|█▍        | 44/303 [00:08<00:31,  8.19it/s] 15%|█▍        | 45/303 [00:09<00:31,  8.20it/s] 15%|█▌        | 46/303 [00:09<00:31,  8.22it/s] 16%|█▌        | 47/303 [00:09<00:31,  8.23it/s] 16%|█▌        | 48/303 [00:09<00:30,  8.23it/s] 16%|█▌        | 49/303 [00:09<00:30,  8.24it/s] 17%|█▋        | 50/303 [00:09<00:30,  8.25it/s] 17%|█▋        | 51/303 [00:09<00:30,  8.25it/s] 17%|█▋        | 52/303 [00:09<00:30,  8.25it/s] 17%|█▋        | 53/303 [00:10<00:30,  8.26it/s] 18%|█▊        | 54/303 [00:10<00:30,  8.25it/s] 18%|█▊        | 55/303 [00:10<00:30,  8.22it/s] 18%|█▊        | 56/303 [00:10<00:30,  8.05it/s] 19%|█▉        | 57/303 [00:10<00:30,  8.12it/s] 19%|█▉        | 58/303 [00:10<00:30,  8.15it/s] 19%|█▉        | 59/303 [00:10<00:29,  8.19it/s] 20%|█▉        | 60/303 [00:10<00:29,  8.21it/s] 20%|██        | 61/303 [00:11<00:29,  8.23it/s] 20%|██        | 62/303 [00:11<00:29,  8.25it/s] 21%|██        | 63/303 [00:11<00:29,  8.25it/s] 21%|██        | 64/303 [00:11<00:29,  8.23it/s] 21%|██▏       | 65/303 [00:11<00:28,  8.24it/s] 22%|██▏       | 66/303 [00:11<00:28,  8.24it/s] 22%|██▏       | 67/303 [00:11<00:28,  8.25it/s] 22%|██▏       | 68/303 [00:11<00:28,  8.25it/s] 23%|██▎       | 69/303 [00:11<00:28,  8.25it/s] 23%|██▎       | 70/303 [00:12<00:28,  8.26it/s] 23%|██▎       | 71/303 [00:12<00:28,  8.07it/s] 24%|██▍       | 72/303 [00:12<00:28,  8.13it/s] 24%|██▍       | 73/303 [00:12<00:28,  8.17it/s] 24%|██▍       | 74/303 [00:12<00:28,  8.18it/s] 25%|██▍       | 75/303 [00:12<00:27,  8.21it/s] 25%|██▌       | 76/303 [00:12<00:27,  8.22it/s] 25%|██▌       | 77/303 [00:12<00:27,  8.24it/s] 26%|██▌       | 78/303 [00:13<00:27,  8.23it/s] 26%|██▌       | 79/303 [00:13<00:27,  8.23it/s] 26%|██▋       | 80/303 [00:13<00:27,  8.23it/s] 27%|██▋       | 81/303 [00:13<00:26,  8.24it/s] 27%|██▋       | 82/303 [00:13<00:26,  8.24it/s] 27%|██▋       | 83/303 [00:13<00:26,  8.25it/s] 28%|██▊       | 84/303 [00:13<00:26,  8.25it/s] 28%|██▊       | 85/303 [00:13<00:26,  8.24it/s] 28%|██▊       | 86/303 [00:14<00:26,  8.06it/s] 29%|██▊       | 87/303 [00:14<00:26,  8.13it/s] 29%|██▉       | 88/303 [00:14<00:26,  8.16it/s] 29%|██▉       | 89/303 [00:14<00:26,  8.17it/s] 30%|██▉       | 90/303 [00:14<00:26,  8.19it/s] 30%|███       | 91/303 [00:14<00:25,  8.21it/s] 30%|███       | 92/303 [00:14<00:25,  8.22it/s] 31%|███       | 93/303 [00:14<00:25,  8.23it/s] 31%|███       | 94/303 [00:15<00:25,  8.22it/s] 31%|███▏      | 95/303 [00:15<00:25,  8.23it/s] 32%|███▏      | 96/303 [00:15<00:25,  8.22it/s] 32%|███▏      | 97/303 [00:15<00:25,  8.22it/s] 32%|███▏      | 98/303 [00:15<00:24,  8.22it/s] 33%|███▎      | 99/303 [00:15<00:24,  8.23it/s] 33%|███▎      | 100/303 [00:15<00:24,  8.22it/s] 33%|███▎      | 101/303 [00:15<00:25,  8.08it/s] 34%|███▎      | 102/303 [00:16<00:24,  8.13it/s] 34%|███▍      | 103/303 [00:16<00:24,  8.16it/s] 34%|███▍      | 104/303 [00:16<00:24,  8.19it/s] 35%|███▍      | 105/303 [00:16<00:24,  8.20it/s] 35%|███▍      | 106/303 [00:16<00:23,  8.22it/s] 35%|███▌      | 107/303 [00:16<00:23,  8.23it/s] 36%|███▌      | 108/303 [00:16<00:23,  8.23it/s] 36%|███▌      | 109/303 [00:16<00:23,  8.23it/s] 36%|███▋      | 110/303 [00:16<00:23,  8.22it/s] 37%|███▋      | 111/303 [00:17<00:23,  8.23it/s] 37%|███▋      | 112/303 [00:17<00:23,  8.21it/s] 37%|███▋      | 113/303 [00:17<00:23,  8.22it/s] 38%|███▊      | 114/303 [00:17<00:22,  8.23it/s] 38%|███▊      | 115/303 [00:17<00:22,  8.24it/s] 38%|███▊      | 116/303 [00:17<00:23,  8.09it/s] 39%|███▊      | 117/303 [00:17<00:22,  8.14it/s] 39%|███▉      | 118/303 [00:17<00:22,  8.17it/s] 39%|███▉      | 119/303 [00:18<00:22,  8.19it/s] 40%|███▉      | 120/303 [00:18<00:22,  8.19it/s] 40%|███▉      | 121/303 [00:18<00:22,  8.21it/s] 40%|████      | 122/303 [00:18<00:22,  8.20it/s] 41%|████      | 123/303 [00:18<00:21,  8.21it/s] 41%|████      | 124/303 [00:18<00:21,  8.21it/s] 41%|████▏     | 125/303 [00:18<00:21,  8.22it/s] 42%|████▏     | 126/303 [00:18<00:21,  8.22it/s] 42%|████▏     | 127/303 [00:19<00:21,  8.22it/s] 42%|████▏     | 128/303 [00:19<00:21,  8.21it/s] 43%|████▎     | 129/303 [00:19<00:21,  8.22it/s] 43%|████▎     | 130/303 [00:19<00:21,  8.18it/s] 43%|████▎     | 131/303 [00:19<00:20,  8.20it/s] 44%|████▎     | 132/303 [00:19<00:20,  8.20it/s] 44%|████▍     | 133/303 [00:19<00:20,  8.22it/s] 44%|████▍     | 134/303 [00:19<00:20,  8.22it/s] 45%|████▍     | 135/303 [00:20<00:20,  8.23it/s] 45%|████▍     | 136/303 [00:20<00:20,  8.22it/s] 45%|████▌     | 137/303 [00:20<00:20,  8.21it/s] 46%|████▌     | 138/303 [00:20<00:20,  8.21it/s] 46%|████▌     | 139/303 [00:20<00:19,  8.21it/s] 46%|████▌     | 140/303 [00:20<00:19,  8.21it/s] 47%|████▋     | 141/303 [00:20<00:19,  8.22it/s] 47%|████▋     | 142/303 [00:20<00:19,  8.23it/s] 47%|████▋     | 143/303 [00:21<00:19,  8.21it/s] 48%|████▊     | 144/303 [00:21<00:19,  8.20it/s] 48%|████▊     | 145/303 [00:21<00:19,  8.21it/s] 48%|████▊     | 146/303 [00:21<00:19,  8.20it/s] 49%|████▊     | 147/303 [00:21<00:18,  8.21it/s] 49%|████▉     | 148/303 [00:21<00:18,  8.22it/s] 49%|████▉     | 149/303 [00:21<00:18,  8.23it/s] 50%|████▉     | 150/303 [00:21<00:18,  8.22it/s] 50%|████▉     | 151/303 [00:21<00:18,  8.21it/s] 50%|█████     | 152/303 [00:22<00:18,  8.20it/s] 50%|█████     | 153/303 [00:22<00:18,  8.02it/s] 51%|█████     | 154/303 [00:22<00:18,  8.08it/s] 51%|█████     | 155/303 [00:22<00:18,  8.11it/s] 51%|█████▏    | 156/303 [00:22<00:18,  8.14it/s] 52%|█████▏    | 157/303 [00:22<00:17,  8.16it/s] 52%|█████▏    | 158/303 [00:22<00:17,  8.17it/s] 52%|█████▏    | 159/303 [00:22<00:17,  8.18it/s] 53%|█████▎    | 160/303 [00:23<00:17,  8.18it/s] 53%|█████▎    | 161/303 [00:23<00:17,  8.20it/s] 53%|█████▎    | 162/303 [00:23<00:17,  8.20it/s] 54%|█████▍    | 163/303 [00:23<00:17,  8.21it/s] 54%|█████▍    | 164/303 [00:23<00:16,  8.21it/s] 54%|█████▍    | 165/303 [00:23<00:16,  8.22it/s] 55%|█████▍    | 166/303 [00:23<00:16,  8.21it/s] 55%|█████▌    | 167/303 [00:23<00:16,  8.21it/s] 55%|█████▌    | 168/303 [00:24<00:16,  8.21it/s] 56%|█████▌    | 169/303 [00:24<00:16,  8.20it/s] 56%|█████▌    | 170/303 [00:24<00:16,  8.21it/s] 56%|█████▋    | 171/303 [00:24<00:16,  8.21it/s] 57%|█████▋    | 172/303 [00:24<00:16,  8.17it/s] 57%|█████▋    | 173/303 [00:24<00:15,  8.18it/s] 57%|█████▋    | 174/303 [00:24<00:15,  8.19it/s] 58%|█████▊    | 175/303 [00:24<00:15,  8.19it/s] 58%|█████▊    | 176/303 [00:25<00:15,  8.19it/s] 58%|█████▊    | 177/303 [00:25<00:15,  8.19it/s] 59%|█████▊    | 178/303 [00:25<00:15,  8.19it/s] 59%|█████▉    | 179/303 [00:25<00:15,  8.19it/s] 59%|█████▉    | 180/303 [00:25<00:15,  8.19it/s] 60%|█████▉    | 181/303 [00:25<00:14,  8.20it/s] 60%|██████    | 182/303 [00:25<00:14,  8.19it/s] 60%|██████    | 183/303 [00:25<00:14,  8.20it/s] 61%|██████    | 184/303 [00:26<00:14,  8.20it/s] 61%|██████    | 185/303 [00:26<00:14,  8.20it/s] 61%|██████▏   | 186/303 [00:26<00:14,  8.20it/s] 62%|██████▏   | 187/303 [00:26<00:14,  8.19it/s] 62%|██████▏   | 188/303 [00:26<00:14,  8.19it/s] 62%|██████▏   | 189/303 [00:26<00:13,  8.20it/s] 63%|██████▎   | 190/303 [00:26<00:13,  8.20it/s] 63%|██████▎   | 191/303 [00:26<00:13,  8.21it/s] 63%|██████▎   | 192/303 [00:27<00:13,  8.19it/s] 64%|██████▎   | 193/303 [00:27<00:13,  8.19it/s] 64%|██████▍   | 194/303 [00:27<00:13,  8.19it/s] 64%|██████▍   | 195/303 [00:27<00:13,  8.18it/s] 65%|██████▍   | 196/303 [00:27<00:13,  8.19it/s] 65%|██████▌   | 197/303 [00:27<00:12,  8.19it/s] 65%|██████▌   | 198/303 [00:27<00:12,  8.18it/s] 66%|██████▌   | 199/303 [00:27<00:12,  8.19it/s] 66%|██████▌   | 200/303 [00:27<00:12,  8.20it/s] 66%|██████▋   | 201/303 [00:28<00:12,  8.21it/s] 67%|██████▋   | 202/303 [00:28<00:12,  8.22it/s] 67%|██████▋   | 203/303 [00:28<00:12,  8.20it/s] 67%|██████▋   | 204/303 [00:28<00:12,  8.20it/s] 68%|██████▊   | 205/303 [00:28<00:11,  8.20it/s] 68%|██████▊   | 206/303 [00:28<00:11,  8.18it/s] 68%|██████▊   | 207/303 [00:28<00:11,  8.19it/s] 69%|██████▊   | 208/303 [00:28<00:11,  8.18it/s] 69%|██████▉   | 209/303 [00:29<00:11,  8.19it/s] 69%|██████▉   | 210/303 [00:29<00:11,  8.20it/s] 70%|██████▉   | 211/303 [00:29<00:11,  8.20it/s] 70%|██████▉   | 212/303 [00:29<00:11,  8.20it/s] 70%|███████   | 213/303 [00:29<00:10,  8.21it/s] 71%|███████   | 214/303 [00:29<00:10,  8.21it/s] 71%|███████   | 215/303 [00:29<00:10,  8.21it/s] 71%|███████▏  | 216/303 [00:29<00:10,  8.20it/s] 72%|███████▏  | 217/303 [00:30<00:10,  8.19it/s] 72%|███████▏  | 218/303 [00:30<00:10,  8.19it/s] 72%|███████▏  | 219/303 [00:30<00:10,  8.19it/s] 73%|███████▎  | 220/303 [00:30<00:10,  8.20it/s] 73%|███████▎  | 221/303 [00:30<00:09,  8.20it/s] 73%|███████▎  | 222/303 [00:30<00:09,  8.19it/s] 74%|███████▎  | 223/303 [00:30<00:09,  8.17it/s] 74%|███████▍  | 224/303 [00:30<00:09,  8.19it/s] 74%|███████▍  | 225/303 [00:31<00:09,  8.18it/s] 75%|███████▍  | 226/303 [00:31<00:09,  8.19it/s] 75%|███████▍  | 227/303 [00:31<00:09,  8.19it/s] 75%|███████▌  | 228/303 [00:31<00:09,  8.19it/s] 76%|███████▌  | 229/303 [00:31<00:09,  8.19it/s] 76%|███████▌  | 230/303 [00:31<00:08,  8.17it/s] 76%|███████▌  | 231/303 [00:31<00:08,  8.18it/s] 77%|███████▋  | 232/303 [00:31<00:08,  8.18it/s] 77%|███████▋  | 233/303 [00:32<00:08,  8.18it/s] 77%|███████▋  | 234/303 [00:32<00:08,  8.18it/s] 78%|███████▊  | 235/303 [00:32<00:08,  8.19it/s] 78%|███████▊  | 236/303 [00:32<00:08,  8.17it/s] 78%|███████▊  | 237/303 [00:32<00:08,  8.18it/s] 79%|███████▊  | 238/303 [00:32<00:07,  8.18it/s] 79%|███████▉  | 239/303 [00:32<00:07,  8.18it/s] 79%|███████▉  | 240/303 [00:32<00:07,  8.17it/s] 80%|███████▉  | 241/303 [00:32<00:07,  8.17it/s] 80%|███████▉  | 242/303 [00:33<00:07,  8.17it/s] 80%|████████  | 243/303 [00:33<00:07,  8.18it/s] 81%|████████  | 244/303 [00:33<00:07,  8.16it/s] 81%|████████  | 245/303 [00:33<00:07,  8.16it/s] 81%|████████  | 246/303 [00:33<00:06,  8.17it/s] 82%|████████▏ | 247/303 [00:33<00:06,  8.16it/s] 82%|████████▏ | 248/303 [00:33<00:06,  8.17it/s] 82%|████████▏ | 249/303 [00:33<00:06,  8.16it/s] 83%|████████▎ | 250/303 [00:34<00:06,  8.17it/s] 83%|████████▎ | 251/303 [00:34<00:06,  8.17it/s] 83%|████████▎ | 252/303 [00:34<00:06,  8.17it/s] 83%|████████▎ | 253/303 [00:34<00:06,  8.16it/s] 84%|████████▍ | 254/303 [00:34<00:05,  8.17it/s] 84%|████████▍ | 255/303 [00:34<00:05,  8.17it/s] 84%|████████▍ | 256/303 [00:34<00:05,  8.17it/s] 85%|████████▍ | 257/303 [00:34<00:05,  8.16it/s] 85%|████████▌ | 258/303 [00:35<00:05,  8.17it/s] 85%|████████▌ | 259/303 [00:35<00:05,  8.17it/s] 86%|████████▌ | 260/303 [00:35<00:05,  8.16it/s] 86%|████████▌ | 261/303 [00:35<00:05,  8.16it/s] 86%|████████▋ | 262/303 [00:35<00:05,  8.16it/s] 87%|████████▋ | 263/303 [00:35<00:04,  8.16it/s] 87%|████████▋ | 264/303 [00:35<00:04,  8.15it/s] 87%|████████▋ | 265/303 [00:35<00:04,  8.15it/s] 88%|████████▊ | 266/303 [00:36<00:04,  8.16it/s] 88%|████████▊ | 267/303 [00:36<00:04,  8.16it/s] 88%|████████▊ | 268/303 [00:36<00:04,  8.16it/s] 89%|████████▉ | 269/303 [00:36<00:04,  8.15it/s] 89%|████████▉ | 270/303 [00:36<00:04,  8.16it/s] 89%|████████▉ | 271/303 [00:36<00:03,  8.15it/s] 90%|████████▉ | 272/303 [00:36<00:03,  8.14it/s] 90%|█████████ | 273/303 [00:36<00:03,  8.14it/s] 90%|█████████ | 274/303 [00:37<00:03,  8.15it/s] 91%|█████████ | 275/303 [00:37<00:03,  8.15it/s] 91%|█████████ | 276/303 [00:37<00:03,  8.16it/s] 91%|█████████▏| 277/303 [00:37<00:03,  8.17it/s] 92%|█████████▏| 278/303 [00:37<00:03,  8.17it/s] 92%|█████████▏| 279/303 [00:37<00:02,  8.16it/s] 92%|█████████▏| 280/303 [00:37<00:02,  8.16it/s] 93%|█████████▎| 281/303 [00:37<00:02,  8.15it/s] 93%|█████████▎| 282/303 [00:38<00:02,  8.15it/s] 93%|█████████▎| 283/303 [00:38<00:02,  8.16it/s] 94%|█████████▎| 284/303 [00:38<00:02,  8.16it/s] 94%|█████████▍| 285/303 [00:38<00:02,  8.16it/s] 94%|█████████▍| 286/303 [00:38<00:02,  8.16it/s] 95%|█████████▍| 287/303 [00:38<00:01,  8.15it/s] 95%|█████████▌| 288/303 [00:38<00:01,  8.14it/s] 95%|█████████▌| 289/303 [00:38<00:01,  8.13it/s] 96%|█████████▌| 290/303 [00:38<00:01,  8.15it/s] 96%|█████████▌| 291/303 [00:39<00:01,  8.18it/s] 96%|█████████▋| 292/303 [00:39<00:01,  8.21it/s] 97%|█████████▋| 293/303 [00:39<00:01,  8.23it/s] 97%|█████████▋| 294/303 [00:39<00:01,  8.24it/s] 97%|█████████▋| 295/303 [00:39<00:00,  8.23it/s] 98%|█████████▊| 296/303 [00:39<00:00,  8.22it/s] 98%|█████████▊| 297/303 [00:39<00:00,  8.20it/s] 98%|█████████▊| 298/303 [00:39<00:00,  8.14it/s] 99%|█████████▊| 299/303 [00:40<00:00,  8.16it/s] 99%|█████████▉| 300/303 [00:40<00:00,  8.17it/s] 99%|█████████▉| 301/303 [00:40<00:00,  8.19it/s]100%|█████████▉| 302/303 [00:40<00:00,  8.19it/s]100%|██████████| 303/303 [00:40<00:00,  8.20it/s]100%|██████████| 303/303 [00:40<00:00,  7.45it/s]
=> result
* total: 30,300
* correct: 17,571
* accuracy: 58.0%
* error: 42.0%
* macro_f1: 53.0%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft ucf101 1 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/ucf101/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/ucf101/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:UCF101
Loading dataset: UCF101
Reading split from /shared/s2/lab01/dataset/clip/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/ucf101/split_fewshot_taesup/shot_16-seed_1.pkl
1616 1898 3783
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  101
# train_x  1,616
# val      1,898
# test     3,783
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/38 [00:00<?, ?it/s]  3%|▎         | 1/38 [00:03<02:09,  3.50s/it]  5%|▌         | 2/38 [00:03<00:54,  1.52s/it]  8%|▊         | 3/38 [00:03<00:30,  1.14it/s] 11%|█         | 4/38 [00:03<00:19,  1.73it/s] 13%|█▎        | 5/38 [00:03<00:13,  2.42it/s] 16%|█▌        | 6/38 [00:04<00:10,  3.19it/s] 18%|█▊        | 7/38 [00:04<00:07,  3.99it/s] 21%|██        | 8/38 [00:04<00:06,  4.78it/s] 24%|██▎       | 9/38 [00:04<00:05,  5.51it/s] 26%|██▋       | 10/38 [00:04<00:04,  6.14it/s] 29%|██▉       | 11/38 [00:04<00:04,  6.67it/s] 32%|███▏      | 12/38 [00:04<00:03,  7.10it/s] 34%|███▍      | 13/38 [00:04<00:03,  7.42it/s] 37%|███▋      | 14/38 [00:05<00:03,  7.66it/s] 39%|███▉      | 15/38 [00:05<00:02,  7.84it/s] 42%|████▏     | 16/38 [00:05<00:02,  7.97it/s] 45%|████▍     | 17/38 [00:05<00:02,  8.07it/s] 47%|████▋     | 18/38 [00:05<00:02,  8.14it/s] 50%|█████     | 19/38 [00:05<00:02,  8.19it/s] 53%|█████▎    | 20/38 [00:05<00:02,  8.23it/s] 55%|█████▌    | 21/38 [00:05<00:02,  8.26it/s] 58%|█████▊    | 22/38 [00:06<00:01,  8.28it/s] 61%|██████    | 23/38 [00:06<00:01,  8.28it/s] 63%|██████▎   | 24/38 [00:06<00:01,  8.29it/s] 66%|██████▌   | 25/38 [00:06<00:01,  8.29it/s] 68%|██████▊   | 26/38 [00:06<00:01,  8.29it/s] 71%|███████   | 27/38 [00:06<00:01,  8.29it/s] 74%|███████▎  | 28/38 [00:06<00:01,  8.30it/s] 76%|███████▋  | 29/38 [00:06<00:01,  8.30it/s] 79%|███████▉  | 30/38 [00:06<00:00,  8.31it/s] 82%|████████▏ | 31/38 [00:07<00:00,  8.31it/s] 84%|████████▍ | 32/38 [00:07<00:00,  8.31it/s] 87%|████████▋ | 33/38 [00:07<00:00,  8.29it/s] 89%|████████▉ | 34/38 [00:07<00:00,  8.30it/s] 92%|█████████▏| 35/38 [00:07<00:00,  8.31it/s] 95%|█████████▍| 36/38 [00:07<00:00,  8.32it/s] 97%|█████████▋| 37/38 [00:07<00:00,  8.32it/s]100%|██████████| 38/38 [00:07<00:00,  8.67it/s]100%|██████████| 38/38 [00:08<00:00,  4.72it/s]
=> result
* total: 3,783
* correct: 1,840
* accuracy: 48.6%
* error: 51.4%
* macro_f1: 43.3%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft ucf101 2 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/ucf101/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/ucf101/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:UCF101
Loading dataset: UCF101
Reading split from /shared/s2/lab01/dataset/clip/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/ucf101/split_fewshot_taesup/shot_16-seed_2.pkl
1616 1898 3783
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  101
# train_x  1,616
# val      1,898
# test     3,783
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/38 [00:00<?, ?it/s]  3%|▎         | 1/38 [00:03<02:05,  3.39s/it]  5%|▌         | 2/38 [00:03<00:52,  1.47s/it]  8%|▊         | 3/38 [00:03<00:29,  1.17it/s] 11%|█         | 4/38 [00:03<00:19,  1.78it/s] 13%|█▎        | 5/38 [00:03<00:13,  2.48it/s] 16%|█▌        | 6/38 [00:03<00:09,  3.26it/s] 18%|█▊        | 7/38 [00:04<00:07,  4.07it/s] 21%|██        | 8/38 [00:04<00:06,  4.86it/s] 24%|██▎       | 9/38 [00:04<00:05,  5.55it/s] 26%|██▋       | 10/38 [00:04<00:04,  6.18it/s] 29%|██▉       | 11/38 [00:04<00:04,  6.70it/s] 32%|███▏      | 12/38 [00:04<00:03,  7.10it/s] 34%|███▍      | 13/38 [00:04<00:03,  7.42it/s] 37%|███▋      | 14/38 [00:04<00:03,  7.65it/s] 39%|███▉      | 15/38 [00:05<00:02,  7.84it/s] 42%|████▏     | 16/38 [00:05<00:02,  7.97it/s] 45%|████▍     | 17/38 [00:05<00:02,  8.05it/s] 47%|████▋     | 18/38 [00:05<00:02,  8.12it/s] 50%|█████     | 19/38 [00:05<00:02,  8.16it/s] 53%|█████▎    | 20/38 [00:05<00:02,  8.19it/s] 55%|█████▌    | 21/38 [00:05<00:02,  8.20it/s] 58%|█████▊    | 22/38 [00:05<00:01,  8.21it/s] 61%|██████    | 23/38 [00:06<00:01,  8.21it/s] 63%|██████▎   | 24/38 [00:06<00:01,  8.25it/s] 66%|██████▌   | 25/38 [00:06<00:01,  8.27it/s] 68%|██████▊   | 26/38 [00:06<00:01,  8.30it/s] 71%|███████   | 27/38 [00:06<00:01,  8.32it/s] 74%|███████▎  | 28/38 [00:06<00:01,  8.32it/s] 76%|███████▋  | 29/38 [00:06<00:01,  8.32it/s] 79%|███████▉  | 30/38 [00:06<00:00,  8.33it/s] 82%|████████▏ | 31/38 [00:07<00:00,  8.33it/s] 84%|████████▍ | 32/38 [00:07<00:00,  8.34it/s] 87%|████████▋ | 33/38 [00:07<00:00,  8.33it/s] 89%|████████▉ | 34/38 [00:07<00:00,  8.33it/s] 92%|█████████▏| 35/38 [00:07<00:00,  8.33it/s] 95%|█████████▍| 36/38 [00:07<00:00,  8.33it/s] 97%|█████████▋| 37/38 [00:07<00:00,  8.32it/s]100%|██████████| 38/38 [00:07<00:00,  8.70it/s]100%|██████████| 38/38 [00:07<00:00,  4.80it/s]
=> result
* total: 3,783
* correct: 1,957
* accuracy: 51.7%
* error: 48.3%
* macro_f1: 46.2%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft ucf101 3 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/ucf101/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/ucf101/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:UCF101
Loading dataset: UCF101
Reading split from /shared/s2/lab01/dataset/clip/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/ucf101/split_fewshot_taesup/shot_16-seed_3.pkl
1616 1898 3783
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  101
# train_x  1,616
# val      1,898
# test     3,783
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/38 [00:00<?, ?it/s]  3%|▎         | 1/38 [00:03<02:11,  3.56s/it]  5%|▌         | 2/38 [00:03<00:55,  1.54s/it]  8%|▊         | 3/38 [00:03<00:31,  1.12it/s] 11%|█         | 4/38 [00:03<00:19,  1.70it/s] 13%|█▎        | 5/38 [00:04<00:13,  2.39it/s] 16%|█▌        | 6/38 [00:04<00:10,  3.15it/s] 18%|█▊        | 7/38 [00:04<00:07,  3.95it/s] 21%|██        | 8/38 [00:04<00:06,  4.73it/s] 24%|██▎       | 9/38 [00:04<00:05,  5.47it/s] 26%|██▋       | 10/38 [00:04<00:04,  6.13it/s] 29%|██▉       | 11/38 [00:04<00:04,  6.63it/s] 32%|███▏      | 12/38 [00:04<00:03,  7.08it/s] 34%|███▍      | 13/38 [00:05<00:03,  7.43it/s] 37%|███▋      | 14/38 [00:05<00:03,  7.68it/s] 39%|███▉      | 15/38 [00:05<00:02,  7.85it/s] 42%|████▏     | 16/38 [00:05<00:02,  7.97it/s] 45%|████▍     | 17/38 [00:05<00:02,  8.06it/s] 47%|████▋     | 18/38 [00:05<00:02,  8.13it/s] 50%|█████     | 19/38 [00:05<00:02,  8.20it/s] 53%|█████▎    | 20/38 [00:05<00:02,  8.23it/s] 55%|█████▌    | 21/38 [00:05<00:02,  8.26it/s] 58%|█████▊    | 22/38 [00:06<00:01,  8.26it/s] 61%|██████    | 23/38 [00:06<00:01,  8.27it/s] 63%|██████▎   | 24/38 [00:06<00:01,  8.28it/s] 66%|██████▌   | 25/38 [00:06<00:01,  8.30it/s] 68%|██████▊   | 26/38 [00:06<00:01,  8.30it/s] 71%|███████   | 27/38 [00:06<00:01,  8.31it/s] 74%|███████▎  | 28/38 [00:06<00:01,  8.32it/s] 76%|███████▋  | 29/38 [00:06<00:01,  8.33it/s] 79%|███████▉  | 30/38 [00:07<00:00,  8.33it/s] 82%|████████▏ | 31/38 [00:07<00:00,  8.33it/s] 84%|████████▍ | 32/38 [00:07<00:00,  8.33it/s] 87%|████████▋ | 33/38 [00:07<00:00,  8.33it/s] 89%|████████▉ | 34/38 [00:07<00:00,  8.33it/s] 92%|█████████▏| 35/38 [00:07<00:00,  8.33it/s] 95%|█████████▍| 36/38 [00:07<00:00,  8.33it/s] 97%|█████████▋| 37/38 [00:07<00:00,  8.33it/s]100%|██████████| 38/38 [00:08<00:00,  8.70it/s]100%|██████████| 38/38 [00:08<00:00,  4.69it/s]
=> result
* total: 3,783
* correct: 1,775
* accuracy: 46.9%
* error: 53.1%
* macro_f1: 42.7%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft caltech101 1 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/caltech101/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/caltech101/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:Caltech101
Loading dataset: Caltech101
Reading split from /shared/s2/lab01/dataset/clip/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/caltech-101/split_fewshot_taesup/shot_16-seed_1.pkl
1600 1649 2465
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  1,600
# val      1,649
# test     2,465
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:03<01:28,  3.67s/it]  8%|▊         | 2/25 [00:03<00:36,  1.59s/it] 12%|█▏        | 3/25 [00:03<00:20,  1.09it/s] 16%|█▌        | 4/25 [00:04<00:12,  1.66it/s] 20%|██        | 5/25 [00:04<00:08,  2.34it/s] 24%|██▍       | 6/25 [00:04<00:06,  3.09it/s] 28%|██▊       | 7/25 [00:04<00:04,  3.89it/s] 32%|███▏      | 8/25 [00:04<00:03,  4.69it/s] 36%|███▌      | 9/25 [00:04<00:02,  5.43it/s] 40%|████      | 10/25 [00:04<00:02,  6.09it/s] 44%|████▍     | 11/25 [00:04<00:02,  6.63it/s] 48%|████▊     | 12/25 [00:04<00:01,  7.07it/s] 52%|█████▏    | 13/25 [00:05<00:01,  7.42it/s] 56%|█████▌    | 14/25 [00:05<00:01,  7.67it/s] 60%|██████    | 15/25 [00:05<00:01,  7.87it/s] 64%|██████▍   | 16/25 [00:05<00:01,  8.01it/s] 68%|██████▊   | 17/25 [00:05<00:00,  8.11it/s] 72%|███████▏  | 18/25 [00:05<00:00,  8.19it/s] 76%|███████▌  | 19/25 [00:05<00:00,  8.24it/s] 80%|████████  | 20/25 [00:05<00:00,  8.28it/s] 84%|████████▍ | 21/25 [00:06<00:00,  8.31it/s] 88%|████████▊ | 22/25 [00:06<00:00,  8.32it/s] 92%|█████████▏| 23/25 [00:06<00:00,  8.33it/s] 96%|█████████▌| 24/25 [00:06<00:00,  8.33it/s]100%|██████████| 25/25 [00:06<00:00,  3.78it/s]
=> result
* total: 2,465
* correct: 1,446
* accuracy: 58.7%
* error: 41.3%
* macro_f1: 60.3%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft caltech101 2 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/caltech101/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/caltech101/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:Caltech101
Loading dataset: Caltech101
Reading split from /shared/s2/lab01/dataset/clip/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/caltech-101/split_fewshot_taesup/shot_16-seed_2.pkl
1600 1649 2465
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  1,600
# val      1,649
# test     2,465
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:03<01:26,  3.62s/it]  8%|▊         | 2/25 [00:03<00:35,  1.56s/it] 12%|█▏        | 3/25 [00:03<00:19,  1.11it/s] 16%|█▌        | 4/25 [00:03<00:12,  1.68it/s] 20%|██        | 5/25 [00:04<00:08,  2.36it/s] 24%|██▍       | 6/25 [00:04<00:06,  3.12it/s] 28%|██▊       | 7/25 [00:04<00:04,  3.92it/s] 32%|███▏      | 8/25 [00:04<00:03,  4.71it/s] 36%|███▌      | 9/25 [00:04<00:02,  5.45it/s] 40%|████      | 10/25 [00:04<00:02,  6.11it/s] 44%|████▍     | 11/25 [00:04<00:02,  6.66it/s] 48%|████▊     | 12/25 [00:04<00:01,  7.11it/s] 52%|█████▏    | 13/25 [00:05<00:01,  7.44it/s] 56%|█████▌    | 14/25 [00:05<00:01,  7.70it/s] 60%|██████    | 15/25 [00:05<00:01,  7.89it/s] 64%|██████▍   | 16/25 [00:05<00:01,  8.02it/s] 68%|██████▊   | 17/25 [00:05<00:00,  8.13it/s] 72%|███████▏  | 18/25 [00:05<00:00,  8.21it/s] 76%|███████▌  | 19/25 [00:05<00:00,  8.26it/s] 80%|████████  | 20/25 [00:05<00:00,  8.28it/s] 84%|████████▍ | 21/25 [00:06<00:00,  8.30it/s] 88%|████████▊ | 22/25 [00:06<00:00,  8.31it/s] 92%|█████████▏| 23/25 [00:06<00:00,  8.33it/s] 96%|█████████▌| 24/25 [00:06<00:00,  8.33it/s]100%|██████████| 25/25 [00:06<00:00,  3.81it/s]
=> result
* total: 2,465
* correct: 1,685
* accuracy: 68.4%
* error: 31.6%
* macro_f1: 67.8%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft caltech101 3 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/caltech101/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/caltech101/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:Caltech101
Loading dataset: Caltech101
Reading split from /shared/s2/lab01/dataset/clip/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/caltech-101/split_fewshot_taesup/shot_16-seed_3.pkl
1600 1649 2465
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  1,600
# val      1,649
# test     2,465
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:03<01:26,  3.60s/it]  8%|▊         | 2/25 [00:03<00:35,  1.55s/it] 12%|█▏        | 3/25 [00:03<00:19,  1.11it/s] 16%|█▌        | 4/25 [00:03<00:12,  1.69it/s] 20%|██        | 5/25 [00:04<00:08,  2.37it/s] 24%|██▍       | 6/25 [00:04<00:06,  3.14it/s] 28%|██▊       | 7/25 [00:04<00:04,  3.94it/s] 32%|███▏      | 8/25 [00:04<00:03,  4.73it/s] 36%|███▌      | 9/25 [00:04<00:02,  5.39it/s] 40%|████      | 10/25 [00:04<00:02,  6.05it/s] 44%|████▍     | 11/25 [00:04<00:02,  6.61it/s] 48%|████▊     | 12/25 [00:04<00:01,  7.07it/s] 52%|█████▏    | 13/25 [00:05<00:01,  7.42it/s] 56%|█████▌    | 14/25 [00:05<00:01,  7.69it/s] 60%|██████    | 15/25 [00:05<00:01,  7.89it/s] 64%|██████▍   | 16/25 [00:05<00:01,  8.02it/s] 68%|██████▊   | 17/25 [00:05<00:00,  8.12it/s] 72%|███████▏  | 18/25 [00:05<00:00,  8.20it/s] 76%|███████▌  | 19/25 [00:05<00:00,  8.25it/s] 80%|████████  | 20/25 [00:05<00:00,  8.28it/s] 84%|████████▍ | 21/25 [00:06<00:00,  8.30it/s] 88%|████████▊ | 22/25 [00:06<00:00,  8.32it/s] 92%|█████████▏| 23/25 [00:06<00:00,  8.33it/s] 96%|█████████▌| 24/25 [00:06<00:00,  8.34it/s]100%|██████████| 25/25 [00:06<00:00,  3.82it/s]
=> result
* total: 2,465
* correct: 1,245
* accuracy: 50.5%
* error: 49.5%
* macro_f1: 47.5%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft sun397 1 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/sun397/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/sun397/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:SUN397
Loading dataset: SUN397
Reading split from /shared/s2/lab01/dataset/clip/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/sun397/split_fewshot_taesup/shot_16-seed_1.pkl
6352 3970 19850
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  397
# train_x  6,352
# val      3,970
# test     19,850
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/199 [00:00<?, ?it/s]  1%|          | 1/199 [00:06<20:55,  6.34s/it]  1%|          | 2/199 [00:06<08:55,  2.72s/it]  2%|▏         | 3/199 [00:07<06:16,  1.92s/it]  2%|▏         | 4/199 [00:07<04:00,  1.23s/it]  3%|▎         | 5/199 [00:07<02:45,  1.17it/s]  3%|▎         | 6/199 [00:08<02:00,  1.61it/s]  4%|▎         | 7/199 [00:08<01:31,  2.09it/s]  4%|▍         | 8/199 [00:08<01:13,  2.61it/s]  5%|▍         | 9/199 [00:09<02:00,  1.58it/s]  5%|▌         | 10/199 [00:09<01:33,  2.03it/s]  6%|▌         | 11/199 [00:13<04:18,  1.38s/it]  6%|▌         | 12/199 [00:13<03:09,  1.01s/it]  7%|▋         | 13/199 [00:13<02:21,  1.32it/s]  7%|▋         | 14/199 [00:13<01:47,  1.72it/s]  8%|▊         | 15/199 [00:13<01:24,  2.17it/s]  8%|▊         | 16/199 [00:14<01:08,  2.67it/s]  9%|▊         | 17/199 [00:14<00:57,  3.17it/s]  9%|▉         | 18/199 [00:14<00:49,  3.64it/s] 10%|▉         | 19/199 [00:16<02:02,  1.46it/s] 10%|█         | 20/199 [00:16<01:35,  1.88it/s] 11%|█         | 21/199 [00:16<01:15,  2.35it/s] 11%|█         | 22/199 [00:16<01:02,  2.85it/s] 12%|█▏        | 23/199 [00:16<00:52,  3.35it/s] 12%|█▏        | 24/199 [00:16<00:45,  3.81it/s] 13%|█▎        | 25/199 [00:17<00:41,  4.22it/s] 13%|█▎        | 26/199 [00:17<00:37,  4.56it/s] 14%|█▎        | 27/199 [00:21<03:46,  1.32s/it] 14%|█▍        | 28/199 [00:21<02:47,  1.02it/s] 15%|█▍        | 29/199 [00:21<02:05,  1.36it/s] 15%|█▌        | 30/199 [00:21<01:36,  1.76it/s] 16%|█▌        | 31/199 [00:21<01:15,  2.21it/s] 16%|█▌        | 32/199 [00:22<01:01,  2.70it/s] 17%|█▋        | 33/199 [00:22<00:51,  3.21it/s] 17%|█▋        | 34/199 [00:22<00:44,  3.69it/s] 18%|█▊        | 35/199 [00:24<02:19,  1.18it/s] 18%|█▊        | 36/199 [00:24<01:45,  1.54it/s] 19%|█▊        | 37/199 [00:24<01:22,  1.97it/s] 19%|█▉        | 38/199 [00:25<01:05,  2.45it/s] 20%|█▉        | 39/199 [00:25<00:54,  2.95it/s] 20%|██        | 40/199 [00:25<00:46,  3.44it/s] 21%|██        | 41/199 [00:25<00:40,  3.88it/s] 21%|██        | 42/199 [00:25<00:36,  4.28it/s] 22%|██▏       | 43/199 [00:27<02:03,  1.26it/s] 22%|██▏       | 44/199 [00:28<01:34,  1.64it/s] 23%|██▎       | 45/199 [00:28<01:13,  2.08it/s] 23%|██▎       | 46/199 [00:28<00:59,  2.57it/s] 24%|██▎       | 47/199 [00:28<00:49,  3.06it/s] 24%|██▍       | 48/199 [00:28<00:42,  3.55it/s] 25%|██▍       | 49/199 [00:28<00:37,  3.99it/s] 25%|██▌       | 50/199 [00:29<00:34,  4.36it/s] 26%|██▌       | 51/199 [00:32<02:31,  1.03s/it] 26%|██▌       | 52/199 [00:32<01:53,  1.29it/s] 27%|██▋       | 53/199 [00:32<01:26,  1.68it/s] 27%|██▋       | 54/199 [00:32<01:08,  2.13it/s] 28%|██▊       | 55/199 [00:33<01:17,  1.86it/s] 28%|██▊       | 56/199 [00:33<01:01,  2.32it/s] 29%|██▊       | 57/199 [00:33<00:50,  2.82it/s] 29%|██▉       | 58/199 [00:33<00:42,  3.31it/s] 30%|██▉       | 59/199 [00:34<00:52,  2.68it/s] 30%|███       | 60/199 [00:34<00:43,  3.17it/s] 31%|███       | 61/199 [00:34<00:37,  3.65it/s] 31%|███       | 62/199 [00:34<00:33,  4.08it/s] 32%|███▏      | 63/199 [00:36<01:11,  1.91it/s] 32%|███▏      | 64/199 [00:36<00:56,  2.39it/s] 33%|███▎      | 65/199 [00:36<00:46,  2.88it/s] 33%|███▎      | 66/199 [00:38<01:43,  1.28it/s] 34%|███▎      | 67/199 [00:38<01:19,  1.67it/s] 34%|███▍      | 68/199 [00:38<01:11,  1.83it/s] 35%|███▍      | 69/199 [00:38<00:56,  2.29it/s] 35%|███▌      | 70/199 [00:39<00:46,  2.79it/s] 36%|███▌      | 71/199 [00:39<00:39,  3.28it/s] 36%|███▌      | 72/199 [00:39<00:33,  3.74it/s] 37%|███▋      | 73/199 [00:39<00:30,  4.16it/s] 37%|███▋      | 74/199 [00:42<02:04,  1.00it/s] 38%|███▊      | 75/199 [00:44<02:30,  1.21s/it] 38%|███▊      | 76/199 [00:44<01:50,  1.11it/s] 39%|███▊      | 77/199 [00:44<01:23,  1.46it/s] 39%|███▉      | 78/199 [00:44<01:04,  1.88it/s] 40%|███▉      | 79/199 [00:44<00:51,  2.35it/s] 40%|████      | 80/199 [00:45<00:41,  2.85it/s] 41%|████      | 81/199 [00:45<00:35,  3.34it/s] 41%|████      | 82/199 [00:45<00:49,  2.38it/s] 42%|████▏     | 83/199 [00:49<02:50,  1.47s/it] 42%|████▏     | 84/199 [00:50<02:04,  1.08s/it] 43%|████▎     | 85/199 [00:50<01:32,  1.24it/s] 43%|████▎     | 86/199 [00:50<01:10,  1.61it/s] 44%|████▎     | 87/199 [00:50<00:54,  2.05it/s] 44%|████▍     | 88/199 [00:50<00:43,  2.54it/s] 45%|████▍     | 89/199 [00:50<00:36,  3.04it/s] 45%|████▌     | 90/199 [00:51<00:37,  2.87it/s] 46%|████▌     | 91/199 [00:52<01:10,  1.53it/s] 46%|████▌     | 92/199 [00:52<00:54,  1.96it/s] 47%|████▋     | 93/199 [00:53<00:43,  2.43it/s] 47%|████▋     | 94/199 [00:53<00:35,  2.93it/s] 48%|████▊     | 95/199 [00:53<00:30,  3.43it/s] 48%|████▊     | 96/199 [00:53<00:26,  3.89it/s] 49%|████▊     | 97/199 [00:53<00:23,  4.28it/s] 49%|████▉     | 98/199 [00:54<00:32,  3.09it/s] 50%|████▉     | 99/199 [00:57<01:52,  1.13s/it] 50%|█████     | 100/199 [00:57<01:23,  1.19it/s] 51%|█████     | 101/199 [00:57<01:02,  1.56it/s] 51%|█████▏    | 102/199 [00:57<00:48,  1.99it/s] 52%|█████▏    | 103/199 [00:57<00:38,  2.47it/s] 52%|█████▏    | 104/199 [00:58<00:32,  2.97it/s] 53%|█████▎    | 105/199 [00:58<00:27,  3.45it/s] 53%|█████▎    | 106/199 [00:59<00:44,  2.11it/s] 54%|█████▍    | 107/199 [01:00<01:07,  1.37it/s] 54%|█████▍    | 108/199 [01:00<00:51,  1.77it/s] 55%|█████▍    | 109/199 [01:00<00:40,  2.23it/s] 55%|█████▌    | 110/199 [01:01<00:32,  2.72it/s] 56%|█████▌    | 111/199 [01:01<00:27,  3.22it/s] 56%|█████▋    | 112/199 [01:01<00:23,  3.69it/s] 57%|█████▋    | 113/199 [01:01<00:20,  4.12it/s] 57%|█████▋    | 114/199 [01:03<00:52,  1.62it/s] 58%|█████▊    | 115/199 [01:04<01:00,  1.38it/s] 58%|█████▊    | 116/199 [01:04<00:46,  1.78it/s] 59%|█████▉    | 117/199 [01:04<00:36,  2.24it/s] 59%|█████▉    | 118/199 [01:04<00:29,  2.73it/s] 60%|█████▉    | 119/199 [01:04<00:24,  3.23it/s] 60%|██████    | 120/199 [01:05<00:21,  3.70it/s] 61%|██████    | 121/199 [01:05<00:18,  4.12it/s] 61%|██████▏   | 122/199 [01:10<02:08,  1.67s/it] 62%|██████▏   | 123/199 [01:10<01:32,  1.22s/it] 62%|██████▏   | 124/199 [01:10<01:08,  1.10it/s] 63%|██████▎   | 125/199 [01:10<00:50,  1.45it/s] 63%|██████▎   | 126/199 [01:10<00:39,  1.87it/s] 64%|██████▍   | 127/199 [01:11<00:30,  2.34it/s] 64%|██████▍   | 128/199 [01:11<00:25,  2.83it/s] 65%|██████▍   | 129/199 [01:11<00:21,  3.33it/s] 65%|██████▌   | 130/199 [01:13<00:58,  1.17it/s] 66%|██████▌   | 131/199 [01:13<00:44,  1.54it/s] 66%|██████▋   | 132/199 [01:13<00:34,  1.96it/s] 67%|██████▋   | 133/199 [01:14<00:27,  2.44it/s] 67%|██████▋   | 134/199 [01:14<00:22,  2.94it/s] 68%|██████▊   | 135/199 [01:14<00:18,  3.43it/s] 68%|██████▊   | 136/199 [01:14<00:16,  3.88it/s] 69%|██████▉   | 137/199 [01:14<00:14,  4.28it/s] 69%|██████▉   | 138/199 [01:19<01:35,  1.57s/it] 70%|██████▉   | 139/199 [01:19<01:09,  1.15s/it] 70%|███████   | 140/199 [01:19<00:50,  1.16it/s] 71%|███████   | 141/199 [01:20<00:37,  1.53it/s] 71%|███████▏  | 142/199 [01:20<00:29,  1.96it/s] 72%|███████▏  | 143/199 [01:20<00:23,  2.43it/s] 72%|███████▏  | 144/199 [01:20<00:18,  2.93it/s] 73%|███████▎  | 145/199 [01:20<00:15,  3.43it/s] 73%|███████▎  | 146/199 [01:24<01:03,  1.20s/it] 74%|███████▍  | 147/199 [01:24<00:46,  1.12it/s] 74%|███████▍  | 148/199 [01:24<00:34,  1.48it/s] 75%|███████▍  | 149/199 [01:24<00:26,  1.90it/s] 75%|███████▌  | 150/199 [01:24<00:20,  2.37it/s] 76%|███████▌  | 151/199 [01:24<00:16,  2.87it/s] 76%|███████▋  | 152/199 [01:25<00:13,  3.36it/s] 77%|███████▋  | 153/199 [01:25<00:12,  3.82it/s] 77%|███████▋  | 154/199 [01:29<01:02,  1.39s/it] 78%|███████▊  | 155/199 [01:29<00:45,  1.03s/it] 78%|███████▊  | 156/199 [01:29<00:33,  1.29it/s] 79%|███████▉  | 157/199 [01:29<00:24,  1.68it/s] 79%|███████▉  | 158/199 [01:30<00:19,  2.13it/s] 80%|███████▉  | 159/199 [01:30<00:15,  2.62it/s] 80%|████████  | 160/199 [01:30<00:12,  3.12it/s] 81%|████████  | 161/199 [01:30<00:10,  3.60it/s] 81%|████████▏ | 162/199 [01:31<00:21,  1.70it/s] 82%|████████▏ | 163/199 [01:32<00:16,  2.15it/s] 82%|████████▏ | 164/199 [01:32<00:13,  2.64it/s] 83%|████████▎ | 165/199 [01:32<00:10,  3.14it/s] 83%|████████▎ | 166/199 [01:32<00:09,  3.61it/s] 84%|████████▍ | 167/199 [01:32<00:07,  4.05it/s] 84%|████████▍ | 168/199 [01:32<00:07,  4.42it/s] 85%|████████▍ | 169/199 [01:33<00:06,  4.73it/s] 85%|████████▌ | 170/199 [01:34<00:16,  1.77it/s] 86%|████████▌ | 171/199 [01:34<00:12,  2.23it/s] 86%|████████▋ | 172/199 [01:34<00:09,  2.72it/s] 87%|████████▋ | 173/199 [01:35<00:08,  3.22it/s] 87%|████████▋ | 174/199 [01:35<00:06,  3.69it/s] 88%|████████▊ | 175/199 [01:35<00:05,  4.11it/s] 88%|████████▊ | 176/199 [01:35<00:05,  4.47it/s] 89%|████████▉ | 177/199 [01:35<00:04,  4.76it/s] 89%|████████▉ | 178/199 [01:38<00:19,  1.07it/s] 90%|████████▉ | 179/199 [01:38<00:14,  1.41it/s] 90%|█████████ | 180/199 [01:38<00:10,  1.82it/s] 91%|█████████ | 181/199 [01:38<00:07,  2.29it/s] 91%|█████████▏| 182/199 [01:39<00:06,  2.78it/s] 92%|█████████▏| 183/199 [01:39<00:04,  3.27it/s] 92%|█████████▏| 184/199 [01:39<00:04,  3.74it/s] 93%|█████████▎| 185/199 [01:39<00:03,  4.16it/s] 93%|█████████▎| 186/199 [01:41<00:09,  1.32it/s] 94%|█████████▍| 187/199 [01:41<00:06,  1.72it/s] 94%|█████████▍| 188/199 [01:41<00:05,  2.17it/s] 95%|█████████▍| 189/199 [01:42<00:03,  2.66it/s] 95%|█████████▌| 190/199 [01:42<00:02,  3.16it/s] 96%|█████████▌| 191/199 [01:42<00:02,  3.64it/s] 96%|█████████▋| 192/199 [01:42<00:01,  4.08it/s] 97%|█████████▋| 193/199 [01:42<00:01,  4.45it/s] 97%|█████████▋| 194/199 [01:44<00:02,  1.68it/s] 98%|█████████▊| 195/199 [01:44<00:02,  1.63it/s] 98%|█████████▊| 196/199 [01:45<00:01,  2.07it/s] 99%|█████████▉| 197/199 [01:45<00:00,  2.56it/s] 99%|█████████▉| 198/199 [01:45<00:00,  2.66it/s]100%|██████████| 199/199 [01:45<00:00,  3.31it/s]100%|██████████| 199/199 [01:45<00:00,  1.88it/s]
=> result
* total: 19,850
* correct: 8,254
* accuracy: 41.6%
* error: 58.4%
* macro_f1: 37.1%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft sun397 2 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/sun397/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/sun397/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:SUN397
Loading dataset: SUN397
Reading split from /shared/s2/lab01/dataset/clip/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/sun397/split_fewshot_taesup/shot_16-seed_2.pkl
6352 3970 19850
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  397
# train_x  6,352
# val      3,970
# test     19,850
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/199 [00:00<?, ?it/s]  1%|          | 1/199 [00:06<22:08,  6.71s/it]  1%|          | 2/199 [00:06<09:24,  2.87s/it]  2%|▏         | 3/199 [00:07<05:26,  1.67s/it]  2%|▏         | 4/199 [00:07<03:30,  1.08s/it]  3%|▎         | 5/199 [00:07<02:26,  1.33it/s]  3%|▎         | 6/199 [00:07<01:50,  1.74it/s]  4%|▎         | 7/199 [00:07<01:25,  2.25it/s]  4%|▍         | 8/199 [00:08<01:22,  2.30it/s]  5%|▍         | 9/199 [00:09<02:37,  1.21it/s]  5%|▌         | 10/199 [00:10<01:58,  1.59it/s]  6%|▌         | 11/199 [00:13<05:01,  1.60s/it]  6%|▌         | 12/199 [00:14<03:38,  1.17s/it]  7%|▋         | 13/199 [00:14<02:41,  1.15it/s]  7%|▋         | 14/199 [00:14<02:02,  1.51it/s]  8%|▊         | 15/199 [00:14<01:34,  1.94it/s]  8%|▊         | 16/199 [00:14<01:15,  2.41it/s]  9%|▊         | 17/199 [00:15<01:02,  2.91it/s]  9%|▉         | 18/199 [00:15<00:53,  3.40it/s] 10%|▉         | 19/199 [00:17<02:24,  1.25it/s] 10%|█         | 20/199 [00:17<01:50,  1.63it/s] 11%|█         | 21/199 [00:17<01:25,  2.07it/s] 11%|█         | 22/199 [00:17<01:09,  2.56it/s] 12%|█▏        | 23/199 [00:17<00:57,  3.06it/s] 12%|█▏        | 24/199 [00:18<00:49,  3.54it/s] 13%|█▎        | 25/199 [00:18<00:43,  3.98it/s] 13%|█▎        | 26/199 [00:18<00:39,  4.36it/s] 14%|█▎        | 27/199 [00:21<02:52,  1.00s/it] 14%|█▍        | 28/199 [00:21<02:08,  1.33it/s] 15%|█▍        | 29/199 [00:21<01:38,  1.72it/s] 15%|█▌        | 30/199 [00:21<01:17,  2.17it/s] 16%|█▌        | 31/199 [00:21<01:03,  2.67it/s] 16%|█▌        | 32/199 [00:22<00:52,  3.17it/s] 17%|█▋        | 33/199 [00:22<00:45,  3.64it/s] 17%|█▋        | 34/199 [00:22<00:40,  4.07it/s] 18%|█▊        | 35/199 [00:23<01:39,  1.65it/s] 18%|█▊        | 36/199 [00:24<01:17,  2.09it/s] 19%|█▊        | 37/199 [00:24<01:02,  2.58it/s] 19%|█▉        | 38/199 [00:24<00:52,  3.08it/s] 20%|█▉        | 39/199 [00:24<00:45,  3.55it/s] 20%|██        | 40/199 [00:24<00:39,  3.99it/s] 21%|██        | 41/199 [00:25<00:36,  4.38it/s] 21%|██        | 42/199 [00:25<00:33,  4.69it/s] 22%|██▏       | 43/199 [00:27<02:13,  1.17it/s] 22%|██▏       | 44/199 [00:29<02:46,  1.07s/it] 23%|██▎       | 45/199 [00:29<02:04,  1.24it/s] 23%|██▎       | 46/199 [00:29<01:34,  1.62it/s] 24%|██▎       | 47/199 [00:29<01:13,  2.06it/s] 24%|██▍       | 48/199 [00:29<00:59,  2.54it/s] 25%|██▍       | 49/199 [00:30<00:49,  3.04it/s] 25%|██▌       | 50/199 [00:30<00:42,  3.52it/s] 26%|██▌       | 51/199 [00:30<00:54,  2.72it/s] 26%|██▌       | 52/199 [00:33<02:52,  1.17s/it] 27%|██▋       | 53/199 [00:34<02:07,  1.14it/s] 27%|██▋       | 54/199 [00:34<01:36,  1.50it/s] 28%|██▊       | 55/199 [00:34<01:14,  1.92it/s] 28%|██▊       | 56/199 [00:34<00:59,  2.40it/s] 29%|██▊       | 57/199 [00:34<00:49,  2.90it/s] 29%|██▉       | 58/199 [00:34<00:41,  3.39it/s] 30%|██▉       | 59/199 [00:35<00:36,  3.85it/s] 30%|███       | 60/199 [00:38<03:06,  1.34s/it] 31%|███       | 61/199 [00:39<02:16,  1.01it/s] 31%|███       | 62/199 [00:39<01:42,  1.34it/s] 32%|███▏      | 63/199 [00:39<01:18,  1.74it/s] 32%|███▏      | 64/199 [00:39<01:01,  2.19it/s] 33%|███▎      | 65/199 [00:39<00:49,  2.69it/s] 33%|███▎      | 66/199 [00:39<00:41,  3.18it/s] 34%|███▎      | 67/199 [00:40<00:36,  3.66it/s] 34%|███▍      | 68/199 [00:43<02:45,  1.26s/it] 35%|███▍      | 69/199 [00:43<02:01,  1.07it/s] 35%|███▌      | 70/199 [00:44<01:31,  1.41it/s] 36%|███▌      | 71/199 [00:44<01:10,  1.82it/s] 36%|███▌      | 72/199 [00:44<00:55,  2.29it/s] 37%|███▋      | 73/199 [00:44<00:45,  2.79it/s] 37%|███▋      | 74/199 [00:44<00:38,  3.28it/s] 38%|███▊      | 75/199 [00:44<00:33,  3.74it/s] 38%|███▊      | 76/199 [00:46<01:03,  1.95it/s] 39%|███▊      | 77/199 [00:46<00:50,  2.42it/s] 39%|███▉      | 78/199 [00:46<00:41,  2.91it/s] 40%|███▉      | 79/199 [00:46<00:35,  3.40it/s] 40%|████      | 80/199 [00:46<00:30,  3.86it/s] 41%|████      | 81/199 [00:46<00:27,  4.25it/s] 41%|████      | 82/199 [00:47<00:25,  4.58it/s] 42%|████▏     | 83/199 [00:47<00:23,  4.85it/s] 42%|████▏     | 84/199 [00:49<01:48,  1.06it/s] 43%|████▎     | 85/199 [00:50<01:21,  1.39it/s] 43%|████▎     | 86/199 [00:50<01:02,  1.80it/s] 44%|████▎     | 87/199 [00:50<00:49,  2.26it/s] 44%|████▍     | 88/199 [00:50<00:40,  2.75it/s] 45%|████▍     | 89/199 [00:50<00:33,  3.25it/s] 45%|████▌     | 90/199 [00:51<00:29,  3.71it/s] 46%|████▌     | 91/199 [00:51<00:26,  4.12it/s] 46%|████▌     | 92/199 [00:57<03:30,  1.97s/it] 47%|████▋     | 93/199 [00:57<02:31,  1.43s/it] 47%|████▋     | 94/199 [00:57<01:50,  1.05s/it] 48%|████▊     | 95/199 [00:57<01:22,  1.26it/s] 48%|████▊     | 96/199 [00:57<01:02,  1.65it/s] 49%|████▊     | 97/199 [00:58<00:48,  2.09it/s] 49%|████▉     | 98/199 [00:58<00:39,  2.57it/s] 50%|████▉     | 99/199 [00:58<00:32,  3.07it/s] 50%|█████     | 100/199 [01:02<02:33,  1.55s/it] 51%|█████     | 101/199 [01:03<01:51,  1.14s/it] 51%|█████▏    | 102/199 [01:03<01:22,  1.18it/s] 52%|█████▏    | 103/199 [01:03<01:02,  1.54it/s] 52%|█████▏    | 104/199 [01:03<00:48,  1.98it/s] 53%|█████▎    | 105/199 [01:03<00:38,  2.45it/s] 53%|█████▎    | 106/199 [01:03<00:31,  2.95it/s] 54%|█████▍    | 107/199 [01:04<00:26,  3.41it/s] 54%|█████▍    | 108/199 [01:08<02:25,  1.60s/it] 55%|█████▍    | 109/199 [01:08<01:45,  1.17s/it] 55%|█████▌    | 110/199 [01:09<01:17,  1.14it/s] 56%|█████▌    | 111/199 [01:09<00:58,  1.50it/s] 56%|█████▋    | 112/199 [01:09<00:45,  1.93it/s] 57%|█████▋    | 113/199 [01:09<00:35,  2.40it/s] 57%|█████▋    | 114/199 [01:09<00:29,  2.89it/s] 58%|█████▊    | 115/199 [01:10<00:24,  3.38it/s] 58%|█████▊    | 116/199 [01:13<01:52,  1.36s/it] 59%|█████▉    | 117/199 [01:14<01:22,  1.01s/it] 59%|█████▉    | 118/199 [01:14<01:01,  1.32it/s] 60%|█████▉    | 119/199 [01:14<00:46,  1.71it/s] 60%|██████    | 120/199 [01:14<00:36,  2.16it/s] 61%|██████    | 121/199 [01:14<00:29,  2.65it/s] 61%|██████▏   | 122/199 [01:14<00:24,  3.15it/s] 62%|██████▏   | 123/199 [01:15<00:20,  3.62it/s] 62%|██████▏   | 124/199 [01:18<01:21,  1.09s/it] 63%|██████▎   | 125/199 [01:18<01:00,  1.23it/s] 63%|██████▎   | 126/199 [01:18<00:45,  1.60it/s] 64%|██████▍   | 127/199 [01:18<00:35,  2.04it/s] 64%|██████▍   | 128/199 [01:18<00:28,  2.52it/s] 65%|██████▍   | 129/199 [01:19<00:23,  3.02it/s] 65%|██████▌   | 130/199 [01:19<00:19,  3.50it/s] 66%|██████▌   | 131/199 [01:19<00:17,  3.94it/s] 66%|██████▋   | 132/199 [01:21<00:51,  1.30it/s] 67%|██████▋   | 133/199 [01:21<00:39,  1.69it/s] 67%|██████▋   | 134/199 [01:21<00:30,  2.13it/s] 68%|██████▊   | 135/199 [01:21<00:24,  2.62it/s] 68%|██████▊   | 136/199 [01:22<00:20,  3.12it/s] 69%|██████▉   | 137/199 [01:22<00:17,  3.59it/s] 69%|██████▉   | 138/199 [01:22<00:15,  4.02it/s] 70%|██████▉   | 139/199 [01:22<00:13,  4.39it/s] 70%|███████   | 140/199 [01:24<00:49,  1.20it/s] 71%|███████   | 141/199 [01:25<00:37,  1.56it/s] 71%|███████▏  | 142/199 [01:25<00:28,  2.00it/s] 72%|███████▏  | 143/199 [01:25<00:22,  2.48it/s] 72%|███████▏  | 144/199 [01:25<00:18,  2.97it/s] 73%|███████▎  | 145/199 [01:25<00:15,  3.46it/s] 73%|███████▎  | 146/199 [01:25<00:13,  3.91it/s] 74%|███████▍  | 147/199 [01:26<00:12,  4.30it/s] 74%|███████▍  | 148/199 [01:26<00:21,  2.35it/s] 75%|███████▍  | 149/199 [01:27<00:17,  2.84it/s] 75%|███████▌  | 150/199 [01:27<00:14,  3.33it/s] 76%|███████▌  | 151/199 [01:27<00:12,  3.78it/s] 76%|███████▋  | 152/199 [01:27<00:11,  4.19it/s] 77%|███████▋  | 153/199 [01:27<00:10,  4.53it/s] 77%|███████▋  | 154/199 [01:30<00:44,  1.00it/s] 78%|███████▊  | 155/199 [01:30<00:33,  1.33it/s] 78%|███████▊  | 156/199 [01:31<00:28,  1.52it/s] 79%|███████▉  | 157/199 [01:31<00:21,  1.95it/s] 79%|███████▉  | 158/199 [01:32<00:27,  1.50it/s] 80%|███████▉  | 159/199 [01:32<00:20,  1.92it/s] 80%|████████  | 160/199 [01:32<00:16,  2.39it/s] 81%|████████  | 161/199 [01:33<00:13,  2.89it/s] 81%|████████▏ | 162/199 [01:33<00:16,  2.31it/s] 82%|████████▏ | 163/199 [01:33<00:12,  2.80it/s] 82%|████████▏ | 164/199 [01:34<00:10,  3.29it/s] 83%|████████▎ | 165/199 [01:34<00:09,  3.75it/s] 83%|████████▎ | 166/199 [01:37<00:37,  1.15s/it] 84%|████████▍ | 167/199 [01:37<00:27,  1.17it/s] 84%|████████▍ | 168/199 [01:37<00:20,  1.53it/s] 85%|████████▍ | 169/199 [01:37<00:15,  1.96it/s] 85%|████████▌ | 170/199 [01:38<00:11,  2.43it/s] 86%|████████▌ | 171/199 [01:38<00:09,  2.93it/s] 86%|████████▋ | 172/199 [01:38<00:07,  3.42it/s] 87%|████████▋ | 173/199 [01:38<00:06,  3.87it/s] 87%|████████▋ | 174/199 [01:42<00:30,  1.23s/it] 88%|████████▊ | 175/199 [01:42<00:21,  1.10it/s] 88%|████████▊ | 176/199 [01:42<00:15,  1.45it/s] 89%|████████▉ | 177/199 [01:42<00:11,  1.86it/s] 89%|████████▉ | 178/199 [01:42<00:09,  2.33it/s] 90%|████████▉ | 179/199 [01:43<00:07,  2.82it/s] 90%|█████████ | 180/199 [01:43<00:05,  3.31it/s] 91%|█████████ | 181/199 [01:43<00:04,  3.77it/s] 91%|█████████▏| 182/199 [01:45<00:16,  1.05it/s] 92%|█████████▏| 183/199 [01:46<00:11,  1.38it/s] 92%|█████████▏| 184/199 [01:46<00:08,  1.79it/s] 93%|█████████▎| 185/199 [01:46<00:06,  2.25it/s] 93%|█████████▎| 186/199 [01:46<00:04,  2.74it/s] 94%|█████████▍| 187/199 [01:46<00:03,  3.25it/s] 94%|█████████▍| 188/199 [01:47<00:02,  3.72it/s] 95%|█████████▍| 189/199 [01:47<00:02,  4.14it/s] 95%|█████████▌| 190/199 [01:49<00:08,  1.02it/s] 96%|█████████▌| 191/199 [01:50<00:05,  1.35it/s] 96%|█████████▋| 192/199 [01:50<00:03,  1.75it/s] 97%|█████████▋| 193/199 [01:50<00:02,  2.21it/s] 97%|█████████▋| 194/199 [01:50<00:01,  2.71it/s] 98%|█████████▊| 195/199 [01:50<00:01,  3.21it/s] 98%|█████████▊| 196/199 [01:50<00:00,  3.68it/s] 99%|█████████▉| 197/199 [01:51<00:00,  4.11it/s] 99%|█████████▉| 198/199 [01:53<00:00,  1.05it/s]100%|██████████| 199/199 [01:53<00:00,  1.41it/s]100%|██████████| 199/199 [01:53<00:00,  1.75it/s]
=> result
* total: 19,850
* correct: 9,290
* accuracy: 46.8%
* error: 53.2%
* macro_f1: 42.7%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft sun397 3 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/sun397/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/sun397/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:SUN397
Loading dataset: SUN397
Reading split from /shared/s2/lab01/dataset/clip/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/sun397/split_fewshot_taesup/shot_16-seed_3.pkl
6352 3970 19850
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  397
# train_x  6,352
# val      3,970
# test     19,850
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/199 [00:00<?, ?it/s]  1%|          | 1/199 [00:06<21:37,  6.56s/it]  1%|          | 2/199 [00:06<09:12,  2.80s/it]  2%|▏         | 3/199 [00:06<05:14,  1.60s/it]  2%|▏         | 4/199 [00:07<03:22,  1.04s/it]  3%|▎         | 5/199 [00:07<02:21,  1.37it/s]  3%|▎         | 6/199 [00:07<01:44,  1.85it/s]  4%|▎         | 7/199 [00:07<01:21,  2.37it/s]  4%|▍         | 8/199 [00:07<01:05,  2.91it/s]  5%|▍         | 9/199 [00:10<03:21,  1.06s/it]  5%|▌         | 10/199 [00:10<02:28,  1.27it/s]  6%|▌         | 11/199 [00:13<04:30,  1.44s/it]  6%|▌         | 12/199 [00:13<03:17,  1.06s/it]  7%|▋         | 13/199 [00:13<02:26,  1.27it/s]  7%|▋         | 14/199 [00:14<01:51,  1.65it/s]  8%|▊         | 15/199 [00:14<01:27,  2.10it/s]  8%|▊         | 16/199 [00:14<01:10,  2.59it/s]  9%|▊         | 17/199 [00:14<00:58,  3.09it/s]  9%|▉         | 18/199 [00:14<00:50,  3.57it/s] 10%|▉         | 19/199 [00:16<02:15,  1.33it/s] 10%|█         | 20/199 [00:16<01:44,  1.72it/s] 11%|█         | 21/199 [00:16<01:22,  2.15it/s] 11%|█         | 22/199 [00:17<01:07,  2.64it/s] 12%|█▏        | 23/199 [00:17<00:56,  3.14it/s] 12%|█▏        | 24/199 [00:17<00:48,  3.61it/s] 13%|█▎        | 25/199 [00:18<01:16,  2.28it/s] 13%|█▎        | 26/199 [00:18<01:02,  2.77it/s] 14%|█▎        | 27/199 [00:20<02:04,  1.38it/s] 14%|█▍        | 28/199 [00:20<01:35,  1.78it/s] 15%|█▍        | 29/199 [00:20<01:15,  2.24it/s] 15%|█▌        | 30/199 [00:20<01:01,  2.73it/s] 16%|█▌        | 31/199 [00:20<00:52,  3.22it/s] 16%|█▌        | 32/199 [00:20<00:45,  3.69it/s] 17%|█▋        | 33/199 [00:21<00:53,  3.13it/s] 17%|█▋        | 34/199 [00:21<00:45,  3.61it/s] 18%|█▊        | 35/199 [00:23<01:46,  1.54it/s] 18%|█▊        | 36/199 [00:23<01:22,  1.97it/s] 19%|█▊        | 37/199 [00:23<01:06,  2.45it/s] 19%|█▉        | 38/199 [00:23<00:54,  2.95it/s] 20%|█▉        | 39/199 [00:23<00:46,  3.43it/s] 20%|██        | 40/199 [00:24<00:40,  3.89it/s] 21%|██        | 41/199 [00:24<00:36,  4.28it/s] 21%|██        | 42/199 [00:24<00:34,  4.60it/s] 22%|██▏       | 43/199 [00:27<02:38,  1.02s/it] 22%|██▏       | 44/199 [00:27<01:58,  1.30it/s] 23%|██▎       | 45/199 [00:27<01:30,  1.70it/s] 23%|██▎       | 46/199 [00:27<01:11,  2.15it/s] 24%|██▎       | 47/199 [00:27<00:57,  2.63it/s] 24%|██▍       | 48/199 [00:28<00:48,  3.13it/s] 25%|██▍       | 49/199 [00:28<00:41,  3.61it/s] 25%|██▌       | 50/199 [00:28<00:36,  4.04it/s] 26%|██▌       | 51/199 [00:32<03:05,  1.25s/it] 26%|██▌       | 52/199 [00:32<02:16,  1.07it/s] 27%|██▋       | 53/199 [00:32<01:42,  1.42it/s] 27%|██▋       | 54/199 [00:32<01:19,  1.83it/s] 28%|██▊       | 55/199 [00:32<01:02,  2.29it/s] 28%|██▊       | 56/199 [00:33<00:51,  2.78it/s] 29%|██▊       | 57/199 [00:33<00:43,  3.28it/s] 29%|██▉       | 58/199 [00:33<00:37,  3.75it/s] 30%|██▉       | 59/199 [00:35<02:03,  1.14it/s] 30%|███       | 60/199 [00:35<01:33,  1.49it/s] 31%|███       | 61/199 [00:36<01:12,  1.91it/s] 31%|███       | 62/199 [00:36<00:57,  2.39it/s] 32%|███▏      | 63/199 [00:36<00:47,  2.89it/s] 32%|███▏      | 64/199 [00:36<00:39,  3.38it/s] 33%|███▎      | 65/199 [00:36<00:34,  3.84it/s] 33%|███▎      | 66/199 [00:36<00:31,  4.24it/s] 34%|███▎      | 67/199 [00:39<01:49,  1.21it/s] 34%|███▍      | 68/199 [00:39<01:22,  1.58it/s] 35%|███▍      | 69/199 [00:39<01:04,  2.02it/s] 35%|███▌      | 70/199 [00:39<00:51,  2.50it/s] 36%|███▌      | 71/199 [00:39<00:42,  2.99it/s] 36%|███▌      | 72/199 [00:40<00:36,  3.48it/s] 37%|███▋      | 73/199 [00:40<00:32,  3.93it/s] 37%|███▋      | 74/199 [00:40<00:29,  4.28it/s] 38%|███▊      | 75/199 [00:44<03:08,  1.52s/it] 38%|███▊      | 76/199 [00:45<02:17,  1.12s/it] 39%|███▊      | 77/199 [00:45<01:41,  1.20it/s] 39%|███▉      | 78/199 [00:45<01:17,  1.57it/s] 40%|███▉      | 79/199 [00:45<00:59,  2.00it/s] 40%|████      | 80/199 [00:45<00:47,  2.48it/s] 41%|████      | 81/199 [00:45<00:39,  2.98it/s] 41%|████      | 82/199 [00:46<00:33,  3.47it/s] 42%|████▏     | 83/199 [00:48<01:43,  1.12it/s] 42%|████▏     | 84/199 [00:48<01:18,  1.47it/s] 43%|████▎     | 85/199 [00:48<01:00,  1.89it/s] 43%|████▎     | 86/199 [00:48<00:47,  2.36it/s] 44%|████▎     | 87/199 [00:49<00:39,  2.85it/s] 44%|████▍     | 88/199 [00:49<00:33,  3.35it/s] 45%|████▍     | 89/199 [00:49<00:28,  3.80it/s] 45%|████▌     | 90/199 [00:49<00:25,  4.21it/s] 46%|████▌     | 91/199 [00:51<01:01,  1.76it/s] 46%|████▌     | 92/199 [00:51<00:48,  2.22it/s] 47%|████▋     | 93/199 [00:51<00:39,  2.71it/s] 47%|████▋     | 94/199 [00:51<00:32,  3.21it/s] 48%|████▊     | 95/199 [00:51<00:28,  3.68it/s] 48%|████▊     | 96/199 [00:51<00:25,  4.10it/s] 49%|████▊     | 97/199 [00:52<00:22,  4.46it/s] 49%|████▉     | 98/199 [00:52<00:21,  4.74it/s] 50%|████▉     | 99/199 [00:55<01:51,  1.12s/it] 50%|█████     | 100/199 [00:55<01:22,  1.20it/s] 51%|█████     | 101/199 [00:55<01:02,  1.57it/s] 51%|█████▏    | 102/199 [00:56<00:48,  2.00it/s] 52%|█████▏    | 103/199 [00:56<00:38,  2.47it/s] 52%|█████▏    | 104/199 [00:56<00:32,  2.96it/s] 53%|█████▎    | 105/199 [00:56<00:27,  3.45it/s] 53%|█████▎    | 106/199 [00:56<00:23,  3.89it/s] 54%|█████▍    | 107/199 [00:58<01:11,  1.29it/s] 54%|█████▍    | 108/199 [00:58<00:54,  1.67it/s] 55%|█████▍    | 109/199 [00:59<00:42,  2.12it/s] 55%|█████▌    | 110/199 [00:59<00:34,  2.60it/s] 56%|█████▌    | 111/199 [00:59<00:28,  3.10it/s] 56%|█████▋    | 112/199 [00:59<00:24,  3.57it/s] 57%|█████▋    | 113/199 [00:59<00:21,  3.99it/s] 57%|█████▋    | 114/199 [01:00<00:19,  4.36it/s] 58%|█████▊    | 115/199 [01:02<01:16,  1.09it/s] 58%|█████▊    | 116/199 [01:02<00:57,  1.44it/s] 59%|█████▉    | 117/199 [01:02<00:44,  1.85it/s] 59%|█████▉    | 118/199 [01:03<00:34,  2.32it/s] 60%|█████▉    | 119/199 [01:03<00:28,  2.81it/s] 60%|██████    | 120/199 [01:03<00:23,  3.30it/s] 61%|██████    | 121/199 [01:04<00:39,  1.95it/s] 61%|██████▏   | 122/199 [01:04<00:31,  2.42it/s] 62%|██████▏   | 123/199 [01:06<01:13,  1.03it/s] 62%|██████▏   | 124/199 [01:07<00:55,  1.36it/s] 63%|██████▎   | 125/199 [01:07<00:41,  1.76it/s] 63%|██████▎   | 126/199 [01:07<00:32,  2.22it/s] 64%|██████▍   | 127/199 [01:07<00:26,  2.70it/s] 64%|██████▍   | 128/199 [01:07<00:22,  3.19it/s] 65%|██████▍   | 129/199 [01:08<00:30,  2.31it/s] 65%|██████▌   | 130/199 [01:08<00:24,  2.80it/s] 66%|██████▌   | 131/199 [01:10<00:44,  1.52it/s] 66%|██████▋   | 132/199 [01:10<00:34,  1.94it/s] 67%|██████▋   | 133/199 [01:10<00:27,  2.42it/s] 67%|██████▋   | 134/199 [01:10<00:22,  2.91it/s] 68%|██████▊   | 135/199 [01:10<00:18,  3.39it/s] 68%|██████▊   | 136/199 [01:10<00:16,  3.84it/s] 69%|██████▉   | 137/199 [01:12<00:36,  1.69it/s] 69%|██████▉   | 138/199 [01:12<00:28,  2.14it/s] 70%|██████▉   | 139/199 [01:12<00:22,  2.63it/s] 70%|███████   | 140/199 [01:12<00:18,  3.13it/s] 71%|███████   | 141/199 [01:13<00:16,  3.61it/s] 71%|███████▏  | 142/199 [01:13<00:14,  4.04it/s] 72%|███████▏  | 143/199 [01:13<00:12,  4.41it/s] 72%|███████▏  | 144/199 [01:13<00:11,  4.71it/s] 73%|███████▎  | 145/199 [01:17<01:15,  1.39s/it] 73%|███████▎  | 146/199 [01:17<00:54,  1.03s/it] 74%|███████▍  | 147/199 [01:18<00:40,  1.30it/s] 74%|███████▍  | 148/199 [01:18<00:30,  1.68it/s] 75%|███████▍  | 149/199 [01:18<00:23,  2.13it/s] 75%|███████▌  | 150/199 [01:18<00:18,  2.62it/s] 76%|███████▌  | 151/199 [01:18<00:15,  3.12it/s] 76%|███████▋  | 152/199 [01:18<00:13,  3.60it/s] 77%|███████▋  | 153/199 [01:20<00:29,  1.57it/s] 77%|███████▋  | 154/199 [01:20<00:22,  2.00it/s] 78%|███████▊  | 155/199 [01:20<00:17,  2.48it/s] 78%|███████▊  | 156/199 [01:20<00:14,  2.98it/s] 79%|███████▉  | 157/199 [01:21<00:12,  3.47it/s] 79%|███████▉  | 158/199 [01:21<00:10,  3.92it/s] 80%|███████▉  | 159/199 [01:21<00:09,  4.30it/s] 80%|████████  | 160/199 [01:21<00:08,  4.61it/s] 81%|████████  | 161/199 [01:22<00:16,  2.37it/s] 81%|████████▏ | 162/199 [01:22<00:12,  2.87it/s] 82%|████████▏ | 163/199 [01:22<00:10,  3.35it/s] 82%|████████▏ | 164/199 [01:23<00:09,  3.81it/s] 83%|████████▎ | 165/199 [01:23<00:08,  4.21it/s] 83%|████████▎ | 166/199 [01:23<00:07,  4.54it/s] 84%|████████▍ | 167/199 [01:24<00:13,  2.44it/s] 84%|████████▍ | 168/199 [01:24<00:10,  2.94it/s] 85%|████████▍ | 169/199 [01:24<00:08,  3.43it/s] 85%|████████▌ | 170/199 [01:24<00:07,  3.87it/s] 86%|████████▌ | 171/199 [01:25<00:06,  4.27it/s] 86%|████████▋ | 172/199 [01:25<00:11,  2.33it/s] 87%|████████▋ | 173/199 [01:26<00:09,  2.82it/s] 87%|████████▋ | 174/199 [01:26<00:10,  2.49it/s] 88%|████████▊ | 175/199 [01:27<00:10,  2.24it/s] 88%|████████▊ | 176/199 [01:27<00:08,  2.73it/s] 89%|████████▉ | 177/199 [01:27<00:06,  3.23it/s] 89%|████████▉ | 178/199 [01:27<00:05,  3.70it/s] 90%|████████▉ | 179/199 [01:27<00:04,  4.12it/s] 90%|█████████ | 180/199 [01:28<00:06,  3.01it/s] 91%|█████████ | 181/199 [01:28<00:05,  3.49it/s] 91%|█████████▏| 182/199 [01:29<00:10,  1.64it/s] 92%|█████████▏| 183/199 [01:31<00:12,  1.30it/s] 92%|█████████▏| 184/199 [01:31<00:08,  1.68it/s] 93%|█████████▎| 185/199 [01:31<00:06,  2.13it/s] 93%|█████████▎| 186/199 [01:31<00:04,  2.61it/s] 94%|█████████▍| 187/199 [01:31<00:03,  3.11it/s] 94%|█████████▍| 188/199 [01:31<00:03,  3.58it/s] 95%|█████████▍| 189/199 [01:32<00:02,  4.01it/s] 95%|█████████▌| 190/199 [01:34<00:06,  1.34it/s] 96%|█████████▌| 191/199 [01:34<00:05,  1.53it/s] 96%|█████████▋| 192/199 [01:34<00:03,  1.96it/s] 97%|█████████▋| 193/199 [01:34<00:02,  2.44it/s] 97%|█████████▋| 194/199 [01:35<00:01,  2.94it/s] 98%|█████████▊| 195/199 [01:35<00:01,  3.43it/s] 98%|█████████▊| 196/199 [01:35<00:00,  3.88it/s] 99%|█████████▉| 197/199 [01:35<00:00,  4.27it/s] 99%|█████████▉| 198/199 [01:38<00:00,  1.04it/s]100%|██████████| 199/199 [01:38<00:00,  1.40it/s]100%|██████████| 199/199 [01:38<00:00,  2.02it/s]
=> result
* total: 19,850
* correct: 8,282
* accuracy: 41.7%
* error: 58.3%
* macro_f1: 36.6%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers stanford_cars oxford_pets food101 ucf101 caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft imagenet 1 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/imagenet.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/imagenet/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: ImageNet
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/imagenet/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:ImageNet
Loading dataset: ImageNet
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/ImageNet/split_fewshot_taesup/shot_16-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  --------
Dataset    ImageNet
# classes  1,000
# train_x  16,000
# val      50,000
# test     50,000
---------  --------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed1/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:04<36:37,  4.40s/it]  0%|          | 2/500 [00:04<16:28,  1.98s/it]  1%|          | 3/500 [00:04<10:02,  1.21s/it]  1%|          | 4/500 [00:05<07:00,  1.18it/s]  1%|          | 5/500 [00:05<05:20,  1.54it/s]  1%|          | 6/500 [00:05<04:19,  1.90it/s]  1%|▏         | 7/500 [00:06<03:41,  2.23it/s]  2%|▏         | 8/500 [00:06<03:16,  2.50it/s]  2%|▏         | 9/500 [00:06<02:59,  2.73it/s]  2%|▏         | 10/500 [00:07<02:48,  2.91it/s]  2%|▏         | 11/500 [00:07<02:40,  3.05it/s]  2%|▏         | 12/500 [00:07<02:34,  3.16it/s]  3%|▎         | 13/500 [00:07<02:30,  3.23it/s]  3%|▎         | 14/500 [00:08<02:27,  3.29it/s]  3%|▎         | 15/500 [00:08<02:25,  3.32it/s]  3%|▎         | 16/500 [00:08<02:24,  3.35it/s]  3%|▎         | 17/500 [00:09<02:23,  3.37it/s]  4%|▎         | 18/500 [00:09<02:22,  3.38it/s]  4%|▍         | 19/500 [00:09<02:21,  3.39it/s]  4%|▍         | 20/500 [00:09<02:21,  3.40it/s]  4%|▍         | 21/500 [00:10<02:20,  3.40it/s]  4%|▍         | 22/500 [00:10<02:20,  3.39it/s]  5%|▍         | 23/500 [00:10<02:20,  3.40it/s]  5%|▍         | 24/500 [00:11<02:20,  3.40it/s](null): _log_init: Unable to open logfile `/var/log/slurm/slurmd.log': No such file or directory
  5%|▌         | 25/500 [00:11<02:19,  3.40it/s]  5%|▌         | 26/500 [00:11<02:19,  3.40it/s]  5%|▌         | 27/500 [00:12<02:19,  3.40it/s]  6%|▌         | 28/500 [00:12<02:18,  3.40it/s]  6%|▌         | 29/500 [00:12<02:18,  3.41it/s]  6%|▌         | 30/500 [00:12<02:17,  3.41it/s]  6%|▌         | 31/500 [00:13<02:17,  3.41it/s]  6%|▋         | 32/500 [00:13<02:17,  3.41it/s]  7%|▋         | 33/500 [00:13<02:17,  3.41it/s]  7%|▋         | 34/500 [00:14<02:16,  3.41it/s]  7%|▋         | 35/500 [00:14<02:16,  3.41it/s]  7%|▋         | 36/500 [00:14<02:16,  3.41it/s]  7%|▋         | 37/500 [00:14<02:16,  3.40it/s]  8%|▊         | 38/500 [00:15<02:15,  3.41it/s]  8%|▊         | 39/500 [00:15<02:14,  3.42it/s]  8%|▊         | 40/500 [00:15<02:14,  3.42it/s]  8%|▊         | 41/500 [00:16<02:14,  3.42it/s]  8%|▊         | 42/500 [00:16<02:14,  3.41it/s]  9%|▊         | 43/500 [00:16<02:14,  3.41it/s]  9%|▉         | 44/500 [00:16<02:13,  3.41it/s]  9%|▉         | 45/500 [00:17<02:13,  3.41it/s]  9%|▉         | 46/500 [00:17<02:13,  3.41it/s]  9%|▉         | 47/500 [00:17<02:12,  3.41it/s] 10%|▉         | 48/500 [00:18<02:12,  3.41it/s] 10%|▉         | 49/500 [00:18<02:12,  3.40it/s] 10%|█         | 50/500 [00:18<02:12,  3.40it/s] 10%|█         | 51/500 [00:19<02:11,  3.40it/s] 10%|█         | 52/500 [00:19<02:11,  3.41it/s] 11%|█         | 53/500 [00:19<02:11,  3.40it/s] 11%|█         | 54/500 [00:19<02:11,  3.38it/s] 11%|█         | 55/500 [00:20<02:11,  3.39it/s] 11%|█         | 56/500 [00:20<02:11,  3.39it/s] 11%|█▏        | 57/500 [00:20<02:10,  3.39it/s] 12%|█▏        | 58/500 [00:21<02:10,  3.40it/s] 12%|█▏        | 59/500 [00:21<02:09,  3.40it/s] 12%|█▏        | 60/500 [00:21<02:09,  3.40it/s] 12%|█▏        | 61/500 [00:22<02:09,  3.40it/s] 12%|█▏        | 62/500 [00:22<02:09,  3.39it/s] 13%|█▎        | 63/500 [00:22<02:08,  3.39it/s] 13%|█▎        | 64/500 [00:22<02:08,  3.39it/s] 13%|█▎        | 65/500 [00:23<02:07,  3.40it/s] 13%|█▎        | 66/500 [00:23<02:07,  3.40it/s] 13%|█▎        | 67/500 [00:23<02:07,  3.40it/s] 14%|█▎        | 68/500 [00:24<02:07,  3.39it/s] 14%|█▍        | 69/500 [00:24<02:07,  3.39it/s] 14%|█▍        | 70/500 [00:24<02:06,  3.40it/s] 14%|█▍        | 71/500 [00:24<02:06,  3.40it/s] 14%|█▍        | 72/500 [00:25<02:05,  3.40it/s] 15%|█▍        | 73/500 [00:25<02:05,  3.41it/s] 15%|█▍        | 74/500 [00:25<02:05,  3.41it/s] 15%|█▌        | 75/500 [00:26<02:04,  3.40it/s] 15%|█▌        | 76/500 [00:26<02:04,  3.40it/s] 15%|█▌        | 77/500 [00:26<02:04,  3.40it/s] 16%|█▌        | 78/500 [00:27<02:03,  3.41it/s] 16%|█▌        | 79/500 [00:27<02:03,  3.41it/s] 16%|█▌        | 80/500 [00:27<02:03,  3.41it/s] 16%|█▌        | 81/500 [00:27<02:02,  3.41it/s] 16%|█▋        | 82/500 [00:28<02:02,  3.41it/s] 17%|█▋        | 83/500 [00:28<02:02,  3.40it/s] 17%|█▋        | 84/500 [00:28<02:02,  3.40it/s] 17%|█▋        | 85/500 [00:29<02:02,  3.40it/s] 17%|█▋        | 86/500 [00:29<02:01,  3.40it/s] 17%|█▋        | 87/500 [00:29<02:01,  3.39it/s] 18%|█▊        | 88/500 [00:29<02:01,  3.39it/s] 18%|█▊        | 89/500 [00:30<02:01,  3.39it/s] 18%|█▊        | 90/500 [00:30<02:01,  3.38it/s] 18%|█▊        | 91/500 [00:30<02:00,  3.38it/s] 18%|█▊        | 92/500 [00:31<02:00,  3.39it/s] 19%|█▊        | 93/500 [00:31<02:00,  3.39it/s] 19%|█▉        | 94/500 [00:31<01:59,  3.38it/s] 19%|█▉        | 95/500 [00:32<01:59,  3.38it/s] 19%|█▉        | 96/500 [00:32<01:59,  3.39it/s] 19%|█▉        | 97/500 [00:32<01:59,  3.39it/s] 20%|█▉        | 98/500 [00:32<01:58,  3.39it/s] 20%|█▉        | 99/500 [00:33<01:58,  3.39it/s] 20%|██        | 100/500 [00:33<01:57,  3.39it/s] 20%|██        | 101/500 [00:33<01:57,  3.39it/s] 20%|██        | 102/500 [00:34<01:57,  3.39it/s] 21%|██        | 103/500 [00:34<01:57,  3.39it/s] 21%|██        | 104/500 [00:34<01:57,  3.38it/s] 21%|██        | 105/500 [00:34<01:56,  3.38it/s] 21%|██        | 106/500 [00:35<01:56,  3.38it/s] 21%|██▏       | 107/500 [00:35<01:56,  3.38it/s] 22%|██▏       | 108/500 [00:35<01:55,  3.38it/s] 22%|██▏       | 109/500 [00:36<01:55,  3.38it/s] 22%|██▏       | 110/500 [00:36<01:55,  3.38it/s] 22%|██▏       | 111/500 [00:36<01:54,  3.38it/s] 22%|██▏       | 112/500 [00:37<01:54,  3.38it/s] 23%|██▎       | 113/500 [00:37<01:54,  3.39it/s] 23%|██▎       | 114/500 [00:37<01:53,  3.39it/s] 23%|██▎       | 115/500 [00:37<01:53,  3.39it/s] 23%|██▎       | 116/500 [00:38<01:53,  3.39it/s] 23%|██▎       | 117/500 [00:38<01:52,  3.39it/s] 24%|██▎       | 118/500 [00:38<01:52,  3.39it/s] 24%|██▍       | 119/500 [00:39<01:52,  3.39it/s] 24%|██▍       | 120/500 [00:39<01:52,  3.39it/s] 24%|██▍       | 121/500 [00:39<01:51,  3.39it/s] 24%|██▍       | 122/500 [00:39<01:51,  3.39it/s] 25%|██▍       | 123/500 [00:40<01:51,  3.39it/s] 25%|██▍       | 124/500 [00:40<01:50,  3.39it/s] 25%|██▌       | 125/500 [00:40<01:50,  3.39it/s] 25%|██▌       | 126/500 [00:41<01:50,  3.39it/s] 25%|██▌       | 127/500 [00:41<01:50,  3.38it/s] 26%|██▌       | 128/500 [00:41<01:50,  3.38it/s] 26%|██▌       | 129/500 [00:42<01:49,  3.37it/s] 26%|██▌       | 130/500 [00:42<01:49,  3.37it/s] 26%|██▌       | 131/500 [00:42<01:49,  3.37it/s] 26%|██▋       | 132/500 [00:42<01:49,  3.37it/s] 27%|██▋       | 133/500 [00:43<01:48,  3.37it/s] 27%|██▋       | 134/500 [00:43<01:49,  3.34it/s] 27%|██▋       | 135/500 [00:43<01:49,  3.35it/s] 27%|██▋       | 136/500 [00:44<01:48,  3.36it/s] 27%|██▋       | 137/500 [00:44<01:47,  3.36it/s] 28%|██▊       | 138/500 [00:44<01:47,  3.37it/s] 28%|██▊       | 139/500 [00:45<01:47,  3.37it/s] 28%|██▊       | 140/500 [00:45<01:46,  3.37it/s] 28%|██▊       | 141/500 [00:45<01:46,  3.37it/s] 28%|██▊       | 142/500 [00:45<01:46,  3.37it/s] 29%|██▊       | 143/500 [00:46<01:45,  3.37it/s] 29%|██▉       | 144/500 [00:46<01:45,  3.37it/s] 29%|██▉       | 145/500 [00:46<01:45,  3.37it/s] 29%|██▉       | 146/500 [00:47<01:44,  3.37it/s] 29%|██▉       | 147/500 [00:47<01:45,  3.36it/s] 30%|██▉       | 148/500 [00:47<01:44,  3.36it/s] 30%|██▉       | 149/500 [00:47<01:44,  3.37it/s] 30%|███       | 150/500 [00:48<01:43,  3.37it/s] 30%|███       | 151/500 [00:48<01:43,  3.37it/s] 30%|███       | 152/500 [00:48<01:43,  3.37it/s] 31%|███       | 153/500 [00:49<01:42,  3.37it/s] 31%|███       | 154/500 [00:49<01:42,  3.37it/s] 31%|███       | 155/500 [00:49<01:42,  3.37it/s] 31%|███       | 156/500 [00:50<01:41,  3.37it/s] 31%|███▏      | 157/500 [00:50<01:41,  3.37it/s] 32%|███▏      | 158/500 [00:50<01:41,  3.36it/s] 32%|███▏      | 159/500 [00:50<01:41,  3.37it/s] 32%|███▏      | 160/500 [00:51<01:40,  3.37it/s] 32%|███▏      | 161/500 [00:51<01:40,  3.37it/s] 32%|███▏      | 162/500 [00:51<01:40,  3.37it/s] 33%|███▎      | 163/500 [00:52<01:39,  3.37it/s] 33%|███▎      | 164/500 [00:52<01:39,  3.37it/s] 33%|███▎      | 165/500 [00:52<01:39,  3.37it/s] 33%|███▎      | 166/500 [00:53<01:38,  3.37it/s] 33%|███▎      | 167/500 [00:53<01:38,  3.38it/s] 34%|███▎      | 168/500 [00:53<01:38,  3.38it/s] 34%|███▍      | 169/500 [00:53<01:37,  3.38it/s] 34%|███▍      | 170/500 [00:54<01:37,  3.39it/s] 34%|███▍      | 171/500 [00:54<01:37,  3.39it/s] 34%|███▍      | 172/500 [00:54<01:36,  3.39it/s] 35%|███▍      | 173/500 [00:55<01:36,  3.39it/s] 35%|███▍      | 174/500 [00:55<01:36,  3.38it/s] 35%|███▌      | 175/500 [00:55<01:36,  3.37it/s] 35%|███▌      | 176/500 [00:55<01:36,  3.37it/s] 35%|███▌      | 177/500 [00:56<01:35,  3.37it/s] 36%|███▌      | 178/500 [00:56<01:35,  3.36it/s] 36%|███▌      | 179/500 [00:56<01:35,  3.36it/s] 36%|███▌      | 180/500 [00:57<01:35,  3.36it/s] 36%|███▌      | 181/500 [00:57<01:34,  3.36it/s] 36%|███▋      | 182/500 [00:57<01:34,  3.36it/s] 37%|███▋      | 183/500 [00:58<01:34,  3.36it/s] 37%|███▋      | 184/500 [00:58<01:33,  3.37it/s] 37%|███▋      | 185/500 [00:58<01:33,  3.37it/s] 37%|███▋      | 186/500 [00:58<01:33,  3.37it/s] 37%|███▋      | 187/500 [00:59<01:32,  3.37it/s] 38%|███▊      | 188/500 [00:59<01:32,  3.37it/s] 38%|███▊      | 189/500 [00:59<01:32,  3.37it/s] 38%|███▊      | 190/500 [01:00<01:31,  3.37it/s] 38%|███▊      | 191/500 [01:00<01:31,  3.37it/s] 38%|███▊      | 192/500 [01:00<01:31,  3.37it/s] 39%|███▊      | 193/500 [01:01<01:31,  3.37it/s] 39%|███▉      | 194/500 [01:01<01:30,  3.38it/s] 39%|███▉      | 195/500 [01:01<01:30,  3.38it/s] 39%|███▉      | 196/500 [01:01<01:29,  3.38it/s] 39%|███▉      | 197/500 [01:02<01:29,  3.38it/s] 40%|███▉      | 198/500 [01:02<01:29,  3.36it/s] 40%|███▉      | 199/500 [01:02<01:29,  3.37it/s] 40%|████      | 200/500 [01:03<01:28,  3.38it/s] 40%|████      | 201/500 [01:03<01:28,  3.38it/s] 40%|████      | 202/500 [01:03<01:28,  3.39it/s] 41%|████      | 203/500 [01:04<01:27,  3.38it/s] 41%|████      | 204/500 [01:04<01:27,  3.38it/s] 41%|████      | 205/500 [01:04<01:27,  3.38it/s] 41%|████      | 206/500 [01:04<01:27,  3.37it/s] 41%|████▏     | 207/500 [01:05<01:26,  3.38it/s] 42%|████▏     | 208/500 [01:05<01:26,  3.38it/s] 42%|████▏     | 209/500 [01:05<01:26,  3.38it/s] 42%|████▏     | 210/500 [01:06<01:25,  3.38it/s] 42%|████▏     | 211/500 [01:06<01:25,  3.38it/s] 42%|████▏     | 212/500 [01:06<01:25,  3.37it/s] 43%|████▎     | 213/500 [01:06<01:25,  3.38it/s] 43%|████▎     | 214/500 [01:07<01:24,  3.37it/s] 43%|████▎     | 215/500 [01:07<01:24,  3.37it/s] 43%|████▎     | 216/500 [01:07<01:24,  3.37it/s] 43%|████▎     | 217/500 [01:08<01:23,  3.37it/s] 44%|████▎     | 218/500 [01:08<01:23,  3.37it/s] 44%|████▍     | 219/500 [01:08<01:23,  3.38it/s] 44%|████▍     | 220/500 [01:09<01:22,  3.38it/s] 44%|████▍     | 221/500 [01:09<01:22,  3.38it/s] 44%|████▍     | 222/500 [01:09<01:22,  3.38it/s] 45%|████▍     | 223/500 [01:09<01:21,  3.39it/s] 45%|████▍     | 224/500 [01:10<01:21,  3.39it/s] 45%|████▌     | 225/500 [01:10<01:21,  3.39it/s] 45%|████▌     | 226/500 [01:10<01:20,  3.39it/s] 45%|████▌     | 227/500 [01:11<01:20,  3.39it/s] 46%|████▌     | 228/500 [01:11<01:20,  3.39it/s] 46%|████▌     | 229/500 [01:11<01:20,  3.38it/s] 46%|████▌     | 230/500 [01:11<01:19,  3.38it/s] 46%|████▌     | 231/500 [01:12<01:19,  3.39it/s] 46%|████▋     | 232/500 [01:12<01:19,  3.39it/s] 47%|████▋     | 233/500 [01:12<01:18,  3.39it/s] 47%|████▋     | 234/500 [01:13<01:18,  3.39it/s] 47%|████▋     | 235/500 [01:13<01:18,  3.39it/s] 47%|████▋     | 236/500 [01:13<01:18,  3.38it/s] 47%|████▋     | 237/500 [01:14<01:17,  3.38it/s] 48%|████▊     | 238/500 [01:14<01:17,  3.38it/s] 48%|████▊     | 239/500 [01:14<01:17,  3.38it/s] 48%|████▊     | 240/500 [01:14<01:16,  3.38it/s] 48%|████▊     | 241/500 [01:15<01:16,  3.38it/s] 48%|████▊     | 242/500 [01:15<01:16,  3.37it/s] 49%|████▊     | 243/500 [01:15<01:16,  3.37it/s] 49%|████▉     | 244/500 [01:16<01:16,  3.36it/s] 49%|████▉     | 245/500 [01:16<01:15,  3.37it/s] 49%|████▉     | 246/500 [01:16<01:15,  3.37it/s] 49%|████▉     | 247/500 [01:17<01:15,  3.37it/s] 50%|████▉     | 248/500 [01:17<01:14,  3.37it/s] 50%|████▉     | 249/500 [01:17<01:14,  3.37it/s] 50%|█████     | 250/500 [01:17<01:14,  3.37it/s] 50%|█████     | 251/500 [01:18<01:13,  3.37it/s] 50%|█████     | 252/500 [01:18<01:14,  3.35it/s] 51%|█████     | 253/500 [01:18<01:13,  3.36it/s] 51%|█████     | 254/500 [01:19<01:13,  3.36it/s] 51%|█████     | 255/500 [01:19<01:12,  3.36it/s] 51%|█████     | 256/500 [01:19<01:12,  3.36it/s] 51%|█████▏    | 257/500 [01:20<01:12,  3.36it/s] 52%|█████▏    | 258/500 [01:20<01:11,  3.36it/s] 52%|█████▏    | 259/500 [01:20<01:11,  3.35it/s] 52%|█████▏    | 260/500 [01:20<01:11,  3.36it/s] 52%|█████▏    | 261/500 [01:21<01:11,  3.36it/s] 52%|█████▏    | 262/500 [01:21<01:10,  3.36it/s] 53%|█████▎    | 263/500 [01:21<01:10,  3.36it/s] 53%|█████▎    | 264/500 [01:22<01:10,  3.36it/s] 53%|█████▎    | 265/500 [01:22<01:09,  3.36it/s] 53%|█████▎    | 266/500 [01:22<01:09,  3.37it/s] 53%|█████▎    | 267/500 [01:22<01:09,  3.36it/s] 54%|█████▎    | 268/500 [01:23<01:08,  3.37it/s] 54%|█████▍    | 269/500 [01:23<01:08,  3.37it/s] 54%|█████▍    | 270/500 [01:23<01:08,  3.37it/s] 54%|█████▍    | 271/500 [01:24<01:08,  3.37it/s] 54%|█████▍    | 272/500 [01:24<01:07,  3.37it/s] 55%|█████▍    | 273/500 [01:24<01:07,  3.37it/s] 55%|█████▍    | 274/500 [01:25<01:07,  3.37it/s] 55%|█████▌    | 275/500 [01:25<01:06,  3.37it/s] 55%|█████▌    | 276/500 [01:25<01:06,  3.37it/s] 55%|█████▌    | 277/500 [01:25<01:06,  3.37it/s] 56%|█████▌    | 278/500 [01:26<01:06,  3.35it/s] 56%|█████▌    | 279/500 [01:26<01:05,  3.36it/s] 56%|█████▌    | 280/500 [01:26<01:05,  3.35it/s] 56%|█████▌    | 281/500 [01:27<01:05,  3.36it/s] 56%|█████▋    | 282/500 [01:27<01:04,  3.36it/s] 57%|█████▋    | 283/500 [01:27<01:04,  3.36it/s] 57%|█████▋    | 284/500 [01:28<01:04,  3.36it/s] 57%|█████▋    | 285/500 [01:28<01:03,  3.36it/s] 57%|█████▋    | 286/500 [01:28<01:03,  3.36it/s] 57%|█████▋    | 287/500 [01:28<01:03,  3.36it/s] 58%|█████▊    | 288/500 [01:29<01:03,  3.36it/s] 58%|█████▊    | 289/500 [01:29<01:02,  3.37it/s] 58%|█████▊    | 290/500 [01:29<01:02,  3.37it/s] 58%|█████▊    | 291/500 [01:30<01:02,  3.36it/s] 58%|█████▊    | 292/500 [01:30<01:01,  3.36it/s] 59%|█████▊    | 293/500 [01:30<01:01,  3.36it/s] 59%|█████▉    | 294/500 [01:31<01:01,  3.36it/s] 59%|█████▉    | 295/500 [01:31<01:01,  3.36it/s] 59%|█████▉    | 296/500 [01:31<01:00,  3.36it/s] 59%|█████▉    | 297/500 [01:31<01:00,  3.36it/s] 60%|█████▉    | 298/500 [01:32<01:00,  3.36it/s] 60%|█████▉    | 299/500 [01:32<01:00,  3.35it/s] 60%|██████    | 300/500 [01:32<00:59,  3.36it/s] 60%|██████    | 301/500 [01:33<00:59,  3.36it/s] 60%|██████    | 302/500 [01:33<00:58,  3.36it/s] 61%|██████    | 303/500 [01:33<00:58,  3.36it/s] 61%|██████    | 304/500 [01:33<00:58,  3.36it/s] 61%|██████    | 305/500 [01:34<00:57,  3.36it/s] 61%|██████    | 306/500 [01:34<00:57,  3.37it/s] 61%|██████▏   | 307/500 [01:34<00:57,  3.36it/s] 62%|██████▏   | 308/500 [01:35<00:57,  3.36it/s] 62%|██████▏   | 309/500 [01:35<00:56,  3.36it/s] 62%|██████▏   | 310/500 [01:35<00:56,  3.36it/s] 62%|██████▏   | 311/500 [01:36<00:56,  3.36it/s] 62%|██████▏   | 312/500 [01:36<00:56,  3.36it/s] 63%|██████▎   | 313/500 [01:36<00:55,  3.36it/s] 63%|██████▎   | 314/500 [01:36<00:55,  3.36it/s] 63%|██████▎   | 315/500 [01:37<00:55,  3.36it/s] 63%|██████▎   | 316/500 [01:37<00:54,  3.35it/s] 63%|██████▎   | 317/500 [01:37<00:54,  3.36it/s] 64%|██████▎   | 318/500 [01:38<00:54,  3.36it/s] 64%|██████▍   | 319/500 [01:38<00:53,  3.36it/s] 64%|██████▍   | 320/500 [01:38<00:53,  3.37it/s] 64%|██████▍   | 321/500 [01:39<00:53,  3.37it/s] 64%|██████▍   | 322/500 [01:39<00:52,  3.37it/s] 65%|██████▍   | 323/500 [01:39<00:52,  3.37it/s] 65%|██████▍   | 324/500 [01:39<00:52,  3.36it/s] 65%|██████▌   | 325/500 [01:40<00:52,  3.36it/s] 65%|██████▌   | 326/500 [01:40<00:51,  3.37it/s] 65%|██████▌   | 327/500 [01:40<00:51,  3.37it/s] 66%|██████▌   | 328/500 [01:41<00:50,  3.37it/s] 66%|██████▌   | 329/500 [01:41<00:50,  3.37it/s] 66%|██████▌   | 330/500 [01:41<00:50,  3.37it/s] 66%|██████▌   | 331/500 [01:42<00:50,  3.37it/s] 66%|██████▋   | 332/500 [01:42<00:49,  3.37it/s] 67%|██████▋   | 333/500 [01:42<00:49,  3.36it/s] 67%|██████▋   | 334/500 [01:42<00:49,  3.36it/s] 67%|██████▋   | 335/500 [01:43<00:49,  3.37it/s] 67%|██████▋   | 336/500 [01:43<00:48,  3.37it/s] 67%|██████▋   | 337/500 [01:43<00:48,  3.37it/s] 68%|██████▊   | 338/500 [01:44<00:48,  3.37it/s] 68%|██████▊   | 339/500 [01:44<00:47,  3.37it/s] 68%|██████▊   | 340/500 [01:44<00:47,  3.38it/s] 68%|██████▊   | 341/500 [01:44<00:47,  3.37it/s] 68%|██████▊   | 342/500 [01:45<00:46,  3.37it/s] 69%|██████▊   | 343/500 [01:45<00:46,  3.37it/s] 69%|██████▉   | 344/500 [01:45<00:46,  3.37it/s] 69%|██████▉   | 345/500 [01:46<00:46,  3.37it/s] 69%|██████▉   | 346/500 [01:46<00:45,  3.37it/s] 69%|██████▉   | 347/500 [01:46<00:45,  3.37it/s] 70%|██████▉   | 348/500 [01:47<00:45,  3.37it/s] 70%|██████▉   | 349/500 [01:47<00:44,  3.37it/s] 70%|███████   | 350/500 [01:47<00:44,  3.37it/s] 70%|███████   | 351/500 [01:47<00:44,  3.37it/s] 70%|███████   | 352/500 [01:48<00:44,  3.35it/s] 71%|███████   | 353/500 [01:48<00:43,  3.35it/s] 71%|███████   | 354/500 [01:48<00:43,  3.36it/s] 71%|███████   | 355/500 [01:49<00:43,  3.36it/s] 71%|███████   | 356/500 [01:49<00:42,  3.36it/s] 71%|███████▏  | 357/500 [01:49<00:42,  3.36it/s] 72%|███████▏  | 358/500 [01:50<00:42,  3.36it/s] 72%|███████▏  | 359/500 [01:50<00:41,  3.36it/s] 72%|███████▏  | 360/500 [01:50<00:41,  3.36it/s] 72%|███████▏  | 361/500 [01:50<00:41,  3.35it/s] 72%|███████▏  | 362/500 [01:51<00:41,  3.35it/s] 73%|███████▎  | 363/500 [01:51<00:40,  3.35it/s] 73%|███████▎  | 364/500 [01:51<00:40,  3.35it/s] 73%|███████▎  | 365/500 [01:52<00:40,  3.35it/s] 73%|███████▎  | 366/500 [01:52<00:39,  3.36it/s] 73%|███████▎  | 367/500 [01:52<00:39,  3.36it/s] 74%|███████▎  | 368/500 [01:53<00:39,  3.36it/s] 74%|███████▍  | 369/500 [01:53<00:39,  3.36it/s] 74%|███████▍  | 370/500 [01:53<00:38,  3.36it/s] 74%|███████▍  | 371/500 [01:53<00:38,  3.36it/s] 74%|███████▍  | 372/500 [01:54<00:38,  3.36it/s] 75%|███████▍  | 373/500 [01:54<00:37,  3.36it/s] 75%|███████▍  | 374/500 [01:54<00:37,  3.36it/s] 75%|███████▌  | 375/500 [01:55<00:37,  3.36it/s] 75%|███████▌  | 376/500 [01:55<00:36,  3.36it/s] 75%|███████▌  | 377/500 [01:55<00:36,  3.36it/s] 76%|███████▌  | 378/500 [01:55<00:36,  3.37it/s] 76%|███████▌  | 379/500 [01:56<00:35,  3.37it/s] 76%|███████▌  | 380/500 [01:56<00:35,  3.37it/s] 76%|███████▌  | 381/500 [01:56<00:35,  3.37it/s] 76%|███████▋  | 382/500 [01:57<00:35,  3.36it/s] 77%|███████▋  | 383/500 [01:57<00:34,  3.37it/s] 77%|███████▋  | 384/500 [01:57<00:34,  3.36it/s] 77%|███████▋  | 385/500 [01:58<00:34,  3.36it/s] 77%|███████▋  | 386/500 [01:58<00:33,  3.37it/s] 77%|███████▋  | 387/500 [01:58<00:33,  3.36it/s] 78%|███████▊  | 388/500 [01:58<00:33,  3.36it/s] 78%|███████▊  | 389/500 [01:59<00:33,  3.36it/s] 78%|███████▊  | 390/500 [01:59<00:32,  3.36it/s] 78%|███████▊  | 391/500 [01:59<00:32,  3.32it/s] 78%|███████▊  | 392/500 [02:00<00:32,  3.32it/s] 79%|███████▊  | 393/500 [02:00<00:32,  3.34it/s] 79%|███████▉  | 394/500 [02:00<00:31,  3.35it/s] 79%|███████▉  | 395/500 [02:01<00:31,  3.36it/s] 79%|███████▉  | 396/500 [02:01<00:30,  3.37it/s] 79%|███████▉  | 397/500 [02:01<00:30,  3.36it/s] 80%|███████▉  | 398/500 [02:01<00:30,  3.36it/s] 80%|███████▉  | 399/500 [02:02<00:30,  3.37it/s] 80%|████████  | 400/500 [02:02<00:29,  3.37it/s] 80%|████████  | 401/500 [02:02<00:29,  3.38it/s] 80%|████████  | 402/500 [02:03<00:29,  3.37it/s] 81%|████████  | 403/500 [02:03<00:28,  3.37it/s] 81%|████████  | 404/500 [02:03<00:28,  3.37it/s] 81%|████████  | 405/500 [02:04<00:28,  3.37it/s] 81%|████████  | 406/500 [02:04<00:27,  3.36it/s] 81%|████████▏ | 407/500 [02:04<00:27,  3.37it/s] 82%|████████▏ | 408/500 [02:04<00:27,  3.36it/s] 82%|████████▏ | 409/500 [02:05<00:27,  3.36it/s] 82%|████████▏ | 410/500 [02:05<00:26,  3.37it/s] 82%|████████▏ | 411/500 [02:05<00:26,  3.36it/s] 82%|████████▏ | 412/500 [02:06<00:26,  3.36it/s] 83%|████████▎ | 413/500 [02:06<00:25,  3.36it/s] 83%|████████▎ | 414/500 [02:06<00:25,  3.37it/s] 83%|████████▎ | 415/500 [02:06<00:25,  3.36it/s] 83%|████████▎ | 416/500 [02:07<00:24,  3.37it/s] 83%|████████▎ | 417/500 [02:07<00:24,  3.37it/s] 84%|████████▎ | 418/500 [02:07<00:24,  3.37it/s] 84%|████████▍ | 419/500 [02:08<00:24,  3.37it/s] 84%|████████▍ | 420/500 [02:08<00:23,  3.37it/s] 84%|████████▍ | 421/500 [02:08<00:23,  3.37it/s] 84%|████████▍ | 422/500 [02:09<00:23,  3.36it/s] 85%|████████▍ | 423/500 [02:09<00:22,  3.37it/s] 85%|████████▍ | 424/500 [02:09<00:22,  3.37it/s] 85%|████████▌ | 425/500 [02:09<00:22,  3.36it/s] 85%|████████▌ | 426/500 [02:10<00:22,  3.36it/s] 85%|████████▌ | 427/500 [02:10<00:21,  3.36it/s] 86%|████████▌ | 428/500 [02:10<00:21,  3.36it/s] 86%|████████▌ | 429/500 [02:11<00:21,  3.36it/s] 86%|████████▌ | 430/500 [02:11<00:20,  3.36it/s] 86%|████████▌ | 431/500 [02:11<00:20,  3.36it/s] 86%|████████▋ | 432/500 [02:12<00:20,  3.36it/s] 87%|████████▋ | 433/500 [02:12<00:19,  3.36it/s] 87%|████████▋ | 434/500 [02:12<00:19,  3.36it/s] 87%|████████▋ | 435/500 [02:12<00:19,  3.36it/s] 87%|████████▋ | 436/500 [02:13<00:19,  3.36it/s] 87%|████████▋ | 437/500 [02:13<00:18,  3.36it/s] 88%|████████▊ | 438/500 [02:13<00:18,  3.36it/s] 88%|████████▊ | 439/500 [02:14<00:18,  3.36it/s] 88%|████████▊ | 440/500 [02:14<00:17,  3.37it/s] 88%|████████▊ | 441/500 [02:14<00:17,  3.37it/s] 88%|████████▊ | 442/500 [02:15<00:17,  3.36it/s] 89%|████████▊ | 443/500 [02:15<00:16,  3.36it/s] 89%|████████▉ | 444/500 [02:15<00:16,  3.37it/s] 89%|████████▉ | 445/500 [02:15<00:16,  3.36it/s] 89%|████████▉ | 446/500 [02:16<00:16,  3.36it/s] 89%|████████▉ | 447/500 [02:16<00:15,  3.36it/s] 90%|████████▉ | 448/500 [02:16<00:15,  3.37it/s] 90%|████████▉ | 449/500 [02:17<00:15,  3.37it/s] 90%|█████████ | 450/500 [02:17<00:14,  3.37it/s] 90%|█████████ | 451/500 [02:17<00:14,  3.36it/s] 90%|█████████ | 452/500 [02:17<00:14,  3.36it/s] 91%|█████████ | 453/500 [02:18<00:14,  3.36it/s] 91%|█████████ | 454/500 [02:18<00:13,  3.36it/s] 91%|█████████ | 455/500 [02:18<00:13,  3.37it/s] 91%|█████████ | 456/500 [02:19<00:13,  3.36it/s] 91%|█████████▏| 457/500 [02:19<00:12,  3.36it/s] 92%|█████████▏| 458/500 [02:19<00:12,  3.36it/s] 92%|█████████▏| 459/500 [02:20<00:12,  3.36it/s] 92%|█████████▏| 460/500 [02:20<00:11,  3.36it/s] 92%|█████████▏| 461/500 [02:20<00:11,  3.36it/s] 92%|█████████▏| 462/500 [02:20<00:11,  3.36it/s] 93%|█████████▎| 463/500 [02:21<00:10,  3.37it/s] 93%|█████████▎| 464/500 [02:21<00:10,  3.37it/s] 93%|█████████▎| 465/500 [02:21<00:10,  3.37it/s] 93%|█████████▎| 466/500 [02:22<00:10,  3.37it/s] 93%|█████████▎| 467/500 [02:22<00:09,  3.37it/s] 94%|█████████▎| 468/500 [02:22<00:09,  3.36it/s] 94%|█████████▍| 469/500 [02:23<00:09,  3.36it/s] 94%|█████████▍| 470/500 [02:23<00:08,  3.36it/s] 94%|█████████▍| 471/500 [02:23<00:08,  3.36it/s] 94%|█████████▍| 472/500 [02:23<00:08,  3.36it/s] 95%|█████████▍| 473/500 [02:24<00:08,  3.37it/s] 95%|█████████▍| 474/500 [02:24<00:07,  3.36it/s] 95%|█████████▌| 475/500 [02:24<00:07,  3.36it/s] 95%|█████████▌| 476/500 [02:25<00:07,  3.36it/s] 95%|█████████▌| 477/500 [02:25<00:06,  3.36it/s] 96%|█████████▌| 478/500 [02:25<00:06,  3.36it/s] 96%|█████████▌| 479/500 [02:26<00:06,  3.36it/s] 96%|█████████▌| 480/500 [02:26<00:05,  3.36it/s] 96%|█████████▌| 481/500 [02:26<00:05,  3.36it/s] 96%|█████████▋| 482/500 [02:26<00:05,  3.36it/s] 97%|█████████▋| 483/500 [02:27<00:05,  3.35it/s] 97%|█████████▋| 484/500 [02:27<00:04,  3.35it/s] 97%|█████████▋| 485/500 [02:27<00:04,  3.35it/s] 97%|█████████▋| 486/500 [02:28<00:04,  3.36it/s] 97%|█████████▋| 487/500 [02:28<00:03,  3.36it/s] 98%|█████████▊| 488/500 [02:28<00:03,  3.37it/s] 98%|█████████▊| 489/500 [02:28<00:03,  3.37it/s] 98%|█████████▊| 490/500 [02:29<00:02,  3.37it/s] 98%|█████████▊| 491/500 [02:29<00:02,  3.38it/s] 98%|█████████▊| 492/500 [02:29<00:02,  3.38it/s] 99%|█████████▊| 493/500 [02:30<00:02,  3.38it/s] 99%|█████████▉| 494/500 [02:30<00:01,  3.38it/s] 99%|█████████▉| 495/500 [02:30<00:01,  3.38it/s] 99%|█████████▉| 496/500 [02:31<00:01,  3.38it/s] 99%|█████████▉| 497/500 [02:31<00:00,  3.38it/s]100%|█████████▉| 498/500 [02:31<00:00,  3.38it/s]100%|█████████▉| 499/500 [02:31<00:00,  3.38it/s]100%|██████████| 500/500 [02:32<00:00,  3.38it/s]100%|██████████| 500/500 [02:32<00:00,  3.28it/s]
=> result
* total: 50,000
* correct: 20,975
* accuracy: 42.0%
* error: 58.0%
* macro_f1: 36.8%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft imagenet 2 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/imagenet.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/imagenet/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: ImageNet
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/imagenet/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:ImageNet
Loading dataset: ImageNet
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/ImageNet/split_fewshot_taesup/shot_16-seed_2.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  --------
Dataset    ImageNet
# classes  1,000
# train_x  16,000
# val      50,000
# test     50,000
---------  --------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed2/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:04<36:53,  4.44s/it]  0%|          | 2/500 [00:04<16:34,  2.00s/it]  1%|          | 3/500 [00:05<10:05,  1.22s/it]  1%|          | 4/500 [00:05<07:02,  1.17it/s]  1%|          | 5/500 [00:05<05:22,  1.54it/s]  1%|          | 6/500 [00:05<04:21,  1.89it/s]  1%|▏         | 7/500 [00:06<03:42,  2.21it/s]  2%|▏         | 8/500 [00:06<03:18,  2.48it/s]  2%|▏         | 9/500 [00:06<03:00,  2.71it/s]  2%|▏         | 10/500 [00:07<02:48,  2.90it/s]  2%|▏         | 11/500 [00:07<02:40,  3.04it/s]  2%|▏         | 12/500 [00:07<02:35,  3.14it/s]  3%|▎         | 13/500 [00:07<02:31,  3.22it/s]  3%|▎         | 14/500 [00:08<02:28,  3.28it/s]  3%|▎         | 15/500 [00:08<02:26,  3.32it/s]  3%|▎         | 16/500 [00:08<02:24,  3.34it/s]  3%|▎         | 17/500 [00:09<02:23,  3.36it/s]  4%|▎         | 18/500 [00:09<02:22,  3.37it/s]  4%|▍         | 19/500 [00:09<02:22,  3.39it/s]  4%|▍         | 20/500 [00:10<02:21,  3.40it/s]  4%|▍         | 21/500 [00:10<02:20,  3.40it/s]  4%|▍         | 22/500 [00:10<02:20,  3.39it/s]  5%|▍         | 23/500 [00:10<02:20,  3.40it/s]  5%|▍         | 24/500 [00:11<02:20,  3.40it/s]  5%|▌         | 25/500 [00:11<02:19,  3.40it/s]  5%|▌         | 26/500 [00:11<02:19,  3.40it/s]  5%|▌         | 27/500 [00:12<02:19,  3.40it/s]  6%|▌         | 28/500 [00:12<02:18,  3.41it/s]  6%|▌         | 29/500 [00:12<02:18,  3.41it/s]  6%|▌         | 30/500 [00:12<02:17,  3.41it/s]  6%|▌         | 31/500 [00:13<02:17,  3.41it/s]  6%|▋         | 32/500 [00:13<02:17,  3.41it/s]  7%|▋         | 33/500 [00:13<02:17,  3.41it/s]  7%|▋         | 34/500 [00:14<02:16,  3.41it/s]  7%|▋         | 35/500 [00:14<02:16,  3.41it/s]  7%|▋         | 36/500 [00:14<02:15,  3.42it/s]  7%|▋         | 37/500 [00:14<02:15,  3.42it/s]  8%|▊         | 38/500 [00:15<02:15,  3.41it/s]  8%|▊         | 39/500 [00:15<02:15,  3.40it/s]  8%|▊         | 40/500 [00:15<02:15,  3.40it/s]  8%|▊         | 41/500 [00:16<02:14,  3.40it/s]  8%|▊         | 42/500 [00:16<02:14,  3.40it/s]  9%|▊         | 43/500 [00:16<02:14,  3.40it/s]  9%|▉         | 44/500 [00:17<02:13,  3.40it/s]  9%|▉         | 45/500 [00:17<02:14,  3.39it/s]  9%|▉         | 46/500 [00:17<02:13,  3.39it/s]  9%|▉         | 47/500 [00:17<02:13,  3.40it/s] 10%|▉         | 48/500 [00:18<02:13,  3.39it/s] 10%|▉         | 49/500 [00:18<02:12,  3.40it/s] 10%|█         | 50/500 [00:18<02:12,  3.40it/s] 10%|█         | 51/500 [00:19<02:12,  3.39it/s] 10%|█         | 52/500 [00:19<02:12,  3.39it/s] 11%|█         | 53/500 [00:19<02:11,  3.39it/s] 11%|█         | 54/500 [00:19<02:11,  3.40it/s] 11%|█         | 55/500 [00:20<02:10,  3.40it/s] 11%|█         | 56/500 [00:20<02:10,  3.40it/s] 11%|█▏        | 57/500 [00:20<02:10,  3.40it/s] 12%|█▏        | 58/500 [00:21<02:10,  3.39it/s] 12%|█▏        | 59/500 [00:21<02:09,  3.40it/s] 12%|█▏        | 60/500 [00:21<02:09,  3.40it/s] 12%|█▏        | 61/500 [00:22<02:09,  3.40it/s] 12%|█▏        | 62/500 [00:22<02:09,  3.39it/s] 13%|█▎        | 63/500 [00:22<02:08,  3.40it/s] 13%|█▎        | 64/500 [00:22<02:08,  3.40it/s] 13%|█▎        | 65/500 [00:23<02:07,  3.40it/s] 13%|█▎        | 66/500 [00:23<02:07,  3.40it/s] 13%|█▎        | 67/500 [00:23<02:07,  3.39it/s] 14%|█▎        | 68/500 [00:24<02:07,  3.39it/s] 14%|█▍        | 69/500 [00:24<02:06,  3.40it/s] 14%|█▍        | 70/500 [00:24<02:06,  3.40it/s] 14%|█▍        | 71/500 [00:24<02:06,  3.40it/s] 14%|█▍        | 72/500 [00:25<02:06,  3.40it/s] 15%|█▍        | 73/500 [00:25<02:05,  3.40it/s] 15%|█▍        | 74/500 [00:25<02:05,  3.39it/s] 15%|█▌        | 75/500 [00:26<02:05,  3.39it/s] 15%|█▌        | 76/500 [00:26<02:05,  3.38it/s] 15%|█▌        | 77/500 [00:26<02:04,  3.38it/s] 16%|█▌        | 78/500 [00:27<02:04,  3.38it/s] 16%|█▌        | 79/500 [00:27<02:04,  3.39it/s] 16%|█▌        | 80/500 [00:27<02:03,  3.39it/s] 16%|█▌        | 81/500 [00:27<02:03,  3.38it/s] 16%|█▋        | 82/500 [00:28<02:03,  3.38it/s] 17%|█▋        | 83/500 [00:28<02:03,  3.39it/s] 17%|█▋        | 84/500 [00:28<02:02,  3.39it/s] 17%|█▋        | 85/500 [00:29<02:02,  3.39it/s] 17%|█▋        | 86/500 [00:29<02:02,  3.39it/s] 17%|█▋        | 87/500 [00:29<02:01,  3.39it/s] 18%|█▊        | 88/500 [00:30<02:01,  3.38it/s] 18%|█▊        | 89/500 [00:30<02:01,  3.39it/s] 18%|█▊        | 90/500 [00:30<02:00,  3.40it/s] 18%|█▊        | 91/500 [00:30<02:00,  3.40it/s] 18%|█▊        | 92/500 [00:31<02:00,  3.39it/s] 19%|█▊        | 93/500 [00:31<02:00,  3.39it/s] 19%|█▉        | 94/500 [00:31<02:00,  3.38it/s] 19%|█▉        | 95/500 [00:32<02:00,  3.37it/s] 19%|█▉        | 96/500 [00:32<01:59,  3.38it/s] 19%|█▉        | 97/500 [00:32<01:59,  3.37it/s] 20%|█▉        | 98/500 [00:32<01:59,  3.38it/s] 20%|█▉        | 99/500 [00:33<01:58,  3.38it/s] 20%|██        | 100/500 [00:33<01:58,  3.38it/s] 20%|██        | 101/500 [00:33<01:58,  3.38it/s] 20%|██        | 102/500 [00:34<01:57,  3.38it/s] 21%|██        | 103/500 [00:34<01:57,  3.38it/s] 21%|██        | 104/500 [00:34<01:57,  3.38it/s] 21%|██        | 105/500 [00:35<01:56,  3.38it/s] 21%|██        | 106/500 [00:35<01:56,  3.38it/s] 21%|██▏       | 107/500 [00:35<01:56,  3.38it/s] 22%|██▏       | 108/500 [00:35<01:55,  3.38it/s] 22%|██▏       | 109/500 [00:36<01:55,  3.37it/s] 22%|██▏       | 110/500 [00:36<01:55,  3.38it/s] 22%|██▏       | 111/500 [00:36<01:55,  3.38it/s] 22%|██▏       | 112/500 [00:37<01:54,  3.38it/s] 23%|██▎       | 113/500 [00:37<01:54,  3.38it/s] 23%|██▎       | 114/500 [00:37<01:54,  3.38it/s] 23%|██▎       | 115/500 [00:38<01:53,  3.38it/s] 23%|██▎       | 116/500 [00:38<01:53,  3.37it/s] 23%|██▎       | 117/500 [00:38<01:53,  3.38it/s] 24%|██▎       | 118/500 [00:38<01:52,  3.38it/s] 24%|██▍       | 119/500 [00:39<01:52,  3.38it/s] 24%|██▍       | 120/500 [00:39<01:52,  3.38it/s] 24%|██▍       | 121/500 [00:39<01:52,  3.38it/s] 24%|██▍       | 122/500 [00:40<01:51,  3.38it/s] 25%|██▍       | 123/500 [00:40<01:51,  3.38it/s] 25%|██▍       | 124/500 [00:40<01:51,  3.38it/s] 25%|██▌       | 125/500 [00:40<01:50,  3.38it/s] 25%|██▌       | 126/500 [00:41<01:50,  3.38it/s] 25%|██▌       | 127/500 [00:41<01:50,  3.38it/s] 26%|██▌       | 128/500 [00:41<01:49,  3.39it/s] 26%|██▌       | 129/500 [00:42<01:49,  3.38it/s] 26%|██▌       | 130/500 [00:42<01:49,  3.39it/s] 26%|██▌       | 131/500 [00:42<01:49,  3.37it/s] 26%|██▋       | 132/500 [00:43<01:49,  3.37it/s] 27%|██▋       | 133/500 [00:43<01:48,  3.37it/s] 27%|██▋       | 134/500 [00:43<01:48,  3.37it/s] 27%|██▋       | 135/500 [00:43<01:48,  3.37it/s] 27%|██▋       | 136/500 [00:44<01:47,  3.37it/s] 27%|██▋       | 137/500 [00:44<01:47,  3.37it/s] 28%|██▊       | 138/500 [00:44<01:47,  3.38it/s] 28%|██▊       | 139/500 [00:45<01:46,  3.38it/s] 28%|██▊       | 140/500 [00:45<01:46,  3.38it/s] 28%|██▊       | 141/500 [00:45<01:46,  3.38it/s] 28%|██▊       | 142/500 [00:45<01:45,  3.38it/s] 29%|██▊       | 143/500 [00:46<01:45,  3.38it/s] 29%|██▉       | 144/500 [00:46<01:45,  3.38it/s] 29%|██▉       | 145/500 [00:46<01:44,  3.38it/s] 29%|██▉       | 146/500 [00:47<01:44,  3.38it/s] 29%|██▉       | 147/500 [00:47<01:44,  3.38it/s] 30%|██▉       | 148/500 [00:47<01:44,  3.37it/s] 30%|██▉       | 149/500 [00:48<01:44,  3.37it/s] 30%|███       | 150/500 [00:48<01:43,  3.38it/s] 30%|███       | 151/500 [00:48<01:43,  3.38it/s] 30%|███       | 152/500 [00:48<01:42,  3.38it/s] 31%|███       | 153/500 [00:49<01:42,  3.38it/s] 31%|███       | 154/500 [00:49<01:42,  3.38it/s] 31%|███       | 155/500 [00:49<01:42,  3.38it/s] 31%|███       | 156/500 [00:50<01:41,  3.38it/s] 31%|███▏      | 157/500 [00:50<01:41,  3.38it/s] 32%|███▏      | 158/500 [00:50<01:41,  3.37it/s] 32%|███▏      | 159/500 [00:51<01:41,  3.37it/s] 32%|███▏      | 160/500 [00:51<01:40,  3.37it/s] 32%|███▏      | 161/500 [00:51<01:40,  3.37it/s] 32%|███▏      | 162/500 [00:51<01:40,  3.37it/s] 33%|███▎      | 163/500 [00:52<01:40,  3.37it/s] 33%|███▎      | 164/500 [00:52<01:39,  3.36it/s] 33%|███▎      | 165/500 [00:52<01:39,  3.36it/s] 33%|███▎      | 166/500 [00:53<01:39,  3.37it/s] 33%|███▎      | 167/500 [00:53<01:39,  3.36it/s] 34%|███▎      | 168/500 [00:53<01:38,  3.36it/s] 34%|███▍      | 169/500 [00:54<01:38,  3.36it/s] 34%|███▍      | 170/500 [00:54<01:37,  3.37it/s] 34%|███▍      | 171/500 [00:54<01:37,  3.37it/s] 34%|███▍      | 172/500 [00:54<01:37,  3.36it/s] 35%|███▍      | 173/500 [00:55<01:37,  3.37it/s] 35%|███▍      | 174/500 [00:55<01:36,  3.36it/s] 35%|███▌      | 175/500 [00:55<01:36,  3.37it/s] 35%|███▌      | 176/500 [00:56<01:36,  3.36it/s] 35%|███▌      | 177/500 [00:56<01:36,  3.36it/s] 36%|███▌      | 178/500 [00:56<01:35,  3.36it/s] 36%|███▌      | 179/500 [00:56<01:35,  3.36it/s] 36%|███▌      | 180/500 [00:57<01:35,  3.36it/s] 36%|███▌      | 181/500 [00:57<01:34,  3.36it/s] 36%|███▋      | 182/500 [00:57<01:34,  3.36it/s] 37%|███▋      | 183/500 [00:58<01:34,  3.36it/s] 37%|███▋      | 184/500 [00:58<01:34,  3.36it/s] 37%|███▋      | 185/500 [00:58<01:33,  3.36it/s] 37%|███▋      | 186/500 [00:59<01:33,  3.36it/s] 37%|███▋      | 187/500 [00:59<01:33,  3.36it/s] 38%|███▊      | 188/500 [00:59<01:32,  3.37it/s] 38%|███▊      | 189/500 [00:59<01:32,  3.37it/s] 38%|███▊      | 190/500 [01:00<01:31,  3.37it/s] 38%|███▊      | 191/500 [01:00<01:31,  3.37it/s] 38%|███▊      | 192/500 [01:00<01:31,  3.37it/s] 39%|███▊      | 193/500 [01:01<01:31,  3.36it/s] 39%|███▉      | 194/500 [01:01<01:30,  3.37it/s] 39%|███▉      | 195/500 [01:01<01:30,  3.37it/s] 39%|███▉      | 196/500 [01:02<01:30,  3.37it/s] 39%|███▉      | 197/500 [01:02<01:29,  3.37it/s] 40%|███▉      | 198/500 [01:02<01:29,  3.37it/s] 40%|███▉      | 199/500 [01:02<01:29,  3.37it/s] 40%|████      | 200/500 [01:03<01:29,  3.37it/s] 40%|████      | 201/500 [01:03<01:28,  3.37it/s] 40%|████      | 202/500 [01:03<01:28,  3.38it/s] 41%|████      | 203/500 [01:04<01:28,  3.37it/s] 41%|████      | 204/500 [01:04<01:27,  3.37it/s] 41%|████      | 205/500 [01:04<01:27,  3.36it/s] 41%|████      | 206/500 [01:04<01:27,  3.37it/s] 41%|████▏     | 207/500 [01:05<01:26,  3.37it/s] 42%|████▏     | 208/500 [01:05<01:26,  3.37it/s] 42%|████▏     | 209/500 [01:05<01:26,  3.38it/s] 42%|████▏     | 210/500 [01:06<01:25,  3.37it/s] 42%|████▏     | 211/500 [01:06<01:26,  3.35it/s] 42%|████▏     | 212/500 [01:06<01:25,  3.36it/s] 43%|████▎     | 213/500 [01:07<01:25,  3.35it/s] 43%|████▎     | 214/500 [01:07<01:25,  3.36it/s] 43%|████▎     | 215/500 [01:07<01:24,  3.36it/s] 43%|████▎     | 216/500 [01:07<01:24,  3.36it/s] 43%|████▎     | 217/500 [01:08<01:24,  3.37it/s] 44%|████▎     | 218/500 [01:08<01:23,  3.37it/s] 44%|████▍     | 219/500 [01:08<01:23,  3.38it/s] 44%|████▍     | 220/500 [01:09<01:22,  3.37it/s] 44%|████▍     | 221/500 [01:09<01:22,  3.36it/s] 44%|████▍     | 222/500 [01:09<01:22,  3.37it/s] 45%|████▍     | 223/500 [01:10<01:22,  3.34it/s] 45%|████▍     | 224/500 [01:10<01:22,  3.34it/s] 45%|████▌     | 225/500 [01:10<01:22,  3.35it/s] 45%|████▌     | 226/500 [01:10<01:21,  3.35it/s] 45%|████▌     | 227/500 [01:11<01:21,  3.35it/s] 46%|████▌     | 228/500 [01:11<01:21,  3.36it/s] 46%|████▌     | 229/500 [01:11<01:20,  3.36it/s] 46%|████▌     | 230/500 [01:12<01:20,  3.36it/s] 46%|████▌     | 231/500 [01:12<01:19,  3.36it/s] 46%|████▋     | 232/500 [01:12<01:19,  3.36it/s] 47%|████▋     | 233/500 [01:13<01:19,  3.36it/s] 47%|████▋     | 234/500 [01:13<01:19,  3.36it/s] 47%|████▋     | 235/500 [01:13<01:18,  3.36it/s] 47%|████▋     | 236/500 [01:13<01:18,  3.36it/s] 47%|████▋     | 237/500 [01:14<01:18,  3.36it/s] 48%|████▊     | 238/500 [01:14<01:17,  3.37it/s] 48%|████▊     | 239/500 [01:14<01:17,  3.37it/s] 48%|████▊     | 240/500 [01:15<01:17,  3.37it/s] 48%|████▊     | 241/500 [01:15<01:16,  3.36it/s] 48%|████▊     | 242/500 [01:15<01:16,  3.37it/s] 49%|████▊     | 243/500 [01:15<01:16,  3.37it/s] 49%|████▉     | 244/500 [01:16<01:15,  3.37it/s] 49%|████▉     | 245/500 [01:16<01:16,  3.34it/s] 49%|████▉     | 246/500 [01:16<01:15,  3.35it/s] 49%|████▉     | 247/500 [01:17<01:15,  3.36it/s] 50%|████▉     | 248/500 [01:17<01:14,  3.36it/s] 50%|████▉     | 249/500 [01:17<01:14,  3.37it/s] 50%|█████     | 250/500 [01:18<01:14,  3.37it/s] 50%|█████     | 251/500 [01:18<01:13,  3.37it/s] 50%|█████     | 252/500 [01:18<01:13,  3.37it/s] 51%|█████     | 253/500 [01:18<01:13,  3.36it/s] 51%|█████     | 254/500 [01:19<01:13,  3.36it/s] 51%|█████     | 255/500 [01:19<01:12,  3.37it/s] 51%|█████     | 256/500 [01:19<01:12,  3.36it/s] 51%|█████▏    | 257/500 [01:20<01:12,  3.37it/s] 52%|█████▏    | 258/500 [01:20<01:11,  3.37it/s] 52%|█████▏    | 259/500 [01:20<01:11,  3.37it/s] 52%|█████▏    | 260/500 [01:21<01:11,  3.37it/s] 52%|█████▏    | 261/500 [01:21<01:10,  3.37it/s] 52%|█████▏    | 262/500 [01:21<01:10,  3.37it/s] 53%|█████▎    | 263/500 [01:21<01:10,  3.37it/s] 53%|█████▎    | 264/500 [01:22<01:10,  3.37it/s] 53%|█████▎    | 265/500 [01:22<01:09,  3.37it/s] 53%|█████▎    | 266/500 [01:22<01:09,  3.37it/s] 53%|█████▎    | 267/500 [01:23<01:09,  3.37it/s] 54%|█████▎    | 268/500 [01:23<01:09,  3.36it/s] 54%|█████▍    | 269/500 [01:23<01:08,  3.36it/s] 54%|█████▍    | 270/500 [01:24<01:08,  3.36it/s] 54%|█████▍    | 271/500 [01:24<01:08,  3.35it/s] 54%|█████▍    | 272/500 [01:24<01:07,  3.36it/s] 55%|█████▍    | 273/500 [01:24<01:07,  3.36it/s] 55%|█████▍    | 274/500 [01:25<01:07,  3.36it/s] 55%|█████▌    | 275/500 [01:25<01:06,  3.36it/s] 55%|█████▌    | 276/500 [01:25<01:06,  3.36it/s] 55%|█████▌    | 277/500 [01:26<01:06,  3.36it/s] 56%|█████▌    | 278/500 [01:26<01:06,  3.36it/s] 56%|█████▌    | 279/500 [01:26<01:05,  3.36it/s] 56%|█████▌    | 280/500 [01:27<01:05,  3.36it/s] 56%|█████▌    | 281/500 [01:27<01:05,  3.36it/s] 56%|█████▋    | 282/500 [01:27<01:04,  3.36it/s] 57%|█████▋    | 283/500 [01:27<01:04,  3.36it/s] 57%|█████▋    | 284/500 [01:28<01:04,  3.36it/s] 57%|█████▋    | 285/500 [01:28<01:04,  3.36it/s] 57%|█████▋    | 286/500 [01:28<01:03,  3.36it/s] 57%|█████▋    | 287/500 [01:29<01:03,  3.36it/s] 58%|█████▊    | 288/500 [01:29<01:03,  3.36it/s] 58%|█████▊    | 289/500 [01:29<01:02,  3.36it/s] 58%|█████▊    | 290/500 [01:29<01:02,  3.36it/s] 58%|█████▊    | 291/500 [01:30<01:02,  3.36it/s] 58%|█████▊    | 292/500 [01:30<01:01,  3.36it/s] 59%|█████▊    | 293/500 [01:30<01:01,  3.36it/s] 59%|█████▉    | 294/500 [01:31<01:01,  3.36it/s] 59%|█████▉    | 295/500 [01:31<01:01,  3.35it/s] 59%|█████▉    | 296/500 [01:31<01:00,  3.35it/s] 59%|█████▉    | 297/500 [01:32<01:00,  3.35it/s] 60%|█████▉    | 298/500 [01:32<01:00,  3.35it/s] 60%|█████▉    | 299/500 [01:32<00:59,  3.36it/s] 60%|██████    | 300/500 [01:32<00:59,  3.36it/s] 60%|██████    | 301/500 [01:33<00:59,  3.36it/s] 60%|██████    | 302/500 [01:33<00:59,  3.36it/s] 61%|██████    | 303/500 [01:33<00:58,  3.35it/s] 61%|██████    | 304/500 [01:34<00:58,  3.35it/s] 61%|██████    | 305/500 [01:34<00:58,  3.35it/s] 61%|██████    | 306/500 [01:34<00:57,  3.35it/s] 61%|██████▏   | 307/500 [01:35<00:57,  3.36it/s] 62%|██████▏   | 308/500 [01:35<00:57,  3.36it/s] 62%|██████▏   | 309/500 [01:35<00:56,  3.36it/s] 62%|██████▏   | 310/500 [01:35<00:56,  3.36it/s] 62%|██████▏   | 311/500 [01:36<00:56,  3.36it/s] 62%|██████▏   | 312/500 [01:36<00:55,  3.36it/s] 63%|██████▎   | 313/500 [01:36<00:55,  3.36it/s] 63%|██████▎   | 314/500 [01:37<00:55,  3.36it/s] 63%|██████▎   | 315/500 [01:37<00:55,  3.36it/s] 63%|██████▎   | 316/500 [01:37<00:54,  3.36it/s] 63%|██████▎   | 317/500 [01:38<00:54,  3.33it/s] 64%|██████▎   | 318/500 [01:38<00:54,  3.34it/s] 64%|██████▍   | 319/500 [01:38<00:53,  3.35it/s] 64%|██████▍   | 320/500 [01:38<00:53,  3.36it/s] 64%|██████▍   | 321/500 [01:39<00:53,  3.35it/s] 64%|██████▍   | 322/500 [01:39<00:52,  3.36it/s] 65%|██████▍   | 323/500 [01:39<00:52,  3.36it/s] 65%|██████▍   | 324/500 [01:40<00:52,  3.36it/s] 65%|██████▌   | 325/500 [01:40<00:52,  3.36it/s] 65%|██████▌   | 326/500 [01:40<00:51,  3.36it/s] 65%|██████▌   | 327/500 [01:41<00:51,  3.36it/s] 66%|██████▌   | 328/500 [01:41<00:51,  3.36it/s] 66%|██████▌   | 329/500 [01:41<00:50,  3.36it/s] 66%|██████▌   | 330/500 [01:41<00:50,  3.36it/s] 66%|██████▌   | 331/500 [01:42<00:50,  3.36it/s] 66%|██████▋   | 332/500 [01:42<00:50,  3.36it/s] 67%|██████▋   | 333/500 [01:42<00:49,  3.36it/s] 67%|██████▋   | 334/500 [01:43<00:49,  3.36it/s] 67%|██████▋   | 335/500 [01:43<00:49,  3.36it/s] 67%|██████▋   | 336/500 [01:43<00:48,  3.36it/s] 67%|██████▋   | 337/500 [01:43<00:48,  3.36it/s] 68%|██████▊   | 338/500 [01:44<00:48,  3.36it/s] 68%|██████▊   | 339/500 [01:44<00:47,  3.36it/s] 68%|██████▊   | 340/500 [01:44<00:47,  3.36it/s] 68%|██████▊   | 341/500 [01:45<00:47,  3.35it/s] 68%|██████▊   | 342/500 [01:45<00:47,  3.35it/s] 69%|██████▊   | 343/500 [01:45<00:46,  3.36it/s] 69%|██████▉   | 344/500 [01:46<00:46,  3.35it/s] 69%|██████▉   | 345/500 [01:46<00:46,  3.35it/s] 69%|██████▉   | 346/500 [01:46<00:45,  3.35it/s] 69%|██████▉   | 347/500 [01:46<00:45,  3.35it/s] 70%|██████▉   | 348/500 [01:47<00:45,  3.35it/s] 70%|██████▉   | 349/500 [01:47<00:45,  3.35it/s] 70%|███████   | 350/500 [01:47<00:44,  3.35it/s] 70%|███████   | 351/500 [01:48<00:44,  3.36it/s] 70%|███████   | 352/500 [01:48<00:44,  3.35it/s] 71%|███████   | 353/500 [01:48<00:43,  3.35it/s] 71%|███████   | 354/500 [01:49<00:43,  3.35it/s] 71%|███████   | 355/500 [01:49<00:43,  3.35it/s] 71%|███████   | 356/500 [01:49<00:42,  3.36it/s] 71%|███████▏  | 357/500 [01:49<00:42,  3.36it/s] 72%|███████▏  | 358/500 [01:50<00:42,  3.36it/s] 72%|███████▏  | 359/500 [01:50<00:41,  3.36it/s] 72%|███████▏  | 360/500 [01:50<00:41,  3.36it/s] 72%|███████▏  | 361/500 [01:51<00:41,  3.36it/s] 72%|███████▏  | 362/500 [01:51<00:41,  3.36it/s] 73%|███████▎  | 363/500 [01:51<00:40,  3.36it/s] 73%|███████▎  | 364/500 [01:52<00:40,  3.36it/s] 73%|███████▎  | 365/500 [01:52<00:40,  3.35it/s] 73%|███████▎  | 366/500 [01:52<00:39,  3.36it/s] 73%|███████▎  | 367/500 [01:52<00:39,  3.36it/s] 74%|███████▎  | 368/500 [01:53<00:39,  3.36it/s] 74%|███████▍  | 369/500 [01:53<00:38,  3.36it/s] 74%|███████▍  | 370/500 [01:53<00:38,  3.36it/s] 74%|███████▍  | 371/500 [01:54<00:38,  3.36it/s] 74%|███████▍  | 372/500 [01:54<00:38,  3.36it/s] 75%|███████▍  | 373/500 [01:54<00:37,  3.35it/s] 75%|███████▍  | 374/500 [01:55<00:37,  3.35it/s] 75%|███████▌  | 375/500 [01:55<00:37,  3.36it/s] 75%|███████▌  | 376/500 [01:55<00:36,  3.35it/s] 75%|███████▌  | 377/500 [01:55<00:36,  3.35it/s] 76%|███████▌  | 378/500 [01:56<00:36,  3.35it/s] 76%|███████▌  | 379/500 [01:56<00:36,  3.35it/s] 76%|███████▌  | 380/500 [01:56<00:35,  3.35it/s] 76%|███████▌  | 381/500 [01:57<00:35,  3.35it/s] 76%|███████▋  | 382/500 [01:57<00:35,  3.36it/s] 77%|███████▋  | 383/500 [01:57<00:34,  3.36it/s] 77%|███████▋  | 384/500 [01:57<00:34,  3.36it/s] 77%|███████▋  | 385/500 [01:58<00:34,  3.36it/s] 77%|███████▋  | 386/500 [01:58<00:33,  3.36it/s] 77%|███████▋  | 387/500 [01:58<00:33,  3.36it/s] 78%|███████▊  | 388/500 [01:59<00:33,  3.36it/s] 78%|███████▊  | 389/500 [01:59<00:33,  3.36it/s] 78%|███████▊  | 390/500 [01:59<00:32,  3.36it/s] 78%|███████▊  | 391/500 [02:00<00:32,  3.36it/s] 78%|███████▊  | 392/500 [02:00<00:32,  3.36it/s] 79%|███████▊  | 393/500 [02:00<00:31,  3.36it/s] 79%|███████▉  | 394/500 [02:00<00:31,  3.35it/s] 79%|███████▉  | 395/500 [02:01<00:31,  3.35it/s] 79%|███████▉  | 396/500 [02:01<00:31,  3.35it/s] 79%|███████▉  | 397/500 [02:01<00:30,  3.35it/s] 80%|███████▉  | 398/500 [02:02<00:30,  3.35it/s] 80%|███████▉  | 399/500 [02:02<00:30,  3.35it/s] 80%|████████  | 400/500 [02:02<00:29,  3.35it/s] 80%|████████  | 401/500 [02:03<00:29,  3.35it/s] 80%|████████  | 402/500 [02:03<00:29,  3.35it/s] 81%|████████  | 403/500 [02:03<00:28,  3.36it/s] 81%|████████  | 404/500 [02:03<00:28,  3.36it/s] 81%|████████  | 405/500 [02:04<00:28,  3.36it/s] 81%|████████  | 406/500 [02:04<00:28,  3.36it/s] 81%|████████▏ | 407/500 [02:04<00:27,  3.35it/s] 82%|████████▏ | 408/500 [02:05<00:27,  3.35it/s] 82%|████████▏ | 409/500 [02:05<00:27,  3.36it/s] 82%|████████▏ | 410/500 [02:05<00:26,  3.36it/s] 82%|████████▏ | 411/500 [02:06<00:26,  3.36it/s] 82%|████████▏ | 412/500 [02:06<00:26,  3.36it/s] 83%|████████▎ | 413/500 [02:06<00:25,  3.36it/s] 83%|████████▎ | 414/500 [02:06<00:25,  3.36it/s] 83%|████████▎ | 415/500 [02:07<00:25,  3.36it/s] 83%|████████▎ | 416/500 [02:07<00:25,  3.36it/s] 83%|████████▎ | 417/500 [02:07<00:24,  3.36it/s] 84%|████████▎ | 418/500 [02:08<00:24,  3.36it/s] 84%|████████▍ | 419/500 [02:08<00:24,  3.36it/s] 84%|████████▍ | 420/500 [02:08<00:23,  3.36it/s] 84%|████████▍ | 421/500 [02:09<00:23,  3.36it/s] 84%|████████▍ | 422/500 [02:09<00:23,  3.36it/s] 85%|████████▍ | 423/500 [02:09<00:22,  3.36it/s] 85%|████████▍ | 424/500 [02:09<00:22,  3.36it/s] 85%|████████▌ | 425/500 [02:10<00:22,  3.29it/s] 85%|████████▌ | 426/500 [02:10<00:22,  3.32it/s] 85%|████████▌ | 427/500 [02:10<00:21,  3.33it/s] 86%|████████▌ | 428/500 [02:11<00:21,  3.34it/s] 86%|████████▌ | 429/500 [02:11<00:21,  3.34it/s] 86%|████████▌ | 430/500 [02:11<00:20,  3.35it/s] 86%|████████▌ | 431/500 [02:12<00:20,  3.35it/s] 86%|████████▋ | 432/500 [02:12<00:20,  3.35it/s] 87%|████████▋ | 433/500 [02:12<00:19,  3.36it/s] 87%|████████▋ | 434/500 [02:12<00:19,  3.36it/s] 87%|████████▋ | 435/500 [02:13<00:19,  3.36it/s] 87%|████████▋ | 436/500 [02:13<00:19,  3.34it/s] 87%|████████▋ | 437/500 [02:13<00:18,  3.34it/s] 88%|████████▊ | 438/500 [02:14<00:18,  3.35it/s] 88%|████████▊ | 439/500 [02:14<00:18,  3.35it/s] 88%|████████▊ | 440/500 [02:14<00:17,  3.35it/s] 88%|████████▊ | 441/500 [02:14<00:17,  3.35it/s] 88%|████████▊ | 442/500 [02:15<00:17,  3.36it/s] 89%|████████▊ | 443/500 [02:15<00:16,  3.36it/s] 89%|████████▉ | 444/500 [02:15<00:16,  3.36it/s] 89%|████████▉ | 445/500 [02:16<00:16,  3.36it/s] 89%|████████▉ | 446/500 [02:16<00:16,  3.36it/s] 89%|████████▉ | 447/500 [02:16<00:15,  3.36it/s] 90%|████████▉ | 448/500 [02:17<00:15,  3.35it/s] 90%|████████▉ | 449/500 [02:17<00:15,  3.36it/s] 90%|█████████ | 450/500 [02:17<00:14,  3.36it/s] 90%|█████████ | 451/500 [02:17<00:14,  3.35it/s] 90%|█████████ | 452/500 [02:18<00:14,  3.36it/s] 91%|█████████ | 453/500 [02:18<00:13,  3.36it/s] 91%|█████████ | 454/500 [02:18<00:13,  3.36it/s] 91%|█████████ | 455/500 [02:19<00:13,  3.36it/s] 91%|█████████ | 456/500 [02:19<00:13,  3.36it/s] 91%|█████████▏| 457/500 [02:19<00:12,  3.35it/s] 92%|█████████▏| 458/500 [02:20<00:12,  3.36it/s] 92%|█████████▏| 459/500 [02:20<00:12,  3.36it/s] 92%|█████████▏| 460/500 [02:20<00:11,  3.35it/s] 92%|█████████▏| 461/500 [02:20<00:11,  3.35it/s] 92%|█████████▏| 462/500 [02:21<00:11,  3.35it/s] 93%|█████████▎| 463/500 [02:21<00:11,  3.35it/s] 93%|█████████▎| 464/500 [02:21<00:10,  3.35it/s] 93%|█████████▎| 465/500 [02:22<00:10,  3.35it/s] 93%|█████████▎| 466/500 [02:22<00:10,  3.36it/s] 93%|█████████▎| 467/500 [02:22<00:09,  3.36it/s] 94%|█████████▎| 468/500 [02:23<00:09,  3.36it/s] 94%|█████████▍| 469/500 [02:23<00:09,  3.36it/s] 94%|█████████▍| 470/500 [02:23<00:08,  3.36it/s] 94%|█████████▍| 471/500 [02:23<00:08,  3.36it/s] 94%|█████████▍| 472/500 [02:24<00:08,  3.36it/s] 95%|█████████▍| 473/500 [02:24<00:08,  3.36it/s] 95%|█████████▍| 474/500 [02:24<00:07,  3.36it/s] 95%|█████████▌| 475/500 [02:25<00:07,  3.36it/s] 95%|█████████▌| 476/500 [02:25<00:07,  3.36it/s] 95%|█████████▌| 477/500 [02:25<00:06,  3.36it/s] 96%|█████████▌| 478/500 [02:26<00:06,  3.36it/s] 96%|█████████▌| 479/500 [02:26<00:06,  3.36it/s] 96%|█████████▌| 480/500 [02:26<00:05,  3.36it/s] 96%|█████████▌| 481/500 [02:26<00:05,  3.36it/s] 96%|█████████▋| 482/500 [02:27<00:05,  3.35it/s] 97%|█████████▋| 483/500 [02:27<00:05,  3.36it/s] 97%|█████████▋| 484/500 [02:27<00:04,  3.36it/s] 97%|█████████▋| 485/500 [02:28<00:04,  3.36it/s] 97%|█████████▋| 486/500 [02:28<00:04,  3.36it/s] 97%|█████████▋| 487/500 [02:28<00:03,  3.36it/s] 98%|█████████▊| 488/500 [02:28<00:03,  3.36it/s] 98%|█████████▊| 489/500 [02:29<00:03,  3.36it/s] 98%|█████████▊| 490/500 [02:29<00:02,  3.36it/s] 98%|█████████▊| 491/500 [02:29<00:02,  3.36it/s] 98%|█████████▊| 492/500 [02:30<00:02,  3.36it/s] 99%|█████████▊| 493/500 [02:30<00:02,  3.36it/s] 99%|█████████▉| 494/500 [02:30<00:01,  3.36it/s] 99%|█████████▉| 495/500 [02:31<00:01,  3.36it/s] 99%|█████████▉| 496/500 [02:31<00:01,  3.37it/s] 99%|█████████▉| 497/500 [02:31<00:00,  3.37it/s]100%|█████████▉| 498/500 [02:31<00:00,  3.37it/s]100%|█████████▉| 499/500 [02:32<00:00,  3.37it/s]100%|██████████| 500/500 [02:32<00:00,  3.36it/s]100%|██████████| 500/500 [02:32<00:00,  3.28it/s]
=> result
* total: 50,000
* correct: 25,166
* accuracy: 50.3%
* error: 49.7%
* macro_f1: 46.9%
+ for seed in 1 2 3
+ sh scripts/coop/crossdataset_test.sh fgvc_aircraft imagenet 3 0 vit_b16_ctxv1 16 200 CoOp
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoOp/vit_b16_ctxv1.yaml
dataset_config_file: configs/datasets/imagenet.yaml
eval_only: True
head: 
load_epoch: 200
model_dir: output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/coop/crossdataset/test_target/source_fgvc_aircraft/imagenet/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: CoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: ImageNet
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/coop/crossdataset/test_target/source_fgvc_aircraft/imagenet/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoOp
  RPO:
    CTX_INIT: 
    K1: 1
    K2: 1
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:CoOp
Loading trainer: CoOp
requested:ImageNet
Loading dataset: ImageNet
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/ImageNet/split_fewshot_taesup/shot_16-seed_3.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  --------
Dataset    ImageNet
# classes  1,000
# train_x  16,000
# val      50,000
# test     50,000
---------  --------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/coop/crossdataset/train_source/fgvc_aircraft/shots_16/CoOp/vit_b16_ctxv1/seed3/prompt_learner/model.pth.tar-200" (epoch = 200)
Evaluate on the *test* set
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:04<40:06,  4.82s/it]  0%|          | 2/500 [00:05<17:53,  2.16s/it]  1%|          | 3/500 [00:05<10:48,  1.31s/it]  1%|          | 4/500 [00:05<07:29,  1.10it/s]  1%|          | 5/500 [00:05<05:39,  1.46it/s]  1%|          | 6/500 [00:06<04:32,  1.81it/s]  1%|▏         | 7/500 [00:06<03:50,  2.14it/s]  2%|▏         | 8/500 [00:06<03:22,  2.43it/s]  2%|▏         | 9/500 [00:07<03:04,  2.67it/s]  2%|▏         | 10/500 [00:07<02:51,  2.86it/s]  2%|▏         | 11/500 [00:07<02:42,  3.01it/s]  2%|▏         | 12/500 [00:08<02:36,  3.12it/s]  3%|▎         | 13/500 [00:08<02:32,  3.20it/s]  3%|▎         | 14/500 [00:08<02:29,  3.26it/s]  3%|▎         | 15/500 [00:08<02:26,  3.30it/s]  3%|▎         | 16/500 [00:09<02:25,  3.33it/s]  3%|▎         | 17/500 [00:09<02:24,  3.35it/s]  4%|▎         | 18/500 [00:09<02:23,  3.37it/s]  4%|▍         | 19/500 [00:10<02:22,  3.38it/s]  4%|▍         | 20/500 [00:10<02:21,  3.39it/s]  4%|▍         | 21/500 [00:10<02:21,  3.39it/s]  4%|▍         | 22/500 [00:10<02:20,  3.40it/s]  5%|▍         | 23/500 [00:11<02:20,  3.40it/s]  5%|▍         | 24/500 [00:11<02:19,  3.41it/s]  5%|▌         | 25/500 [00:11<02:19,  3.41it/s]  5%|▌         | 26/500 [00:12<02:19,  3.41it/s]  5%|▌         | 27/500 [00:12<02:18,  3.41it/s]  6%|▌         | 28/500 [00:12<02:18,  3.41it/s]  6%|▌         | 29/500 [00:13<02:18,  3.41it/s]  6%|▌         | 30/500 [00:13<02:17,  3.41it/s]  6%|▌         | 31/500 [00:13<02:17,  3.41it/s]  6%|▋         | 32/500 [00:13<02:17,  3.40it/s]  7%|▋         | 33/500 [00:14<02:17,  3.40it/s]  7%|▋         | 34/500 [00:14<02:16,  3.41it/s]  7%|▋         | 35/500 [00:14<02:16,  3.41it/s]  7%|▋         | 36/500 [00:15<02:16,  3.41it/s]  7%|▋         | 37/500 [00:15<02:15,  3.41it/s]  8%|▊         | 38/500 [00:15<02:16,  3.38it/s]  8%|▊         | 39/500 [00:15<02:15,  3.39it/s]  8%|▊         | 40/500 [00:16<02:15,  3.39it/s]  8%|▊         | 41/500 [00:16<02:15,  3.39it/s]  8%|▊         | 42/500 [00:16<02:14,  3.40it/s]  9%|▊         | 43/500 [00:17<02:14,  3.41it/s]  9%|▉         | 44/500 [00:17<02:13,  3.41it/s]  9%|▉         | 45/500 [00:17<02:13,  3.41it/s]  9%|▉         | 46/500 [00:18<02:13,  3.41it/s]  9%|▉         | 47/500 [00:18<02:12,  3.41it/s] 10%|▉         | 48/500 [00:18<02:12,  3.42it/s] 10%|▉         | 49/500 [00:18<02:12,  3.42it/s] 10%|█         | 50/500 [00:19<02:11,  3.41it/s] 10%|█         | 51/500 [00:19<02:11,  3.41it/s] 10%|█         | 52/500 [00:19<02:11,  3.41it/s] 11%|█         | 53/500 [00:20<02:11,  3.41it/s] 11%|█         | 54/500 [00:20<02:10,  3.41it/s] 11%|█         | 55/500 [00:20<02:10,  3.41it/s] 11%|█         | 56/500 [00:20<02:10,  3.41it/s] 11%|█▏        | 57/500 [00:21<02:10,  3.41it/s] 12%|█▏        | 58/500 [00:21<02:09,  3.41it/s] 12%|█▏        | 59/500 [00:21<02:09,  3.41it/s] 12%|█▏        | 60/500 [00:22<02:08,  3.41it/s] 12%|█▏        | 61/500 [00:22<02:08,  3.41it/s] 12%|█▏        | 62/500 [00:22<02:09,  3.39it/s] 13%|█▎        | 63/500 [00:23<02:08,  3.40it/s] 13%|█▎        | 64/500 [00:23<02:08,  3.39it/s] 13%|█▎        | 65/500 [00:23<02:08,  3.39it/s] 13%|█▎        | 66/500 [00:23<02:07,  3.40it/s] 13%|█▎        | 67/500 [00:24<02:07,  3.40it/s] 14%|█▎        | 68/500 [00:24<02:06,  3.41it/s] 14%|█▍        | 69/500 [00:24<02:06,  3.41it/s] 14%|█▍        | 70/500 [00:25<02:07,  3.38it/s] 14%|█▍        | 71/500 [00:25<02:06,  3.39it/s] 14%|█▍        | 72/500 [00:25<02:06,  3.38it/s] 15%|█▍        | 73/500 [00:25<02:06,  3.39it/s] 15%|█▍        | 74/500 [00:26<02:05,  3.39it/s] 15%|█▌        | 75/500 [00:26<02:05,  3.39it/s] 15%|█▌        | 76/500 [00:26<02:05,  3.38it/s] 15%|█▌        | 77/500 [00:27<02:04,  3.39it/s] 16%|█▌        | 78/500 [00:27<02:04,  3.40it/s] 16%|█▌        | 79/500 [00:27<02:03,  3.40it/s] 16%|█▌        | 80/500 [00:28<02:03,  3.40it/s] 16%|█▌        | 81/500 [00:28<02:03,  3.40it/s] 16%|█▋        | 82/500 [00:28<02:03,  3.39it/s] 17%|█▋        | 83/500 [00:28<02:02,  3.39it/s] 17%|█▋        | 84/500 [00:29<02:02,  3.39it/s] 17%|█▋        | 85/500 [00:29<02:02,  3.38it/s] 17%|█▋        | 86/500 [00:29<02:02,  3.38it/s] 17%|█▋        | 87/500 [00:30<02:02,  3.39it/s] 18%|█▊        | 88/500 [00:30<02:02,  3.37it/s] 18%|█▊        | 89/500 [00:30<02:01,  3.38it/s] 18%|█▊        | 90/500 [00:30<02:01,  3.38it/s] 18%|█▊        | 91/500 [00:31<02:00,  3.38it/s] 18%|█▊        | 92/500 [00:31<02:00,  3.39it/s] 19%|█▊        | 93/500 [00:31<02:00,  3.39it/s] 19%|█▉        | 94/500 [00:32<01:59,  3.39it/s] 19%|█▉        | 95/500 [00:32<01:59,  3.39it/s] 19%|█▉        | 96/500 [00:32<01:59,  3.39it/s] 19%|█▉        | 97/500 [00:33<01:58,  3.39it/s] 20%|█▉        | 98/500 [00:33<01:58,  3.39it/s] 20%|█▉        | 99/500 [00:33<01:58,  3.39it/s] 20%|██        | 100/500 [00:33<01:58,  3.38it/s] 20%|██        | 101/500 [00:34<01:59,  3.35it/s] 20%|██        | 102/500 [00:34<01:58,  3.36it/s] 21%|██        | 103/500 [00:34<01:57,  3.37it/s] 21%|██        | 104/500 [00:35<01:57,  3.38it/s] 21%|██        | 105/500 [00:35<01:56,  3.39it/s] 21%|██        | 106/500 [00:35<01:57,  3.35it/s] 21%|██▏       | 107/500 [00:36<01:56,  3.37it/s] 22%|██▏       | 108/500 [00:36<01:56,  3.37it/s] 22%|██▏       | 109/500 [00:36<01:55,  3.38it/s] 22%|██▏       | 110/500 [00:36<01:55,  3.38it/s] 22%|██▏       | 111/500 [00:37<01:54,  3.38it/s] 22%|██▏       | 112/500 [00:37<01:54,  3.38it/s] 23%|██▎       | 113/500 [00:37<01:54,  3.38it/s] 23%|██▎       | 114/500 [00:38<01:54,  3.37it/s] 23%|██▎       | 115/500 [00:38<01:54,  3.37it/s] 23%|██▎       | 116/500 [00:38<01:54,  3.37it/s] 23%|██▎       | 117/500 [00:38<01:53,  3.37it/s] 24%|██▎       | 118/500 [00:39<01:53,  3.37it/s] 24%|██▍       | 119/500 [00:39<01:53,  3.37it/s] 24%|██▍       | 120/500 [00:39<01:52,  3.37it/s] 24%|██▍       | 121/500 [00:40<01:52,  3.37it/s] 24%|██▍       | 122/500 [00:40<01:52,  3.37it/s] 25%|██▍       | 123/500 [00:40<01:51,  3.37it/s] 25%|██▍       | 124/500 [00:41<01:51,  3.37it/s] 25%|██▌       | 125/500 [00:41<01:51,  3.37it/s] 25%|██▌       | 126/500 [00:41<01:50,  3.37it/s] 25%|██▌       | 127/500 [00:41<01:51,  3.35it/s] 26%|██▌       | 128/500 [00:42<01:50,  3.35it/s] 26%|██▌       | 129/500 [00:42<01:50,  3.36it/s] 26%|██▌       | 130/500 [00:42<01:50,  3.36it/s] 26%|██▌       | 131/500 [00:43<01:49,  3.36it/s] 26%|██▋       | 132/500 [00:43<01:49,  3.37it/s] 27%|██▋       | 133/500 [00:43<01:49,  3.36it/s] 27%|██▋       | 134/500 [00:44<01:48,  3.37it/s] 27%|██▋       | 135/500 [00:44<01:48,  3.37it/s] 27%|██▋       | 136/500 [00:44<01:48,  3.37it/s] 27%|██▋       | 137/500 [00:44<01:47,  3.36it/s] 28%|██▊       | 138/500 [00:45<01:47,  3.37it/s] 28%|██▊       | 139/500 [00:45<01:47,  3.37it/s] 28%|██▊       | 140/500 [00:45<01:46,  3.37it/s] 28%|██▊       | 141/500 [00:46<01:46,  3.37it/s] 28%|██▊       | 142/500 [00:46<01:46,  3.37it/s] 29%|██▊       | 143/500 [00:46<01:45,  3.37it/s] 29%|██▉       | 144/500 [00:46<01:45,  3.37it/s] 29%|██▉       | 145/500 [00:47<01:45,  3.37it/s] 29%|██▉       | 146/500 [00:47<01:44,  3.38it/s] 29%|██▉       | 147/500 [00:47<01:44,  3.38it/s] 30%|██▉       | 148/500 [00:48<01:44,  3.38it/s] 30%|██▉       | 149/500 [00:48<01:43,  3.38it/s] 30%|███       | 150/500 [00:48<01:43,  3.38it/s] 30%|███       | 151/500 [00:49<01:43,  3.38it/s] 30%|███       | 152/500 [00:49<01:43,  3.37it/s] 31%|███       | 153/500 [00:49<01:42,  3.37it/s] 31%|███       | 154/500 [00:49<01:42,  3.37it/s] 31%|███       | 155/500 [00:50<01:42,  3.38it/s] 31%|███       | 156/500 [00:50<01:41,  3.38it/s] 31%|███▏      | 157/500 [00:50<01:41,  3.37it/s] 32%|███▏      | 158/500 [00:51<01:41,  3.37it/s] 32%|███▏      | 159/500 [00:51<01:41,  3.37it/s] 32%|███▏      | 160/500 [00:51<01:40,  3.38it/s] 32%|███▏      | 161/500 [00:52<01:40,  3.38it/s] 32%|███▏      | 162/500 [00:52<01:40,  3.38it/s] 33%|███▎      | 163/500 [00:52<01:39,  3.38it/s] 33%|███▎      | 164/500 [00:52<01:39,  3.38it/s] 33%|███▎      | 165/500 [00:53<01:39,  3.38it/s] 33%|███▎      | 166/500 [00:53<01:38,  3.38it/s] 33%|███▎      | 167/500 [00:53<01:38,  3.38it/s] 34%|███▎      | 168/500 [00:54<01:38,  3.38it/s] 34%|███▍      | 169/500 [00:54<01:37,  3.39it/s] 34%|███▍      | 170/500 [00:54<01:37,  3.38it/s] 34%|███▍      | 171/500 [00:54<01:37,  3.38it/s] 34%|███▍      | 172/500 [00:55<01:37,  3.38it/s] 35%|███▍      | 173/500 [00:55<01:36,  3.38it/s] 35%|███▍      | 174/500 [00:55<01:36,  3.38it/s] 35%|███▌      | 175/500 [00:56<01:36,  3.38it/s] 35%|███▌      | 176/500 [00:56<01:35,  3.38it/s] 35%|███▌      | 177/500 [00:56<01:35,  3.38it/s] 36%|███▌      | 178/500 [00:57<01:35,  3.38it/s] 36%|███▌      | 179/500 [00:57<01:34,  3.38it/s] 36%|███▌      | 180/500 [00:57<01:34,  3.37it/s] 36%|███▌      | 181/500 [00:57<01:34,  3.37it/s] 36%|███▋      | 182/500 [00:58<01:34,  3.37it/s] 37%|███▋      | 183/500 [00:58<01:34,  3.37it/s] 37%|███▋      | 184/500 [00:58<01:34,  3.36it/s] 37%|███▋      | 185/500 [00:59<01:33,  3.35it/s] 37%|███▋      | 186/500 [00:59<01:33,  3.35it/s] 37%|███▋      | 187/500 [00:59<01:33,  3.35it/s] 38%|███▊      | 188/500 [01:00<01:33,  3.35it/s] 38%|███▊      | 189/500 [01:00<01:32,  3.36it/s] 38%|███▊      | 190/500 [01:00<01:32,  3.36it/s] 38%|███▊      | 191/500 [01:00<01:31,  3.36it/s] 38%|███▊      | 192/500 [01:01<01:31,  3.36it/s] 39%|███▊      | 193/500 [01:01<01:31,  3.36it/s] 39%|███▉      | 194/500 [01:01<01:31,  3.36it/s] 39%|███▉      | 195/500 [01:02<01:30,  3.37it/s] 39%|███▉      | 196/500 [01:02<01:30,  3.37it/s] 39%|███▉      | 197/500 [01:02<01:30,  3.37it/s] 40%|███▉      | 198/500 [01:03<01:29,  3.36it/s] 40%|███▉      | 199/500 [01:03<01:29,  3.36it/s] 40%|████      | 200/500 [01:03<01:29,  3.36it/s] 40%|████      | 201/500 [01:03<01:28,  3.36it/s] 40%|████      | 202/500 [01:04<01:28,  3.36it/s] 41%|████      | 203/500 [01:04<01:28,  3.36it/s] 41%|████      | 204/500 [01:04<01:27,  3.37it/s] 41%|████      | 205/500 [01:05<01:27,  3.37it/s] 41%|████      | 206/500 [01:05<01:27,  3.37it/s] 41%|████▏     | 207/500 [01:05<01:27,  3.37it/s] 42%|████▏     | 208/500 [01:05<01:26,  3.36it/s] 42%|████▏     | 209/500 [01:06<01:26,  3.37it/s] 42%|████▏     | 210/500 [01:06<01:26,  3.37it/s] 42%|████▏     | 211/500 [01:06<01:25,  3.37it/s] 42%|████▏     | 212/500 [01:07<01:25,  3.37it/s] 43%|████▎     | 213/500 [01:07<01:25,  3.37it/s] 43%|████▎     | 214/500 [01:07<01:24,  3.37it/s] 43%|████▎     | 215/500 [01:08<01:24,  3.37it/s] 43%|████▎     | 216/500 [01:08<01:24,  3.37it/s] 43%|████▎     | 217/500 [01:08<01:24,  3.36it/s] 44%|████▎     | 218/500 [01:08<01:23,  3.36it/s] 44%|████▍     | 219/500 [01:09<01:23,  3.36it/s] 44%|████▍     | 220/500 [01:09<01:23,  3.37it/s] 44%|████▍     | 221/500 [01:09<01:22,  3.37it/s] 44%|████▍     | 222/500 [01:10<01:22,  3.37it/s] 45%|████▍     | 223/500 [01:10<01:22,  3.37it/s] 45%|████▍     | 224/500 [01:10<01:22,  3.36it/s] 45%|████▌     | 225/500 [01:11<01:21,  3.36it/s] 45%|████▌     | 226/500 [01:11<01:21,  3.37it/s] 45%|████▌     | 227/500 [01:11<01:21,  3.37it/s] 46%|████▌     | 228/500 [01:11<01:20,  3.37it/s] 46%|████▌     | 229/500 [01:12<01:20,  3.37it/s] 46%|████▌     | 230/500 [01:12<01:20,  3.37it/s] 46%|████▌     | 231/500 [01:12<01:19,  3.37it/s] 46%|████▋     | 232/500 [01:13<01:19,  3.36it/s] 47%|████▋     | 233/500 [01:13<01:19,  3.36it/s] 47%|████▋     | 234/500 [01:13<01:19,  3.36it/s] 47%|████▋     | 235/500 [01:14<01:18,  3.36it/s] 47%|████▋     | 236/500 [01:14<01:18,  3.37it/s] 47%|████▋     | 237/500 [01:14<01:18,  3.37it/s] 48%|████▊     | 238/500 [01:14<01:17,  3.37it/s] 48%|████▊     | 239/500 [01:15<01:18,  3.33it/s] 48%|████▊     | 240/500 [01:15<01:18,  3.31it/s] 48%|████▊     | 241/500 [01:15<01:17,  3.33it/s] 48%|████▊     | 242/500 [01:16<01:17,  3.34it/s] 49%|████▊     | 243/500 [01:16<01:16,  3.35it/s] 49%|████▉     | 244/500 [01:16<01:16,  3.36it/s] 49%|████▉     | 245/500 [01:16<01:15,  3.36it/s] 49%|████▉     | 246/500 [01:17<01:15,  3.36it/s] 49%|████▉     | 247/500 [01:17<01:15,  3.37it/s] 50%|████▉     | 248/500 [01:17<01:15,  3.36it/s] 50%|████▉     | 249/500 [01:18<01:14,  3.35it/s] 50%|█████     | 250/500 [01:18<01:14,  3.36it/s] 50%|█████     | 251/500 [01:18<01:14,  3.36it/s] 50%|█████     | 252/500 [01:19<01:13,  3.36it/s] 51%|█████     | 253/500 [01:19<01:13,  3.36it/s] 51%|█████     | 254/500 [01:19<01:13,  3.36it/s] 51%|█████     | 255/500 [01:19<01:12,  3.36it/s] 51%|█████     | 256/500 [01:20<01:12,  3.36it/s] 51%|█████▏    | 257/500 [01:20<01:12,  3.36it/s] 52%|█████▏    | 258/500 [01:20<01:12,  3.36it/s] 52%|█████▏    | 259/500 [01:21<01:11,  3.36it/s] 52%|█████▏    | 260/500 [01:21<01:11,  3.36it/s] 52%|█████▏    | 261/500 [01:21<01:10,  3.37it/s] 52%|█████▏    | 262/500 [01:22<01:10,  3.38it/s] 53%|█████▎    | 263/500 [01:22<01:10,  3.38it/s] 53%|█████▎    | 264/500 [01:22<01:10,  3.37it/s] 53%|█████▎    | 265/500 [01:22<01:09,  3.37it/s] 53%|█████▎    | 266/500 [01:23<01:09,  3.37it/s] 53%|█████▎    | 267/500 [01:23<01:09,  3.36it/s] 54%|█████▎    | 268/500 [01:23<01:09,  3.36it/s] 54%|█████▍    | 269/500 [01:24<01:08,  3.36it/s] 54%|█████▍    | 270/500 [01:24<01:08,  3.37it/s] 54%|█████▍    | 271/500 [01:24<01:08,  3.36it/s] 54%|█████▍    | 272/500 [01:25<01:07,  3.36it/s] 55%|█████▍    | 273/500 [01:25<01:07,  3.37it/s] 55%|█████▍    | 274/500 [01:25<01:07,  3.36it/s] 55%|█████▌    | 275/500 [01:25<01:06,  3.36it/s] 55%|█████▌    | 276/500 [01:26<01:06,  3.37it/s] 55%|█████▌    | 277/500 [01:26<01:06,  3.37it/s] 56%|█████▌    | 278/500 [01:26<01:05,  3.37it/s] 56%|█████▌    | 279/500 [01:27<01:05,  3.37it/s] 56%|█████▌    | 280/500 [01:27<01:05,  3.37it/s] 56%|█████▌    | 281/500 [01:27<01:05,  3.37it/s] 56%|█████▋    | 282/500 [01:27<01:04,  3.36it/s] 57%|█████▋    | 283/500 [01:28<01:04,  3.35it/s] 57%|█████▋    | 284/500 [01:28<01:04,  3.36it/s] 57%|█████▋    | 285/500 [01:28<01:04,  3.36it/s] 57%|█████▋    | 286/500 [01:29<01:03,  3.36it/s] 57%|█████▋    | 287/500 [01:29<01:03,  3.36it/s] 58%|█████▊    | 288/500 [01:29<01:03,  3.36it/s] 58%|█████▊    | 289/500 [01:30<01:02,  3.36it/s] 58%|█████▊    | 290/500 [01:30<01:02,  3.36it/s] 58%|█████▊    | 291/500 [01:30<01:02,  3.37it/s] 58%|█████▊    | 292/500 [01:30<01:01,  3.37it/s] 59%|█████▊    | 293/500 [01:31<01:01,  3.37it/s] 59%|█████▉    | 294/500 [01:31<01:01,  3.37it/s] 59%|█████▉    | 295/500 [01:31<01:00,  3.37it/s] 59%|█████▉    | 296/500 [01:32<01:00,  3.37it/s] 59%|█████▉    | 297/500 [01:32<01:00,  3.37it/s] 60%|█████▉    | 298/500 [01:32<01:00,  3.37it/s] 60%|█████▉    | 299/500 [01:33<00:59,  3.36it/s] 60%|██████    | 300/500 [01:33<00:59,  3.36it/s] 60%|██████    | 301/500 [01:33<00:59,  3.36it/s] 60%|██████    | 302/500 [01:33<00:58,  3.36it/s] 61%|██████    | 303/500 [01:34<00:58,  3.36it/s] 61%|██████    | 304/500 [01:34<00:58,  3.36it/s] 61%|██████    | 305/500 [01:34<00:57,  3.36it/s] 61%|██████    | 306/500 [01:35<00:57,  3.36it/s] 61%|██████▏   | 307/500 [01:35<00:57,  3.35it/s] 62%|██████▏   | 308/500 [01:35<00:57,  3.35it/s] 62%|██████▏   | 309/500 [01:36<00:56,  3.36it/s] 62%|██████▏   | 310/500 [01:36<00:56,  3.36it/s] 62%|██████▏   | 311/500 [01:36<00:56,  3.36it/s] 62%|██████▏   | 312/500 [01:36<00:55,  3.36it/s] 63%|██████▎   | 313/500 [01:37<00:55,  3.37it/s] 63%|██████▎   | 314/500 [01:37<00:55,  3.37it/s] 63%|██████▎   | 315/500 [01:37<00:54,  3.37it/s] 63%|██████▎   | 316/500 [01:38<00:54,  3.37it/s] 63%|██████▎   | 317/500 [01:38<00:54,  3.37it/s] 64%|██████▎   | 318/500 [01:38<00:54,  3.37it/s] 64%|██████▍   | 319/500 [01:38<00:53,  3.37it/s] 64%|██████▍   | 320/500 [01:39<00:53,  3.38it/s] 64%|██████▍   | 321/500 [01:39<00:53,  3.37it/s] 64%|██████▍   | 322/500 [01:39<00:52,  3.37it/s] 65%|██████▍   | 323/500 [01:40<00:52,  3.37it/s] 65%|██████▍   | 324/500 [01:40<00:52,  3.37it/s] 65%|██████▌   | 325/500 [01:40<00:51,  3.37it/s] 65%|██████▌   | 326/500 [01:41<00:51,  3.37it/s] 65%|██████▌   | 327/500 [01:41<00:51,  3.37it/s] 66%|██████▌   | 328/500 [01:41<00:51,  3.37it/s] 66%|██████▌   | 329/500 [01:41<00:50,  3.37it/s] 66%|██████▌   | 330/500 [01:42<00:50,  3.37it/s] 66%|██████▌   | 331/500 [01:42<00:50,  3.34it/s] 66%|██████▋   | 332/500 [01:42<00:50,  3.35it/s] 67%|██████▋   | 333/500 [01:43<00:49,  3.35it/s] 67%|██████▋   | 334/500 [01:43<00:49,  3.35it/s] 67%|██████▋   | 335/500 [01:43<00:49,  3.36it/s] 67%|██████▋   | 336/500 [01:44<00:48,  3.36it/s] 67%|██████▋   | 337/500 [01:44<00:48,  3.36it/s] 68%|██████▊   | 338/500 [01:44<00:48,  3.36it/s] 68%|██████▊   | 339/500 [01:44<00:47,  3.37it/s] 68%|██████▊   | 340/500 [01:45<00:47,  3.37it/s] 68%|██████▊   | 341/500 [01:45<00:47,  3.37it/s] 68%|██████▊   | 342/500 [01:45<00:46,  3.37it/s] 69%|██████▊   | 343/500 [01:46<00:46,  3.37it/s] 69%|██████▉   | 344/500 [01:46<00:46,  3.37it/s] 69%|██████▉   | 345/500 [01:46<00:46,  3.37it/s] 69%|██████▉   | 346/500 [01:47<00:45,  3.36it/s] 69%|██████▉   | 347/500 [01:47<00:45,  3.36it/s] 70%|██████▉   | 348/500 [01:47<00:45,  3.36it/s] 70%|██████▉   | 349/500 [01:47<00:44,  3.36it/s] 70%|███████   | 350/500 [01:48<00:44,  3.36it/s] 70%|███████   | 351/500 [01:48<00:44,  3.34it/s] 70%|███████   | 352/500 [01:48<00:44,  3.35it/s] 71%|███████   | 353/500 [01:49<00:43,  3.35it/s] 71%|███████   | 354/500 [01:49<00:43,  3.36it/s] 71%|███████   | 355/500 [01:49<00:43,  3.35it/s] 71%|███████   | 356/500 [01:49<00:42,  3.36it/s] 71%|███████▏  | 357/500 [01:50<00:42,  3.36it/s] 72%|███████▏  | 358/500 [01:50<00:42,  3.36it/s] 72%|███████▏  | 359/500 [01:50<00:41,  3.36it/s] 72%|███████▏  | 360/500 [01:51<00:41,  3.36it/s] 72%|███████▏  | 361/500 [01:51<00:41,  3.37it/s] 72%|███████▏  | 362/500 [01:51<00:41,  3.36it/s] 73%|███████▎  | 363/500 [01:52<00:40,  3.36it/s] 73%|███████▎  | 364/500 [01:52<00:40,  3.36it/s] 73%|███████▎  | 365/500 [01:52<00:40,  3.36it/s] 73%|███████▎  | 366/500 [01:52<00:39,  3.36it/s] 73%|███████▎  | 367/500 [01:53<00:39,  3.36it/s] 74%|███████▎  | 368/500 [01:53<00:39,  3.36it/s] 74%|███████▍  | 369/500 [01:53<00:38,  3.36it/s] 74%|███████▍  | 370/500 [01:54<00:38,  3.37it/s] 74%|███████▍  | 371/500 [01:54<00:38,  3.37it/s] 74%|███████▍  | 372/500 [01:54<00:38,  3.36it/s] 75%|███████▍  | 373/500 [01:55<00:37,  3.36it/s] 75%|███████▍  | 374/500 [01:55<00:37,  3.36it/s] 75%|███████▌  | 375/500 [01:55<00:37,  3.36it/s] 75%|███████▌  | 376/500 [01:55<00:36,  3.36it/s] 75%|███████▌  | 377/500 [01:56<00:36,  3.35it/s] 76%|███████▌  | 378/500 [01:56<00:36,  3.36it/s] 76%|███████▌  | 379/500 [01:56<00:36,  3.35it/s] 76%|███████▌  | 380/500 [01:57<00:35,  3.35it/s] 76%|███████▌  | 381/500 [01:57<00:35,  3.36it/s] 76%|███████▋  | 382/500 [01:57<00:35,  3.36it/s] 77%|███████▋  | 383/500 [01:58<00:34,  3.36it/s] 77%|███████▋  | 384/500 [01:58<00:34,  3.35it/s] 77%|███████▋  | 385/500 [01:58<00:34,  3.36it/s] 77%|███████▋  | 386/500 [01:58<00:34,  3.35it/s] 77%|███████▋  | 387/500 [01:59<00:33,  3.35it/s] 78%|███████▊  | 388/500 [01:59<00:33,  3.35it/s] 78%|███████▊  | 389/500 [01:59<00:33,  3.35it/s] 78%|███████▊  | 390/500 [02:00<00:32,  3.35it/s] 78%|███████▊  | 391/500 [02:00<00:32,  3.35it/s] 78%|███████▊  | 392/500 [02:00<00:32,  3.34it/s] 79%|███████▊  | 393/500 [02:01<00:32,  3.33it/s] 79%|███████▉  | 394/500 [02:01<00:31,  3.34it/s] 79%|███████▉  | 395/500 [02:01<00:31,  3.35it/s] 79%|███████▉  | 396/500 [02:01<00:31,  3.35it/s] 79%|███████▉  | 397/500 [02:02<00:30,  3.36it/s] 80%|███████▉  | 398/500 [02:02<00:30,  3.36it/s] 80%|███████▉  | 399/500 [02:02<00:30,  3.36it/s] 80%|████████  | 400/500 [02:03<00:29,  3.37it/s] 80%|████████  | 401/500 [02:03<00:29,  3.37it/s] 80%|████████  | 402/500 [02:03<00:29,  3.37it/s] 81%|████████  | 403/500 [02:03<00:28,  3.38it/s] 81%|████████  | 404/500 [02:04<00:28,  3.38it/s] 81%|████████  | 405/500 [02:04<00:28,  3.38it/s] 81%|████████  | 406/500 [02:04<00:27,  3.38it/s] 81%|████████▏ | 407/500 [02:05<00:27,  3.38it/s] 82%|████████▏ | 408/500 [02:05<00:27,  3.38it/s] 82%|████████▏ | 409/500 [02:05<00:26,  3.38it/s] 82%|████████▏ | 410/500 [02:06<00:26,  3.37it/s] 82%|████████▏ | 411/500 [02:06<00:26,  3.37it/s] 82%|████████▏ | 412/500 [02:06<00:26,  3.37it/s] 83%|████████▎ | 413/500 [02:06<00:25,  3.37it/s] 83%|████████▎ | 414/500 [02:07<00:25,  3.36it/s] 83%|████████▎ | 415/500 [02:07<00:25,  3.37it/s] 83%|████████▎ | 416/500 [02:07<00:24,  3.37it/s] 83%|████████▎ | 417/500 [02:08<00:24,  3.37it/s] 84%|████████▎ | 418/500 [02:08<00:24,  3.37it/s] 84%|████████▍ | 419/500 [02:08<00:24,  3.36it/s] 84%|████████▍ | 420/500 [02:09<00:23,  3.37it/s] 84%|████████▍ | 421/500 [02:09<00:23,  3.37it/s] 84%|████████▍ | 422/500 [02:09<00:23,  3.37it/s] 85%|████████▍ | 423/500 [02:09<00:22,  3.37it/s] 85%|████████▍ | 424/500 [02:10<00:22,  3.38it/s] 85%|████████▌ | 425/500 [02:10<00:22,  3.37it/s] 85%|████████▌ | 426/500 [02:10<00:21,  3.37it/s] 85%|████████▌ | 427/500 [02:11<00:21,  3.37it/s] 86%|████████▌ | 428/500 [02:11<00:21,  3.37it/s] 86%|████████▌ | 429/500 [02:11<00:21,  3.37it/s] 86%|████████▌ | 430/500 [02:11<00:20,  3.38it/s] 86%|████████▌ | 431/500 [02:12<00:20,  3.38it/s] 86%|████████▋ | 432/500 [02:12<00:20,  3.37it/s] 87%|████████▋ | 433/500 [02:12<00:19,  3.36it/s] 87%|████████▋ | 434/500 [02:13<00:19,  3.36it/s] 87%|████████▋ | 435/500 [02:13<00:19,  3.36it/s] 87%|████████▋ | 436/500 [02:13<00:19,  3.36it/s] 87%|████████▋ | 437/500 [02:14<00:18,  3.36it/s] 88%|████████▊ | 438/500 [02:14<00:18,  3.36it/s] 88%|████████▊ | 439/500 [02:14<00:18,  3.36it/s] 88%|████████▊ | 440/500 [02:14<00:17,  3.36it/s] 88%|████████▊ | 441/500 [02:15<00:17,  3.36it/s] 88%|████████▊ | 442/500 [02:15<00:17,  3.36it/s] 89%|████████▊ | 443/500 [02:15<00:16,  3.36it/s] 89%|████████▉ | 444/500 [02:16<00:16,  3.36it/s] 89%|████████▉ | 445/500 [02:16<00:16,  3.36it/s] 89%|████████▉ | 446/500 [02:16<00:16,  3.36it/s] 89%|████████▉ | 447/500 [02:17<00:15,  3.35it/s] 90%|████████▉ | 448/500 [02:17<00:15,  3.36it/s] 90%|████████▉ | 449/500 [02:17<00:15,  3.36it/s] 90%|█████████ | 450/500 [02:17<00:14,  3.36it/s] 90%|█████████ | 451/500 [02:18<00:14,  3.36it/s] 90%|█████████ | 452/500 [02:18<00:14,  3.37it/s] 91%|█████████ | 453/500 [02:18<00:13,  3.36it/s] 91%|█████████ | 454/500 [02:19<00:13,  3.36it/s] 91%|█████████ | 455/500 [02:19<00:13,  3.36it/s] 91%|█████████ | 456/500 [02:19<00:13,  3.36it/s] 91%|█████████▏| 457/500 [02:20<00:12,  3.36it/s] 92%|█████████▏| 458/500 [02:20<00:12,  3.37it/s] 92%|█████████▏| 459/500 [02:20<00:12,  3.33it/s] 92%|█████████▏| 460/500 [02:20<00:11,  3.34it/s] 92%|█████████▏| 461/500 [02:21<00:11,  3.35it/s] 92%|█████████▏| 462/500 [02:21<00:11,  3.36it/s] 93%|█████████▎| 463/500 [02:21<00:11,  3.36it/s] 93%|█████████▎| 464/500 [02:22<00:10,  3.36it/s] 93%|█████████▎| 465/500 [02:22<00:10,  3.37it/s] 93%|█████████▎| 466/500 [02:22<00:10,  3.37it/s] 93%|█████████▎| 467/500 [02:23<00:09,  3.37it/s] 94%|█████████▎| 468/500 [02:23<00:09,  3.37it/s] 94%|█████████▍| 469/500 [02:23<00:09,  3.38it/s] 94%|█████████▍| 470/500 [02:23<00:08,  3.38it/s] 94%|█████████▍| 471/500 [02:24<00:08,  3.38it/s] 94%|█████████▍| 472/500 [02:24<00:08,  3.38it/s] 95%|█████████▍| 473/500 [02:24<00:08,  3.37it/s] 95%|█████████▍| 474/500 [02:25<00:07,  3.37it/s] 95%|█████████▌| 475/500 [02:25<00:07,  3.37it/s] 95%|█████████▌| 476/500 [02:25<00:07,  3.37it/s] 95%|█████████▌| 477/500 [02:25<00:06,  3.37it/s] 96%|█████████▌| 478/500 [02:26<00:06,  3.37it/s] 96%|█████████▌| 479/500 [02:26<00:06,  3.37it/s] 96%|█████████▌| 480/500 [02:26<00:05,  3.36it/s] 96%|█████████▌| 481/500 [02:27<00:05,  3.36it/s] 96%|█████████▋| 482/500 [02:27<00:05,  3.36it/s] 97%|█████████▋| 483/500 [02:27<00:05,  3.36it/s] 97%|█████████▋| 484/500 [02:28<00:04,  3.36it/s] 97%|█████████▋| 485/500 [02:28<00:04,  3.36it/s] 97%|█████████▋| 486/500 [02:28<00:04,  3.37it/s] 97%|█████████▋| 487/500 [02:28<00:03,  3.37it/s] 98%|█████████▊| 488/500 [02:29<00:03,  3.37it/s] 98%|█████████▊| 489/500 [02:29<00:03,  3.38it/s] 98%|█████████▊| 490/500 [02:29<00:02,  3.38it/s] 98%|█████████▊| 491/500 [02:30<00:02,  3.37it/s] 98%|█████████▊| 492/500 [02:30<00:02,  3.38it/s] 99%|█████████▊| 493/500 [02:30<00:02,  3.38it/s] 99%|█████████▉| 494/500 [02:31<00:01,  3.39it/s] 99%|█████████▉| 495/500 [02:31<00:01,  3.38it/s] 99%|█████████▉| 496/500 [02:31<00:01,  3.38it/s] 99%|█████████▉| 497/500 [02:31<00:00,  3.38it/s]100%|█████████▉| 498/500 [02:32<00:00,  3.38it/s]100%|█████████▉| 499/500 [02:32<00:00,  3.38it/s]100%|██████████| 500/500 [02:32<00:00,  3.38it/s]100%|██████████| 500/500 [02:32<00:00,  3.27it/s]
=> result
* total: 50,000
* correct: 22,439
* accuracy: 44.9%
* error: 55.1%
* macro_f1: 40.9%
