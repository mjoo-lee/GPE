set -e
set -x

eval "$(conda shell.bash hook)"
++ conda shell.bash hook
+ eval 'export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
export CONDA_EXE='/home/s2/mjoolee/anaconda/bin/conda'
++ export CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
++ CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
export _CE_M=''
++ export _CE_M=
++ _CE_M=
export _CE_CONDA=''
++ export _CE_CONDA=
++ _CE_CONDA=
export CONDA_PYTHON_EXE='/home/s2/mjoolee/anaconda/bin/python'
++ export CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
++ CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We're not allowing PS1 to be unbound. It must at least be set.
    # However, we're not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi
++ '[' -z x ']'

conda activate base
++ conda activate base
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate base
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate base
+++ /home/s2/mjoolee/anaconda/bin/conda shell.posix activate base
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\'''
PS1='(base) '
+++ PS1='(base) '
export PATH='/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'
+++ export PATH=/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
+++ PATH=/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
export CONDA_SHLVL='1'
+++ export CONDA_SHLVL=1
+++ CONDA_SHLVL=1
export CONDA_PROMPT_MODIFIER='(base) '
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
conda activate dassl
+ conda activate dassl
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate dassl
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate dassl
++ /home/s2/mjoolee/anaconda/bin/conda shell.posix activate dassl
+ ask_conda='PS1='\''(dassl) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_PREFIX='\''/home/s2/mjoolee/anaconda/envs/dassl'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''dassl'\''
export CONDA_PROMPT_MODIFIER='\''(dassl) '\''
export CONDA_PREFIX_1='\''/home/s2/mjoolee/anaconda'\''
export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\'''
+ eval 'PS1='\''(dassl) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_PREFIX='\''/home/s2/mjoolee/anaconda/envs/dassl'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''dassl'\''
export CONDA_PROMPT_MODIFIER='\''(dassl) '\''
export CONDA_PREFIX_1='\''/home/s2/mjoolee/anaconda'\''
export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\'''
PS1='(dassl) '
++ PS1='(dassl) '
export PATH='/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'
++ export PATH=/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
++ PATH=/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
export CONDA_PREFIX='/home/s2/mjoolee/anaconda/envs/dassl'
++ export CONDA_PREFIX=/home/s2/mjoolee/anaconda/envs/dassl
++ CONDA_PREFIX=/home/s2/mjoolee/anaconda/envs/dassl
export CONDA_SHLVL='2'
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
export CONDA_DEFAULT_ENV='dassl'
++ export CONDA_DEFAULT_ENV=dassl
++ CONDA_DEFAULT_ENV=dassl
export CONDA_PROMPT_MODIFIER='(dassl) '
++ export 'CONDA_PROMPT_MODIFIER=(dassl) '
++ CONDA_PROMPT_MODIFIER='(dassl) '
export CONDA_PREFIX_1='/home/s2/mjoolee/anaconda'
++ export CONDA_PREFIX_1=/home/s2/mjoolee/anaconda
++ CONDA_PREFIX_1=/home/s2/mjoolee/anaconda
export CONDA_EXE='/home/s2/mjoolee/anaconda/bin/conda'
++ export CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
++ CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
export _CE_M=''
++ export _CE_M=
++ _CE_M=
export _CE_CONDA=''
++ export _CE_CONDA=
++ _CE_CONDA=
export CONDA_PYTHON_EXE='/home/s2/mjoolee/anaconda/bin/python'
++ export CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
++ CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r

GPU=0
+ GPU=0
SHOT=16
+ SHOT=16

for dataset in eurosat dtd fgvc_aircraft oxford_flowers
do
    for seed in 1 2 3
    do
    sh scripts/rpo_prime/base2new_train_sdl.sh ${dataset} ${seed} ${GPU} main_tmp1_0.1sdl ${SHOT}
    #sh scripts/rpo_prime/base2new_test.sh ${dataset} ${seed} ${GPU} main_9_9 ${SHOT} base
    sh scripts/rpo_prime/base2new_test_sdl.sh ${dataset} ${seed} ${GPU} main_tmp1_0.1sdl ${SHOT} new
    done
done
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh eurosat 1 0 main_tmp1_0.1sdl 16
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
80 2800 4200
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      2,800
# test     4,200
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model.pth.tar-30"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 30
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/tensorboard)
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar" (epoch = 27)
Evaluate on the *test* set
  0%|          | 0/42 [00:00<?, ?it/s]  2%|▏         | 1/42 [00:04<03:15,  4.76s/it]  5%|▍         | 2/42 [00:04<01:22,  2.06s/it]  7%|▋         | 3/42 [00:05<00:46,  1.20s/it] 10%|▉         | 4/42 [00:05<00:31,  1.22it/s] 12%|█▏        | 5/42 [00:05<00:21,  1.68it/s] 14%|█▍        | 6/42 [00:05<00:16,  2.19it/s] 17%|█▋        | 7/42 [00:05<00:13,  2.64it/s] 19%|█▉        | 8/42 [00:06<00:11,  2.96it/s] 21%|██▏       | 9/42 [00:06<00:09,  3.44it/s] 24%|██▍       | 10/42 [00:06<00:08,  3.75it/s] 26%|██▌       | 11/42 [00:06<00:07,  4.15it/s] 29%|██▊       | 12/42 [00:06<00:06,  4.40it/s] 31%|███       | 13/42 [00:07<00:05,  4.90it/s] 33%|███▎      | 14/42 [00:07<00:05,  5.32it/s] 36%|███▌      | 15/42 [00:07<00:04,  5.66it/s] 38%|███▊      | 16/42 [00:07<00:04,  5.92it/s] 40%|████      | 17/42 [00:07<00:04,  6.12it/s] 43%|████▎     | 18/42 [00:07<00:03,  6.27it/s] 45%|████▌     | 19/42 [00:08<00:03,  6.38it/s] 48%|████▊     | 20/42 [00:08<00:03,  6.46it/s] 50%|█████     | 21/42 [00:08<00:03,  6.51it/s] 52%|█████▏    | 22/42 [00:08<00:03,  6.55it/s] 55%|█████▍    | 23/42 [00:08<00:02,  6.58it/s] 57%|█████▋    | 24/42 [00:08<00:02,  6.60it/s] 60%|█████▉    | 25/42 [00:08<00:02,  6.61it/s] 62%|██████▏   | 26/42 [00:09<00:02,  6.62it/s] 64%|██████▍   | 27/42 [00:09<00:02,  6.63it/s] 67%|██████▋   | 28/42 [00:09<00:02,  6.63it/s] 69%|██████▉   | 29/42 [00:09<00:01,  6.64it/s] 71%|███████▏  | 30/42 [00:09<00:01,  6.64it/s] 74%|███████▍  | 31/42 [00:09<00:01,  6.64it/s] 76%|███████▌  | 32/42 [00:09<00:01,  6.64it/s] 79%|███████▊  | 33/42 [00:10<00:01,  6.63it/s] 81%|████████  | 34/42 [00:10<00:01,  6.63it/s] 83%|████████▎ | 35/42 [00:10<00:01,  6.64it/s] 86%|████████▌ | 36/42 [00:10<00:00,  6.64it/s] 88%|████████▊ | 37/42 [00:10<00:00,  6.64it/s] 90%|█████████ | 38/42 [00:10<00:00,  6.63it/s] 93%|█████████▎| 39/42 [00:11<00:00,  6.64it/s] 95%|█████████▌| 40/42 [00:11<00:00,  6.64it/s] 98%|█████████▊| 41/42 [00:11<00:00,  6.64it/s]100%|██████████| 42/42 [00:11<00:00,  6.64it/s]100%|██████████| 42/42 [00:11<00:00,  3.61it/s]
=> result
* total: 4,200
* correct: 3,813
* accuracy: 90.8%
* error: 9.2%
* macro_f1: 90.7%
Elapsed: 0:00:12
+ sh scripts/rpo_prime/base2new_test_sdl.sh eurosat 1 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
80 2600 3900
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      2,600
# test     3,900
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar" (epoch = 27)
Evaluate on the *test* set
  0%|          | 0/39 [00:00<?, ?it/s]  3%|▎         | 1/39 [00:04<02:57,  4.67s/it]  5%|▌         | 2/39 [00:04<01:14,  2.01s/it]  8%|▊         | 3/39 [00:04<00:41,  1.17s/it] 10%|█         | 4/39 [00:05<00:26,  1.30it/s] 13%|█▎        | 5/39 [00:05<00:19,  1.74it/s] 15%|█▌        | 6/39 [00:05<00:14,  2.20it/s] 18%|█▊        | 7/39 [00:05<00:12,  2.66it/s] 21%|██        | 8/39 [00:06<00:10,  2.99it/s] 23%|██▎       | 9/39 [00:06<00:08,  3.52it/s] 26%|██▌       | 10/39 [00:06<00:07,  4.11it/s] 28%|██▊       | 11/39 [00:06<00:06,  4.66it/s] 31%|███       | 12/39 [00:06<00:05,  5.05it/s] 33%|███▎      | 13/39 [00:06<00:04,  5.44it/s] 36%|███▌      | 14/39 [00:06<00:04,  5.75it/s] 38%|███▊      | 15/39 [00:07<00:04,  5.99it/s] 41%|████      | 16/39 [00:07<00:03,  6.11it/s] 44%|████▎     | 17/39 [00:07<00:03,  6.25it/s] 46%|████▌     | 18/39 [00:07<00:03,  6.36it/s] 49%|████▊     | 19/39 [00:07<00:03,  6.44it/s] 51%|█████▏    | 20/39 [00:07<00:02,  6.50it/s] 54%|█████▍    | 21/39 [00:08<00:02,  6.54it/s] 56%|█████▋    | 22/39 [00:08<00:02,  6.57it/s] 59%|█████▉    | 23/39 [00:08<00:02,  6.59it/s] 62%|██████▏   | 24/39 [00:08<00:02,  6.56it/s] 64%|██████▍   | 25/39 [00:08<00:02,  6.58it/s] 67%|██████▋   | 26/39 [00:08<00:01,  6.59it/s] 69%|██████▉   | 27/39 [00:08<00:01,  6.60it/s] 72%|███████▏  | 28/39 [00:09<00:01,  6.61it/s] 74%|███████▍  | 29/39 [00:09<00:01,  6.62it/s] 77%|███████▋  | 30/39 [00:09<00:01,  6.62it/s] 79%|███████▉  | 31/39 [00:09<00:01,  6.63it/s] 82%|████████▏ | 32/39 [00:09<00:01,  6.63it/s] 85%|████████▍ | 33/39 [00:09<00:00,  6.63it/s] 87%|████████▋ | 34/39 [00:10<00:00,  6.63it/s] 90%|████████▉ | 35/39 [00:10<00:00,  6.63it/s] 92%|█████████▏| 36/39 [00:10<00:00,  6.62it/s] 95%|█████████▍| 37/39 [00:10<00:00,  6.63it/s] 97%|█████████▋| 38/39 [00:10<00:00,  6.63it/s]100%|██████████| 39/39 [00:10<00:00,  6.63it/s]100%|██████████| 39/39 [00:10<00:00,  3.58it/s]
=> result
* total: 3,900
* correct: 2,235
* accuracy: 57.3%
* error: 42.7%
* macro_f1: 54.3%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh eurosat 2 0 main_tmp1_0.1sdl 16
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
80 2800 4200
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      2,800
# test     4,200
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-30"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 30
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/tensorboard)
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 21)
Evaluate on the *test* set
  0%|          | 0/42 [00:00<?, ?it/s]  2%|▏         | 1/42 [00:04<03:08,  4.60s/it]  5%|▍         | 2/42 [00:04<01:21,  2.05s/it]  7%|▋         | 3/42 [00:05<00:46,  1.20s/it] 10%|▉         | 4/42 [00:05<00:31,  1.21it/s] 12%|█▏        | 5/42 [00:05<00:23,  1.60it/s] 14%|█▍        | 6/42 [00:05<00:17,  2.08it/s] 17%|█▋        | 7/42 [00:06<00:14,  2.46it/s] 19%|█▉        | 8/42 [00:06<00:12,  2.75it/s] 21%|██▏       | 9/42 [00:06<00:10,  3.26it/s] 24%|██▍       | 10/42 [00:06<00:09,  3.44it/s] 26%|██▌       | 11/42 [00:06<00:08,  3.64it/s] 29%|██▊       | 12/42 [00:07<00:07,  4.22it/s] 31%|███       | 13/42 [00:07<00:06,  4.74it/s] 33%|███▎      | 14/42 [00:07<00:05,  5.19it/s] 36%|███▌      | 15/42 [00:07<00:04,  5.56it/s] 38%|███▊      | 16/42 [00:07<00:04,  5.84it/s] 40%|████      | 17/42 [00:07<00:04,  6.05it/s] 43%|████▎     | 18/42 [00:08<00:03,  6.21it/s] 45%|████▌     | 19/42 [00:08<00:03,  6.33it/s] 48%|████▊     | 20/42 [00:08<00:03,  6.42it/s] 50%|█████     | 21/42 [00:08<00:03,  6.48it/s] 52%|█████▏    | 22/42 [00:08<00:03,  6.52it/s] 55%|█████▍    | 23/42 [00:08<00:02,  6.55it/s] 57%|█████▋    | 24/42 [00:08<00:02,  6.58it/s] 60%|█████▉    | 25/42 [00:09<00:02,  6.59it/s] 62%|██████▏   | 26/42 [00:09<00:02,  6.60it/s] 64%|██████▍   | 27/42 [00:09<00:02,  6.61it/s] 67%|██████▋   | 28/42 [00:09<00:02,  6.61it/s] 69%|██████▉   | 29/42 [00:09<00:01,  6.62it/s] 71%|███████▏  | 30/42 [00:09<00:01,  6.62it/s] 74%|███████▍  | 31/42 [00:09<00:01,  6.62it/s] 76%|███████▌  | 32/42 [00:10<00:01,  6.62it/s] 79%|███████▊  | 33/42 [00:10<00:01,  6.63it/s] 81%|████████  | 34/42 [00:10<00:01,  6.63it/s] 83%|████████▎ | 35/42 [00:10<00:01,  6.63it/s] 86%|████████▌ | 36/42 [00:10<00:00,  6.62it/s] 88%|████████▊ | 37/42 [00:10<00:00,  6.62it/s] 90%|█████████ | 38/42 [00:11<00:00,  6.62it/s] 93%|█████████▎| 39/42 [00:11<00:00,  6.63it/s] 95%|█████████▌| 40/42 [00:11<00:00,  6.62it/s] 98%|█████████▊| 41/42 [00:11<00:00,  6.62it/s]100%|██████████| 42/42 [00:11<00:00,  6.62it/s]100%|██████████| 42/42 [00:11<00:00,  3.57it/s]
=> result
* total: 4,200
* correct: 3,816
* accuracy: 90.9%
* error: 9.1%
* macro_f1: 90.9%
Elapsed: 0:00:12
+ sh scripts/rpo_prime/base2new_test_sdl.sh eurosat 2 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
80 2600 3900
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      2,600
# test     3,900
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 21)
Evaluate on the *test* set
  0%|          | 0/39 [00:00<?, ?it/s]  3%|▎         | 1/39 [00:04<02:54,  4.60s/it]  5%|▌         | 2/39 [00:04<01:13,  1.99s/it]  8%|▊         | 3/39 [00:05<00:43,  1.21s/it] 10%|█         | 4/39 [00:05<00:29,  1.19it/s] 13%|█▎        | 5/39 [00:05<00:20,  1.67it/s] 15%|█▌        | 6/39 [00:05<00:15,  2.07it/s] 18%|█▊        | 7/39 [00:06<00:13,  2.43it/s] 21%|██        | 8/39 [00:06<00:10,  2.91it/s] 23%|██▎       | 9/39 [00:06<00:08,  3.52it/s] 26%|██▌       | 10/39 [00:06<00:07,  4.12it/s] 28%|██▊       | 11/39 [00:06<00:06,  4.66it/s] 31%|███       | 12/39 [00:06<00:05,  5.12it/s] 33%|███▎      | 13/39 [00:06<00:04,  5.50it/s] 36%|███▌      | 14/39 [00:07<00:04,  5.76it/s] 38%|███▊      | 15/39 [00:07<00:04,  6.00it/s] 41%|████      | 16/39 [00:07<00:03,  6.17it/s] 44%|████▎     | 17/39 [00:07<00:03,  6.29it/s] 46%|████▌     | 18/39 [00:07<00:03,  6.39it/s] 49%|████▊     | 19/39 [00:07<00:03,  6.46it/s] 51%|█████▏    | 20/39 [00:08<00:02,  6.51it/s] 54%|█████▍    | 21/39 [00:08<00:02,  6.54it/s] 56%|█████▋    | 22/39 [00:08<00:02,  6.56it/s] 59%|█████▉    | 23/39 [00:08<00:02,  6.58it/s] 62%|██████▏   | 24/39 [00:08<00:02,  6.59it/s] 64%|██████▍   | 25/39 [00:08<00:02,  6.60it/s] 67%|██████▋   | 26/39 [00:08<00:01,  6.57it/s] 69%|██████▉   | 27/39 [00:09<00:01,  6.59it/s] 72%|███████▏  | 28/39 [00:09<00:01,  6.59it/s] 74%|███████▍  | 29/39 [00:09<00:01,  6.60it/s] 77%|███████▋  | 30/39 [00:09<00:01,  6.61it/s] 79%|███████▉  | 31/39 [00:09<00:01,  6.61it/s] 82%|████████▏ | 32/39 [00:09<00:01,  6.62it/s] 85%|████████▍ | 33/39 [00:09<00:00,  6.62it/s] 87%|████████▋ | 34/39 [00:10<00:00,  6.62it/s] 90%|████████▉ | 35/39 [00:10<00:00,  6.62it/s] 92%|█████████▏| 36/39 [00:10<00:00,  6.62it/s] 95%|█████████▍| 37/39 [00:10<00:00,  6.62it/s] 97%|█████████▋| 38/39 [00:10<00:00,  6.61it/s]100%|██████████| 39/39 [00:10<00:00,  6.61it/s]100%|██████████| 39/39 [00:11<00:00,  3.54it/s]
=> result
* total: 3,900
* correct: 2,657
* accuracy: 68.1%
* error: 31.9%
* macro_f1: 67.3%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh eurosat 3 0 main_tmp1_0.1sdl 16
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
80 2800 4200
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      2,800
# test     4,200
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-30"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 30
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/tensorboard)
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 25)
Evaluate on the *test* set
  0%|          | 0/42 [00:00<?, ?it/s]  2%|▏         | 1/42 [00:04<03:05,  4.52s/it]  5%|▍         | 2/42 [00:04<01:18,  1.96s/it]  7%|▋         | 3/42 [00:04<00:44,  1.15s/it] 10%|▉         | 4/42 [00:05<00:30,  1.24it/s] 12%|█▏        | 5/42 [00:05<00:21,  1.72it/s] 14%|█▍        | 6/42 [00:05<00:16,  2.24it/s] 17%|█▋        | 7/42 [00:05<00:13,  2.57it/s] 19%|█▉        | 8/42 [00:05<00:11,  3.05it/s] 21%|██▏       | 9/42 [00:06<00:10,  3.22it/s] 24%|██▍       | 10/42 [00:06<00:08,  3.59it/s] 26%|██▌       | 11/42 [00:06<00:08,  3.67it/s] 29%|██▊       | 12/42 [00:06<00:07,  4.24it/s] 31%|███       | 13/42 [00:07<00:06,  4.76it/s] 33%|███▎      | 14/42 [00:07<00:05,  5.20it/s] 36%|███▌      | 15/42 [00:07<00:04,  5.56it/s] 38%|███▊      | 16/42 [00:07<00:04,  5.84it/s] 40%|████      | 17/42 [00:07<00:04,  6.06it/s] 43%|████▎     | 18/42 [00:07<00:03,  6.19it/s] 45%|████▌     | 19/42 [00:07<00:03,  6.31it/s] 48%|████▊     | 20/42 [00:08<00:03,  6.40it/s] 50%|█████     | 21/42 [00:08<00:03,  6.47it/s] 52%|█████▏    | 22/42 [00:08<00:03,  6.51it/s] 55%|█████▍    | 23/42 [00:08<00:02,  6.54it/s] 57%|█████▋    | 24/42 [00:08<00:02,  6.56it/s] 60%|█████▉    | 25/42 [00:08<00:02,  6.58it/s] 62%|██████▏   | 26/42 [00:08<00:02,  6.59it/s] 64%|██████▍   | 27/42 [00:09<00:02,  6.60it/s] 67%|██████▋   | 28/42 [00:09<00:02,  6.61it/s] 69%|██████▉   | 29/42 [00:09<00:01,  6.61it/s] 71%|███████▏  | 30/42 [00:09<00:01,  6.61it/s] 74%|███████▍  | 31/42 [00:09<00:01,  6.56it/s] 76%|███████▌  | 32/42 [00:09<00:01,  6.58it/s] 79%|███████▊  | 33/42 [00:10<00:01,  6.59it/s] 81%|████████  | 34/42 [00:10<00:01,  6.60it/s] 83%|████████▎ | 35/42 [00:10<00:01,  6.61it/s] 86%|████████▌ | 36/42 [00:10<00:00,  6.61it/s] 88%|████████▊ | 37/42 [00:10<00:00,  6.61it/s] 90%|█████████ | 38/42 [00:10<00:00,  6.61it/s] 93%|█████████▎| 39/42 [00:10<00:00,  6.62it/s] 95%|█████████▌| 40/42 [00:11<00:00,  6.62it/s] 98%|█████████▊| 41/42 [00:11<00:00,  6.62it/s]100%|██████████| 42/42 [00:11<00:00,  6.60it/s]100%|██████████| 42/42 [00:11<00:00,  3.64it/s]
=> result
* total: 4,200
* correct: 3,816
* accuracy: 90.9%
* error: 9.1%
* macro_f1: 90.8%
Elapsed: 0:00:12
+ sh scripts/rpo_prime/base2new_test_sdl.sh eurosat 3 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
80 2600 3900
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      2,600
# test     3,900
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 25)
Evaluate on the *test* set
  0%|          | 0/39 [00:00<?, ?it/s]  3%|▎         | 1/39 [00:04<02:54,  4.60s/it]  5%|▌         | 2/39 [00:04<01:13,  2.00s/it]  8%|▊         | 3/39 [00:05<00:43,  1.21s/it] 10%|█         | 4/39 [00:05<00:28,  1.23it/s] 13%|█▎        | 5/39 [00:05<00:20,  1.62it/s] 15%|█▌        | 6/39 [00:05<00:15,  2.08it/s] 18%|█▊        | 7/39 [00:05<00:12,  2.48it/s] 21%|██        | 8/39 [00:06<00:11,  2.78it/s] 23%|██▎       | 9/39 [00:06<00:08,  3.40it/s] 26%|██▌       | 10/39 [00:06<00:07,  4.00it/s] 28%|██▊       | 11/39 [00:06<00:06,  4.55it/s] 31%|███       | 12/39 [00:06<00:05,  5.02it/s] 33%|███▎      | 13/39 [00:07<00:04,  5.42it/s] 36%|███▌      | 14/39 [00:07<00:04,  5.73it/s] 38%|███▊      | 15/39 [00:07<00:04,  5.97it/s] 41%|████      | 16/39 [00:07<00:03,  6.16it/s] 44%|████▎     | 17/39 [00:07<00:03,  6.25it/s] 46%|████▌     | 18/39 [00:07<00:03,  6.35it/s] 49%|████▊     | 19/39 [00:07<00:03,  6.42it/s] 51%|█████▏    | 20/39 [00:08<00:02,  6.47it/s] 54%|█████▍    | 21/39 [00:08<00:02,  6.51it/s] 56%|█████▋    | 22/39 [00:08<00:02,  6.54it/s] 59%|█████▉    | 23/39 [00:08<00:02,  6.56it/s] 62%|██████▏   | 24/39 [00:08<00:02,  6.57it/s] 64%|██████▍   | 25/39 [00:08<00:02,  6.57it/s] 67%|██████▋   | 26/39 [00:08<00:01,  6.58it/s] 69%|██████▉   | 27/39 [00:09<00:01,  6.59it/s] 72%|███████▏  | 28/39 [00:09<00:01,  6.59it/s] 74%|███████▍  | 29/39 [00:09<00:01,  6.60it/s] 77%|███████▋  | 30/39 [00:09<00:01,  6.60it/s] 79%|███████▉  | 31/39 [00:09<00:01,  6.60it/s] 82%|████████▏ | 32/39 [00:09<00:01,  6.60it/s] 85%|████████▍ | 33/39 [00:10<00:00,  6.59it/s] 87%|████████▋ | 34/39 [00:10<00:00,  6.60it/s] 90%|████████▉ | 35/39 [00:10<00:00,  6.60it/s] 92%|█████████▏| 36/39 [00:10<00:00,  6.60it/s] 95%|█████████▍| 37/39 [00:10<00:00,  6.60it/s] 97%|█████████▋| 38/39 [00:10<00:00,  6.60it/s]100%|██████████| 39/39 [00:10<00:00,  6.60it/s]100%|██████████| 39/39 [00:11<00:00,  3.52it/s]
=> result
* total: 3,900
* correct: 2,533
* accuracy: 64.9%
* error: 35.1%
* macro_f1: 63.3%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh dtd 1 0 main_tmp1_0.1sdl 16
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
384 576 864
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  24
# train_x  384
# val      576
# test     864
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model.pth.tar-30"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 30
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/tensorboard)
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar" (epoch = 25)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:04<00:36,  4.58s/it] 22%|██▏       | 2/9 [00:04<00:13,  1.99s/it] 33%|███▎      | 3/9 [00:04<00:06,  1.15s/it] 44%|████▍     | 4/9 [00:05<00:03,  1.32it/s] 56%|█████▌    | 5/9 [00:05<00:02,  1.85it/s] 67%|██████▋   | 6/9 [00:05<00:01,  2.44it/s] 78%|███████▊  | 7/9 [00:05<00:00,  3.06it/s] 89%|████████▉ | 8/9 [00:05<00:00,  3.67it/s]100%|██████████| 9/9 [00:05<00:00,  4.51it/s]100%|██████████| 9/9 [00:05<00:00,  1.53it/s]
=> result
* total: 864
* correct: 692
* accuracy: 80.1%
* error: 19.9%
* macro_f1: 79.9%
Elapsed: 0:00:06
+ sh scripts/rpo_prime/base2new_test_sdl.sh dtd 1 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
368 552 828
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  23
# train_x  368
# val      552
# test     828
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar" (epoch = 25)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:04<00:35,  4.41s/it] 22%|██▏       | 2/9 [00:04<00:13,  1.91s/it] 33%|███▎      | 3/9 [00:04<00:06,  1.11s/it] 44%|████▍     | 4/9 [00:04<00:03,  1.37it/s] 56%|█████▌    | 5/9 [00:05<00:02,  1.91it/s] 67%|██████▋   | 6/9 [00:05<00:01,  2.51it/s] 78%|███████▊  | 7/9 [00:05<00:00,  3.13it/s] 89%|████████▉ | 8/9 [00:05<00:00,  3.75it/s]100%|██████████| 9/9 [00:05<00:00,  1.59it/s]
=> result
* total: 828
* correct: 532
* accuracy: 64.3%
* error: 35.7%
* macro_f1: 64.1%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh dtd 2 0 main_tmp1_0.1sdl 16
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
384 576 864
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  24
# train_x  384
# val      576
# test     864
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-30"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 30
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/tensorboard)
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 27)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:04<00:33,  4.24s/it] 22%|██▏       | 2/9 [00:04<00:12,  1.85s/it] 33%|███▎      | 3/9 [00:04<00:06,  1.07s/it] 44%|████▍     | 4/9 [00:04<00:03,  1.40it/s] 56%|█████▌    | 5/9 [00:04<00:02,  1.96it/s] 67%|██████▋   | 6/9 [00:05<00:01,  2.56it/s] 78%|███████▊  | 7/9 [00:05<00:00,  3.15it/s] 89%|████████▉ | 8/9 [00:05<00:00,  3.75it/s]100%|██████████| 9/9 [00:05<00:00,  4.60it/s]100%|██████████| 9/9 [00:05<00:00,  1.61it/s]
=> result
* total: 864
* correct: 709
* accuracy: 82.1%
* error: 17.9%
* macro_f1: 81.9%
Elapsed: 0:00:06
+ sh scripts/rpo_prime/base2new_test_sdl.sh dtd 2 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
368 552 828
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  23
# train_x  368
# val      552
# test     828
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 27)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:04<00:33,  4.22s/it] 22%|██▏       | 2/9 [00:04<00:12,  1.83s/it] 33%|███▎      | 3/9 [00:04<00:06,  1.07s/it] 44%|████▍     | 4/9 [00:04<00:03,  1.42it/s] 56%|█████▌    | 5/9 [00:04<00:02,  1.97it/s] 67%|██████▋   | 6/9 [00:04<00:01,  2.58it/s] 78%|███████▊  | 7/9 [00:05<00:00,  3.20it/s] 89%|████████▉ | 8/9 [00:05<00:00,  3.81it/s]100%|██████████| 9/9 [00:05<00:00,  1.64it/s]
=> result
* total: 828
* correct: 526
* accuracy: 63.5%
* error: 36.5%
* macro_f1: 62.7%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh dtd 3 0 main_tmp1_0.1sdl 16
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
384 576 864
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  24
# train_x  384
# val      576
# test     864
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-30"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 30
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/tensorboard)
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 27)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:04<00:34,  4.25s/it] 22%|██▏       | 2/9 [00:04<00:12,  1.85s/it] 33%|███▎      | 3/9 [00:04<00:06,  1.08s/it] 44%|████▍     | 4/9 [00:04<00:03,  1.40it/s] 56%|█████▌    | 5/9 [00:04<00:02,  1.95it/s] 67%|██████▋   | 6/9 [00:05<00:01,  2.55it/s] 78%|███████▊  | 7/9 [00:05<00:00,  3.18it/s] 89%|████████▉ | 8/9 [00:05<00:00,  3.74it/s]100%|██████████| 9/9 [00:05<00:00,  4.57it/s]100%|██████████| 9/9 [00:05<00:00,  1.61it/s]
=> result
* total: 864
* correct: 687
* accuracy: 79.5%
* error: 20.5%
* macro_f1: 79.3%
Elapsed: 0:00:06
+ sh scripts/rpo_prime/base2new_test_sdl.sh dtd 3 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
368 552 828
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  23
# train_x  368
# val      552
# test     828
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 27)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:04<00:34,  4.35s/it] 22%|██▏       | 2/9 [00:04<00:13,  1.88s/it] 33%|███▎      | 3/9 [00:04<00:06,  1.10s/it] 44%|████▍     | 4/9 [00:04<00:03,  1.38it/s] 56%|█████▌    | 5/9 [00:04<00:02,  1.93it/s] 67%|██████▋   | 6/9 [00:05<00:01,  2.52it/s] 78%|███████▊  | 7/9 [00:05<00:00,  3.15it/s] 89%|████████▉ | 8/9 [00:05<00:00,  3.76it/s]100%|██████████| 9/9 [00:05<00:00,  1.60it/s]
=> result
* total: 828
* correct: 538
* accuracy: 65.0%
* error: 35.0%
* macro_f1: 64.5%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh fgvc_aircraft 1 0 main_tmp1_0.1sdl 16
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
800 1667 1666
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      1,667
# test     1,666
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model.pth.tar-10"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 10
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [11/30] batch [20/200] time 0.360 (0.504) data 0.000 (0.070) loss 2.4375 (2.2226) lr 7.5000e-03 eta 0:33:25
epoch [11/30] batch [40/200] time 0.371 (0.428) data 0.000 (0.035) loss 0.9507 (2.2305) lr 7.5000e-03 eta 0:28:13
epoch [11/30] batch [60/200] time 0.343 (0.402) data 0.000 (0.023) loss 2.2070 (2.1390) lr 7.5000e-03 eta 0:26:25
epoch [11/30] batch [80/200] time 0.353 (0.390) data 0.000 (0.018) loss 2.0352 (2.1598) lr 7.5000e-03 eta 0:25:29
epoch [11/30] batch [100/200] time 0.343 (0.382) data 0.000 (0.014) loss 1.0557 (2.1140) lr 7.5000e-03 eta 0:24:50
epoch [11/30] batch [120/200] time 0.357 (0.377) data 0.000 (0.012) loss 1.5186 (2.1149) lr 7.5000e-03 eta 0:24:23
epoch [11/30] batch [140/200] time 0.337 (0.374) data 0.000 (0.010) loss 1.7539 (2.1007) lr 7.5000e-03 eta 0:24:04
epoch [11/30] batch [160/200] time 0.351 (0.372) data 0.000 (0.009) loss 1.5898 (2.0871) lr 7.5000e-03 eta 0:23:47
epoch [11/30] batch [180/200] time 0.299 (0.366) data 0.000 (0.008) loss 2.7441 (2.1028) lr 7.5000e-03 eta 0:23:16
epoch [11/30] batch [200/200] time 0.296 (0.359) data 0.000 (0.007) loss 2.4648 (2.0954) lr 7.0337e-03 eta 0:22:43
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:32,  5.79s/it] 12%|█▏        | 2/17 [00:06<00:40,  2.68s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.54s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.39it/s] 35%|███▌      | 6/17 [00:07<00:05,  1.84it/s] 41%|████      | 7/17 [00:07<00:04,  2.32it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.79it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.23it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.78it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.29it/s] 71%|███████   | 12/17 [00:08<00:01,  4.72it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.08it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.36it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.53it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.70it/s]100%|██████████| 17/17 [00:08<00:00,  6.30it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 610
* accuracy: 36.6%
* error: 63.4%
* macro_f1: 33.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [12/30] batch [20/200] time 0.356 (0.405) data 0.000 (0.048) loss 2.0879 (1.8507) lr 7.0337e-03 eta 0:25:30
epoch [12/30] batch [40/200] time 0.348 (0.377) data 0.000 (0.024) loss 1.7090 (1.8739) lr 7.0337e-03 eta 0:23:37
epoch [12/30] batch [60/200] time 0.345 (0.369) data 0.000 (0.016) loss 2.6055 (1.8915) lr 7.0337e-03 eta 0:22:59
epoch [12/30] batch [80/200] time 0.374 (0.363) data 0.000 (0.012) loss 1.7295 (1.9002) lr 7.0337e-03 eta 0:22:31
epoch [12/30] batch [100/200] time 0.353 (0.360) data 0.000 (0.010) loss 2.4297 (1.9408) lr 7.0337e-03 eta 0:22:12
epoch [12/30] batch [120/200] time 0.359 (0.358) data 0.000 (0.008) loss 1.0010 (1.9183) lr 7.0337e-03 eta 0:21:57
epoch [12/30] batch [140/200] time 0.339 (0.357) data 0.000 (0.007) loss 2.9551 (1.9184) lr 7.0337e-03 eta 0:21:47
epoch [12/30] batch [160/200] time 0.347 (0.357) data 0.000 (0.006) loss 1.3525 (1.9138) lr 7.0337e-03 eta 0:21:38
epoch [12/30] batch [180/200] time 0.296 (0.352) data 0.000 (0.006) loss 1.6357 (1.9150) lr 7.0337e-03 eta 0:21:16
epoch [12/30] batch [200/200] time 0.301 (0.347) data 0.000 (0.005) loss 2.0059 (1.9443) lr 6.5451e-03 eta 0:20:48
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.50s/it] 12%|█▏        | 2/17 [00:05<00:38,  2.55s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.51s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.02s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.71it/s] 41%|████      | 7/17 [00:07<00:04,  2.07it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.57it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.15it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.68it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.19it/s] 71%|███████   | 12/17 [00:08<00:01,  4.64it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.01it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.31it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.54it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.71it/s]100%|██████████| 17/17 [00:08<00:00,  6.32it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 575
* accuracy: 34.5%
* error: 65.5%
* macro_f1: 30.7%

epoch [13/30] batch [20/200] time 0.373 (0.401) data 0.000 (0.045) loss 1.9854 (2.1027) lr 6.5451e-03 eta 0:23:55
epoch [13/30] batch [40/200] time 0.360 (0.377) data 0.000 (0.023) loss 2.0254 (1.9535) lr 6.5451e-03 eta 0:22:20
epoch [13/30] batch [60/200] time 0.356 (0.367) data 0.000 (0.015) loss 1.8770 (2.0222) lr 6.5451e-03 eta 0:21:38
epoch [13/30] batch [80/200] time 0.336 (0.362) data 0.000 (0.011) loss 2.0078 (2.0178) lr 6.5451e-03 eta 0:21:12
epoch [13/30] batch [100/200] time 0.348 (0.359) data 0.000 (0.009) loss 3.8633 (2.0605) lr 6.5451e-03 eta 0:20:54
epoch [13/30] batch [120/200] time 0.350 (0.357) data 0.000 (0.008) loss 1.9229 (2.0725) lr 6.5451e-03 eta 0:20:41
epoch [13/30] batch [140/200] time 0.357 (0.357) data 0.000 (0.007) loss 1.8809 (2.0622) lr 6.5451e-03 eta 0:20:33
epoch [13/30] batch [160/200] time 0.360 (0.356) data 0.000 (0.006) loss 2.0977 (2.0322) lr 6.5451e-03 eta 0:20:24
epoch [13/30] batch [180/200] time 0.304 (0.352) data 0.000 (0.005) loss 2.3535 (2.0436) lr 6.5451e-03 eta 0:20:05
epoch [13/30] batch [200/200] time 0.298 (0.347) data 0.000 (0.005) loss 1.6074 (2.0407) lr 6.0396e-03 eta 0:19:38
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:29,  5.61s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.55s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.51s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.00s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.38it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.75it/s] 41%|████      | 7/17 [00:07<00:04,  2.11it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.52it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.09it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.65it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.17it/s] 71%|███████   | 12/17 [00:08<00:01,  4.62it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.99it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.28it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.51it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.68it/s]100%|██████████| 17/17 [00:08<00:00,  6.29it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 553
* accuracy: 33.2%
* error: 66.8%
* macro_f1: 29.9%

epoch [14/30] batch [20/200] time 0.334 (0.400) data 0.000 (0.046) loss 2.4238 (2.1304) lr 6.0396e-03 eta 0:22:33
epoch [14/30] batch [40/200] time 0.346 (0.377) data 0.000 (0.023) loss 2.6172 (2.0039) lr 6.0396e-03 eta 0:21:05
epoch [14/30] batch [60/200] time 0.342 (0.367) data 0.000 (0.015) loss 2.0859 (1.9085) lr 6.0396e-03 eta 0:20:25
epoch [14/30] batch [80/200] time 0.338 (0.364) data 0.000 (0.012) loss 2.0938 (1.9413) lr 6.0396e-03 eta 0:20:07
epoch [14/30] batch [100/200] time 0.356 (0.362) data 0.000 (0.009) loss 1.5957 (1.9816) lr 6.0396e-03 eta 0:19:54
epoch [14/30] batch [120/200] time 0.351 (0.359) data 0.000 (0.008) loss 2.3066 (1.9924) lr 6.0396e-03 eta 0:19:39
epoch [14/30] batch [140/200] time 0.344 (0.357) data 0.000 (0.007) loss 1.4814 (1.9939) lr 6.0396e-03 eta 0:19:24
epoch [14/30] batch [160/200] time 0.363 (0.357) data 0.000 (0.006) loss 1.9180 (2.0319) lr 6.0396e-03 eta 0:19:16
epoch [14/30] batch [180/200] time 0.295 (0.353) data 0.000 (0.005) loss 1.8301 (2.0430) lr 6.0396e-03 eta 0:18:55
epoch [14/30] batch [200/200] time 0.295 (0.347) data 0.000 (0.005) loss 2.6230 (2.0440) lr 5.5226e-03 eta 0:18:29
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:30,  5.63s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.50s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.48s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.00s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.35it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.73it/s] 41%|████      | 7/17 [00:07<00:04,  2.08it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.53it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.09it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.65it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.17it/s] 71%|███████   | 12/17 [00:08<00:01,  4.62it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.99it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.29it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.52it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.69it/s]100%|██████████| 17/17 [00:08<00:00,  6.29it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 591
* accuracy: 35.5%
* error: 64.5%
* macro_f1: 32.9%

epoch [15/30] batch [20/200] time 0.353 (0.412) data 0.000 (0.047) loss 2.1289 (1.9480) lr 5.5226e-03 eta 0:21:51
epoch [15/30] batch [40/200] time 0.349 (0.381) data 0.000 (0.024) loss 1.8799 (1.9138) lr 5.5226e-03 eta 0:20:03
epoch [15/30] batch [60/200] time 0.337 (0.369) data 0.000 (0.016) loss 1.8945 (1.9596) lr 5.5226e-03 eta 0:19:20
epoch [15/30] batch [80/200] time 0.362 (0.364) data 0.000 (0.012) loss 2.3340 (1.9465) lr 5.5226e-03 eta 0:18:56
epoch [15/30] batch [100/200] time 0.339 (0.362) data 0.000 (0.010) loss 1.6768 (1.9177) lr 5.5226e-03 eta 0:18:42
epoch [15/30] batch [120/200] time 0.356 (0.361) data 0.000 (0.008) loss 2.0117 (1.9729) lr 5.5226e-03 eta 0:18:30
epoch [15/30] batch [140/200] time 0.373 (0.359) data 0.000 (0.007) loss 2.7988 (1.9870) lr 5.5226e-03 eta 0:18:18
epoch [15/30] batch [160/200] time 0.345 (0.358) data 0.000 (0.006) loss 2.8887 (1.9959) lr 5.5226e-03 eta 0:18:08
epoch [15/30] batch [180/200] time 0.297 (0.354) data 0.000 (0.006) loss 2.1133 (1.9862) lr 5.5226e-03 eta 0:17:48
epoch [15/30] batch [200/200] time 0.294 (0.348) data 0.000 (0.005) loss 2.2305 (1.9852) lr 5.0000e-03 eta 0:17:24
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.47s/it] 12%|█▏        | 2/17 [00:05<00:38,  2.54s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.46s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.04it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.47it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.92it/s] 41%|████      | 7/17 [00:06<00:04,  2.34it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.76it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.24it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.64it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.16it/s] 71%|███████   | 12/17 [00:07<00:01,  4.61it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.99it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.28it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.51it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.69it/s]100%|██████████| 17/17 [00:08<00:00,  6.25it/s]100%|██████████| 17/17 [00:08<00:00,  1.93it/s]=> result
* total: 1,667
* correct: 604
* accuracy: 36.2%
* error: 63.8%
* macro_f1: 33.3%

epoch [16/30] batch [20/200] time 0.337 (0.405) data 0.000 (0.047) loss 2.6660 (1.8263) lr 5.0000e-03 eta 0:20:05
epoch [16/30] batch [40/200] time 0.351 (0.376) data 0.000 (0.024) loss 0.8833 (1.8523) lr 5.0000e-03 eta 0:18:32
epoch [16/30] batch [60/200] time 0.349 (0.368) data 0.000 (0.016) loss 1.5693 (1.8252) lr 5.0000e-03 eta 0:18:01
epoch [16/30] batch [80/200] time 0.370 (0.363) data 0.000 (0.012) loss 1.5000 (1.8360) lr 5.0000e-03 eta 0:17:40
epoch [16/30] batch [100/200] time 0.340 (0.360) data 0.000 (0.010) loss 2.0938 (1.9057) lr 5.0000e-03 eta 0:17:24
epoch [16/30] batch [120/200] time 0.335 (0.358) data 0.000 (0.008) loss 2.2930 (1.9221) lr 5.0000e-03 eta 0:17:11
epoch [16/30] batch [140/200] time 0.358 (0.358) data 0.000 (0.007) loss 1.7295 (1.8987) lr 5.0000e-03 eta 0:17:04
epoch [16/30] batch [160/200] time 0.352 (0.357) data 0.000 (0.006) loss 1.3848 (1.8907) lr 5.0000e-03 eta 0:16:55
epoch [16/30] batch [180/200] time 0.296 (0.353) data 0.000 (0.005) loss 2.2148 (1.9043) lr 5.0000e-03 eta 0:16:35
epoch [16/30] batch [200/200] time 0.299 (0.347) data 0.000 (0.005) loss 1.6250 (1.9193) lr 4.4774e-03 eta 0:16:12
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:31,  5.73s/it] 12%|█▏        | 2/17 [00:06<00:37,  2.53s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.50s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.71it/s] 41%|████      | 7/17 [00:07<00:04,  2.08it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.63it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.20it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.76it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.26it/s] 71%|███████   | 12/17 [00:08<00:01,  4.70it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.06it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.35it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.51it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.69it/s]100%|██████████| 17/17 [00:08<00:00,  6.29it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 613
* accuracy: 36.8%
* error: 63.2%
* macro_f1: 34.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [17/30] batch [20/200] time 0.338 (0.399) data 0.000 (0.045) loss 2.0820 (1.9472) lr 4.4774e-03 eta 0:18:28
epoch [17/30] batch [40/200] time 0.360 (0.375) data 0.000 (0.023) loss 1.9834 (2.0437) lr 4.4774e-03 eta 0:17:14
epoch [17/30] batch [60/200] time 0.351 (0.366) data 0.000 (0.015) loss 1.9014 (1.9918) lr 4.4774e-03 eta 0:16:44
epoch [17/30] batch [80/200] time 0.355 (0.364) data 0.000 (0.011) loss 1.7578 (1.9604) lr 4.4774e-03 eta 0:16:30
epoch [17/30] batch [100/200] time 0.374 (0.363) data 0.000 (0.009) loss 1.6758 (1.9811) lr 4.4774e-03 eta 0:16:18
epoch [17/30] batch [120/200] time 0.357 (0.361) data 0.000 (0.008) loss 1.4199 (1.9764) lr 4.4774e-03 eta 0:16:08
epoch [17/30] batch [140/200] time 0.348 (0.360) data 0.000 (0.007) loss 1.6670 (1.9755) lr 4.4774e-03 eta 0:15:56
epoch [17/30] batch [160/200] time 0.365 (0.358) data 0.000 (0.006) loss 1.9863 (1.9604) lr 4.4774e-03 eta 0:15:46
epoch [17/30] batch [180/200] time 0.301 (0.354) data 0.000 (0.005) loss 2.1230 (1.9684) lr 4.4774e-03 eta 0:15:28
epoch [17/30] batch [200/200] time 0.297 (0.349) data 0.000 (0.005) loss 2.6289 (1.9880) lr 3.9604e-03 eta 0:15:06
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.56s/it] 12%|█▏        | 2/17 [00:05<00:38,  2.55s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.51s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.70it/s] 41%|████      | 7/17 [00:07<00:04,  2.06it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.60it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.17it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.73it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.23it/s] 71%|███████   | 12/17 [00:08<00:01,  4.67it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.04it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.32it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.54it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.71it/s]100%|██████████| 17/17 [00:08<00:00,  6.31it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 600
* accuracy: 36.0%
* error: 64.0%
* macro_f1: 32.7%

epoch [18/30] batch [20/200] time 0.372 (0.407) data 0.000 (0.046) loss 1.1992 (2.0788) lr 3.9604e-03 eta 0:17:28
epoch [18/30] batch [40/200] time 0.368 (0.380) data 0.000 (0.023) loss 2.3027 (1.9803) lr 3.9604e-03 eta 0:16:12
epoch [18/30] batch [60/200] time 0.357 (0.371) data 0.000 (0.016) loss 1.3369 (1.9026) lr 3.9604e-03 eta 0:15:42
epoch [18/30] batch [80/200] time 0.345 (0.366) data 0.000 (0.012) loss 1.4570 (1.9106) lr 3.9604e-03 eta 0:15:21
epoch [18/30] batch [100/200] time 0.339 (0.362) data 0.000 (0.009) loss 1.3818 (1.9271) lr 3.9604e-03 eta 0:15:05
epoch [18/30] batch [120/200] time 0.334 (0.360) data 0.000 (0.008) loss 2.3535 (1.9204) lr 3.9604e-03 eta 0:14:53
epoch [18/30] batch [140/200] time 0.369 (0.359) data 0.000 (0.007) loss 2.6367 (1.9475) lr 3.9604e-03 eta 0:14:43
epoch [18/30] batch [160/200] time 0.336 (0.358) data 0.000 (0.006) loss 1.7393 (1.9674) lr 3.9604e-03 eta 0:14:34
epoch [18/30] batch [180/200] time 0.299 (0.354) data 0.000 (0.005) loss 0.6577 (1.9661) lr 3.9604e-03 eta 0:14:15
epoch [18/30] batch [200/200] time 0.299 (0.348) data 0.000 (0.005) loss 2.4062 (1.9671) lr 3.4549e-03 eta 0:13:56
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.53s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.57s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.03it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.44it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.89it/s] 41%|████      | 7/17 [00:07<00:04,  2.30it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.77it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.02it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.58it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.10it/s] 71%|███████   | 12/17 [00:08<00:01,  4.56it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.96it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.20it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.45it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.64it/s]100%|██████████| 17/17 [00:08<00:00,  6.25it/s]100%|██████████| 17/17 [00:08<00:00,  1.90it/s]=> result
* total: 1,667
* correct: 626
* accuracy: 37.6%
* error: 62.4%
* macro_f1: 34.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [19/30] batch [20/200] time 0.347 (0.404) data 0.000 (0.044) loss 1.8057 (2.1367) lr 3.4549e-03 eta 0:16:02
epoch [19/30] batch [40/200] time 0.346 (0.378) data 0.000 (0.022) loss 1.6621 (2.0797) lr 3.4549e-03 eta 0:14:51
epoch [19/30] batch [60/200] time 0.356 (0.368) data 0.000 (0.015) loss 1.9502 (2.0249) lr 3.4549e-03 eta 0:14:20
epoch [19/30] batch [80/200] time 0.334 (0.363) data 0.000 (0.011) loss 1.8721 (2.0404) lr 3.4549e-03 eta 0:14:02
epoch [19/30] batch [100/200] time 0.329 (0.361) data 0.000 (0.009) loss 1.0928 (2.0098) lr 3.4549e-03 eta 0:13:50
epoch [19/30] batch [120/200] time 0.354 (0.360) data 0.000 (0.008) loss 2.0586 (1.9918) lr 3.4549e-03 eta 0:13:41
epoch [19/30] batch [140/200] time 0.338 (0.359) data 0.000 (0.007) loss 1.4854 (2.0340) lr 3.4549e-03 eta 0:13:30
epoch [19/30] batch [160/200] time 0.352 (0.358) data 0.000 (0.006) loss 1.7090 (2.0198) lr 3.4549e-03 eta 0:13:21
epoch [19/30] batch [180/200] time 0.296 (0.353) data 0.000 (0.005) loss 1.5625 (1.9996) lr 3.4549e-03 eta 0:13:04
epoch [19/30] batch [200/200] time 0.297 (0.348) data 0.000 (0.005) loss 1.5938 (1.9834) lr 2.9663e-03 eta 0:12:44
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:29,  5.62s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.51s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.71it/s] 41%|████      | 7/17 [00:07<00:04,  2.07it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.54it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.11it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.66it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.18it/s] 71%|███████   | 12/17 [00:08<00:01,  4.63it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.00it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.30it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.43it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.62it/s]100%|██████████| 17/17 [00:08<00:00,  6.23it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 612
* accuracy: 36.7%
* error: 63.3%
* macro_f1: 33.0%

epoch [20/30] batch [20/200] time 0.343 (0.402) data 0.000 (0.045) loss 2.4512 (2.1141) lr 2.9663e-03 eta 0:14:36
epoch [20/30] batch [40/200] time 0.361 (0.374) data 0.000 (0.023) loss 1.4287 (1.9989) lr 2.9663e-03 eta 0:13:28
epoch [20/30] batch [60/200] time 0.348 (0.367) data 0.000 (0.015) loss 0.9268 (1.9909) lr 2.9663e-03 eta 0:13:05
epoch [20/30] batch [80/200] time 0.357 (0.362) data 0.000 (0.011) loss 2.8027 (1.9232) lr 2.9663e-03 eta 0:12:48
epoch [20/30] batch [100/200] time 0.346 (0.361) data 0.000 (0.009) loss 1.3721 (1.9369) lr 2.9663e-03 eta 0:12:37
epoch [20/30] batch [120/200] time 0.352 (0.359) data 0.000 (0.008) loss 1.9443 (1.9615) lr 2.9663e-03 eta 0:12:27
epoch [20/30] batch [140/200] time 0.342 (0.358) data 0.000 (0.007) loss 2.0137 (1.9910) lr 2.9663e-03 eta 0:12:17
epoch [20/30] batch [160/200] time 0.388 (0.357) data 0.000 (0.006) loss 1.9414 (1.9676) lr 2.9663e-03 eta 0:12:08
epoch [20/30] batch [180/200] time 0.294 (0.353) data 0.000 (0.005) loss 1.8574 (1.9444) lr 2.9663e-03 eta 0:11:52
epoch [20/30] batch [200/200] time 0.296 (0.347) data 0.000 (0.005) loss 1.9648 (1.9552) lr 2.5000e-03 eta 0:11:34
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.56s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.58s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.53s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.03s/it] 29%|██▉       | 5/17 [00:06<00:09,  1.32it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.70it/s] 41%|████      | 7/17 [00:07<00:04,  2.05it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.59it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.16it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.72it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.23it/s] 71%|███████   | 12/17 [00:08<00:01,  4.67it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.04it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.32it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.55it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.71it/s]100%|██████████| 17/17 [00:09<00:00,  6.31it/s]100%|██████████| 17/17 [00:09<00:00,  1.86it/s]=> result
* total: 1,667
* correct: 629
* accuracy: 37.7%
* error: 62.3%
* macro_f1: 35.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model.pth.tar-20

epoch [21/30] batch [20/200] time 0.344 (0.403) data 0.000 (0.047) loss 2.2812 (1.8886) lr 2.5000e-03 eta 0:13:17
epoch [21/30] batch [40/200] time 0.362 (0.377) data 0.000 (0.024) loss 1.2930 (1.9006) lr 2.5000e-03 eta 0:12:19
epoch [21/30] batch [60/200] time 0.350 (0.371) data 0.000 (0.016) loss 2.4238 (1.9146) lr 2.5000e-03 eta 0:12:00
epoch [21/30] batch [80/200] time 0.337 (0.365) data 0.000 (0.012) loss 2.4844 (1.8690) lr 2.5000e-03 eta 0:11:40
epoch [21/30] batch [100/200] time 0.340 (0.361) data 0.000 (0.010) loss 2.0039 (1.8890) lr 2.5000e-03 eta 0:11:26
epoch [21/30] batch [120/200] time 0.352 (0.360) data 0.000 (0.008) loss 1.6699 (1.9019) lr 2.5000e-03 eta 0:11:17
epoch [21/30] batch [140/200] time 0.352 (0.359) data 0.000 (0.007) loss 2.2754 (1.9206) lr 2.5000e-03 eta 0:11:07
epoch [21/30] batch [160/200] time 0.348 (0.357) data 0.000 (0.006) loss 1.5273 (1.9033) lr 2.5000e-03 eta 0:10:57
epoch [21/30] batch [180/200] time 0.298 (0.353) data 0.000 (0.005) loss 1.4502 (1.9114) lr 2.5000e-03 eta 0:10:43
epoch [21/30] batch [200/200] time 0.294 (0.348) data 0.000 (0.005) loss 1.1660 (1.8934) lr 2.0611e-03 eta 0:10:26
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.52s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.56s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.52s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.03s/it] 29%|██▉       | 5/17 [00:06<00:09,  1.32it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.69it/s] 41%|████      | 7/17 [00:07<00:04,  2.05it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.60it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.17it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.73it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.23it/s] 71%|███████   | 12/17 [00:08<00:01,  4.68it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.04it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.33it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.49it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.67it/s]100%|██████████| 17/17 [00:08<00:00,  6.27it/s]100%|██████████| 17/17 [00:09<00:00,  1.86it/s]=> result
* total: 1,667
* correct: 611
* accuracy: 36.7%
* error: 63.3%
* macro_f1: 33.2%

epoch [22/30] batch [20/200] time 0.337 (0.405) data 0.000 (0.053) loss 2.4648 (1.9602) lr 2.0611e-03 eta 0:12:00
epoch [22/30] batch [40/200] time 0.360 (0.378) data 0.000 (0.027) loss 1.8574 (1.8745) lr 2.0611e-03 eta 0:11:05
epoch [22/30] batch [60/200] time 0.360 (0.369) data 0.000 (0.018) loss 2.5898 (1.9621) lr 2.0611e-03 eta 0:10:42
epoch [22/30] batch [80/200] time 0.375 (0.366) data 0.000 (0.014) loss 1.6953 (1.9913) lr 2.0611e-03 eta 0:10:29
epoch [22/30] batch [100/200] time 0.347 (0.364) data 0.000 (0.011) loss 1.3965 (1.9408) lr 2.0611e-03 eta 0:10:18
epoch [22/30] batch [120/200] time 0.351 (0.362) data 0.000 (0.009) loss 0.9277 (1.9051) lr 2.0611e-03 eta 0:10:07
epoch [22/30] batch [140/200] time 0.367 (0.361) data 0.000 (0.008) loss 1.8887 (1.9229) lr 2.0611e-03 eta 0:09:58
epoch [22/30] batch [160/200] time 0.347 (0.359) data 0.000 (0.007) loss 1.6230 (1.9266) lr 2.0611e-03 eta 0:09:48
epoch [22/30] batch [180/200] time 0.298 (0.355) data 0.000 (0.006) loss 0.7119 (1.9283) lr 2.0611e-03 eta 0:09:34
epoch [22/30] batch [200/200] time 0.294 (0.349) data 0.000 (0.006) loss 2.3457 (1.9218) lr 1.6543e-03 eta 0:09:17
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:31,  5.74s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.55s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.48s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.02it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.44it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.91it/s] 41%|████      | 7/17 [00:07<00:04,  2.37it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.89it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.38it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.79it/s] 65%|██████▍   | 11/17 [00:07<00:01,  3.99it/s] 71%|███████   | 12/17 [00:08<00:01,  4.47it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.87it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.20it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.45it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.55it/s]100%|██████████| 17/17 [00:08<00:00,  6.17it/s]100%|██████████| 17/17 [00:08<00:00,  1.91it/s]=> result
* total: 1,667
* correct: 621
* accuracy: 37.3%
* error: 62.7%
* macro_f1: 34.5%

epoch [23/30] batch [20/200] time 0.348 (0.406) data 0.000 (0.045) loss 1.5850 (1.9650) lr 1.6543e-03 eta 0:10:40
epoch [23/30] batch [40/200] time 0.333 (0.377) data 0.000 (0.023) loss 1.9062 (1.9574) lr 1.6543e-03 eta 0:09:48
epoch [23/30] batch [60/200] time 0.345 (0.368) data 0.000 (0.015) loss 2.1074 (1.9233) lr 1.6543e-03 eta 0:09:26
epoch [23/30] batch [80/200] time 0.348 (0.363) data 0.000 (0.012) loss 2.0039 (1.9125) lr 1.6543e-03 eta 0:09:11
epoch [23/30] batch [100/200] time 0.343 (0.360) data 0.000 (0.009) loss 2.3438 (1.8807) lr 1.6543e-03 eta 0:08:59
epoch [23/30] batch [120/200] time 0.348 (0.359) data 0.000 (0.008) loss 2.0605 (1.9269) lr 1.6543e-03 eta 0:08:51
epoch [23/30] batch [140/200] time 0.370 (0.358) data 0.000 (0.007) loss 1.7344 (1.9258) lr 1.6543e-03 eta 0:08:42
epoch [23/30] batch [160/200] time 0.352 (0.357) data 0.000 (0.006) loss 2.0977 (1.9769) lr 1.6543e-03 eta 0:08:34
epoch [23/30] batch [180/200] time 0.295 (0.353) data 0.000 (0.005) loss 1.6973 (1.9776) lr 1.6543e-03 eta 0:08:20
epoch [23/30] batch [200/200] time 0.294 (0.347) data 0.000 (0.005) loss 1.6055 (1.9584) lr 1.2843e-03 eta 0:08:05
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.53s/it] 12%|█▏        | 2/17 [00:05<00:38,  2.53s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.50s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.02s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.70it/s] 41%|████      | 7/17 [00:07<00:04,  2.06it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.46it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.03it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.59it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.10it/s] 71%|███████   | 12/17 [00:08<00:01,  4.56it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.95it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.25it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.49it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.67it/s]100%|██████████| 17/17 [00:09<00:00,  6.27it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 635
* accuracy: 38.1%
* error: 61.9%
* macro_f1: 35.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [24/30] batch [20/200] time 0.336 (0.398) data 0.000 (0.044) loss 0.6914 (1.7635) lr 1.2843e-03 eta 0:09:08
epoch [24/30] batch [40/200] time 0.469 (0.376) data 0.000 (0.022) loss 2.8770 (1.8295) lr 1.2843e-03 eta 0:08:31
epoch [24/30] batch [60/200] time 0.376 (0.368) data 0.000 (0.015) loss 1.3770 (1.8388) lr 1.2843e-03 eta 0:08:13
epoch [24/30] batch [80/200] time 0.341 (0.364) data 0.000 (0.011) loss 2.8906 (1.8393) lr 1.2843e-03 eta 0:07:59
epoch [24/30] batch [100/200] time 0.346 (0.361) data 0.000 (0.009) loss 2.2793 (1.8625) lr 1.2843e-03 eta 0:07:49
epoch [24/30] batch [120/200] time 0.362 (0.360) data 0.000 (0.008) loss 1.0527 (1.8659) lr 1.2843e-03 eta 0:07:40
epoch [24/30] batch [140/200] time 0.371 (0.360) data 0.000 (0.007) loss 1.3057 (1.8966) lr 1.2843e-03 eta 0:07:33
epoch [24/30] batch [160/200] time 0.346 (0.358) data 0.000 (0.006) loss 1.8037 (1.9016) lr 1.2843e-03 eta 0:07:24
epoch [24/30] batch [180/200] time 0.295 (0.354) data 0.000 (0.005) loss 2.3066 (1.8769) lr 1.2843e-03 eta 0:07:11
epoch [24/30] batch [200/200] time 0.297 (0.348) data 0.000 (0.005) loss 1.7266 (1.8737) lr 9.5492e-04 eta 0:06:57
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.48s/it] 12%|█▏        | 2/17 [00:05<00:38,  2.56s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.51s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.00it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.43it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.88it/s] 41%|████      | 7/17 [00:07<00:04,  2.23it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.54it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.11it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.67it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.19it/s] 71%|███████   | 12/17 [00:08<00:01,  4.64it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.01it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.31it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.54it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.71it/s]100%|██████████| 17/17 [00:08<00:00,  6.26it/s]100%|██████████| 17/17 [00:08<00:00,  1.89it/s]=> result
* total: 1,667
* correct: 631
* accuracy: 37.9%
* error: 62.1%
* macro_f1: 35.1%

epoch [25/30] batch [20/200] time 0.345 (0.406) data 0.000 (0.049) loss 1.6816 (1.5714) lr 9.5492e-04 eta 0:07:59
epoch [25/30] batch [40/200] time 0.347 (0.376) data 0.000 (0.024) loss 1.8330 (1.6860) lr 9.5492e-04 eta 0:07:16
epoch [25/30] batch [60/200] time 0.366 (0.369) data 0.000 (0.016) loss 1.8994 (1.7645) lr 9.5492e-04 eta 0:07:00
epoch [25/30] batch [80/200] time 0.361 (0.364) data 0.000 (0.012) loss 1.8691 (1.7940) lr 9.5492e-04 eta 0:06:48
epoch [25/30] batch [100/200] time 0.353 (0.362) data 0.000 (0.010) loss 0.9028 (1.8154) lr 9.5492e-04 eta 0:06:38
epoch [25/30] batch [120/200] time 0.357 (0.360) data 0.000 (0.008) loss 2.5391 (1.8042) lr 9.5492e-04 eta 0:06:28
epoch [25/30] batch [140/200] time 0.340 (0.358) data 0.000 (0.007) loss 2.4453 (1.8217) lr 9.5492e-04 eta 0:06:19
epoch [25/30] batch [160/200] time 0.356 (0.357) data 0.000 (0.006) loss 1.8154 (1.8462) lr 9.5492e-04 eta 0:06:10
epoch [25/30] batch [180/200] time 0.294 (0.353) data 0.000 (0.006) loss 2.6602 (1.8609) lr 9.5492e-04 eta 0:06:00
epoch [25/30] batch [200/200] time 0.295 (0.347) data 0.000 (0.005) loss 1.3730 (1.8751) lr 6.6987e-04 eta 0:05:47
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:29,  5.60s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.55s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.02it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.43it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.90it/s] 41%|████      | 7/17 [00:07<00:04,  2.33it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.82it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.31it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.73it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.14it/s] 71%|███████   | 12/17 [00:07<00:01,  4.60it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.98it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.23it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.48it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.66it/s]100%|██████████| 17/17 [00:08<00:00,  6.27it/s]100%|██████████| 17/17 [00:08<00:00,  1.91it/s]=> result
* total: 1,667
* correct: 625
* accuracy: 37.5%
* error: 62.5%
* macro_f1: 34.8%

epoch [26/30] batch [20/200] time 0.341 (0.402) data 0.000 (0.048) loss 1.7705 (1.7480) lr 6.6987e-04 eta 0:06:33
epoch [26/30] batch [40/200] time 0.334 (0.376) data 0.000 (0.024) loss 0.6206 (1.7923) lr 6.6987e-04 eta 0:06:01
epoch [26/30] batch [60/200] time 0.360 (0.368) data 0.000 (0.016) loss 1.8965 (1.7837) lr 6.6987e-04 eta 0:05:46
epoch [26/30] batch [80/200] time 0.346 (0.363) data 0.000 (0.012) loss 1.3779 (1.8116) lr 6.6987e-04 eta 0:05:33
epoch [26/30] batch [100/200] time 0.337 (0.362) data 0.000 (0.010) loss 3.3906 (1.8617) lr 6.6987e-04 eta 0:05:25
epoch [26/30] batch [120/200] time 0.364 (0.360) data 0.000 (0.008) loss 1.8818 (1.8470) lr 6.6987e-04 eta 0:05:16
epoch [26/30] batch [140/200] time 0.343 (0.358) data 0.000 (0.007) loss 1.1084 (1.8390) lr 6.6987e-04 eta 0:05:08
epoch [26/30] batch [160/200] time 0.349 (0.358) data 0.000 (0.006) loss 2.8750 (1.8433) lr 6.6987e-04 eta 0:05:00
epoch [26/30] batch [180/200] time 0.295 (0.353) data 0.000 (0.006) loss 1.7559 (1.8611) lr 6.6987e-04 eta 0:04:49
epoch [26/30] batch [200/200] time 0.373 (0.348) data 0.000 (0.005) loss 1.6602 (1.8578) lr 4.3227e-04 eta 0:04:38
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:30,  5.66s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.54s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.51s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.71it/s] 41%|████      | 7/17 [00:07<00:04,  2.08it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.63it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.20it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.75it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.26it/s] 71%|███████   | 12/17 [00:08<00:01,  4.70it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.06it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.34it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.56it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.72it/s]100%|██████████| 17/17 [00:08<00:00,  6.31it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 629
* accuracy: 37.7%
* error: 62.3%
* macro_f1: 34.9%

epoch [27/30] batch [20/200] time 0.339 (0.401) data 0.000 (0.044) loss 1.2109 (1.7498) lr 4.3227e-04 eta 0:05:12
epoch [27/30] batch [40/200] time 0.346 (0.374) data 0.000 (0.022) loss 2.6348 (2.0099) lr 4.3227e-04 eta 0:04:44
epoch [27/30] batch [60/200] time 0.343 (0.366) data 0.000 (0.015) loss 1.8447 (1.9149) lr 4.3227e-04 eta 0:04:30
epoch [27/30] batch [80/200] time 0.350 (0.360) data 0.000 (0.011) loss 2.0527 (1.8868) lr 4.3227e-04 eta 0:04:19
epoch [27/30] batch [100/200] time 0.349 (0.358) data 0.000 (0.009) loss 1.6250 (1.8701) lr 4.3227e-04 eta 0:04:10
epoch [27/30] batch [120/200] time 0.341 (0.356) data 0.000 (0.008) loss 1.5068 (1.8525) lr 4.3227e-04 eta 0:04:02
epoch [27/30] batch [140/200] time 0.369 (0.356) data 0.000 (0.007) loss 1.7549 (1.8488) lr 4.3227e-04 eta 0:03:55
epoch [27/30] batch [160/200] time 0.352 (0.356) data 0.000 (0.006) loss 2.2031 (1.8696) lr 4.3227e-04 eta 0:03:48
epoch [27/30] batch [180/200] time 0.295 (0.352) data 0.000 (0.005) loss 2.4453 (1.8759) lr 4.3227e-04 eta 0:03:38
epoch [27/30] batch [200/200] time 0.297 (0.346) data 0.000 (0.005) loss 3.5977 (1.8773) lr 2.4472e-04 eta 0:03:27
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:25,  5.33s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.53s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.50s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:06<00:06,  1.71it/s] 41%|████      | 7/17 [00:07<00:04,  2.08it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.62it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.19it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.74it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.25it/s] 71%|███████   | 12/17 [00:08<00:01,  4.68it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.03it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.32it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.53it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.69it/s]100%|██████████| 17/17 [00:08<00:00,  6.27it/s]100%|██████████| 17/17 [00:08<00:00,  1.89it/s]=> result
* total: 1,667
* correct: 636
* accuracy: 38.2%
* error: 61.8%
* macro_f1: 35.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [28/30] batch [20/200] time 0.347 (0.408) data 0.000 (0.046) loss 2.5742 (2.0564) lr 2.4472e-04 eta 0:03:56
epoch [28/30] batch [40/200] time 0.336 (0.376) data 0.000 (0.023) loss 1.4541 (1.9254) lr 2.4472e-04 eta 0:03:30
epoch [28/30] batch [60/200] time 0.343 (0.368) data 0.000 (0.015) loss 1.9727 (1.9269) lr 2.4472e-04 eta 0:03:18
epoch [28/30] batch [80/200] time 0.341 (0.365) data 0.000 (0.012) loss 1.9307 (1.9002) lr 2.4472e-04 eta 0:03:09
epoch [28/30] batch [100/200] time 0.349 (0.362) data 0.000 (0.009) loss 1.4082 (1.9103) lr 2.4472e-04 eta 0:03:01
epoch [28/30] batch [120/200] time 0.346 (0.360) data 0.000 (0.008) loss 2.4258 (1.8815) lr 2.4472e-04 eta 0:02:52
epoch [28/30] batch [140/200] time 0.355 (0.359) data 0.000 (0.007) loss 1.2773 (1.8842) lr 2.4472e-04 eta 0:02:45
epoch [28/30] batch [160/200] time 0.339 (0.359) data 0.000 (0.006) loss 2.6895 (1.8796) lr 2.4472e-04 eta 0:02:37
epoch [28/30] batch [180/200] time 0.299 (0.354) data 0.000 (0.005) loss 1.9580 (1.8766) lr 2.4472e-04 eta 0:02:28
epoch [28/30] batch [200/200] time 0.296 (0.348) data 0.000 (0.005) loss 3.0430 (1.8617) lr 1.0926e-04 eta 0:02:19
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.46s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.52s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.72it/s] 41%|████      | 7/17 [00:07<00:04,  2.09it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.59it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.16it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.71it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.23it/s] 71%|███████   | 12/17 [00:08<00:01,  4.67it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.03it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.33it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.55it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.71it/s]100%|██████████| 17/17 [00:08<00:00,  6.32it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 633
* accuracy: 38.0%
* error: 62.0%
* macro_f1: 35.2%

epoch [29/30] batch [20/200] time 0.363 (0.407) data 0.000 (0.050) loss 2.2051 (1.6947) lr 1.0926e-04 eta 0:02:34
epoch [29/30] batch [40/200] time 0.345 (0.377) data 0.000 (0.025) loss 2.2656 (1.8212) lr 1.0926e-04 eta 0:02:15
epoch [29/30] batch [60/200] time 0.356 (0.370) data 0.000 (0.017) loss 2.1895 (1.8036) lr 1.0926e-04 eta 0:02:05
epoch [29/30] batch [80/200] time 0.367 (0.365) data 0.000 (0.013) loss 1.7578 (1.7967) lr 1.0926e-04 eta 0:01:56
epoch [29/30] batch [100/200] time 0.356 (0.362) data 0.000 (0.010) loss 2.2422 (1.7670) lr 1.0926e-04 eta 0:01:48
epoch [29/30] batch [120/200] time 0.345 (0.360) data 0.000 (0.009) loss 0.8711 (1.7609) lr 1.0926e-04 eta 0:01:40
epoch [29/30] batch [140/200] time 0.333 (0.359) data 0.000 (0.007) loss 1.7695 (1.7735) lr 1.0926e-04 eta 0:01:33
epoch [29/30] batch [160/200] time 0.355 (0.358) data 0.000 (0.006) loss 1.2490 (1.7765) lr 1.0926e-04 eta 0:01:25
epoch [29/30] batch [180/200] time 0.294 (0.353) data 0.000 (0.006) loss 1.3760 (1.7973) lr 1.0926e-04 eta 0:01:17
epoch [29/30] batch [200/200] time 0.295 (0.347) data 0.000 (0.005) loss 1.6982 (1.8052) lr 2.7391e-05 eta 0:01:09
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.55s/it] 12%|█▏        | 2/17 [00:05<00:38,  2.54s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.47s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.03it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.44it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.91it/s] 41%|████      | 7/17 [00:06<00:04,  2.40it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.86it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.29it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.65it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.17it/s] 71%|███████   | 12/17 [00:07<00:01,  4.62it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.99it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.29it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.43it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.62it/s]100%|██████████| 17/17 [00:08<00:00,  6.24it/s]100%|██████████| 17/17 [00:08<00:00,  1.92it/s]=> result
* total: 1,667
* correct: 634
* accuracy: 38.0%
* error: 62.0%
* macro_f1: 35.1%

epoch [30/30] batch [20/200] time 0.358 (0.404) data 0.000 (0.047) loss 2.9023 (2.1597) lr 2.7391e-05 eta 0:01:12
epoch [30/30] batch [40/200] time 0.340 (0.379) data 0.000 (0.023) loss 1.4199 (1.9493) lr 2.7391e-05 eta 0:01:00
epoch [30/30] batch [60/200] time 0.339 (0.368) data 0.000 (0.016) loss 2.4980 (1.8856) lr 2.7391e-05 eta 0:00:51
epoch [30/30] batch [80/200] time 0.355 (0.365) data 0.000 (0.012) loss 1.4414 (1.9419) lr 2.7391e-05 eta 0:00:43
epoch [30/30] batch [100/200] time 0.346 (0.362) data 0.000 (0.010) loss 1.7012 (1.9488) lr 2.7391e-05 eta 0:00:36
epoch [30/30] batch [120/200] time 0.346 (0.360) data 0.000 (0.008) loss 2.1777 (1.8974) lr 2.7391e-05 eta 0:00:28
epoch [30/30] batch [140/200] time 0.350 (0.359) data 0.000 (0.007) loss 1.6260 (1.8976) lr 2.7391e-05 eta 0:00:21
epoch [30/30] batch [160/200] time 0.361 (0.358) data 0.000 (0.006) loss 1.5625 (1.8728) lr 2.7391e-05 eta 0:00:14
epoch [30/30] batch [180/200] time 0.296 (0.354) data 0.000 (0.005) loss 2.6504 (1.8858) lr 2.7391e-05 eta 0:00:07
epoch [30/30] batch [200/200] time 0.299 (0.348) data 0.000 (0.005) loss 2.1699 (1.8615) lr 0.0000e+00 eta 0:00:00
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.50s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.49s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.48s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.00s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.40it/s] 35%|███▌      | 6/17 [00:06<00:06,  1.77it/s] 41%|████      | 7/17 [00:07<00:04,  2.14it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.47it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.01it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.57it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.10it/s] 71%|███████   | 12/17 [00:08<00:01,  4.55it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.94it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.24it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.48it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.66it/s]100%|██████████| 17/17 [00:08<00:00,  6.27it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]
=> result
* total: 1,667
* correct: 637
* accuracy: 38.2%
* error: 61.8%
* macro_f1: 35.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:31,  5.72s/it] 12%|█▏        | 2/17 [00:06<00:37,  2.52s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.46s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.04it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.45it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.92it/s] 41%|████      | 7/17 [00:07<00:04,  2.37it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.81it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.32it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.73it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.00it/s] 71%|███████   | 12/17 [00:07<00:01,  4.47it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.87it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.19it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.44it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.63it/s]100%|██████████| 17/17 [00:08<00:00,  6.26it/s]100%|██████████| 17/17 [00:08<00:00,  1.91it/s]
=> result
* total: 1,666
* correct: 651
* accuracy: 39.1%
* error: 60.9%
* macro_f1: 36.3%
Elapsed: 0:26:26
+ sh scripts/rpo_prime/base2new_test_sdl.sh fgvc_aircraft 1 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
800 1666 1667
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      1,666
# test     1,667
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:07<02:06,  7.92s/it] 12%|█▏        | 2/17 [00:08<00:50,  3.37s/it] 18%|█▊        | 3/17 [00:08<00:26,  1.90s/it] 24%|██▎       | 4/17 [00:08<00:15,  1.22s/it] 29%|██▉       | 5/17 [00:08<00:10,  1.20it/s] 35%|███▌      | 6/17 [00:08<00:06,  1.65it/s] 41%|████      | 7/17 [00:08<00:04,  2.16it/s] 47%|████▋     | 8/17 [00:09<00:03,  2.73it/s] 53%|█████▎    | 9/17 [00:09<00:02,  3.30it/s] 59%|█████▉    | 10/17 [00:09<00:01,  3.85it/s] 65%|██████▍   | 11/17 [00:09<00:01,  4.35it/s] 71%|███████   | 12/17 [00:09<00:01,  4.77it/s] 76%|███████▋  | 13/17 [00:09<00:00,  5.12it/s] 82%|████████▏ | 14/17 [00:10<00:00,  5.39it/s] 88%|████████▊ | 15/17 [00:10<00:00,  5.59it/s] 94%|█████████▍| 16/17 [00:10<00:00,  5.75it/s]100%|██████████| 17/17 [00:10<00:00,  6.34it/s]100%|██████████| 17/17 [00:10<00:00,  1.60it/s]
=> result
* total: 1,667
* correct: 567
* accuracy: 34.0%
* error: 66.0%
* macro_f1: 31.5%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh fgvc_aircraft 2 0 main_tmp1_0.1sdl 16
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
800 1667 1666
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      1,667
# test     1,666
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/30] batch [20/200] time 0.359 (0.499) data 0.000 (0.074) loss 2.3223 (2.6177) lr 1.0000e-02 eta 0:49:42
epoch [1/30] batch [40/200] time 0.342 (0.426) data 0.000 (0.037) loss 3.2285 (2.6189) lr 1.0000e-02 eta 0:42:20
epoch [1/30] batch [60/200] time 0.342 (0.400) data 0.000 (0.025) loss 2.3555 (2.5868) lr 1.0000e-02 eta 0:39:37
epoch [1/30] batch [80/200] time 0.358 (0.387) data 0.000 (0.019) loss 1.7910 (2.4932) lr 1.0000e-02 eta 0:38:11
epoch [1/30] batch [100/200] time 0.361 (0.380) data 0.000 (0.015) loss 1.3320 (2.4306) lr 1.0000e-02 eta 0:37:23
epoch [1/30] batch [120/200] time 0.344 (0.375) data 0.000 (0.013) loss 2.7773 (2.4316) lr 1.0000e-02 eta 0:36:44
epoch [1/30] batch [140/200] time 0.343 (0.371) data 0.000 (0.011) loss 2.5898 (2.4132) lr 1.0000e-02 eta 0:36:14
epoch [1/30] batch [160/200] time 0.349 (0.369) data 0.000 (0.009) loss 1.8828 (2.3869) lr 1.0000e-02 eta 0:35:53
epoch [1/30] batch [180/200] time 0.297 (0.363) data 0.000 (0.008) loss 2.2559 (2.3929) lr 1.0000e-02 eta 0:35:13
epoch [1/30] batch [200/200] time 0.296 (0.356) data 0.000 (0.008) loss 1.7119 (2.3975) lr 9.9726e-03 eta 0:34:26
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:32,  5.75s/it] 12%|█▏        | 2/17 [00:06<00:40,  2.69s/it] 18%|█▊        | 3/17 [00:06<00:22,  1.59s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.07s/it] 29%|██▉       | 5/17 [00:07<00:09,  1.27it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.63it/s] 41%|████      | 7/17 [00:07<00:04,  2.14it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.70it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.27it/s] 59%|█████▉    | 10/17 [00:08<00:01,  3.82it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.32it/s] 71%|███████   | 12/17 [00:08<00:01,  4.75it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.10it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.37it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.57it/s] 94%|█████████▍| 16/17 [00:09<00:00,  5.73it/s]100%|██████████| 17/17 [00:09<00:00,  6.32it/s]100%|██████████| 17/17 [00:09<00:00,  1.83it/s]=> result
* total: 1,667
* correct: 528
* accuracy: 31.7%
* error: 68.3%
* macro_f1: 27.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [2/30] batch [20/200] time 0.360 (0.405) data 0.000 (0.047) loss 2.2969 (2.2482) lr 9.9726e-03 eta 0:38:59
epoch [2/30] batch [40/200] time 0.337 (0.374) data 0.000 (0.023) loss 2.6660 (2.2066) lr 9.9726e-03 eta 0:35:55
epoch [2/30] batch [60/200] time 0.342 (0.366) data 0.000 (0.016) loss 2.2520 (2.2084) lr 9.9726e-03 eta 0:34:58
epoch [2/30] batch [80/200] time 0.356 (0.361) data 0.000 (0.012) loss 2.5508 (2.2002) lr 9.9726e-03 eta 0:34:26
epoch [2/30] batch [100/200] time 0.357 (0.359) data 0.000 (0.010) loss 3.1895 (2.2022) lr 9.9726e-03 eta 0:34:05
epoch [2/30] batch [120/200] time 0.343 (0.357) data 0.000 (0.008) loss 2.3398 (2.2168) lr 9.9726e-03 eta 0:33:46
epoch [2/30] batch [140/200] time 0.356 (0.357) data 0.000 (0.007) loss 2.4746 (2.2177) lr 9.9726e-03 eta 0:33:41
epoch [2/30] batch [160/200] time 0.346 (0.356) data 0.000 (0.006) loss 0.9932 (2.2075) lr 9.9726e-03 eta 0:33:27
epoch [2/30] batch [180/200] time 0.295 (0.351) data 0.000 (0.005) loss 3.0801 (2.2382) lr 9.9726e-03 eta 0:32:55
epoch [2/30] batch [200/200] time 0.292 (0.346) data 0.000 (0.005) loss 1.4238 (2.2249) lr 9.8907e-03 eta 0:32:17
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:26,  5.43s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.51s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.00s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.39it/s] 35%|███▌      | 6/17 [00:06<00:06,  1.77it/s] 41%|████      | 7/17 [00:07<00:04,  2.12it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.48it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.05it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.61it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.13it/s] 71%|███████   | 12/17 [00:08<00:01,  4.59it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.97it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.27it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.51it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.68it/s]100%|██████████| 17/17 [00:08<00:00,  6.29it/s]100%|██████████| 17/17 [00:09<00:00,  1.89it/s]=> result
* total: 1,667
* correct: 538
* accuracy: 32.3%
* error: 67.7%
* macro_f1: 28.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [3/30] batch [20/200] time 0.353 (0.397) data 0.000 (0.046) loss 1.7207 (2.2220) lr 9.8907e-03 eta 0:36:56
epoch [3/30] batch [40/200] time 0.336 (0.377) data 0.000 (0.023) loss 2.6094 (2.1706) lr 9.8907e-03 eta 0:34:53
epoch [3/30] batch [60/200] time 0.343 (0.367) data 0.000 (0.016) loss 2.7090 (2.1984) lr 9.8907e-03 eta 0:33:53
epoch [3/30] batch [80/200] time 0.339 (0.362) data 0.000 (0.012) loss 2.7188 (2.2468) lr 9.8907e-03 eta 0:33:18
epoch [3/30] batch [100/200] time 0.352 (0.359) data 0.000 (0.010) loss 1.5186 (2.1923) lr 9.8907e-03 eta 0:32:52
epoch [3/30] batch [120/200] time 0.343 (0.357) data 0.000 (0.008) loss 2.2598 (2.2092) lr 9.8907e-03 eta 0:32:36
epoch [3/30] batch [140/200] time 0.356 (0.356) data 0.000 (0.007) loss 1.0400 (2.1946) lr 9.8907e-03 eta 0:32:21
epoch [3/30] batch [160/200] time 0.357 (0.355) data 0.000 (0.006) loss 1.9756 (2.2216) lr 9.8907e-03 eta 0:32:11
epoch [3/30] batch [180/200] time 0.295 (0.351) data 0.000 (0.005) loss 2.3574 (2.2106) lr 9.8907e-03 eta 0:31:41
epoch [3/30] batch [200/200] time 0.292 (0.345) data 0.000 (0.005) loss 2.4258 (2.2045) lr 9.7553e-03 eta 0:31:03
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.54s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.55s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.48s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.04it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.40it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.85it/s] 41%|████      | 7/17 [00:07<00:04,  2.31it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.64it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.09it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.64it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.16it/s] 71%|███████   | 12/17 [00:08<00:01,  4.61it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.99it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.25it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.49it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.67it/s]100%|██████████| 17/17 [00:08<00:00,  6.28it/s]100%|██████████| 17/17 [00:08<00:00,  1.90it/s]=> result
* total: 1,667
* correct: 545
* accuracy: 32.7%
* error: 67.3%
* macro_f1: 28.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [4/30] batch [20/200] time 0.363 (0.399) data 0.000 (0.043) loss 2.2891 (2.2748) lr 9.7553e-03 eta 0:35:44
epoch [4/30] batch [40/200] time 0.365 (0.373) data 0.000 (0.022) loss 2.7559 (2.2906) lr 9.7553e-03 eta 0:33:18
epoch [4/30] batch [60/200] time 0.339 (0.365) data 0.000 (0.014) loss 2.0703 (2.1875) lr 9.7553e-03 eta 0:32:29
epoch [4/30] batch [80/200] time 0.346 (0.361) data 0.000 (0.011) loss 2.3730 (2.1447) lr 9.7553e-03 eta 0:32:02
epoch [4/30] batch [100/200] time 0.360 (0.358) data 0.000 (0.009) loss 1.3330 (2.2034) lr 9.7553e-03 eta 0:31:38
epoch [4/30] batch [120/200] time 0.339 (0.357) data 0.000 (0.007) loss 1.3936 (2.2141) lr 9.7553e-03 eta 0:31:26
epoch [4/30] batch [140/200] time 0.353 (0.356) data 0.000 (0.006) loss 2.3145 (2.2361) lr 9.7553e-03 eta 0:31:12
epoch [4/30] batch [160/200] time 0.356 (0.356) data 0.000 (0.006) loss 2.0605 (2.2154) lr 9.7553e-03 eta 0:31:02
epoch [4/30] batch [180/200] time 0.295 (0.351) data 0.000 (0.005) loss 1.8994 (2.2205) lr 9.7553e-03 eta 0:30:33
epoch [4/30] batch [200/200] time 0.297 (0.346) data 0.000 (0.005) loss 2.7227 (2.1945) lr 9.5677e-03 eta 0:29:57
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:30,  5.67s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.55s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.51s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.02s/it] 29%|██▉       | 5/17 [00:06<00:09,  1.33it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.69it/s] 41%|████      | 7/17 [00:07<00:04,  2.05it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.60it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.17it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.73it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.24it/s] 71%|███████   | 12/17 [00:08<00:01,  4.68it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.04it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.33it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.46it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.64it/s]100%|██████████| 17/17 [00:09<00:00,  6.25it/s]100%|██████████| 17/17 [00:09<00:00,  1.86it/s]=> result
* total: 1,667
* correct: 566
* accuracy: 34.0%
* error: 66.0%
* macro_f1: 30.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [5/30] batch [20/200] time 0.348 (0.406) data 0.000 (0.044) loss 4.0820 (2.1958) lr 9.5677e-03 eta 0:35:01
epoch [5/30] batch [40/200] time 0.352 (0.378) data 0.000 (0.022) loss 2.3379 (2.2669) lr 9.5677e-03 eta 0:32:28
epoch [5/30] batch [60/200] time 0.336 (0.368) data 0.000 (0.015) loss 2.4121 (2.2086) lr 9.5677e-03 eta 0:31:30
epoch [5/30] batch [80/200] time 0.332 (0.363) data 0.000 (0.011) loss 2.7910 (2.2290) lr 9.5677e-03 eta 0:30:59
epoch [5/30] batch [100/200] time 0.365 (0.361) data 0.000 (0.009) loss 3.3027 (2.2452) lr 9.5677e-03 eta 0:30:42
epoch [5/30] batch [120/200] time 0.362 (0.360) data 0.000 (0.008) loss 2.1055 (2.2472) lr 9.5677e-03 eta 0:30:26
epoch [5/30] batch [140/200] time 0.357 (0.358) data 0.000 (0.007) loss 1.9004 (2.2300) lr 9.5677e-03 eta 0:30:12
epoch [5/30] batch [160/200] time 0.356 (0.357) data 0.000 (0.006) loss 1.9863 (2.2175) lr 9.5677e-03 eta 0:30:00
epoch [5/30] batch [180/200] time 0.293 (0.353) data 0.000 (0.005) loss 2.4961 (2.2066) lr 9.5677e-03 eta 0:29:32
epoch [5/30] batch [200/200] time 0.297 (0.347) data 0.000 (0.005) loss 1.4951 (2.2039) lr 9.3301e-03 eta 0:28:55
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.48s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.52s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.72it/s] 41%|████      | 7/17 [00:07<00:04,  2.15it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.68it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.25it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.80it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.30it/s] 71%|███████   | 12/17 [00:08<00:01,  4.73it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.09it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.29it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.52it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.69it/s]100%|██████████| 17/17 [00:08<00:00,  6.29it/s]100%|██████████| 17/17 [00:08<00:00,  1.89it/s]=> result
* total: 1,667
* correct: 564
* accuracy: 33.8%
* error: 66.2%
* macro_f1: 30.3%

epoch [6/30] batch [20/200] time 0.353 (0.398) data 0.000 (0.043) loss 1.8525 (1.9531) lr 9.3301e-03 eta 0:33:01
epoch [6/30] batch [40/200] time 0.337 (0.373) data 0.000 (0.022) loss 2.5117 (2.0073) lr 9.3301e-03 eta 0:30:51
epoch [6/30] batch [60/200] time 0.352 (0.366) data 0.000 (0.014) loss 1.7793 (2.1166) lr 9.3301e-03 eta 0:30:06
epoch [6/30] batch [80/200] time 0.343 (0.362) data 0.000 (0.011) loss 1.9053 (2.0515) lr 9.3301e-03 eta 0:29:39
epoch [6/30] batch [100/200] time 0.360 (0.360) data 0.000 (0.009) loss 2.1191 (2.0734) lr 9.3301e-03 eta 0:29:22
epoch [6/30] batch [120/200] time 0.373 (0.359) data 0.000 (0.007) loss 2.2969 (2.1158) lr 9.3301e-03 eta 0:29:09
epoch [6/30] batch [140/200] time 0.354 (0.359) data 0.000 (0.006) loss 1.8154 (2.1317) lr 9.3301e-03 eta 0:29:02
epoch [6/30] batch [160/200] time 0.337 (0.358) data 0.000 (0.006) loss 2.5820 (2.1491) lr 9.3301e-03 eta 0:28:52
epoch [6/30] batch [180/200] time 0.292 (0.353) data 0.000 (0.005) loss 2.2695 (2.1424) lr 9.3301e-03 eta 0:28:21
epoch [6/30] batch [200/200] time 0.300 (0.347) data 0.000 (0.005) loss 2.0039 (2.1297) lr 9.0451e-03 eta 0:27:45
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:26,  5.43s/it] 12%|█▏        | 2/17 [00:05<00:38,  2.56s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.48s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.03it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.44it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.90it/s] 41%|████      | 7/17 [00:06<00:04,  2.42it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.81it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.24it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.59it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.11it/s] 71%|███████   | 12/17 [00:07<00:01,  4.57it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.95it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.27it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.50it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.68it/s]100%|██████████| 17/17 [00:08<00:00,  6.29it/s]100%|██████████| 17/17 [00:08<00:00,  1.93it/s]=> result
* total: 1,667
* correct: 561
* accuracy: 33.7%
* error: 66.3%
* macro_f1: 30.2%

epoch [7/30] batch [20/200] time 0.336 (0.398) data 0.000 (0.044) loss 2.2148 (2.0940) lr 9.0451e-03 eta 0:31:41
epoch [7/30] batch [40/200] time 0.355 (0.371) data 0.000 (0.022) loss 2.1406 (2.0806) lr 9.0451e-03 eta 0:29:26
epoch [7/30] batch [60/200] time 0.454 (0.366) data 0.000 (0.015) loss 4.1719 (2.1030) lr 9.0451e-03 eta 0:28:56
epoch [7/30] batch [80/200] time 0.345 (0.362) data 0.000 (0.011) loss 2.5254 (2.1047) lr 9.0451e-03 eta 0:28:29
epoch [7/30] batch [100/200] time 0.347 (0.360) data 0.000 (0.009) loss 3.0723 (2.1377) lr 9.0451e-03 eta 0:28:10
epoch [7/30] batch [120/200] time 0.362 (0.359) data 0.000 (0.008) loss 1.8184 (2.1474) lr 9.0451e-03 eta 0:27:57
epoch [7/30] batch [140/200] time 0.345 (0.357) data 0.000 (0.007) loss 2.5293 (2.1416) lr 9.0451e-03 eta 0:27:45
epoch [7/30] batch [160/200] time 0.350 (0.357) data 0.000 (0.006) loss 1.4756 (2.1284) lr 9.0451e-03 eta 0:27:34
epoch [7/30] batch [180/200] time 0.301 (0.352) data 0.000 (0.005) loss 3.6777 (2.1291) lr 9.0451e-03 eta 0:27:06
epoch [7/30] batch [200/200] time 0.292 (0.346) data 0.000 (0.005) loss 2.3672 (2.1005) lr 8.7157e-03 eta 0:26:32
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:29,  5.59s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.51s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.71it/s] 41%|████      | 7/17 [00:07<00:04,  2.08it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.48it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.05it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.61it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.14it/s] 71%|███████   | 12/17 [00:08<00:01,  4.59it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.97it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.28it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.51it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.68it/s]100%|██████████| 17/17 [00:08<00:00,  6.28it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 585
* accuracy: 35.1%
* error: 64.9%
* macro_f1: 32.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [8/30] batch [20/200] time 0.343 (0.396) data 0.000 (0.044) loss 2.0137 (2.1332) lr 8.7157e-03 eta 0:30:14
epoch [8/30] batch [40/200] time 0.355 (0.373) data 0.000 (0.022) loss 1.9854 (2.1991) lr 8.7157e-03 eta 0:28:19
epoch [8/30] batch [60/200] time 0.342 (0.365) data 0.000 (0.015) loss 2.5215 (2.1677) lr 8.7157e-03 eta 0:27:36
epoch [8/30] batch [80/200] time 0.353 (0.361) data 0.000 (0.011) loss 1.9688 (2.2192) lr 8.7157e-03 eta 0:27:11
epoch [8/30] batch [100/200] time 0.338 (0.359) data 0.000 (0.009) loss 1.9375 (2.1993) lr 8.7157e-03 eta 0:26:54
epoch [8/30] batch [120/200] time 0.353 (0.358) data 0.000 (0.008) loss 2.7070 (2.1761) lr 8.7157e-03 eta 0:26:41
epoch [8/30] batch [140/200] time 0.347 (0.357) data 0.000 (0.006) loss 1.8125 (2.1664) lr 8.7157e-03 eta 0:26:31
epoch [8/30] batch [160/200] time 0.361 (0.356) data 0.000 (0.006) loss 2.4199 (2.1436) lr 8.7157e-03 eta 0:26:20
epoch [8/30] batch [180/200] time 0.292 (0.352) data 0.000 (0.005) loss 2.7891 (2.1290) lr 8.7157e-03 eta 0:25:54
epoch [8/30] batch [200/200] time 0.296 (0.346) data 0.000 (0.005) loss 3.0215 (2.1327) lr 8.3457e-03 eta 0:25:24
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.53s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.53s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.50s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.71it/s] 41%|████      | 7/17 [00:07<00:04,  2.06it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.59it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.16it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.72it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.23it/s] 71%|███████   | 12/17 [00:08<00:01,  4.67it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.04it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.33it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.56it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.71it/s]100%|██████████| 17/17 [00:08<00:00,  6.32it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 584
* accuracy: 35.0%
* error: 65.0%
* macro_f1: 32.6%

epoch [9/30] batch [20/200] time 0.348 (0.399) data 0.000 (0.044) loss 1.8711 (2.1139) lr 8.3457e-03 eta 0:29:08
epoch [9/30] batch [40/200] time 0.342 (0.375) data 0.000 (0.022) loss 2.7500 (2.2369) lr 8.3457e-03 eta 0:27:17
epoch [9/30] batch [60/200] time 0.339 (0.366) data 0.000 (0.015) loss 1.3027 (2.1716) lr 8.3457e-03 eta 0:26:26
epoch [9/30] batch [80/200] time 0.357 (0.361) data 0.000 (0.011) loss 2.9746 (2.1456) lr 8.3457e-03 eta 0:26:01
epoch [9/30] batch [100/200] time 0.349 (0.359) data 0.000 (0.009) loss 2.2148 (2.1081) lr 8.3457e-03 eta 0:25:43
epoch [9/30] batch [120/200] time 0.357 (0.359) data 0.000 (0.008) loss 2.1055 (2.1023) lr 8.3457e-03 eta 0:25:34
epoch [9/30] batch [140/200] time 0.362 (0.358) data 0.000 (0.007) loss 2.0938 (2.1042) lr 8.3457e-03 eta 0:25:25
epoch [9/30] batch [160/200] time 0.340 (0.358) data 0.000 (0.006) loss 1.9297 (2.0956) lr 8.3457e-03 eta 0:25:15
epoch [9/30] batch [180/200] time 0.292 (0.353) data 0.000 (0.005) loss 0.9868 (2.0886) lr 8.3457e-03 eta 0:24:49
epoch [9/30] batch [200/200] time 0.292 (0.347) data 0.000 (0.005) loss 1.9150 (2.1174) lr 7.9389e-03 eta 0:24:16
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.55s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.53s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.50s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.42it/s] 35%|███▌      | 6/17 [00:06<00:06,  1.78it/s] 41%|████      | 7/17 [00:07<00:04,  2.14it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.49it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.06it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.62it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.14it/s] 71%|███████   | 12/17 [00:08<00:01,  4.59it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.97it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.19it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.44it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.63it/s]100%|██████████| 17/17 [00:08<00:00,  6.25it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 569
* accuracy: 34.1%
* error: 65.9%
* macro_f1: 30.4%

epoch [10/30] batch [20/200] time 0.339 (0.411) data 0.000 (0.047) loss 2.5371 (1.8344) lr 7.9389e-03 eta 0:28:37
epoch [10/30] batch [40/200] time 0.351 (0.380) data 0.000 (0.023) loss 1.8389 (1.8902) lr 7.9389e-03 eta 0:26:20
epoch [10/30] batch [60/200] time 0.341 (0.370) data 0.000 (0.016) loss 1.9180 (2.0194) lr 7.9389e-03 eta 0:25:29
epoch [10/30] batch [80/200] time 0.342 (0.364) data 0.000 (0.012) loss 2.3633 (2.0165) lr 7.9389e-03 eta 0:25:00
epoch [10/30] batch [100/200] time 0.349 (0.361) data 0.000 (0.010) loss 1.1016 (2.0268) lr 7.9389e-03 eta 0:24:39
epoch [10/30] batch [120/200] time 0.348 (0.359) data 0.000 (0.008) loss 3.4023 (2.0758) lr 7.9389e-03 eta 0:24:24
epoch [10/30] batch [140/200] time 0.344 (0.358) data 0.000 (0.007) loss 1.7285 (2.0590) lr 7.9389e-03 eta 0:24:13
epoch [10/30] batch [160/200] time 0.368 (0.357) data 0.000 (0.006) loss 2.0000 (2.0726) lr 7.9389e-03 eta 0:24:04
epoch [10/30] batch [180/200] time 0.292 (0.353) data 0.000 (0.005) loss 3.8164 (2.0928) lr 7.9389e-03 eta 0:23:38
epoch [10/30] batch [200/200] time 0.294 (0.347) data 0.000 (0.005) loss 2.3789 (2.0908) lr 7.5000e-03 eta 0:23:07
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.49s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.58s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.02it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.39it/s] 35%|███▌      | 6/17 [00:06<00:06,  1.81it/s] 41%|████      | 7/17 [00:07<00:04,  2.29it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.80it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.23it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.77it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.28it/s] 71%|███████   | 12/17 [00:07<00:01,  4.71it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.07it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.24it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.48it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.66it/s]100%|██████████| 17/17 [00:08<00:00,  6.26it/s]100%|██████████| 17/17 [00:08<00:00,  1.91it/s]=> result
* total: 1,667
* correct: 596
* accuracy: 35.8%
* error: 64.2%
* macro_f1: 31.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-10

epoch [11/30] batch [20/200] time 0.348 (0.398) data 0.000 (0.042) loss 1.4922 (2.0157) lr 7.5000e-03 eta 0:26:24
epoch [11/30] batch [40/200] time 0.343 (0.372) data 0.000 (0.021) loss 1.3350 (2.0567) lr 7.5000e-03 eta 0:24:32
epoch [11/30] batch [60/200] time 0.348 (0.367) data 0.000 (0.014) loss 1.9648 (2.0152) lr 7.5000e-03 eta 0:24:06
epoch [11/30] batch [80/200] time 0.344 (0.362) data 0.000 (0.011) loss 2.8574 (2.0911) lr 7.5000e-03 eta 0:23:39
epoch [11/30] batch [100/200] time 0.339 (0.359) data 0.000 (0.009) loss 2.6934 (2.0462) lr 7.5000e-03 eta 0:23:19
epoch [11/30] batch [120/200] time 0.343 (0.358) data 0.000 (0.007) loss 1.8711 (2.0335) lr 7.5000e-03 eta 0:23:07
epoch [11/30] batch [140/200] time 0.354 (0.356) data 0.000 (0.006) loss 2.9336 (2.0529) lr 7.5000e-03 eta 0:22:53
epoch [11/30] batch [160/200] time 0.363 (0.356) data 0.000 (0.005) loss 2.2383 (2.0623) lr 7.5000e-03 eta 0:22:45
epoch [11/30] batch [180/200] time 0.294 (0.352) data 0.000 (0.005) loss 1.7451 (2.0680) lr 7.5000e-03 eta 0:22:24
epoch [11/30] batch [200/200] time 0.291 (0.346) data 0.000 (0.004) loss 2.7422 (2.0610) lr 7.0337e-03 eta 0:21:55
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:30,  5.68s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.51s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.70it/s] 41%|████      | 7/17 [00:07<00:04,  2.06it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.61it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.18it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.74it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.25it/s] 71%|███████   | 12/17 [00:08<00:01,  4.69it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.06it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.34it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.57it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.73it/s]100%|██████████| 17/17 [00:08<00:00,  6.33it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 585
* accuracy: 35.1%
* error: 64.9%
* macro_f1: 31.0%

epoch [12/30] batch [20/200] time 0.332 (0.392) data 0.000 (0.043) loss 1.8076 (2.1354) lr 7.0337e-03 eta 0:24:41
epoch [12/30] batch [40/200] time 0.342 (0.369) data 0.000 (0.022) loss 0.6235 (2.0040) lr 7.0337e-03 eta 0:23:09
epoch [12/30] batch [60/200] time 0.346 (0.361) data 0.000 (0.015) loss 1.3213 (1.9518) lr 7.0337e-03 eta 0:22:31
epoch [12/30] batch [80/200] time 0.338 (0.358) data 0.000 (0.011) loss 3.0391 (2.0049) lr 7.0337e-03 eta 0:22:13
epoch [12/30] batch [100/200] time 0.356 (0.357) data 0.000 (0.009) loss 1.6523 (2.0238) lr 7.0337e-03 eta 0:22:00
epoch [12/30] batch [120/200] time 0.326 (0.355) data 0.000 (0.007) loss 1.8242 (1.9900) lr 7.0337e-03 eta 0:21:47
epoch [12/30] batch [140/200] time 0.337 (0.355) data 0.000 (0.006) loss 2.3965 (2.0095) lr 7.0337e-03 eta 0:21:38
epoch [12/30] batch [160/200] time 0.361 (0.354) data 0.000 (0.006) loss 2.4883 (2.0291) lr 7.0337e-03 eta 0:21:30
epoch [12/30] batch [180/200] time 0.293 (0.351) data 0.000 (0.005) loss 2.5156 (2.0175) lr 7.0337e-03 eta 0:21:09
epoch [12/30] batch [200/200] time 0.291 (0.345) data 0.000 (0.005) loss 2.5410 (2.0318) lr 6.5451e-03 eta 0:20:41
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:26,  5.39s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.53s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.50s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.02it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.38it/s] 35%|███▌      | 6/17 [00:06<00:06,  1.74it/s] 41%|████      | 7/17 [00:07<00:04,  2.11it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.47it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.03it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.60it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.12it/s] 71%|███████   | 12/17 [00:08<00:01,  4.58it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.96it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.27it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.51it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.69it/s]100%|██████████| 17/17 [00:08<00:00,  6.28it/s]100%|██████████| 17/17 [00:09<00:00,  1.89it/s]=> result
* total: 1,667
* correct: 595
* accuracy: 35.7%
* error: 64.3%
* macro_f1: 32.4%

epoch [13/30] batch [20/200] time 0.340 (0.398) data 0.000 (0.047) loss 2.0098 (2.0330) lr 6.5451e-03 eta 0:23:46
epoch [13/30] batch [40/200] time 0.352 (0.374) data 0.000 (0.024) loss 3.0352 (2.0552) lr 6.5451e-03 eta 0:22:10
epoch [13/30] batch [60/200] time 0.346 (0.364) data 0.000 (0.016) loss 1.9443 (2.0252) lr 6.5451e-03 eta 0:21:28
epoch [13/30] batch [80/200] time 0.370 (0.360) data 0.000 (0.012) loss 1.3379 (2.0281) lr 6.5451e-03 eta 0:21:07
epoch [13/30] batch [100/200] time 0.343 (0.360) data 0.000 (0.010) loss 3.0156 (2.0934) lr 6.5451e-03 eta 0:20:59
epoch [13/30] batch [120/200] time 0.355 (0.359) data 0.000 (0.008) loss 2.5352 (2.0682) lr 6.5451e-03 eta 0:20:47
epoch [13/30] batch [140/200] time 0.369 (0.357) data 0.000 (0.007) loss 1.8311 (2.0835) lr 6.5451e-03 eta 0:20:36
epoch [13/30] batch [160/200] time 0.361 (0.357) data 0.000 (0.006) loss 2.8301 (2.0691) lr 6.5451e-03 eta 0:20:28
epoch [13/30] batch [180/200] time 0.292 (0.352) data 0.000 (0.005) loss 1.2480 (2.0684) lr 6.5451e-03 eta 0:20:05
epoch [13/30] batch [200/200] time 0.294 (0.346) data 0.000 (0.005) loss 1.2881 (2.0601) lr 6.0396e-03 eta 0:19:38
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.46s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.50s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.48s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.00s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.35it/s] 35%|███▌      | 6/17 [00:06<00:06,  1.72it/s] 41%|████      | 7/17 [00:07<00:04,  2.09it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.49it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.06it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.62it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.14it/s] 71%|███████   | 12/17 [00:08<00:01,  4.56it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.94it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.24it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.47it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.64it/s]100%|██████████| 17/17 [00:08<00:00,  6.24it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 593
* accuracy: 35.6%
* error: 64.4%
* macro_f1: 32.3%

epoch [14/30] batch [20/200] time 0.362 (0.402) data 0.000 (0.048) loss 1.8506 (1.8891) lr 6.0396e-03 eta 0:22:38
epoch [14/30] batch [40/200] time 0.345 (0.377) data 0.000 (0.024) loss 2.0664 (1.9192) lr 6.0396e-03 eta 0:21:07
epoch [14/30] batch [60/200] time 0.352 (0.368) data 0.000 (0.016) loss 2.7266 (1.9865) lr 6.0396e-03 eta 0:20:28
epoch [14/30] batch [80/200] time 0.366 (0.363) data 0.000 (0.012) loss 2.9023 (1.9707) lr 6.0396e-03 eta 0:20:05
epoch [14/30] batch [100/200] time 0.344 (0.360) data 0.000 (0.010) loss 1.7900 (1.9748) lr 6.0396e-03 eta 0:19:46
epoch [14/30] batch [120/200] time 0.344 (0.357) data 0.000 (0.008) loss 2.1270 (2.0140) lr 6.0396e-03 eta 0:19:32
epoch [14/30] batch [140/200] time 0.344 (0.357) data 0.000 (0.007) loss 1.2334 (1.9861) lr 6.0396e-03 eta 0:19:24
epoch [14/30] batch [160/200] time 0.343 (0.357) data 0.000 (0.006) loss 2.8320 (2.0126) lr 6.0396e-03 eta 0:19:16
epoch [14/30] batch [180/200] time 0.290 (0.352) data 0.000 (0.006) loss 2.4805 (2.0225) lr 6.0396e-03 eta 0:18:54
epoch [14/30] batch [200/200] time 0.294 (0.346) data 0.000 (0.005) loss 2.1270 (2.0317) lr 5.5226e-03 eta 0:18:28
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.47s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.50s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.02it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.42it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.88it/s] 41%|████      | 7/17 [00:06<00:04,  2.38it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.86it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.30it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.67it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.18it/s] 71%|███████   | 12/17 [00:07<00:01,  4.63it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.96it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.27it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.50it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.68it/s]100%|██████████| 17/17 [00:08<00:00,  6.29it/s]100%|██████████| 17/17 [00:08<00:00,  1.93it/s]=> result
* total: 1,667
* correct: 604
* accuracy: 36.2%
* error: 63.8%
* macro_f1: 32.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [15/30] batch [20/200] time 0.368 (0.399) data 0.000 (0.046) loss 2.0117 (1.9700) lr 5.5226e-03 eta 0:21:09
epoch [15/30] batch [40/200] time 0.342 (0.375) data 0.000 (0.023) loss 1.7129 (1.9734) lr 5.5226e-03 eta 0:19:44
epoch [15/30] batch [60/200] time 0.360 (0.366) data 0.000 (0.015) loss 1.1240 (1.9112) lr 5.5226e-03 eta 0:19:08
epoch [15/30] batch [80/200] time 0.349 (0.362) data 0.000 (0.012) loss 1.5898 (1.9556) lr 5.5226e-03 eta 0:18:48
epoch [15/30] batch [100/200] time 0.336 (0.360) data 0.000 (0.009) loss 3.1172 (1.9937) lr 5.5226e-03 eta 0:18:34
epoch [15/30] batch [120/200] time 0.362 (0.358) data 0.000 (0.008) loss 2.3047 (2.0162) lr 5.5226e-03 eta 0:18:21
epoch [15/30] batch [140/200] time 0.352 (0.357) data 0.000 (0.007) loss 1.7012 (2.0204) lr 5.5226e-03 eta 0:18:11
epoch [15/30] batch [160/200] time 0.448 (0.357) data 0.000 (0.006) loss 3.0488 (2.0133) lr 5.5226e-03 eta 0:18:05
epoch [15/30] batch [180/200] time 0.296 (0.353) data 0.000 (0.005) loss 1.3330 (2.0298) lr 5.5226e-03 eta 0:17:44
epoch [15/30] batch [200/200] time 0.293 (0.347) data 0.000 (0.005) loss 1.9580 (2.0317) lr 5.0000e-03 eta 0:17:20
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:29,  5.60s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.52s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.70it/s] 41%|████      | 7/17 [00:07<00:04,  2.06it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.61it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.18it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.74it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.25it/s] 71%|███████   | 12/17 [00:08<00:01,  4.69it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.00it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.30it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.53it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.71it/s]100%|██████████| 17/17 [00:08<00:00,  6.31it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 603
* accuracy: 36.2%
* error: 63.8%
* macro_f1: 32.8%

epoch [16/30] batch [20/200] time 0.336 (0.401) data 0.000 (0.050) loss 1.8545 (2.0098) lr 5.0000e-03 eta 0:19:55
epoch [16/30] batch [40/200] time 0.353 (0.375) data 0.000 (0.025) loss 0.6685 (1.9127) lr 5.0000e-03 eta 0:18:30
epoch [16/30] batch [60/200] time 0.347 (0.365) data 0.000 (0.017) loss 1.7510 (1.9255) lr 5.0000e-03 eta 0:17:54
epoch [16/30] batch [80/200] time 0.344 (0.362) data 0.000 (0.013) loss 0.9897 (1.9740) lr 5.0000e-03 eta 0:17:37
epoch [16/30] batch [100/200] time 0.360 (0.361) data 0.000 (0.010) loss 2.4121 (1.9300) lr 5.0000e-03 eta 0:17:27
epoch [16/30] batch [120/200] time 0.353 (0.359) data 0.000 (0.009) loss 1.6055 (1.9393) lr 5.0000e-03 eta 0:17:14
epoch [16/30] batch [140/200] time 0.353 (0.358) data 0.000 (0.007) loss 2.6621 (1.9522) lr 5.0000e-03 eta 0:17:03
epoch [16/30] batch [160/200] time 0.351 (0.357) data 0.000 (0.006) loss 1.4102 (1.9463) lr 5.0000e-03 eta 0:16:54
epoch [16/30] batch [180/200] time 0.292 (0.353) data 0.000 (0.006) loss 2.2012 (1.9396) lr 5.0000e-03 eta 0:16:35
epoch [16/30] batch [200/200] time 0.290 (0.347) data 0.000 (0.005) loss 1.6924 (1.9448) lr 4.4774e-03 eta 0:16:12
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.48s/it] 12%|█▏        | 2/17 [00:05<00:38,  2.55s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.51s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.02s/it] 29%|██▉       | 5/17 [00:06<00:09,  1.33it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.70it/s] 41%|████      | 7/17 [00:07<00:04,  2.05it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.60it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.18it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.73it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.24it/s] 71%|███████   | 12/17 [00:08<00:01,  4.68it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.05it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.34it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.56it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.72it/s]100%|██████████| 17/17 [00:08<00:00,  6.32it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 598
* accuracy: 35.9%
* error: 64.1%
* macro_f1: 32.5%

epoch [17/30] batch [20/200] time 0.340 (0.402) data 0.000 (0.048) loss 1.6445 (2.0671) lr 4.4774e-03 eta 0:18:38
epoch [17/30] batch [40/200] time 0.358 (0.375) data 0.000 (0.024) loss 1.9092 (2.0294) lr 4.4774e-03 eta 0:17:14
epoch [17/30] batch [60/200] time 0.339 (0.368) data 0.000 (0.016) loss 2.1445 (1.9952) lr 4.4774e-03 eta 0:16:47
epoch [17/30] batch [80/200] time 0.354 (0.363) data 0.000 (0.012) loss 0.7661 (1.9606) lr 4.4774e-03 eta 0:16:28
epoch [17/30] batch [100/200] time 0.347 (0.361) data 0.000 (0.010) loss 2.2949 (1.9294) lr 4.4774e-03 eta 0:16:13
epoch [17/30] batch [120/200] time 0.347 (0.359) data 0.000 (0.008) loss 2.1621 (1.9217) lr 4.4774e-03 eta 0:16:02
epoch [17/30] batch [140/200] time 0.333 (0.358) data 0.000 (0.007) loss 1.5361 (1.9189) lr 4.4774e-03 eta 0:15:51
epoch [17/30] batch [160/200] time 0.358 (0.357) data 0.000 (0.006) loss 1.7314 (1.9165) lr 4.4774e-03 eta 0:15:43
epoch [17/30] batch [180/200] time 0.292 (0.353) data 0.000 (0.006) loss 1.9053 (1.9428) lr 4.4774e-03 eta 0:15:24
epoch [17/30] batch [200/200] time 0.292 (0.347) data 0.000 (0.005) loss 1.0488 (1.9504) lr 3.9604e-03 eta 0:15:01
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:30,  5.64s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.50s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.45s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.05it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.46it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.90it/s] 41%|████      | 7/17 [00:06<00:04,  2.35it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.85it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.35it/s] 59%|█████▉    | 10/17 [00:07<00:02,  3.48it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.01it/s] 71%|███████   | 12/17 [00:07<00:01,  4.48it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.88it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.20it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.44it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.63it/s]100%|██████████| 17/17 [00:08<00:00,  6.23it/s]100%|██████████| 17/17 [00:08<00:00,  1.92it/s]=> result
* total: 1,667
* correct: 622
* accuracy: 37.3%
* error: 62.7%
* macro_f1: 33.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [18/30] batch [20/200] time 0.348 (0.405) data 0.000 (0.048) loss 3.3652 (1.7740) lr 3.9604e-03 eta 0:17:24
epoch [18/30] batch [40/200] time 0.338 (0.378) data 0.000 (0.024) loss 1.9170 (1.8232) lr 3.9604e-03 eta 0:16:08
epoch [18/30] batch [60/200] time 0.341 (0.370) data 0.000 (0.016) loss 1.3438 (1.8590) lr 3.9604e-03 eta 0:15:40
epoch [18/30] batch [80/200] time 0.337 (0.366) data 0.000 (0.012) loss 3.2070 (1.9497) lr 3.9604e-03 eta 0:15:21
epoch [18/30] batch [100/200] time 0.331 (0.362) data 0.000 (0.010) loss 2.1895 (1.9544) lr 3.9604e-03 eta 0:15:05
epoch [18/30] batch [120/200] time 0.344 (0.359) data 0.000 (0.008) loss 0.9116 (1.9946) lr 3.9604e-03 eta 0:14:50
epoch [18/30] batch [140/200] time 0.347 (0.359) data 0.000 (0.007) loss 2.4062 (1.9668) lr 3.9604e-03 eta 0:14:41
epoch [18/30] batch [160/200] time 0.343 (0.358) data 0.000 (0.006) loss 1.1758 (1.9784) lr 3.9604e-03 eta 0:14:34
epoch [18/30] batch [180/200] time 0.292 (0.354) data 0.000 (0.006) loss 1.1738 (1.9583) lr 3.9604e-03 eta 0:14:16
epoch [18/30] batch [200/200] time 0.293 (0.348) data 0.000 (0.005) loss 1.5322 (1.9588) lr 3.4549e-03 eta 0:13:54
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:31,  5.72s/it] 12%|█▏        | 2/17 [00:06<00:37,  2.52s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.50s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.71it/s] 41%|████      | 7/17 [00:07<00:04,  2.07it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.62it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.19it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.75it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.26it/s] 71%|███████   | 12/17 [00:08<00:01,  4.69it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.06it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.35it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.56it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.72it/s]100%|██████████| 17/17 [00:08<00:00,  6.32it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 631
* accuracy: 37.9%
* error: 62.1%
* macro_f1: 35.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [19/30] batch [20/200] time 0.358 (0.399) data 0.000 (0.043) loss 1.9795 (1.9916) lr 3.4549e-03 eta 0:15:50
epoch [19/30] batch [40/200] time 0.346 (0.374) data 0.000 (0.022) loss 1.4248 (1.9184) lr 3.4549e-03 eta 0:14:42
epoch [19/30] batch [60/200] time 0.347 (0.367) data 0.000 (0.015) loss 2.4219 (1.9006) lr 3.4549e-03 eta 0:14:19
epoch [19/30] batch [80/200] time 0.349 (0.362) data 0.000 (0.011) loss 1.8252 (1.9136) lr 3.4549e-03 eta 0:14:00
epoch [19/30] batch [100/200] time 0.363 (0.359) data 0.000 (0.009) loss 1.7920 (1.8771) lr 3.4549e-03 eta 0:13:46
epoch [19/30] batch [120/200] time 0.358 (0.359) data 0.000 (0.007) loss 2.2969 (1.9077) lr 3.4549e-03 eta 0:13:37
epoch [19/30] batch [140/200] time 0.335 (0.358) data 0.000 (0.006) loss 1.2568 (1.8912) lr 3.4549e-03 eta 0:13:28
epoch [19/30] batch [160/200] time 0.344 (0.357) data 0.000 (0.006) loss 2.1777 (1.9117) lr 3.4549e-03 eta 0:13:19
epoch [19/30] batch [180/200] time 0.294 (0.352) data 0.000 (0.005) loss 2.4336 (1.9131) lr 3.4549e-03 eta 0:13:02
epoch [19/30] batch [200/200] time 0.291 (0.347) data 0.000 (0.005) loss 2.3242 (1.9150) lr 2.9663e-03 eta 0:12:43
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.53s/it] 12%|█▏        | 2/17 [00:05<00:38,  2.55s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.51s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.02s/it] 29%|██▉       | 5/17 [00:06<00:09,  1.33it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.70it/s] 41%|████      | 7/17 [00:07<00:04,  2.05it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.59it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.16it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.72it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.23it/s] 71%|███████   | 12/17 [00:08<00:01,  4.67it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.04it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.33it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.55it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.71it/s]100%|██████████| 17/17 [00:08<00:00,  6.31it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 643
* accuracy: 38.6%
* error: 61.4%
* macro_f1: 36.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [20/30] batch [20/200] time 0.350 (0.398) data 0.000 (0.045) loss 2.8984 (1.8824) lr 2.9663e-03 eta 0:14:28
epoch [20/30] batch [40/200] time 0.354 (0.374) data 0.000 (0.023) loss 1.5791 (1.8680) lr 2.9663e-03 eta 0:13:27
epoch [20/30] batch [60/200] time 0.348 (0.365) data 0.000 (0.015) loss 1.7617 (1.8640) lr 2.9663e-03 eta 0:13:00
epoch [20/30] batch [80/200] time 0.348 (0.362) data 0.000 (0.011) loss 2.2480 (1.8347) lr 2.9663e-03 eta 0:12:47
epoch [20/30] batch [100/200] time 0.351 (0.360) data 0.000 (0.009) loss 2.2754 (1.8407) lr 2.9663e-03 eta 0:12:34
epoch [20/30] batch [120/200] time 0.356 (0.359) data 0.000 (0.008) loss 1.5859 (1.8955) lr 2.9663e-03 eta 0:12:25
epoch [20/30] batch [140/200] time 0.334 (0.357) data 0.000 (0.007) loss 3.1016 (1.8949) lr 2.9663e-03 eta 0:12:15
epoch [20/30] batch [160/200] time 0.356 (0.356) data 0.000 (0.006) loss 2.1816 (1.8673) lr 2.9663e-03 eta 0:12:06
epoch [20/30] batch [180/200] time 0.292 (0.352) data 0.000 (0.005) loss 1.8887 (1.8531) lr 2.9663e-03 eta 0:11:51
epoch [20/30] batch [200/200] time 0.294 (0.346) data 0.000 (0.005) loss 2.1953 (1.8536) lr 2.5000e-03 eta 0:11:32
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.48s/it] 12%|█▏        | 2/17 [00:05<00:38,  2.56s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.51s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.03s/it] 29%|██▉       | 5/17 [00:06<00:09,  1.33it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.69it/s] 41%|████      | 7/17 [00:07<00:04,  2.04it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.59it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.16it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.72it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.23it/s] 71%|███████   | 12/17 [00:08<00:01,  4.67it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.04it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.33it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.55it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.71it/s]100%|██████████| 17/17 [00:08<00:00,  6.31it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 641
* accuracy: 38.5%
* error: 61.5%
* macro_f1: 35.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-20

epoch [21/30] batch [20/200] time 0.332 (0.410) data 0.000 (0.042) loss 1.2510 (1.7312) lr 2.5000e-03 eta 0:13:32
epoch [21/30] batch [40/200] time 0.350 (0.380) data 0.000 (0.021) loss 1.4326 (1.7766) lr 2.5000e-03 eta 0:12:25
epoch [21/30] batch [60/200] time 0.348 (0.369) data 0.000 (0.014) loss 1.9023 (1.8846) lr 2.5000e-03 eta 0:11:55
epoch [21/30] batch [80/200] time 0.349 (0.364) data 0.000 (0.011) loss 2.8281 (1.8743) lr 2.5000e-03 eta 0:11:39
epoch [21/30] batch [100/200] time 0.339 (0.362) data 0.000 (0.009) loss 2.9316 (1.8517) lr 2.5000e-03 eta 0:11:27
epoch [21/30] batch [120/200] time 0.344 (0.360) data 0.000 (0.007) loss 1.4756 (1.8723) lr 2.5000e-03 eta 0:11:17
epoch [21/30] batch [140/200] time 0.348 (0.358) data 0.000 (0.006) loss 1.8662 (1.8505) lr 2.5000e-03 eta 0:11:06
epoch [21/30] batch [160/200] time 0.341 (0.357) data 0.000 (0.005) loss 1.8584 (1.8464) lr 2.5000e-03 eta 0:10:57
epoch [21/30] batch [180/200] time 0.296 (0.353) data 0.000 (0.005) loss 1.5020 (1.8476) lr 2.5000e-03 eta 0:10:41
epoch [21/30] batch [200/200] time 0.298 (0.347) data 0.000 (0.004) loss 0.9014 (1.8423) lr 2.0611e-03 eta 0:10:24
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:29,  5.56s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.50s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.02it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.43it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.88it/s] 41%|████      | 7/17 [00:06<00:04,  2.39it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.69it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.19it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.74it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.25it/s] 71%|███████   | 12/17 [00:07<00:01,  4.68it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.99it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.29it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.49it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.67it/s]100%|██████████| 17/17 [00:08<00:00,  6.28it/s]100%|██████████| 17/17 [00:08<00:00,  1.92it/s]=> result
* total: 1,667
* correct: 658
* accuracy: 39.5%
* error: 60.5%
* macro_f1: 36.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [22/30] batch [20/200] time 0.347 (0.402) data 0.000 (0.049) loss 1.6104 (1.7329) lr 2.0611e-03 eta 0:11:56
epoch [22/30] batch [40/200] time 0.335 (0.374) data 0.000 (0.024) loss 1.4678 (1.8151) lr 2.0611e-03 eta 0:10:58
epoch [22/30] batch [60/200] time 0.340 (0.365) data 0.000 (0.016) loss 1.0947 (1.8376) lr 2.0611e-03 eta 0:10:35
epoch [22/30] batch [80/200] time 0.355 (0.362) data 0.000 (0.012) loss 2.2461 (1.8782) lr 2.0611e-03 eta 0:10:22
epoch [22/30] batch [100/200] time 0.366 (0.360) data 0.000 (0.010) loss 1.9258 (1.8761) lr 2.0611e-03 eta 0:10:11
epoch [22/30] batch [120/200] time 0.341 (0.357) data 0.000 (0.008) loss 2.1113 (1.8992) lr 2.0611e-03 eta 0:10:00
epoch [22/30] batch [140/200] time 0.349 (0.357) data 0.000 (0.007) loss 2.0879 (1.8718) lr 2.0611e-03 eta 0:09:52
epoch [22/30] batch [160/200] time 0.344 (0.356) data 0.000 (0.006) loss 1.6582 (1.8469) lr 2.0611e-03 eta 0:09:43
epoch [22/30] batch [180/200] time 0.295 (0.352) data 0.000 (0.006) loss 1.7822 (1.8371) lr 2.0611e-03 eta 0:09:29
epoch [22/30] batch [200/200] time 0.296 (0.346) data 0.000 (0.005) loss 2.9512 (1.8364) lr 1.6543e-03 eta 0:09:13
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:29,  5.62s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.50s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.48s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.02it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.38it/s] 35%|███▌      | 6/17 [00:06<00:06,  1.75it/s] 41%|████      | 7/17 [00:07<00:04,  2.11it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.45it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.01it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.57it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.10it/s] 71%|███████   | 12/17 [00:08<00:01,  4.56it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.95it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.26it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.50it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.68it/s]100%|██████████| 17/17 [00:08<00:00,  6.30it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 651
* accuracy: 39.1%
* error: 60.9%
* macro_f1: 35.9%

epoch [23/30] batch [20/200] time 0.382 (0.399) data 0.000 (0.046) loss 1.4014 (1.9322) lr 1.6543e-03 eta 0:10:30
epoch [23/30] batch [40/200] time 0.349 (0.375) data 0.000 (0.023) loss 2.7656 (1.8785) lr 1.6543e-03 eta 0:09:44
epoch [23/30] batch [60/200] time 0.355 (0.365) data 0.000 (0.015) loss 1.8867 (1.8112) lr 1.6543e-03 eta 0:09:22
epoch [23/30] batch [80/200] time 0.351 (0.363) data 0.000 (0.012) loss 1.5791 (1.8018) lr 1.6543e-03 eta 0:09:11
epoch [23/30] batch [100/200] time 0.341 (0.360) data 0.000 (0.009) loss 2.1484 (1.8231) lr 1.6543e-03 eta 0:08:59
epoch [23/30] batch [120/200] time 0.353 (0.358) data 0.000 (0.008) loss 1.8545 (1.8396) lr 1.6543e-03 eta 0:08:50
epoch [23/30] batch [140/200] time 0.345 (0.357) data 0.000 (0.007) loss 2.2441 (1.8369) lr 1.6543e-03 eta 0:08:41
epoch [23/30] batch [160/200] time 0.358 (0.357) data 0.000 (0.006) loss 1.6621 (1.8623) lr 1.6543e-03 eta 0:08:33
epoch [23/30] batch [180/200] time 0.295 (0.353) data 0.000 (0.005) loss 1.7734 (1.8441) lr 1.6543e-03 eta 0:08:21
epoch [23/30] batch [200/200] time 0.294 (0.347) data 0.000 (0.005) loss 1.7764 (1.8473) lr 1.2843e-03 eta 0:08:05
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.45s/it] 12%|█▏        | 2/17 [00:05<00:38,  2.55s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.51s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.02s/it] 29%|██▉       | 5/17 [00:06<00:09,  1.33it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.70it/s] 41%|████      | 7/17 [00:07<00:04,  2.06it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.61it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.18it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.70it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.21it/s] 71%|███████   | 12/17 [00:08<00:01,  4.66it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.03it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.32it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.54it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.71it/s]100%|██████████| 17/17 [00:08<00:00,  6.31it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 654
* accuracy: 39.2%
* error: 60.8%
* macro_f1: 36.3%

epoch [24/30] batch [20/200] time 0.342 (0.398) data 0.000 (0.043) loss 1.6035 (1.9620) lr 1.2843e-03 eta 0:09:09
epoch [24/30] batch [40/200] time 0.341 (0.373) data 0.000 (0.022) loss 1.7129 (1.8521) lr 1.2843e-03 eta 0:08:27
epoch [24/30] batch [60/200] time 0.346 (0.366) data 0.000 (0.015) loss 2.3867 (1.8996) lr 1.2843e-03 eta 0:08:10
epoch [24/30] batch [80/200] time 0.345 (0.362) data 0.000 (0.011) loss 1.8154 (1.8829) lr 1.2843e-03 eta 0:07:58
epoch [24/30] batch [100/200] time 0.345 (0.360) data 0.000 (0.009) loss 2.0977 (1.8606) lr 1.2843e-03 eta 0:07:48
epoch [24/30] batch [120/200] time 0.349 (0.359) data 0.000 (0.007) loss 1.6006 (1.8427) lr 1.2843e-03 eta 0:07:39
epoch [24/30] batch [140/200] time 0.335 (0.358) data 0.001 (0.006) loss 1.4521 (1.8152) lr 1.2843e-03 eta 0:07:30
epoch [24/30] batch [160/200] time 0.358 (0.357) data 0.000 (0.006) loss 1.3301 (1.8092) lr 1.2843e-03 eta 0:07:22
epoch [24/30] batch [180/200] time 0.296 (0.353) data 0.000 (0.005) loss 1.3643 (1.8151) lr 1.2843e-03 eta 0:07:10
epoch [24/30] batch [200/200] time 0.308 (0.348) data 0.000 (0.005) loss 2.2734 (1.8121) lr 9.5492e-04 eta 0:06:57
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.48s/it] 12%|█▏        | 2/17 [00:05<00:38,  2.56s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.47s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.03it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.43it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.90it/s] 41%|████      | 7/17 [00:06<00:04,  2.41it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.87it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.25it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.68it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.20it/s] 71%|███████   | 12/17 [00:07<00:01,  4.64it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.01it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.30it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.53it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.70it/s]100%|██████████| 17/17 [00:08<00:00,  6.30it/s]100%|██████████| 17/17 [00:08<00:00,  1.93it/s]=> result
* total: 1,667
* correct: 657
* accuracy: 39.4%
* error: 60.6%
* macro_f1: 36.9%

epoch [25/30] batch [20/200] time 0.352 (0.399) data 0.000 (0.043) loss 1.9141 (1.7786) lr 9.5492e-04 eta 0:07:50
epoch [25/30] batch [40/200] time 0.344 (0.373) data 0.000 (0.021) loss 1.9385 (1.8152) lr 9.5492e-04 eta 0:07:12
epoch [25/30] batch [60/200] time 0.349 (0.366) data 0.000 (0.014) loss 0.7524 (1.8280) lr 9.5492e-04 eta 0:06:56
epoch [25/30] batch [80/200] time 0.340 (0.361) data 0.000 (0.011) loss 3.5898 (1.8346) lr 9.5492e-04 eta 0:06:44
epoch [25/30] batch [100/200] time 0.385 (0.359) data 0.000 (0.009) loss 1.8564 (1.8608) lr 9.5492e-04 eta 0:06:35
epoch [25/30] batch [120/200] time 0.360 (0.358) data 0.000 (0.007) loss 2.2031 (1.8420) lr 9.5492e-04 eta 0:06:26
epoch [25/30] batch [140/200] time 0.353 (0.358) data 0.000 (0.006) loss 2.0723 (1.8490) lr 9.5492e-04 eta 0:06:19
epoch [25/30] batch [160/200] time 0.339 (0.357) data 0.000 (0.006) loss 1.9297 (1.8241) lr 9.5492e-04 eta 0:06:10
epoch [25/30] batch [180/200] time 0.294 (0.352) data 0.000 (0.005) loss 1.6064 (1.8198) lr 9.5492e-04 eta 0:05:59
epoch [25/30] batch [200/200] time 0.293 (0.346) data 0.000 (0.004) loss 2.5977 (1.8123) lr 6.6987e-04 eta 0:05:46
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:29,  5.59s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.51s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.71it/s] 41%|████      | 7/17 [00:07<00:04,  2.07it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.57it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.14it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.70it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.22it/s] 71%|███████   | 12/17 [00:08<00:01,  4.66it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.01it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.30it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.53it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.70it/s]100%|██████████| 17/17 [00:08<00:00,  6.26it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 663
* accuracy: 39.8%
* error: 60.2%
* macro_f1: 37.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [26/30] batch [20/200] time 0.356 (0.404) data 0.000 (0.048) loss 1.6885 (1.5653) lr 6.6987e-04 eta 0:06:35
epoch [26/30] batch [40/200] time 0.330 (0.378) data 0.000 (0.024) loss 1.9316 (1.6611) lr 6.6987e-04 eta 0:06:02
epoch [26/30] batch [60/200] time 0.361 (0.368) data 0.000 (0.016) loss 2.1484 (1.6815) lr 6.6987e-04 eta 0:05:45
epoch [26/30] batch [80/200] time 0.334 (0.363) data 0.000 (0.012) loss 0.5479 (1.7175) lr 6.6987e-04 eta 0:05:33
epoch [26/30] batch [100/200] time 0.352 (0.360) data 0.000 (0.010) loss 1.8936 (1.7753) lr 6.6987e-04 eta 0:05:24
epoch [26/30] batch [120/200] time 0.335 (0.358) data 0.000 (0.008) loss 2.4043 (1.8396) lr 6.6987e-04 eta 0:05:15
epoch [26/30] batch [140/200] time 0.358 (0.357) data 0.000 (0.007) loss 1.9414 (1.8532) lr 6.6987e-04 eta 0:05:07
epoch [26/30] batch [160/200] time 0.352 (0.357) data 0.000 (0.006) loss 1.7246 (1.8587) lr 6.6987e-04 eta 0:04:59
epoch [26/30] batch [180/200] time 0.292 (0.352) data 0.000 (0.006) loss 2.4551 (1.8702) lr 6.6987e-04 eta 0:04:48
epoch [26/30] batch [200/200] time 0.294 (0.347) data 0.000 (0.005) loss 0.9297 (1.8458) lr 4.3227e-04 eta 0:04:37
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.50s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.53s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.50s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.33it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.71it/s] 41%|████      | 7/17 [00:07<00:04,  2.11it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.66it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.23it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.79it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.27it/s] 71%|███████   | 12/17 [00:08<00:01,  4.70it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.06it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.35it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.57it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.61it/s]100%|██████████| 17/17 [00:08<00:00,  6.22it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 659
* accuracy: 39.5%
* error: 60.5%
* macro_f1: 37.1%

epoch [27/30] batch [20/200] time 0.353 (0.397) data 0.000 (0.047) loss 1.2754 (1.8005) lr 4.3227e-04 eta 0:05:09
epoch [27/30] batch [40/200] time 0.350 (0.372) data 0.000 (0.023) loss 1.7246 (1.8869) lr 4.3227e-04 eta 0:04:42
epoch [27/30] batch [60/200] time 0.352 (0.367) data 0.000 (0.016) loss 2.1309 (1.9235) lr 4.3227e-04 eta 0:04:31
epoch [27/30] batch [80/200] time 0.341 (0.362) data 0.000 (0.012) loss 1.0645 (1.8786) lr 4.3227e-04 eta 0:04:20
epoch [27/30] batch [100/200] time 0.360 (0.361) data 0.000 (0.010) loss 1.7939 (1.8539) lr 4.3227e-04 eta 0:04:12
epoch [27/30] batch [120/200] time 0.355 (0.358) data 0.000 (0.008) loss 1.0312 (1.8493) lr 4.3227e-04 eta 0:04:03
epoch [27/30] batch [140/200] time 0.352 (0.357) data 0.000 (0.007) loss 2.6660 (1.8492) lr 4.3227e-04 eta 0:03:55
epoch [27/30] batch [160/200] time 0.350 (0.356) data 0.000 (0.006) loss 1.9824 (1.8297) lr 4.3227e-04 eta 0:03:47
epoch [27/30] batch [180/200] time 0.288 (0.351) data 0.000 (0.005) loss 1.2861 (1.8328) lr 4.3227e-04 eta 0:03:37
epoch [27/30] batch [200/200] time 0.291 (0.345) data 0.000 (0.005) loss 1.5049 (1.8441) lr 2.4472e-04 eta 0:03:27
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:29,  5.59s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.49s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.48s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.00s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.35it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.72it/s] 41%|████      | 7/17 [00:07<00:04,  2.08it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.63it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.21it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.76it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.27it/s] 71%|███████   | 12/17 [00:08<00:01,  4.70it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.98it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.28it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.50it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.68it/s]100%|██████████| 17/17 [00:08<00:00,  6.28it/s]100%|██████████| 17/17 [00:09<00:00,  1.89it/s]=> result
* total: 1,667
* correct: 660
* accuracy: 39.6%
* error: 60.4%
* macro_f1: 36.8%

epoch [28/30] batch [20/200] time 0.339 (0.394) data 0.000 (0.048) loss 1.1787 (1.7315) lr 2.4472e-04 eta 0:03:48
epoch [28/30] batch [40/200] time 0.345 (0.373) data 0.000 (0.024) loss 2.3516 (1.7647) lr 2.4472e-04 eta 0:03:28
epoch [28/30] batch [60/200] time 0.363 (0.366) data 0.000 (0.016) loss 0.9546 (1.8184) lr 2.4472e-04 eta 0:03:17
epoch [28/30] batch [80/200] time 0.378 (0.362) data 0.000 (0.012) loss 0.7646 (1.7534) lr 2.4472e-04 eta 0:03:08
epoch [28/30] batch [100/200] time 0.347 (0.361) data 0.000 (0.010) loss 3.1719 (1.7742) lr 2.4472e-04 eta 0:03:00
epoch [28/30] batch [120/200] time 0.336 (0.359) data 0.000 (0.008) loss 2.2285 (1.8016) lr 2.4472e-04 eta 0:02:52
epoch [28/30] batch [140/200] time 0.335 (0.358) data 0.000 (0.007) loss 2.3223 (1.8213) lr 2.4472e-04 eta 0:02:44
epoch [28/30] batch [160/200] time 0.349 (0.357) data 0.000 (0.006) loss 3.4121 (1.8170) lr 2.4472e-04 eta 0:02:37
epoch [28/30] batch [180/200] time 0.295 (0.353) data 0.000 (0.006) loss 2.1172 (1.8125) lr 2.4472e-04 eta 0:02:28
epoch [28/30] batch [200/200] time 0.296 (0.347) data 0.000 (0.005) loss 1.2803 (1.8060) lr 1.0926e-04 eta 0:02:18
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.49s/it] 12%|█▏        | 2/17 [00:05<00:38,  2.55s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.48s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.03it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.43it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.90it/s] 41%|████      | 7/17 [00:06<00:04,  2.40it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.89it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.29it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.56it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.08it/s] 71%|███████   | 12/17 [00:07<00:01,  4.55it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.93it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.24it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.48it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.65it/s]100%|██████████| 17/17 [00:08<00:00,  6.25it/s]100%|██████████| 17/17 [00:08<00:00,  1.92it/s]=> result
* total: 1,667
* correct: 659
* accuracy: 39.5%
* error: 60.5%
* macro_f1: 36.7%

epoch [29/30] batch [20/200] time 0.358 (0.397) data 0.000 (0.046) loss 2.3242 (2.0243) lr 1.0926e-04 eta 0:02:31
epoch [29/30] batch [40/200] time 0.342 (0.373) data 0.000 (0.023) loss 1.6045 (1.9751) lr 1.0926e-04 eta 0:02:14
epoch [29/30] batch [60/200] time 0.347 (0.364) data 0.000 (0.016) loss 1.6523 (1.8879) lr 1.0926e-04 eta 0:02:03
epoch [29/30] batch [80/200] time 0.355 (0.361) data 0.000 (0.012) loss 2.6016 (1.8751) lr 1.0926e-04 eta 0:01:55
epoch [29/30] batch [100/200] time 0.366 (0.359) data 0.000 (0.010) loss 2.5000 (1.8424) lr 1.0926e-04 eta 0:01:47
epoch [29/30] batch [120/200] time 0.347 (0.359) data 0.000 (0.008) loss 2.4375 (1.8449) lr 1.0926e-04 eta 0:01:40
epoch [29/30] batch [140/200] time 0.339 (0.358) data 0.000 (0.007) loss 1.5625 (1.7990) lr 1.0926e-04 eta 0:01:33
epoch [29/30] batch [160/200] time 0.361 (0.357) data 0.000 (0.006) loss 1.5713 (1.7877) lr 1.0926e-04 eta 0:01:25
epoch [29/30] batch [180/200] time 0.296 (0.352) data 0.000 (0.005) loss 1.6592 (1.7936) lr 1.0926e-04 eta 0:01:17
epoch [29/30] batch [200/200] time 0.299 (0.347) data 0.000 (0.005) loss 1.9600 (1.8090) lr 2.7391e-05 eta 0:01:09
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.44s/it] 12%|█▏        | 2/17 [00:05<00:38,  2.55s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.51s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.02s/it] 29%|██▉       | 5/17 [00:06<00:09,  1.33it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.69it/s] 41%|████      | 7/17 [00:07<00:04,  2.04it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.59it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.15it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.70it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.22it/s] 71%|███████   | 12/17 [00:08<00:01,  4.66it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.98it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.28it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.51it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.68it/s]100%|██████████| 17/17 [00:08<00:00,  6.28it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 660
* accuracy: 39.6%
* error: 60.4%
* macro_f1: 36.7%

epoch [30/30] batch [20/200] time 0.349 (0.400) data 0.000 (0.043) loss 1.5264 (1.6463) lr 2.7391e-05 eta 0:01:12
epoch [30/30] batch [40/200] time 0.351 (0.374) data 0.000 (0.022) loss 1.3203 (1.7058) lr 2.7391e-05 eta 0:00:59
epoch [30/30] batch [60/200] time 0.344 (0.366) data 0.000 (0.015) loss 1.6641 (1.7167) lr 2.7391e-05 eta 0:00:51
epoch [30/30] batch [80/200] time 0.341 (0.362) data 0.000 (0.011) loss 2.0918 (1.7847) lr 2.7391e-05 eta 0:00:43
epoch [30/30] batch [100/200] time 0.352 (0.360) data 0.000 (0.009) loss 1.7705 (1.8476) lr 2.7391e-05 eta 0:00:36
epoch [30/30] batch [120/200] time 0.344 (0.359) data 0.000 (0.007) loss 2.4980 (1.8242) lr 2.7391e-05 eta 0:00:28
epoch [30/30] batch [140/200] time 0.418 (0.358) data 0.000 (0.006) loss 1.9336 (1.7929) lr 2.7391e-05 eta 0:00:21
epoch [30/30] batch [160/200] time 0.352 (0.358) data 0.000 (0.006) loss 1.4902 (1.7796) lr 2.7391e-05 eta 0:00:14
epoch [30/30] batch [180/200] time 0.287 (0.353) data 0.000 (0.005) loss 3.0586 (1.7819) lr 2.7391e-05 eta 0:00:07
epoch [30/30] batch [200/200] time 0.289 (0.346) data 0.000 (0.005) loss 0.6929 (1.7754) lr 0.0000e+00 eta 0:00:00
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.54s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.51s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.35it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.72it/s] 41%|████      | 7/17 [00:07<00:04,  2.07it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.61it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.18it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.69it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.21it/s] 71%|███████   | 12/17 [00:08<00:01,  4.65it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.02it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.31it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.54it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.71it/s]100%|██████████| 17/17 [00:08<00:00,  6.31it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]
=> result
* total: 1,667
* correct: 660
* accuracy: 39.6%
* error: 60.4%
* macro_f1: 36.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 25)
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:30,  5.67s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.50s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.46s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.04it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.45it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.90it/s] 41%|████      | 7/17 [00:06<00:04,  2.39it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.90it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.41it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.78it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.20it/s] 71%|███████   | 12/17 [00:07<00:01,  4.64it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.01it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.30it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.52it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.69it/s]100%|██████████| 17/17 [00:08<00:00,  6.30it/s]100%|██████████| 17/17 [00:08<00:00,  1.93it/s]
=> result
* total: 1,666
* correct: 662
* accuracy: 39.7%
* error: 60.3%
* macro_f1: 37.0%
Elapsed: 0:39:26
+ sh scripts/rpo_prime/base2new_test_sdl.sh fgvc_aircraft 2 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
800 1666 1667
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      1,666
# test     1,667
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 25)
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:07<02:07,  7.98s/it] 12%|█▏        | 2/17 [00:08<00:50,  3.38s/it] 18%|█▊        | 3/17 [00:08<00:26,  1.91s/it] 24%|██▎       | 4/17 [00:08<00:15,  1.22s/it] 29%|██▉       | 5/17 [00:08<00:10,  1.19it/s] 35%|███▌      | 6/17 [00:08<00:06,  1.64it/s] 41%|████      | 7/17 [00:08<00:04,  2.15it/s] 47%|████▋     | 8/17 [00:09<00:03,  2.72it/s] 53%|█████▎    | 9/17 [00:09<00:02,  3.29it/s] 59%|█████▉    | 10/17 [00:09<00:01,  3.84it/s] 65%|██████▍   | 11/17 [00:09<00:01,  4.33it/s] 71%|███████   | 12/17 [00:09<00:01,  4.75it/s] 76%|███████▋  | 13/17 [00:09<00:00,  5.10it/s] 82%|████████▏ | 14/17 [00:10<00:00,  5.37it/s] 88%|████████▊ | 15/17 [00:10<00:00,  5.58it/s] 94%|█████████▍| 16/17 [00:10<00:00,  5.73it/s]100%|██████████| 17/17 [00:10<00:00,  6.33it/s]100%|██████████| 17/17 [00:10<00:00,  1.59it/s]
=> result
* total: 1,667
* correct: 567
* accuracy: 34.0%
* error: 66.0%
* macro_f1: 30.5%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh fgvc_aircraft 3 0 main_tmp1_0.1sdl 16
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
800 1667 1666
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      1,667
# test     1,666
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/30] batch [20/200] time 0.347 (0.495) data 0.000 (0.066) loss 1.5908 (2.5364) lr 1.0000e-02 eta 0:49:20
epoch [1/30] batch [40/200] time 0.339 (0.424) data 0.000 (0.033) loss 2.7402 (2.5310) lr 1.0000e-02 eta 0:42:04
epoch [1/30] batch [60/200] time 0.351 (0.401) data 0.000 (0.022) loss 2.6738 (2.4233) lr 1.0000e-02 eta 0:39:40
epoch [1/30] batch [80/200] time 0.352 (0.388) data 0.000 (0.017) loss 2.0586 (2.3313) lr 1.0000e-02 eta 0:38:16
epoch [1/30] batch [100/200] time 0.340 (0.381) data 0.000 (0.013) loss 1.5576 (2.3069) lr 1.0000e-02 eta 0:37:27
epoch [1/30] batch [120/200] time 0.363 (0.376) data 0.000 (0.011) loss 1.1758 (2.2885) lr 1.0000e-02 eta 0:36:52
epoch [1/30] batch [140/200] time 0.338 (0.373) data 0.000 (0.010) loss 2.2891 (2.2977) lr 1.0000e-02 eta 0:36:27
epoch [1/30] batch [160/200] time 0.326 (0.370) data 0.000 (0.008) loss 1.8262 (2.2880) lr 1.0000e-02 eta 0:36:03
epoch [1/30] batch [180/200] time 0.289 (0.364) data 0.000 (0.008) loss 2.8848 (2.3078) lr 1.0000e-02 eta 0:35:18
epoch [1/30] batch [200/200] time 0.308 (0.357) data 0.000 (0.007) loss 1.3340 (2.3081) lr 9.9726e-03 eta 0:34:29
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:34,  5.90s/it] 12%|█▏        | 2/17 [00:06<00:39,  2.66s/it] 18%|█▊        | 3/17 [00:06<00:22,  1.57s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.06s/it] 29%|██▉       | 5/17 [00:07<00:09,  1.29it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.65it/s] 41%|████      | 7/17 [00:07<00:04,  2.16it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.72it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.29it/s] 59%|█████▉    | 10/17 [00:08<00:01,  3.84it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.33it/s] 71%|███████   | 12/17 [00:08<00:01,  4.75it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.10it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.38it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.53it/s] 94%|█████████▍| 16/17 [00:09<00:00,  5.70it/s]100%|██████████| 17/17 [00:09<00:00,  6.29it/s]100%|██████████| 17/17 [00:09<00:00,  1.83it/s]=> result
* total: 1,667
* correct: 524
* accuracy: 31.4%
* error: 68.6%
* macro_f1: 27.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [2/30] batch [20/200] time 0.355 (0.411) data 0.000 (0.049) loss 2.3945 (2.0413) lr 9.9726e-03 eta 0:39:33
epoch [2/30] batch [40/200] time 0.350 (0.380) data 0.000 (0.025) loss 2.4727 (2.1809) lr 9.9726e-03 eta 0:36:26
epoch [2/30] batch [60/200] time 0.349 (0.370) data 0.000 (0.017) loss 1.9629 (2.2932) lr 9.9726e-03 eta 0:35:22
epoch [2/30] batch [80/200] time 0.325 (0.365) data 0.000 (0.012) loss 2.2676 (2.2675) lr 9.9726e-03 eta 0:34:48
epoch [2/30] batch [100/200] time 0.355 (0.362) data 0.000 (0.010) loss 1.9688 (2.2743) lr 9.9726e-03 eta 0:34:24
epoch [2/30] batch [120/200] time 0.354 (0.361) data 0.000 (0.008) loss 2.3027 (2.2574) lr 9.9726e-03 eta 0:34:08
epoch [2/30] batch [140/200] time 0.349 (0.360) data 0.000 (0.007) loss 1.8711 (2.2335) lr 9.9726e-03 eta 0:33:56
epoch [2/30] batch [160/200] time 0.347 (0.359) data 0.001 (0.006) loss 2.5469 (2.2413) lr 9.9726e-03 eta 0:33:45
epoch [2/30] batch [180/200] time 0.298 (0.355) data 0.000 (0.006) loss 2.5742 (2.2418) lr 9.9726e-03 eta 0:33:14
epoch [2/30] batch [200/200] time 0.294 (0.349) data 0.000 (0.005) loss 2.9414 (2.2417) lr 9.8907e-03 eta 0:32:34
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.56s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.52s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.47s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.04it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.45it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.93it/s] 41%|████      | 7/17 [00:06<00:04,  2.36it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.81it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.27it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.65it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.16it/s] 71%|███████   | 12/17 [00:07<00:01,  4.61it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.98it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.28it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.51it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.65it/s]100%|██████████| 17/17 [00:08<00:00,  6.26it/s]100%|██████████| 17/17 [00:08<00:00,  1.93it/s]=> result
* total: 1,667
* correct: 535
* accuracy: 32.1%
* error: 67.9%
* macro_f1: 28.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [3/30] batch [20/200] time 0.365 (0.401) data 0.000 (0.044) loss 2.7617 (2.1011) lr 9.8907e-03 eta 0:37:17
epoch [3/30] batch [40/200] time 0.361 (0.378) data 0.000 (0.022) loss 1.3633 (2.1112) lr 9.8907e-03 eta 0:35:00
epoch [3/30] batch [60/200] time 0.367 (0.368) data 0.000 (0.015) loss 1.8604 (2.0943) lr 9.8907e-03 eta 0:33:58
epoch [3/30] batch [80/200] time 0.327 (0.363) data 0.000 (0.011) loss 2.6113 (2.0945) lr 9.8907e-03 eta 0:33:22
epoch [3/30] batch [100/200] time 0.341 (0.360) data 0.000 (0.009) loss 2.5078 (2.0963) lr 9.8907e-03 eta 0:32:59
epoch [3/30] batch [120/200] time 0.356 (0.357) data 0.000 (0.008) loss 2.3145 (2.1063) lr 9.8907e-03 eta 0:32:38
epoch [3/30] batch [140/200] time 0.348 (0.356) data 0.000 (0.007) loss 2.6934 (2.0942) lr 9.8907e-03 eta 0:32:24
epoch [3/30] batch [160/200] time 0.380 (0.356) data 0.000 (0.006) loss 1.7998 (2.1223) lr 9.8907e-03 eta 0:32:14
epoch [3/30] batch [180/200] time 0.287 (0.351) data 0.000 (0.005) loss 2.4688 (2.1219) lr 9.8907e-03 eta 0:31:41
epoch [3/30] batch [200/200] time 0.288 (0.345) data 0.000 (0.005) loss 1.7871 (2.1215) lr 9.7553e-03 eta 0:31:00
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.46s/it] 12%|█▏        | 2/17 [00:05<00:38,  2.56s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.52s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.03s/it] 29%|██▉       | 5/17 [00:06<00:09,  1.33it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.70it/s] 41%|████      | 7/17 [00:07<00:04,  2.05it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.58it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.15it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.70it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.21it/s] 71%|███████   | 12/17 [00:08<00:01,  4.65it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.01it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.30it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.53it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.69it/s]100%|██████████| 17/17 [00:08<00:00,  6.29it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 557
* accuracy: 33.4%
* error: 66.6%
* macro_f1: 30.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [4/30] batch [20/200] time 0.355 (0.399) data 0.000 (0.043) loss 1.7881 (2.1053) lr 9.7553e-03 eta 0:35:45
epoch [4/30] batch [40/200] time 0.349 (0.371) data 0.000 (0.022) loss 1.8037 (2.1551) lr 9.7553e-03 eta 0:33:09
epoch [4/30] batch [60/200] time 0.341 (0.363) data 0.000 (0.015) loss 1.4844 (2.1426) lr 9.7553e-03 eta 0:32:19
epoch [4/30] batch [80/200] time 0.342 (0.362) data 0.000 (0.011) loss 3.0684 (2.1712) lr 9.7553e-03 eta 0:32:04
epoch [4/30] batch [100/200] time 0.351 (0.360) data 0.000 (0.009) loss 2.4414 (2.1519) lr 9.7553e-03 eta 0:31:45
epoch [4/30] batch [120/200] time 0.345 (0.358) data 0.000 (0.007) loss 2.0996 (2.1755) lr 9.7553e-03 eta 0:31:32
epoch [4/30] batch [140/200] time 0.355 (0.357) data 0.000 (0.006) loss 1.4492 (2.2019) lr 9.7553e-03 eta 0:31:18
epoch [4/30] batch [160/200] time 0.356 (0.357) data 0.000 (0.006) loss 2.5703 (2.2294) lr 9.7553e-03 eta 0:31:09
epoch [4/30] batch [180/200] time 0.298 (0.353) data 0.000 (0.005) loss 1.6338 (2.2202) lr 9.7553e-03 eta 0:30:41
epoch [4/30] batch [200/200] time 0.298 (0.347) data 0.000 (0.005) loss 2.1250 (2.2005) lr 9.5677e-03 eta 0:30:05
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:26,  5.39s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.52s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.50s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.71it/s] 41%|████      | 7/17 [00:07<00:04,  2.06it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.61it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.18it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.73it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.24it/s] 71%|███████   | 12/17 [00:08<00:01,  4.67it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.04it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.32it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.54it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.70it/s]100%|██████████| 17/17 [00:08<00:00,  6.30it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 566
* accuracy: 34.0%
* error: 66.0%
* macro_f1: 30.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [5/30] batch [20/200] time 0.349 (0.403) data 0.000 (0.042) loss 2.7598 (2.3484) lr 9.5677e-03 eta 0:34:48
epoch [5/30] batch [40/200] time 0.344 (0.376) data 0.000 (0.021) loss 2.1895 (2.1884) lr 9.5677e-03 eta 0:32:20
epoch [5/30] batch [60/200] time 0.348 (0.368) data 0.000 (0.014) loss 2.5332 (2.1097) lr 9.5677e-03 eta 0:31:29
epoch [5/30] batch [80/200] time 0.344 (0.363) data 0.000 (0.011) loss 2.3125 (2.0864) lr 9.5677e-03 eta 0:30:59
epoch [5/30] batch [100/200] time 0.358 (0.361) data 0.000 (0.009) loss 1.2490 (2.0919) lr 9.5677e-03 eta 0:30:43
epoch [5/30] batch [120/200] time 0.355 (0.359) data 0.000 (0.007) loss 3.6445 (2.1324) lr 9.5677e-03 eta 0:30:23
epoch [5/30] batch [140/200] time 0.358 (0.358) data 0.000 (0.006) loss 1.7090 (2.1381) lr 9.5677e-03 eta 0:30:12
epoch [5/30] batch [160/200] time 0.340 (0.358) data 0.000 (0.006) loss 1.5078 (2.1548) lr 9.5677e-03 eta 0:30:03
epoch [5/30] batch [180/200] time 0.300 (0.354) data 0.000 (0.005) loss 3.4512 (2.1746) lr 9.5677e-03 eta 0:29:34
epoch [5/30] batch [200/200] time 0.299 (0.348) data 0.000 (0.004) loss 1.3057 (2.1433) lr 9.3301e-03 eta 0:28:58
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:31,  5.69s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.51s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.71it/s] 41%|████      | 7/17 [00:07<00:04,  2.07it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.63it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.20it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.75it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.26it/s] 71%|███████   | 12/17 [00:08<00:01,  4.69it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.05it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.34it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.55it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.71it/s]100%|██████████| 17/17 [00:08<00:00,  6.31it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 561
* accuracy: 33.7%
* error: 66.3%
* macro_f1: 30.1%

epoch [6/30] batch [20/200] time 0.342 (0.401) data 0.000 (0.048) loss 1.7617 (1.9922) lr 9.3301e-03 eta 0:33:16
epoch [6/30] batch [40/200] time 0.360 (0.374) data 0.000 (0.024) loss 2.4609 (2.0259) lr 9.3301e-03 eta 0:30:56
epoch [6/30] batch [60/200] time 0.350 (0.367) data 0.000 (0.016) loss 2.4590 (2.0885) lr 9.3301e-03 eta 0:30:12
epoch [6/30] batch [80/200] time 0.375 (0.363) data 0.000 (0.012) loss 2.8789 (2.1192) lr 9.3301e-03 eta 0:29:45
epoch [6/30] batch [100/200] time 0.345 (0.360) data 0.000 (0.010) loss 2.5664 (2.1462) lr 9.3301e-03 eta 0:29:23
epoch [6/30] batch [120/200] time 0.358 (0.358) data 0.000 (0.008) loss 1.8506 (2.1655) lr 9.3301e-03 eta 0:29:08
epoch [6/30] batch [140/200] time 0.346 (0.358) data 0.000 (0.007) loss 1.8682 (2.1473) lr 9.3301e-03 eta 0:29:00
epoch [6/30] batch [160/200] time 0.355 (0.357) data 0.000 (0.006) loss 1.8086 (2.1425) lr 9.3301e-03 eta 0:28:45
epoch [6/30] batch [180/200] time 0.295 (0.352) data 0.000 (0.006) loss 2.4766 (2.1378) lr 9.3301e-03 eta 0:28:18
epoch [6/30] batch [200/200] time 0.294 (0.347) data 0.000 (0.005) loss 1.5156 (2.1283) lr 9.0451e-03 eta 0:27:44
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.48s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.58s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.03it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.43it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.93it/s] 41%|████      | 7/17 [00:06<00:04,  2.42it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.95it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.32it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.71it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.23it/s] 71%|███████   | 12/17 [00:07<00:01,  4.66it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.03it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.32it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.53it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.70it/s]100%|██████████| 17/17 [00:08<00:00,  6.31it/s]100%|██████████| 17/17 [00:08<00:00,  1.93it/s]=> result
* total: 1,667
* correct: 581
* accuracy: 34.9%
* error: 65.1%
* macro_f1: 30.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [7/30] batch [20/200] time 0.363 (0.395) data 0.000 (0.047) loss 2.4707 (2.1123) lr 9.0451e-03 eta 0:31:26
epoch [7/30] batch [40/200] time 0.357 (0.373) data 0.000 (0.024) loss 0.4604 (2.1706) lr 9.0451e-03 eta 0:29:33
epoch [7/30] batch [60/200] time 0.343 (0.364) data 0.000 (0.016) loss 3.5332 (2.1971) lr 9.0451e-03 eta 0:28:43
epoch [7/30] batch [80/200] time 0.368 (0.361) data 0.000 (0.012) loss 2.9492 (2.1740) lr 9.0451e-03 eta 0:28:23
epoch [7/30] batch [100/200] time 0.378 (0.360) data 0.000 (0.010) loss 2.5176 (2.1965) lr 9.0451e-03 eta 0:28:10
epoch [7/30] batch [120/200] time 0.351 (0.358) data 0.000 (0.008) loss 2.3320 (2.1662) lr 9.0451e-03 eta 0:27:54
epoch [7/30] batch [140/200] time 0.351 (0.356) data 0.000 (0.007) loss 1.6113 (2.1901) lr 9.0451e-03 eta 0:27:40
epoch [7/30] batch [160/200] time 0.346 (0.356) data 0.000 (0.006) loss 2.9844 (2.1734) lr 9.0451e-03 eta 0:27:32
epoch [7/30] batch [180/200] time 0.299 (0.353) data 0.000 (0.005) loss 1.7842 (2.1758) lr 9.0451e-03 eta 0:27:08
epoch [7/30] batch [200/200] time 0.297 (0.347) data 0.000 (0.005) loss 1.6777 (2.1392) lr 8.7157e-03 eta 0:26:35
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.49s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.47s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.47s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.01it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.36it/s] 35%|███▌      | 6/17 [00:06<00:06,  1.72it/s] 41%|████      | 7/17 [00:07<00:04,  2.08it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.45it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.02it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.58it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.11it/s] 71%|███████   | 12/17 [00:08<00:01,  4.56it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.94it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.25it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.48it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.66it/s]100%|██████████| 17/17 [00:08<00:00,  6.27it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 571
* accuracy: 34.3%
* error: 65.7%
* macro_f1: 30.7%

epoch [8/30] batch [20/200] time 0.346 (0.402) data 0.000 (0.047) loss 2.1602 (2.3858) lr 8.7157e-03 eta 0:30:41
epoch [8/30] batch [40/200] time 0.354 (0.376) data 0.000 (0.023) loss 2.4980 (2.2504) lr 8.7157e-03 eta 0:28:34
epoch [8/30] batch [60/200] time 0.338 (0.366) data 0.000 (0.016) loss 2.3281 (2.2281) lr 8.7157e-03 eta 0:27:40
epoch [8/30] batch [80/200] time 0.351 (0.360) data 0.000 (0.012) loss 2.2461 (2.2235) lr 8.7157e-03 eta 0:27:09
epoch [8/30] batch [100/200] time 0.354 (0.359) data 0.000 (0.010) loss 1.6152 (2.1605) lr 8.7157e-03 eta 0:26:55
epoch [8/30] batch [120/200] time 0.374 (0.357) data 0.000 (0.008) loss 2.5020 (2.1775) lr 8.7157e-03 eta 0:26:41
epoch [8/30] batch [140/200] time 0.347 (0.356) data 0.000 (0.007) loss 2.5195 (2.1785) lr 8.7157e-03 eta 0:26:29
epoch [8/30] batch [160/200] time 0.359 (0.356) data 0.000 (0.006) loss 1.8174 (2.1627) lr 8.7157e-03 eta 0:26:21
epoch [8/30] batch [180/200] time 0.294 (0.352) data 0.000 (0.005) loss 1.8818 (2.1424) lr 8.7157e-03 eta 0:25:56
epoch [8/30] batch [200/200] time 0.299 (0.347) data 0.000 (0.005) loss 1.2227 (2.1365) lr 8.3457e-03 eta 0:25:26
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.56s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.52s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.50s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.70it/s] 41%|████      | 7/17 [00:07<00:04,  2.06it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.60it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.16it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.72it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.23it/s] 71%|███████   | 12/17 [00:08<00:01,  4.67it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.94it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.24it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.47it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.65it/s]100%|██████████| 17/17 [00:08<00:00,  6.25it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 584
* accuracy: 35.0%
* error: 65.0%
* macro_f1: 31.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [9/30] batch [20/200] time 0.357 (0.403) data 0.000 (0.047) loss 1.9980 (1.9646) lr 8.3457e-03 eta 0:29:23
epoch [9/30] batch [40/200] time 0.362 (0.378) data 0.000 (0.023) loss 2.0234 (1.8505) lr 8.3457e-03 eta 0:27:28
epoch [9/30] batch [60/200] time 0.336 (0.369) data 0.000 (0.016) loss 2.7090 (1.8903) lr 8.3457e-03 eta 0:26:41
epoch [9/30] batch [80/200] time 0.362 (0.365) data 0.000 (0.012) loss 1.6279 (1.9652) lr 8.3457e-03 eta 0:26:16
epoch [9/30] batch [100/200] time 0.345 (0.362) data 0.000 (0.010) loss 1.7129 (1.9869) lr 8.3457e-03 eta 0:25:55
epoch [9/30] batch [120/200] time 0.352 (0.361) data 0.000 (0.008) loss 2.4004 (2.0211) lr 8.3457e-03 eta 0:25:43
epoch [9/30] batch [140/200] time 0.368 (0.360) data 0.000 (0.007) loss 1.5762 (2.0138) lr 8.3457e-03 eta 0:25:32
epoch [9/30] batch [160/200] time 0.338 (0.358) data 0.000 (0.006) loss 2.2168 (2.0273) lr 8.3457e-03 eta 0:25:19
epoch [9/30] batch [180/200] time 0.295 (0.354) data 0.000 (0.005) loss 1.1504 (2.0457) lr 8.3457e-03 eta 0:24:53
epoch [9/30] batch [200/200] time 0.296 (0.348) data 0.000 (0.005) loss 2.0117 (2.0446) lr 7.9389e-03 eta 0:24:22
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:06<01:42,  6.38s/it] 12%|█▏        | 2/17 [00:07<00:44,  3.00s/it] 18%|█▊        | 3/17 [00:07<00:24,  1.72s/it] 24%|██▎       | 4/17 [00:07<00:14,  1.13s/it] 29%|██▉       | 5/17 [00:07<00:09,  1.27it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.71it/s] 41%|████      | 7/17 [00:08<00:04,  2.18it/s] 47%|████▋     | 8/17 [00:08<00:03,  2.65it/s] 53%|█████▎    | 9/17 [00:08<00:02,  3.14it/s] 59%|█████▉    | 10/17 [00:08<00:01,  3.52it/s] 65%|██████▍   | 11/17 [00:08<00:01,  3.80it/s] 71%|███████   | 12/17 [00:08<00:01,  4.30it/s] 76%|███████▋  | 13/17 [00:09<00:00,  4.72it/s] 82%|████████▏ | 14/17 [00:09<00:00,  5.01it/s] 88%|████████▊ | 15/17 [00:09<00:00,  5.30it/s] 94%|█████████▍| 16/17 [00:09<00:00,  5.52it/s]100%|██████████| 17/17 [00:09<00:00,  6.14it/s]100%|██████████| 17/17 [00:09<00:00,  1.72it/s]=> result
* total: 1,667
* correct: 574
* accuracy: 34.4%
* error: 65.6%
* macro_f1: 31.2%

epoch [10/30] batch [20/200] time 0.342 (0.404) data 0.000 (0.044) loss 4.0469 (2.0903) lr 7.9389e-03 eta 0:28:08
epoch [10/30] batch [40/200] time 0.352 (0.378) data 0.000 (0.022) loss 2.9609 (2.1054) lr 7.9389e-03 eta 0:26:11
epoch [10/30] batch [60/200] time 0.356 (0.369) data 0.000 (0.015) loss 2.0938 (2.0468) lr 7.9389e-03 eta 0:25:29
epoch [10/30] batch [80/200] time 0.361 (0.365) data 0.000 (0.011) loss 1.8779 (2.1041) lr 7.9389e-03 eta 0:25:03
epoch [10/30] batch [100/200] time 0.336 (0.361) data 0.000 (0.009) loss 2.8613 (2.1229) lr 7.9389e-03 eta 0:24:39
epoch [10/30] batch [120/200] time 0.352 (0.359) data 0.000 (0.007) loss 2.2344 (2.1303) lr 7.9389e-03 eta 0:24:23
epoch [10/30] batch [140/200] time 0.340 (0.358) data 0.000 (0.006) loss 2.8750 (2.1124) lr 7.9389e-03 eta 0:24:14
epoch [10/30] batch [160/200] time 0.352 (0.357) data 0.001 (0.006) loss 1.8330 (2.1128) lr 7.9389e-03 eta 0:24:03
epoch [10/30] batch [180/200] time 0.297 (0.353) data 0.000 (0.005) loss 1.2998 (2.1039) lr 7.9389e-03 eta 0:23:38
epoch [10/30] batch [200/200] time 0.299 (0.348) data 0.000 (0.005) loss 2.8828 (2.0928) lr 7.5000e-03 eta 0:23:10
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:32,  5.80s/it] 12%|█▏        | 2/17 [00:06<00:40,  2.70s/it] 18%|█▊        | 3/17 [00:06<00:22,  1.60s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.07s/it] 29%|██▉       | 5/17 [00:07<00:09,  1.28it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.64it/s] 41%|████      | 7/17 [00:07<00:04,  2.00it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.55it/s] 53%|█████▎    | 9/17 [00:08<00:02,  3.11it/s] 59%|█████▉    | 10/17 [00:08<00:01,  3.67it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.18it/s] 71%|███████   | 12/17 [00:08<00:01,  4.63it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.00it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.21it/s] 88%|████████▊ | 15/17 [00:09<00:00,  5.45it/s] 94%|█████████▍| 16/17 [00:09<00:00,  5.64it/s]100%|██████████| 17/17 [00:09<00:00,  6.24it/s]100%|██████████| 17/17 [00:09<00:00,  1.80it/s]=> result
* total: 1,667
* correct: 575
* accuracy: 34.5%
* error: 65.5%
* macro_f1: 31.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-10

epoch [11/30] batch [20/200] time 0.349 (0.401) data 0.000 (0.047) loss 1.3438 (1.8073) lr 7.5000e-03 eta 0:26:37
epoch [11/30] batch [40/200] time 0.346 (0.375) data 0.000 (0.024) loss 2.2891 (2.0330) lr 7.5000e-03 eta 0:24:43
epoch [11/30] batch [60/200] time 0.340 (0.367) data 0.000 (0.016) loss 0.5767 (2.0625) lr 7.5000e-03 eta 0:24:07
epoch [11/30] batch [80/200] time 0.345 (0.363) data 0.000 (0.012) loss 1.3115 (2.0472) lr 7.5000e-03 eta 0:23:41
epoch [11/30] batch [100/200] time 0.344 (0.360) data 0.000 (0.010) loss 1.9131 (2.0194) lr 7.5000e-03 eta 0:23:25
epoch [11/30] batch [120/200] time 0.346 (0.359) data 0.000 (0.008) loss 2.6523 (2.0566) lr 7.5000e-03 eta 0:23:11
epoch [11/30] batch [140/200] time 0.352 (0.357) data 0.000 (0.007) loss 3.1055 (2.0896) lr 7.5000e-03 eta 0:22:59
epoch [11/30] batch [160/200] time 0.348 (0.357) data 0.000 (0.006) loss 1.2090 (2.0939) lr 7.5000e-03 eta 0:22:49
epoch [11/30] batch [180/200] time 0.296 (0.353) data 0.000 (0.006) loss 1.8633 (2.0588) lr 7.5000e-03 eta 0:22:28
epoch [11/30] batch [200/200] time 0.296 (0.347) data 0.000 (0.005) loss 1.5215 (2.0803) lr 7.0337e-03 eta 0:22:00
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:29,  5.57s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.56s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.52s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.03s/it] 29%|██▉       | 5/17 [00:06<00:09,  1.32it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.68it/s] 41%|████      | 7/17 [00:07<00:04,  2.06it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.61it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.18it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.70it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.21it/s] 71%|███████   | 12/17 [00:08<00:01,  4.66it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.03it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.32it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.54it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.67it/s]100%|██████████| 17/17 [00:08<00:00,  6.28it/s]100%|██████████| 17/17 [00:09<00:00,  1.86it/s]=> result
* total: 1,667
* correct: 594
* accuracy: 35.6%
* error: 64.4%
* macro_f1: 32.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [12/30] batch [20/200] time 0.342 (0.397) data 0.000 (0.045) loss 2.2012 (2.0976) lr 7.0337e-03 eta 0:25:01
epoch [12/30] batch [40/200] time 0.375 (0.374) data 0.000 (0.023) loss 2.2129 (2.0000) lr 7.0337e-03 eta 0:23:24
epoch [12/30] batch [60/200] time 0.343 (0.364) data 0.000 (0.015) loss 2.2734 (2.0397) lr 7.0337e-03 eta 0:22:40
epoch [12/30] batch [80/200] time 0.340 (0.361) data 0.000 (0.011) loss 2.1387 (2.0212) lr 7.0337e-03 eta 0:22:24
epoch [12/30] batch [100/200] time 0.359 (0.360) data 0.000 (0.009) loss 2.5020 (2.0315) lr 7.0337e-03 eta 0:22:10
epoch [12/30] batch [120/200] time 0.347 (0.358) data 0.000 (0.008) loss 2.0195 (2.0604) lr 7.0337e-03 eta 0:21:57
epoch [12/30] batch [140/200] time 0.360 (0.357) data 0.000 (0.007) loss 2.7324 (2.0559) lr 7.0337e-03 eta 0:21:47
epoch [12/30] batch [160/200] time 0.349 (0.357) data 0.000 (0.006) loss 1.5645 (2.0732) lr 7.0337e-03 eta 0:21:38
epoch [12/30] batch [180/200] time 0.294 (0.352) data 0.000 (0.005) loss 0.7163 (2.0709) lr 7.0337e-03 eta 0:21:15
epoch [12/30] batch [200/200] time 0.295 (0.347) data 0.000 (0.005) loss 1.5840 (2.0727) lr 6.5451e-03 eta 0:20:47
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:30,  5.65s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.50s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.48s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.71it/s] 41%|████      | 7/17 [00:07<00:04,  2.06it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.61it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.18it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.73it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.24it/s] 71%|███████   | 12/17 [00:08<00:01,  4.68it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.04it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.24it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.47it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.65it/s]100%|██████████| 17/17 [00:08<00:00,  6.25it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 581
* accuracy: 34.9%
* error: 65.1%
* macro_f1: 32.0%

epoch [13/30] batch [20/200] time 0.332 (0.395) data 0.000 (0.043) loss 2.1387 (1.9513) lr 6.5451e-03 eta 0:23:33
epoch [13/30] batch [40/200] time 0.348 (0.373) data 0.000 (0.022) loss 2.4727 (2.0587) lr 6.5451e-03 eta 0:22:07
epoch [13/30] batch [60/200] time 0.344 (0.366) data 0.000 (0.014) loss 1.7676 (2.0463) lr 6.5451e-03 eta 0:21:34
epoch [13/30] batch [80/200] time 0.346 (0.362) data 0.000 (0.011) loss 1.2061 (1.9967) lr 6.5451e-03 eta 0:21:13
epoch [13/30] batch [100/200] time 0.340 (0.360) data 0.000 (0.009) loss 3.3984 (2.0116) lr 6.5451e-03 eta 0:20:59
epoch [13/30] batch [120/200] time 0.347 (0.360) data 0.000 (0.007) loss 1.9160 (2.0243) lr 6.5451e-03 eta 0:20:51
epoch [13/30] batch [140/200] time 0.374 (0.359) data 0.000 (0.006) loss 1.5625 (2.0172) lr 6.5451e-03 eta 0:20:40
epoch [13/30] batch [160/200] time 0.354 (0.358) data 0.000 (0.006) loss 2.1816 (2.0226) lr 6.5451e-03 eta 0:20:31
epoch [13/30] batch [180/200] time 0.295 (0.353) data 0.000 (0.005) loss 2.1270 (2.0288) lr 6.5451e-03 eta 0:20:08
epoch [13/30] batch [200/200] time 0.293 (0.347) data 0.000 (0.005) loss 3.2031 (2.0412) lr 6.0396e-03 eta 0:19:41
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.51s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.56s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.02it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.44it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.88it/s] 41%|████      | 7/17 [00:07<00:04,  2.40it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.90it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.30it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.66it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.17it/s] 71%|███████   | 12/17 [00:07<00:01,  4.61it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.99it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.28it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.51it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.62it/s]100%|██████████| 17/17 [00:08<00:00,  6.23it/s]100%|██████████| 17/17 [00:08<00:00,  1.92it/s]=> result
* total: 1,667
* correct: 576
* accuracy: 34.6%
* error: 65.4%
* macro_f1: 32.0%

epoch [14/30] batch [20/200] time 0.345 (0.395) data 0.000 (0.045) loss 2.8359 (2.1164) lr 6.0396e-03 eta 0:22:14
epoch [14/30] batch [40/200] time 0.451 (0.375) data 0.000 (0.022) loss 1.4912 (2.1771) lr 6.0396e-03 eta 0:20:58
epoch [14/30] batch [60/200] time 0.363 (0.366) data 0.000 (0.015) loss 1.5684 (2.1895) lr 6.0396e-03 eta 0:20:21
epoch [14/30] batch [80/200] time 0.343 (0.362) data 0.000 (0.011) loss 1.5859 (2.1356) lr 6.0396e-03 eta 0:20:00
epoch [14/30] batch [100/200] time 0.348 (0.359) data 0.000 (0.009) loss 2.8730 (2.1115) lr 6.0396e-03 eta 0:19:44
epoch [14/30] batch [120/200] time 0.353 (0.358) data 0.000 (0.008) loss 2.1582 (2.1230) lr 6.0396e-03 eta 0:19:32
epoch [14/30] batch [140/200] time 0.343 (0.357) data 0.000 (0.007) loss 1.4805 (2.1410) lr 6.0396e-03 eta 0:19:24
epoch [14/30] batch [160/200] time 0.344 (0.356) data 0.000 (0.006) loss 2.2070 (2.1376) lr 6.0396e-03 eta 0:19:14
epoch [14/30] batch [180/200] time 0.295 (0.352) data 0.000 (0.005) loss 2.0391 (2.0941) lr 6.0396e-03 eta 0:18:53
epoch [14/30] batch [200/200] time 0.295 (0.346) data 0.000 (0.005) loss 1.0254 (2.0861) lr 5.5226e-03 eta 0:18:28
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:29,  5.57s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.50s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.48s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.35it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.72it/s] 41%|████      | 7/17 [00:07<00:04,  2.09it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.56it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.13it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.68it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.20it/s] 71%|███████   | 12/17 [00:08<00:01,  4.64it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.01it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.30it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.53it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.70it/s]100%|██████████| 17/17 [00:08<00:00,  6.27it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 599
* accuracy: 35.9%
* error: 64.1%
* macro_f1: 32.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [15/30] batch [20/200] time 0.348 (0.402) data 0.000 (0.049) loss 2.4121 (2.1372) lr 5.5226e-03 eta 0:21:17
epoch [15/30] batch [40/200] time 0.345 (0.375) data 0.000 (0.025) loss 2.1367 (1.9378) lr 5.5226e-03 eta 0:19:44
epoch [15/30] batch [60/200] time 0.338 (0.365) data 0.000 (0.017) loss 1.6094 (1.9241) lr 5.5226e-03 eta 0:19:06
epoch [15/30] batch [80/200] time 0.339 (0.360) data 0.000 (0.012) loss 2.4883 (1.8703) lr 5.5226e-03 eta 0:18:43
epoch [15/30] batch [100/200] time 0.355 (0.358) data 0.000 (0.010) loss 2.0742 (1.9116) lr 5.5226e-03 eta 0:18:30
epoch [15/30] batch [120/200] time 0.345 (0.358) data 0.000 (0.008) loss 1.8174 (1.9257) lr 5.5226e-03 eta 0:18:21
epoch [15/30] batch [140/200] time 0.347 (0.357) data 0.000 (0.007) loss 1.8574 (1.9306) lr 5.5226e-03 eta 0:18:11
epoch [15/30] batch [160/200] time 0.346 (0.356) data 0.000 (0.006) loss 2.2578 (1.9378) lr 5.5226e-03 eta 0:18:03
epoch [15/30] batch [180/200] time 0.297 (0.353) data 0.000 (0.006) loss 1.4268 (1.9450) lr 5.5226e-03 eta 0:17:45
epoch [15/30] batch [200/200] time 0.298 (0.347) data 0.000 (0.005) loss 1.5020 (1.9743) lr 5.0000e-03 eta 0:17:21
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.56s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.55s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.51s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.02s/it] 29%|██▉       | 5/17 [00:06<00:09,  1.33it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.70it/s] 41%|████      | 7/17 [00:07<00:04,  2.06it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.60it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.17it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.72it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.23it/s] 71%|███████   | 12/17 [00:08<00:01,  4.67it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.03it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.32it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.54it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.70it/s]100%|██████████| 17/17 [00:08<00:00,  6.30it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 600
* accuracy: 36.0%
* error: 64.0%
* macro_f1: 32.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [16/30] batch [20/200] time 0.351 (0.400) data 0.000 (0.044) loss 1.3037 (1.9107) lr 5.0000e-03 eta 0:19:51
epoch [16/30] batch [40/200] time 0.338 (0.371) data 0.000 (0.022) loss 2.8047 (2.0023) lr 5.0000e-03 eta 0:18:19
epoch [16/30] batch [60/200] time 0.343 (0.363) data 0.000 (0.015) loss 2.5957 (2.0426) lr 5.0000e-03 eta 0:17:46
epoch [16/30] batch [80/200] time 0.350 (0.360) data 0.000 (0.011) loss 1.3906 (2.0321) lr 5.0000e-03 eta 0:17:30
epoch [16/30] batch [100/200] time 0.358 (0.359) data 0.000 (0.009) loss 1.4502 (2.0225) lr 5.0000e-03 eta 0:17:22
epoch [16/30] batch [120/200] time 0.340 (0.358) data 0.000 (0.008) loss 1.7402 (2.0398) lr 5.0000e-03 eta 0:17:10
epoch [16/30] batch [140/200] time 0.350 (0.357) data 0.000 (0.007) loss 2.5234 (2.0183) lr 5.0000e-03 eta 0:17:00
epoch [16/30] batch [160/200] time 0.344 (0.356) data 0.000 (0.006) loss 2.2012 (2.0164) lr 5.0000e-03 eta 0:16:50
epoch [16/30] batch [180/200] time 0.293 (0.352) data 0.000 (0.005) loss 1.9395 (1.9926) lr 5.0000e-03 eta 0:16:32
epoch [16/30] batch [200/200] time 0.379 (0.347) data 0.000 (0.005) loss 2.4629 (2.0019) lr 4.4774e-03 eta 0:16:10
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.46s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.53s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.46s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.03it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.46it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.92it/s] 41%|████      | 7/17 [00:06<00:04,  2.37it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.82it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.30it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.76it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.13it/s] 71%|███████   | 12/17 [00:07<00:01,  4.59it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.96it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.27it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.50it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.68it/s]100%|██████████| 17/17 [00:08<00:00,  6.28it/s]100%|██████████| 17/17 [00:08<00:00,  1.94it/s]=> result
* total: 1,667
* correct: 613
* accuracy: 36.8%
* error: 63.2%
* macro_f1: 33.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [17/30] batch [20/200] time 0.341 (0.402) data 0.000 (0.043) loss 1.9316 (1.9427) lr 4.4774e-03 eta 0:18:37
epoch [17/30] batch [40/200] time 0.373 (0.376) data 0.000 (0.021) loss 1.6377 (1.8951) lr 4.4774e-03 eta 0:17:18
epoch [17/30] batch [60/200] time 0.370 (0.367) data 0.000 (0.014) loss 1.7324 (1.9562) lr 4.4774e-03 eta 0:16:46
epoch [17/30] batch [80/200] time 0.349 (0.363) data 0.000 (0.011) loss 1.9678 (2.0187) lr 4.4774e-03 eta 0:16:26
epoch [17/30] batch [100/200] time 0.347 (0.361) data 0.000 (0.009) loss 0.8877 (2.0642) lr 4.4774e-03 eta 0:16:13
epoch [17/30] batch [120/200] time 0.341 (0.358) data 0.000 (0.007) loss 1.7178 (2.0817) lr 4.4774e-03 eta 0:16:00
epoch [17/30] batch [140/200] time 0.346 (0.357) data 0.000 (0.006) loss 2.2598 (2.0573) lr 4.4774e-03 eta 0:15:49
epoch [17/30] batch [160/200] time 0.349 (0.356) data 0.000 (0.006) loss 2.0078 (2.0448) lr 4.4774e-03 eta 0:15:40
epoch [17/30] batch [180/200] time 0.295 (0.352) data 0.000 (0.005) loss 1.8057 (2.0446) lr 4.4774e-03 eta 0:15:21
epoch [17/30] batch [200/200] time 0.295 (0.346) data 0.000 (0.005) loss 2.0332 (2.0212) lr 3.9604e-03 eta 0:15:00
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.51s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.52s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.71it/s] 41%|████      | 7/17 [00:07<00:04,  2.08it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.61it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.19it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.74it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.25it/s] 71%|███████   | 12/17 [00:08<00:01,  4.69it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.05it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.33it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.55it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.72it/s]100%|██████████| 17/17 [00:08<00:00,  6.31it/s]100%|██████████| 17/17 [00:09<00:00,  1.89it/s]=> result
* total: 1,667
* correct: 596
* accuracy: 35.8%
* error: 64.2%
* macro_f1: 31.8%

epoch [18/30] batch [20/200] time 0.339 (0.405) data 0.000 (0.041) loss 2.3340 (1.9071) lr 3.9604e-03 eta 0:17:23
epoch [18/30] batch [40/200] time 0.374 (0.379) data 0.000 (0.021) loss 2.2031 (1.8619) lr 3.9604e-03 eta 0:16:09
epoch [18/30] batch [60/200] time 0.356 (0.368) data 0.000 (0.014) loss 1.9180 (1.9163) lr 3.9604e-03 eta 0:15:35
epoch [18/30] batch [80/200] time 0.355 (0.363) data 0.000 (0.010) loss 1.8662 (1.9569) lr 3.9604e-03 eta 0:15:16
epoch [18/30] batch [100/200] time 0.351 (0.361) data 0.000 (0.008) loss 1.8926 (1.9367) lr 3.9604e-03 eta 0:15:02
epoch [18/30] batch [120/200] time 0.353 (0.359) data 0.000 (0.007) loss 1.7354 (1.9150) lr 3.9604e-03 eta 0:14:50
epoch [18/30] batch [140/200] time 0.342 (0.357) data 0.000 (0.006) loss 2.3652 (1.9189) lr 3.9604e-03 eta 0:14:39
epoch [18/30] batch [160/200] time 0.349 (0.358) data 0.000 (0.005) loss 2.9238 (1.9787) lr 3.9604e-03 eta 0:14:32
epoch [18/30] batch [180/200] time 0.294 (0.353) data 0.000 (0.005) loss 1.8662 (1.9813) lr 3.9604e-03 eta 0:14:14
epoch [18/30] batch [200/200] time 0.295 (0.347) data 0.000 (0.004) loss 1.5430 (1.9923) lr 3.4549e-03 eta 0:13:53
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:31,  5.69s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.52s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.50s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.34it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.71it/s] 41%|████      | 7/17 [00:07<00:04,  2.06it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.61it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.18it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.73it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.17it/s] 71%|███████   | 12/17 [00:08<00:01,  4.62it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.99it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.22it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.46it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.64it/s]100%|██████████| 17/17 [00:08<00:00,  6.24it/s]100%|██████████| 17/17 [00:09<00:00,  1.86it/s]=> result
* total: 1,667
* correct: 601
* accuracy: 36.1%
* error: 63.9%
* macro_f1: 32.8%

epoch [19/30] batch [20/200] time 0.346 (0.395) data 0.000 (0.044) loss 2.4023 (1.9396) lr 3.4549e-03 eta 0:15:40
epoch [19/30] batch [40/200] time 0.346 (0.372) data 0.000 (0.022) loss 1.0176 (1.8814) lr 3.4549e-03 eta 0:14:37
epoch [19/30] batch [60/200] time 0.362 (0.366) data 0.000 (0.015) loss 2.3203 (1.9047) lr 3.4549e-03 eta 0:14:16
epoch [19/30] batch [80/200] time 0.350 (0.362) data 0.000 (0.011) loss 1.1699 (1.9042) lr 3.4549e-03 eta 0:14:00
epoch [19/30] batch [100/200] time 0.356 (0.360) data 0.000 (0.009) loss 1.7227 (1.9183) lr 3.4549e-03 eta 0:13:47
epoch [19/30] batch [120/200] time 0.362 (0.358) data 0.000 (0.007) loss 2.2129 (1.9054) lr 3.4549e-03 eta 0:13:36
epoch [19/30] batch [140/200] time 0.350 (0.358) data 0.000 (0.006) loss 0.7607 (1.9024) lr 3.4549e-03 eta 0:13:28
epoch [19/30] batch [160/200] time 0.351 (0.357) data 0.000 (0.006) loss 2.4277 (1.9230) lr 3.4549e-03 eta 0:13:19
epoch [19/30] batch [180/200] time 0.296 (0.353) data 0.000 (0.005) loss 1.6826 (1.8944) lr 3.4549e-03 eta 0:13:02
epoch [19/30] batch [200/200] time 0.302 (0.347) data 0.000 (0.005) loss 2.3184 (1.9183) lr 2.9663e-03 eta 0:12:43
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:29,  5.60s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.56s/it] 18%|█▊        | 3/17 [00:06<00:21,  1.52s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.03s/it] 29%|██▉       | 5/17 [00:06<00:09,  1.32it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.69it/s] 41%|████      | 7/17 [00:07<00:04,  2.04it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.59it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.16it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.72it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.22it/s] 71%|███████   | 12/17 [00:08<00:01,  4.66it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.03it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.31it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.53it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.69it/s]100%|██████████| 17/17 [00:09<00:00,  6.28it/s]100%|██████████| 17/17 [00:09<00:00,  1.86it/s]=> result
* total: 1,667
* correct: 615
* accuracy: 36.9%
* error: 63.1%
* macro_f1: 33.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [20/30] batch [20/200] time 0.318 (0.393) data 0.000 (0.043) loss 1.3359 (2.0077) lr 2.9663e-03 eta 0:14:17
epoch [20/30] batch [40/200] time 0.354 (0.372) data 0.000 (0.022) loss 1.8809 (2.0192) lr 2.9663e-03 eta 0:13:23
epoch [20/30] batch [60/200] time 0.351 (0.364) data 0.000 (0.015) loss 1.2930 (2.0413) lr 2.9663e-03 eta 0:12:59
epoch [20/30] batch [80/200] time 0.367 (0.363) data 0.000 (0.011) loss 2.4336 (2.0527) lr 2.9663e-03 eta 0:12:49
epoch [20/30] batch [100/200] time 0.339 (0.360) data 0.000 (0.009) loss 1.3379 (2.0377) lr 2.9663e-03 eta 0:12:36
epoch [20/30] batch [120/200] time 0.361 (0.359) data 0.000 (0.007) loss 1.4629 (2.0079) lr 2.9663e-03 eta 0:12:27
epoch [20/30] batch [140/200] time 0.357 (0.358) data 0.000 (0.006) loss 2.0684 (1.9821) lr 2.9663e-03 eta 0:12:18
epoch [20/30] batch [160/200] time 0.357 (0.358) data 0.000 (0.006) loss 1.5801 (1.9490) lr 2.9663e-03 eta 0:12:09
epoch [20/30] batch [180/200] time 0.296 (0.354) data 0.000 (0.005) loss 1.9072 (1.9231) lr 2.9663e-03 eta 0:11:54
epoch [20/30] batch [200/200] time 0.441 (0.349) data 0.000 (0.005) loss 2.3535 (1.9264) lr 2.5000e-03 eta 0:11:37
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.54s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.57s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.03it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.46it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.93it/s] 41%|████      | 7/17 [00:07<00:04,  2.32it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.85it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.32it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.78it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.12it/s] 71%|███████   | 12/17 [00:07<00:01,  4.58it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.96it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.26it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.39it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.58it/s]100%|██████████| 17/17 [00:08<00:00,  6.19it/s]100%|██████████| 17/17 [00:08<00:00,  1.92it/s]=> result
* total: 1,667
* correct: 615
* accuracy: 36.9%
* error: 63.1%
* macro_f1: 33.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-20

epoch [21/30] batch [20/200] time 0.343 (0.403) data 0.000 (0.044) loss 1.5674 (1.7208) lr 2.5000e-03 eta 0:13:18
epoch [21/30] batch [40/200] time 0.336 (0.376) data 0.000 (0.022) loss 1.8145 (1.9462) lr 2.5000e-03 eta 0:12:16
epoch [21/30] batch [60/200] time 0.361 (0.367) data 0.000 (0.015) loss 2.1797 (1.9558) lr 2.5000e-03 eta 0:11:52
epoch [21/30] batch [80/200] time 0.352 (0.363) data 0.000 (0.011) loss 1.6494 (1.9811) lr 2.5000e-03 eta 0:11:36
epoch [21/30] batch [100/200] time 0.343 (0.361) data 0.000 (0.009) loss 1.8213 (1.9259) lr 2.5000e-03 eta 0:11:25
epoch [21/30] batch [120/200] time 0.352 (0.360) data 0.000 (0.008) loss 0.8418 (1.9158) lr 2.5000e-03 eta 0:11:17
epoch [21/30] batch [140/200] time 0.349 (0.359) data 0.000 (0.007) loss 1.3105 (1.9030) lr 2.5000e-03 eta 0:11:07
epoch [21/30] batch [160/200] time 0.381 (0.358) data 0.000 (0.006) loss 1.6885 (1.8894) lr 2.5000e-03 eta 0:10:58
epoch [21/30] batch [180/200] time 0.294 (0.354) data 0.000 (0.005) loss 1.3066 (1.8777) lr 2.5000e-03 eta 0:10:44
epoch [21/30] batch [200/200] time 0.292 (0.348) data 0.000 (0.005) loss 2.6133 (1.9044) lr 2.0611e-03 eta 0:10:26
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.54s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.50s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.35it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.72it/s] 41%|████      | 7/17 [00:07<00:04,  2.08it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.63it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.20it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.76it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.26it/s] 71%|███████   | 12/17 [00:08<00:01,  4.70it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.06it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.34it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.46it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.64it/s]100%|██████████| 17/17 [00:08<00:00,  6.25it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 610
* accuracy: 36.6%
* error: 63.4%
* macro_f1: 33.7%

epoch [22/30] batch [20/200] time 0.336 (0.398) data 0.000 (0.044) loss 3.0781 (1.8315) lr 2.0611e-03 eta 0:11:49
epoch [22/30] batch [40/200] time 0.384 (0.374) data 0.000 (0.022) loss 1.5947 (1.8199) lr 2.0611e-03 eta 0:10:58
epoch [22/30] batch [60/200] time 0.337 (0.366) data 0.000 (0.015) loss 1.8613 (1.8817) lr 2.0611e-03 eta 0:10:36
epoch [22/30] batch [80/200] time 0.337 (0.362) data 0.000 (0.011) loss 2.0195 (1.8873) lr 2.0611e-03 eta 0:10:22
epoch [22/30] batch [100/200] time 0.359 (0.360) data 0.000 (0.009) loss 3.1348 (1.9108) lr 2.0611e-03 eta 0:10:11
epoch [22/30] batch [120/200] time 0.345 (0.358) data 0.000 (0.008) loss 1.8291 (1.9351) lr 2.0611e-03 eta 0:10:01
epoch [22/30] batch [140/200] time 0.476 (0.357) data 0.000 (0.007) loss 2.1406 (1.9394) lr 2.0611e-03 eta 0:09:52
epoch [22/30] batch [160/200] time 0.341 (0.357) data 0.000 (0.006) loss 1.7529 (1.9458) lr 2.0611e-03 eta 0:09:44
epoch [22/30] batch [180/200] time 0.296 (0.353) data 0.000 (0.005) loss 1.9121 (1.9508) lr 2.0611e-03 eta 0:09:31
epoch [22/30] batch [200/200] time 0.294 (0.347) data 0.000 (0.005) loss 1.5996 (1.9415) lr 1.6543e-03 eta 0:09:15
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.48s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.53s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.47s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.00it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.36it/s] 35%|███▌      | 6/17 [00:06<00:06,  1.73it/s] 41%|████      | 7/17 [00:07<00:04,  2.08it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.43it/s] 53%|█████▎    | 9/17 [00:07<00:02,  2.99it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.55it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.08it/s] 71%|███████   | 12/17 [00:08<00:01,  4.54it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.93it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.23it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.48it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.54it/s]100%|██████████| 17/17 [00:08<00:00,  6.15it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 613
* accuracy: 36.8%
* error: 63.2%
* macro_f1: 33.5%

epoch [23/30] batch [20/200] time 0.355 (0.402) data 0.000 (0.045) loss 3.6738 (2.0060) lr 1.6543e-03 eta 0:10:35
epoch [23/30] batch [40/200] time 0.343 (0.376) data 0.000 (0.023) loss 1.2637 (1.9640) lr 1.6543e-03 eta 0:09:46
epoch [23/30] batch [60/200] time 0.360 (0.367) data 0.000 (0.015) loss 1.6396 (1.9511) lr 1.6543e-03 eta 0:09:24
epoch [23/30] batch [80/200] time 0.347 (0.364) data 0.000 (0.011) loss 2.0078 (1.9138) lr 1.6543e-03 eta 0:09:13
epoch [23/30] batch [100/200] time 0.364 (0.362) data 0.000 (0.009) loss 1.1475 (1.9260) lr 1.6543e-03 eta 0:09:03
epoch [23/30] batch [120/200] time 0.347 (0.359) data 0.000 (0.008) loss 2.5117 (1.8891) lr 1.6543e-03 eta 0:08:51
epoch [23/30] batch [140/200] time 0.366 (0.358) data 0.000 (0.007) loss 2.1973 (1.8878) lr 1.6543e-03 eta 0:08:42
epoch [23/30] batch [160/200] time 0.347 (0.357) data 0.000 (0.006) loss 1.7080 (1.8882) lr 1.6543e-03 eta 0:08:34
epoch [23/30] batch [180/200] time 0.293 (0.353) data 0.000 (0.005) loss 3.1680 (1.8788) lr 1.6543e-03 eta 0:08:21
epoch [23/30] batch [200/200] time 0.294 (0.347) data 0.000 (0.005) loss 2.2109 (1.8839) lr 1.2843e-03 eta 0:08:05
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:26,  5.41s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.52s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.01s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.35it/s] 35%|███▌      | 6/17 [00:06<00:06,  1.72it/s] 41%|████      | 7/17 [00:07<00:04,  2.07it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.56it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.13it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.69it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.20it/s] 71%|███████   | 12/17 [00:08<00:01,  4.65it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.01it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.30it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.52it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.69it/s]100%|██████████| 17/17 [00:08<00:00,  6.28it/s]100%|██████████| 17/17 [00:09<00:00,  1.89it/s]=> result
* total: 1,667
* correct: 601
* accuracy: 36.1%
* error: 63.9%
* macro_f1: 33.1%

epoch [24/30] batch [20/200] time 0.335 (0.401) data 0.000 (0.043) loss 1.7295 (1.8639) lr 1.2843e-03 eta 0:09:12
epoch [24/30] batch [40/200] time 0.343 (0.377) data 0.000 (0.022) loss 2.7910 (1.8593) lr 1.2843e-03 eta 0:08:32
epoch [24/30] batch [60/200] time 0.344 (0.369) data 0.000 (0.015) loss 1.9424 (1.8742) lr 1.2843e-03 eta 0:08:14
epoch [24/30] batch [80/200] time 0.355 (0.364) data 0.000 (0.011) loss 0.8218 (1.8622) lr 1.2843e-03 eta 0:08:00
epoch [24/30] batch [100/200] time 0.368 (0.361) data 0.000 (0.009) loss 0.8862 (1.9155) lr 1.2843e-03 eta 0:07:49
epoch [24/30] batch [120/200] time 0.355 (0.359) data 0.000 (0.007) loss 1.6504 (1.8985) lr 1.2843e-03 eta 0:07:39
epoch [24/30] batch [140/200] time 0.344 (0.357) data 0.000 (0.006) loss 1.6221 (1.9126) lr 1.2843e-03 eta 0:07:30
epoch [24/30] batch [160/200] time 0.354 (0.356) data 0.000 (0.006) loss 2.6621 (1.9203) lr 1.2843e-03 eta 0:07:21
epoch [24/30] batch [180/200] time 0.297 (0.353) data 0.000 (0.005) loss 2.2012 (1.9258) lr 1.2843e-03 eta 0:07:10
epoch [24/30] batch [200/200] time 0.299 (0.348) data 0.000 (0.005) loss 2.0508 (1.9301) lr 9.5492e-04 eta 0:06:57
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.55s/it] 12%|█▏        | 2/17 [00:05<00:38,  2.54s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.46s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.03it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.45it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.94it/s] 41%|████      | 7/17 [00:06<00:04,  2.44it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.87it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.33it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.76it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.26it/s] 71%|███████   | 12/17 [00:07<00:01,  4.69it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.97it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.27it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.49it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.67it/s]100%|██████████| 17/17 [00:08<00:00,  6.26it/s]100%|██████████| 17/17 [00:08<00:00,  1.94it/s]=> result
* total: 1,667
* correct: 618
* accuracy: 37.1%
* error: 62.9%
* macro_f1: 34.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [25/30] batch [20/200] time 0.340 (0.401) data 0.000 (0.046) loss 1.2412 (1.8849) lr 9.5492e-04 eta 0:07:53
epoch [25/30] batch [40/200] time 0.346 (0.374) data 0.000 (0.023) loss 0.9146 (1.7202) lr 9.5492e-04 eta 0:07:13
epoch [25/30] batch [60/200] time 0.369 (0.366) data 0.000 (0.016) loss 0.9961 (1.7945) lr 9.5492e-04 eta 0:06:57
epoch [25/30] batch [80/200] time 0.338 (0.361) data 0.000 (0.012) loss 1.9658 (1.7586) lr 9.5492e-04 eta 0:06:44
epoch [25/30] batch [100/200] time 0.347 (0.359) data 0.000 (0.009) loss 1.8008 (1.8214) lr 9.5492e-04 eta 0:06:34
epoch [25/30] batch [120/200] time 0.354 (0.357) data 0.000 (0.008) loss 1.7305 (1.8116) lr 9.5492e-04 eta 0:06:26
epoch [25/30] batch [140/200] time 0.348 (0.357) data 0.000 (0.007) loss 2.0625 (1.8151) lr 9.5492e-04 eta 0:06:18
epoch [25/30] batch [160/200] time 0.348 (0.356) data 0.000 (0.006) loss 1.0918 (1.8436) lr 9.5492e-04 eta 0:06:10
epoch [25/30] batch [180/200] time 0.302 (0.352) data 0.000 (0.005) loss 1.7471 (1.8564) lr 9.5492e-04 eta 0:05:59
epoch [25/30] batch [200/200] time 0.295 (0.347) data 0.000 (0.005) loss 2.2227 (1.8487) lr 6.6987e-04 eta 0:05:46
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:27,  5.47s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.50s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.49s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.00s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.35it/s] 35%|███▌      | 6/17 [00:06<00:06,  1.71it/s] 41%|████      | 7/17 [00:07<00:04,  2.07it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.54it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.11it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.67it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.12it/s] 71%|███████   | 12/17 [00:08<00:01,  4.58it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.96it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.26it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.50it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.68it/s]100%|██████████| 17/17 [00:08<00:00,  6.28it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 616
* accuracy: 37.0%
* error: 63.0%
* macro_f1: 33.8%

epoch [26/30] batch [20/200] time 0.336 (0.395) data 0.000 (0.044) loss 1.8037 (1.8754) lr 6.6987e-04 eta 0:06:27
epoch [26/30] batch [40/200] time 0.347 (0.376) data 0.000 (0.022) loss 2.9355 (1.8697) lr 6.6987e-04 eta 0:06:00
epoch [26/30] batch [60/200] time 0.341 (0.366) data 0.000 (0.015) loss 1.4160 (1.8324) lr 6.6987e-04 eta 0:05:43
epoch [26/30] batch [80/200] time 0.341 (0.361) data 0.000 (0.011) loss 2.5840 (1.8869) lr 6.6987e-04 eta 0:05:32
epoch [26/30] batch [100/200] time 0.348 (0.360) data 0.000 (0.009) loss 1.6328 (1.8769) lr 6.6987e-04 eta 0:05:23
epoch [26/30] batch [120/200] time 0.360 (0.357) data 0.000 (0.008) loss 1.7676 (1.8902) lr 6.6987e-04 eta 0:05:14
epoch [26/30] batch [140/200] time 0.344 (0.356) data 0.000 (0.007) loss 1.2959 (1.8427) lr 6.6987e-04 eta 0:05:06
epoch [26/30] batch [160/200] time 0.337 (0.356) data 0.000 (0.006) loss 2.7168 (1.8647) lr 6.6987e-04 eta 0:04:58
epoch [26/30] batch [180/200] time 0.296 (0.352) data 0.000 (0.005) loss 2.3027 (1.8873) lr 6.6987e-04 eta 0:04:48
epoch [26/30] batch [200/200] time 0.294 (0.346) data 0.000 (0.005) loss 1.3223 (1.8903) lr 4.3227e-04 eta 0:04:36
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.52s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.47s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.47s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.00it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.39it/s] 35%|███▌      | 6/17 [00:06<00:06,  1.76it/s] 41%|████      | 7/17 [00:07<00:04,  2.14it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.62it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.16it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.72it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.22it/s] 71%|███████   | 12/17 [00:08<00:01,  4.67it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.04it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.26it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.50it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.67it/s]100%|██████████| 17/17 [00:08<00:00,  6.27it/s]100%|██████████| 17/17 [00:08<00:00,  1.90it/s]=> result
* total: 1,667
* correct: 620
* accuracy: 37.2%
* error: 62.8%
* macro_f1: 34.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [27/30] batch [20/200] time 0.334 (0.397) data 0.000 (0.046) loss 2.2871 (1.9490) lr 4.3227e-04 eta 0:05:09
epoch [27/30] batch [40/200] time 0.353 (0.374) data 0.000 (0.023) loss 1.8857 (1.8665) lr 4.3227e-04 eta 0:04:43
epoch [27/30] batch [60/200] time 0.337 (0.368) data 0.000 (0.015) loss 1.8740 (1.8744) lr 4.3227e-04 eta 0:04:32
epoch [27/30] batch [80/200] time 0.350 (0.364) data 0.000 (0.012) loss 1.6787 (1.8486) lr 4.3227e-04 eta 0:04:21
epoch [27/30] batch [100/200] time 0.337 (0.362) data 0.000 (0.009) loss 2.0684 (1.8415) lr 4.3227e-04 eta 0:04:13
epoch [27/30] batch [120/200] time 0.344 (0.359) data 0.000 (0.008) loss 1.3945 (1.8284) lr 4.3227e-04 eta 0:04:04
epoch [27/30] batch [140/200] time 0.339 (0.358) data 0.000 (0.007) loss 0.7344 (1.8305) lr 4.3227e-04 eta 0:03:56
epoch [27/30] batch [160/200] time 0.344 (0.357) data 0.000 (0.006) loss 2.2129 (1.8399) lr 4.3227e-04 eta 0:03:48
epoch [27/30] batch [180/200] time 0.295 (0.353) data 0.000 (0.005) loss 2.7305 (1.8639) lr 4.3227e-04 eta 0:03:38
epoch [27/30] batch [200/200] time 0.298 (0.347) data 0.000 (0.005) loss 2.0664 (1.8706) lr 2.4472e-04 eta 0:03:28
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:28,  5.55s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.56s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.47s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.03it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.46it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.93it/s] 41%|████      | 7/17 [00:06<00:04,  2.42it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.89it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.34it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.74it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.24it/s] 71%|███████   | 12/17 [00:07<00:01,  4.67it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.03it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.32it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.54it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.58it/s]100%|██████████| 17/17 [00:08<00:00,  6.19it/s]100%|██████████| 17/17 [00:08<00:00,  1.93it/s]=> result
* total: 1,667
* correct: 604
* accuracy: 36.2%
* error: 63.8%
* macro_f1: 33.3%

epoch [28/30] batch [20/200] time 0.345 (0.402) data 0.000 (0.044) loss 1.6055 (1.9221) lr 2.4472e-04 eta 0:03:52
epoch [28/30] batch [40/200] time 0.338 (0.376) data 0.000 (0.022) loss 1.5645 (1.9109) lr 2.4472e-04 eta 0:03:30
epoch [28/30] batch [60/200] time 0.372 (0.367) data 0.000 (0.015) loss 3.1113 (1.9063) lr 2.4472e-04 eta 0:03:18
epoch [28/30] batch [80/200] time 0.353 (0.362) data 0.000 (0.011) loss 1.9072 (1.8437) lr 2.4472e-04 eta 0:03:08
epoch [28/30] batch [100/200] time 0.356 (0.360) data 0.000 (0.009) loss 2.4297 (1.8675) lr 2.4472e-04 eta 0:03:00
epoch [28/30] batch [120/200] time 0.351 (0.359) data 0.000 (0.008) loss 1.5986 (1.8854) lr 2.4472e-04 eta 0:02:52
epoch [28/30] batch [140/200] time 0.352 (0.358) data 0.000 (0.007) loss 1.9629 (1.8507) lr 2.4472e-04 eta 0:02:44
epoch [28/30] batch [160/200] time 0.372 (0.358) data 0.000 (0.006) loss 2.5254 (1.8719) lr 2.4472e-04 eta 0:02:37
epoch [28/30] batch [180/200] time 0.296 (0.353) data 0.000 (0.005) loss 1.7393 (1.8882) lr 2.4472e-04 eta 0:02:28
epoch [28/30] batch [200/200] time 0.295 (0.347) data 0.000 (0.005) loss 1.2617 (1.8688) lr 1.0926e-04 eta 0:02:18
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:29,  5.58s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.48s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.47s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.00s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.36it/s] 35%|███▌      | 6/17 [00:06<00:06,  1.72it/s] 41%|████      | 7/17 [00:07<00:04,  2.09it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.45it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.01it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.57it/s] 65%|██████▍   | 11/17 [00:08<00:01,  4.10it/s] 71%|███████   | 12/17 [00:08<00:01,  4.56it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.94it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.25it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.49it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.67it/s]100%|██████████| 17/17 [00:08<00:00,  6.28it/s]100%|██████████| 17/17 [00:09<00:00,  1.87it/s]=> result
* total: 1,667
* correct: 614
* accuracy: 36.8%
* error: 63.2%
* macro_f1: 33.9%

epoch [29/30] batch [20/200] time 0.335 (0.399) data 0.000 (0.044) loss 1.5000 (1.7924) lr 1.0926e-04 eta 0:02:31
epoch [29/30] batch [40/200] time 0.334 (0.373) data 0.000 (0.022) loss 2.1367 (1.8610) lr 1.0926e-04 eta 0:02:14
epoch [29/30] batch [60/200] time 0.356 (0.366) data 0.000 (0.015) loss 1.7100 (1.8998) lr 1.0926e-04 eta 0:02:04
epoch [29/30] batch [80/200] time 0.343 (0.361) data 0.000 (0.011) loss 0.8901 (1.9259) lr 1.0926e-04 eta 0:01:55
epoch [29/30] batch [100/200] time 0.350 (0.359) data 0.000 (0.009) loss 1.9980 (1.9031) lr 1.0926e-04 eta 0:01:47
epoch [29/30] batch [120/200] time 0.340 (0.358) data 0.000 (0.008) loss 1.1328 (1.8707) lr 1.0926e-04 eta 0:01:40
epoch [29/30] batch [140/200] time 0.352 (0.357) data 0.000 (0.007) loss 1.4961 (1.8372) lr 1.0926e-04 eta 0:01:32
epoch [29/30] batch [160/200] time 0.355 (0.357) data 0.000 (0.006) loss 1.2920 (1.8311) lr 1.0926e-04 eta 0:01:25
epoch [29/30] batch [180/200] time 0.287 (0.352) data 0.000 (0.005) loss 2.3008 (1.8404) lr 1.0926e-04 eta 0:01:17
epoch [29/30] batch [200/200] time 0.295 (0.346) data 0.000 (0.005) loss 1.6484 (1.8905) lr 2.7391e-05 eta 0:01:09
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:30,  5.66s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.50s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.48s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.00s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.35it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.72it/s] 41%|████      | 7/17 [00:07<00:04,  2.08it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.61it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.18it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.74it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.24it/s] 71%|███████   | 12/17 [00:08<00:01,  4.62it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.99it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.28it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.51it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.69it/s]100%|██████████| 17/17 [00:08<00:00,  6.28it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]=> result
* total: 1,667
* correct: 615
* accuracy: 36.9%
* error: 63.1%
* macro_f1: 33.9%

epoch [30/30] batch [20/200] time 0.343 (0.401) data 0.000 (0.045) loss 1.9316 (1.7714) lr 2.7391e-05 eta 0:01:12
epoch [30/30] batch [40/200] time 0.340 (0.375) data 0.000 (0.023) loss 2.6406 (1.8636) lr 2.7391e-05 eta 0:01:00
epoch [30/30] batch [60/200] time 0.344 (0.369) data 0.000 (0.015) loss 1.7256 (1.8497) lr 2.7391e-05 eta 0:00:51
epoch [30/30] batch [80/200] time 0.343 (0.365) data 0.000 (0.011) loss 1.6094 (1.8508) lr 2.7391e-05 eta 0:00:43
epoch [30/30] batch [100/200] time 0.341 (0.362) data 0.000 (0.009) loss 1.4355 (1.8334) lr 2.7391e-05 eta 0:00:36
epoch [30/30] batch [120/200] time 0.354 (0.360) data 0.000 (0.008) loss 1.5723 (1.8389) lr 2.7391e-05 eta 0:00:28
epoch [30/30] batch [140/200] time 0.347 (0.359) data 0.000 (0.007) loss 2.1270 (1.8211) lr 2.7391e-05 eta 0:00:21
epoch [30/30] batch [160/200] time 0.338 (0.359) data 0.000 (0.006) loss 0.8638 (1.8368) lr 2.7391e-05 eta 0:00:14
epoch [30/30] batch [180/200] time 0.300 (0.354) data 0.000 (0.005) loss 1.9277 (1.8393) lr 2.7391e-05 eta 0:00:07
epoch [30/30] batch [200/200] time 0.296 (0.348) data 0.000 (0.005) loss 1.8525 (1.8407) lr 0.0000e+00 eta 0:00:00
Evaluate on the *val* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:29,  5.61s/it] 12%|█▏        | 2/17 [00:05<00:37,  2.49s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.48s/it] 24%|██▎       | 4/17 [00:06<00:13,  1.00s/it] 29%|██▉       | 5/17 [00:06<00:08,  1.36it/s] 35%|███▌      | 6/17 [00:07<00:06,  1.72it/s] 41%|████      | 7/17 [00:07<00:04,  2.09it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.61it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.18it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.74it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.23it/s] 71%|███████   | 12/17 [00:08<00:01,  4.67it/s] 76%|███████▋  | 13/17 [00:08<00:00,  5.03it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.31it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.53it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.70it/s]100%|██████████| 17/17 [00:08<00:00,  6.29it/s]100%|██████████| 17/17 [00:09<00:00,  1.88it/s]
=> result
* total: 1,667
* correct: 620
* accuracy: 37.2%
* error: 62.8%
* macro_f1: 34.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 26)
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:05<01:32,  5.81s/it] 12%|█▏        | 2/17 [00:06<00:38,  2.56s/it] 18%|█▊        | 3/17 [00:06<00:20,  1.48s/it] 24%|██▎       | 4/17 [00:06<00:12,  1.02it/s] 29%|██▉       | 5/17 [00:06<00:08,  1.43it/s] 35%|███▌      | 6/17 [00:06<00:05,  1.87it/s] 41%|████      | 7/17 [00:07<00:04,  2.37it/s] 47%|████▋     | 8/17 [00:07<00:03,  2.80it/s] 53%|█████▎    | 9/17 [00:07<00:02,  3.31it/s] 59%|█████▉    | 10/17 [00:07<00:01,  3.69it/s] 65%|██████▍   | 11/17 [00:07<00:01,  4.17it/s] 71%|███████   | 12/17 [00:08<00:01,  4.61it/s] 76%|███████▋  | 13/17 [00:08<00:00,  4.98it/s] 82%|████████▏ | 14/17 [00:08<00:00,  5.28it/s] 88%|████████▊ | 15/17 [00:08<00:00,  5.50it/s] 94%|█████████▍| 16/17 [00:08<00:00,  5.67it/s]100%|██████████| 17/17 [00:08<00:00,  6.29it/s]100%|██████████| 17/17 [00:08<00:00,  1.90it/s]
=> result
* total: 1,666
* correct: 643
* accuracy: 38.6%
* error: 61.4%
* macro_f1: 36.2%
Elapsed: 0:39:31
+ sh scripts/rpo_prime/base2new_test_sdl.sh fgvc_aircraft 3 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
800 1666 1667
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      1,666
# test     1,667
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 26)
Evaluate on the *test* set
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:07<02:07,  7.94s/it] 12%|█▏        | 2/17 [00:08<00:50,  3.37s/it] 18%|█▊        | 3/17 [00:08<00:26,  1.90s/it] 24%|██▎       | 4/17 [00:08<00:15,  1.22s/it] 29%|██▉       | 5/17 [00:08<00:10,  1.19it/s] 35%|███▌      | 6/17 [00:08<00:06,  1.64it/s] 41%|████      | 7/17 [00:08<00:04,  2.15it/s] 47%|████▋     | 8/17 [00:09<00:03,  2.71it/s] 53%|█████▎    | 9/17 [00:09<00:02,  3.28it/s] 59%|█████▉    | 10/17 [00:09<00:01,  3.82it/s] 65%|██████▍   | 11/17 [00:09<00:01,  4.32it/s] 71%|███████   | 12/17 [00:09<00:01,  4.74it/s] 76%|███████▋  | 13/17 [00:09<00:00,  5.09it/s] 82%|████████▏ | 14/17 [00:10<00:00,  5.36it/s] 88%|████████▊ | 15/17 [00:10<00:00,  5.57it/s] 94%|█████████▍| 16/17 [00:10<00:00,  5.73it/s]100%|██████████| 17/17 [00:10<00:00,  6.32it/s]100%|██████████| 17/17 [00:10<00:00,  1.60it/s]
=> result
* total: 1,667
* correct: 569
* accuracy: 34.1%
* error: 65.9%
* macro_f1: 31.5%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh oxford_flowers 1 0 main_tmp1_0.1sdl 16
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
816 696 1053
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      696
# test     1,053
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/30] batch [20/204] time 0.319 (0.462) data 0.000 (0.065) loss 1.1152 (1.5356) lr 1.0000e-02 eta 0:46:56
epoch [1/30] batch [40/204] time 0.330 (0.391) data 0.000 (0.032) loss 0.6782 (1.4414) lr 1.0000e-02 eta 0:39:39
epoch [1/30] batch [60/204] time 0.318 (0.368) data 0.000 (0.022) loss 1.1797 (1.4394) lr 1.0000e-02 eta 0:37:08
epoch [1/30] batch [80/204] time 0.318 (0.356) data 0.000 (0.016) loss 0.4033 (1.3526) lr 1.0000e-02 eta 0:35:48
epoch [1/30] batch [100/204] time 0.317 (0.349) data 0.000 (0.013) loss 1.5479 (1.3778) lr 1.0000e-02 eta 0:35:01
epoch [1/30] batch [120/204] time 0.312 (0.344) data 0.000 (0.011) loss 1.0547 (1.3677) lr 1.0000e-02 eta 0:34:22
epoch [1/30] batch [140/204] time 0.323 (0.340) data 0.000 (0.009) loss 1.1309 (1.3484) lr 1.0000e-02 eta 0:33:54
epoch [1/30] batch [160/204] time 0.317 (0.337) data 0.000 (0.008) loss 0.8691 (1.3164) lr 1.0000e-02 eta 0:33:30
epoch [1/30] batch [180/204] time 0.295 (0.336) data 0.000 (0.007) loss 0.0243 (1.3107) lr 1.0000e-02 eta 0:33:13
epoch [1/30] batch [200/204] time 0.296 (0.332) data 0.000 (0.007) loss 1.6211 (1.3090) lr 1.0000e-02 eta 0:32:43
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:15,  2.55s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.15s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.43it/s] 57%|█████▋    | 4/7 [00:03<00:01,  2.05it/s] 71%|███████▏  | 5/7 [00:03<00:00,  2.69it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.33it/s]100%|██████████| 7/7 [00:03<00:00,  3.94it/s]100%|██████████| 7/7 [00:03<00:00,  1.91it/s]=> result
* total: 696
* correct: 515
* accuracy: 74.0%
* error: 26.0%
* macro_f1: 68.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [2/30] batch [20/204] time 0.318 (0.360) data 0.000 (0.039) loss 1.0918 (1.1614) lr 9.9726e-03 eta 0:35:25
epoch [2/30] batch [40/204] time 0.322 (0.340) data 0.000 (0.020) loss 1.3145 (1.0545) lr 9.9726e-03 eta 0:33:18
epoch [2/30] batch [60/204] time 0.321 (0.333) data 0.000 (0.013) loss 1.9932 (1.0132) lr 9.9726e-03 eta 0:32:31
epoch [2/30] batch [80/204] time 0.316 (0.329) data 0.000 (0.010) loss 0.9365 (1.0190) lr 9.9726e-03 eta 0:32:01
epoch [2/30] batch [100/204] time 0.318 (0.328) data 0.000 (0.008) loss 0.8662 (0.9927) lr 9.9726e-03 eta 0:31:48
epoch [2/30] batch [120/204] time 0.317 (0.327) data 0.000 (0.007) loss 0.2612 (0.9865) lr 9.9726e-03 eta 0:31:34
epoch [2/30] batch [140/204] time 0.328 (0.326) data 0.000 (0.006) loss 0.7480 (1.0014) lr 9.9726e-03 eta 0:31:22
epoch [2/30] batch [160/204] time 0.319 (0.325) data 0.000 (0.005) loss 0.9731 (1.0323) lr 9.9726e-03 eta 0:31:12
epoch [2/30] batch [180/204] time 0.295 (0.324) data 0.000 (0.005) loss 0.0500 (1.0059) lr 9.9726e-03 eta 0:30:56
epoch [2/30] batch [200/204] time 0.296 (0.322) data 0.000 (0.004) loss 1.0918 (1.0025) lr 9.9726e-03 eta 0:30:37
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.32s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.06s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.54it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.18it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.84it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.47it/s]100%|██████████| 7/7 [00:03<00:00,  4.08it/s]100%|██████████| 7/7 [00:03<00:00,  2.04it/s]=> result
* total: 696
* correct: 544
* accuracy: 78.2%
* error: 21.8%
* macro_f1: 74.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [3/30] batch [20/204] time 0.321 (0.363) data 0.000 (0.041) loss 1.8555 (1.0180) lr 9.8907e-03 eta 0:34:27
epoch [3/30] batch [40/204] time 0.315 (0.342) data 0.000 (0.021) loss 0.5854 (0.9657) lr 9.8907e-03 eta 0:32:17
epoch [3/30] batch [60/204] time 0.316 (0.335) data 0.000 (0.014) loss 0.1753 (0.9927) lr 9.8907e-03 eta 0:31:34
epoch [3/30] batch [80/204] time 0.328 (0.331) data 0.000 (0.011) loss 3.1348 (0.9425) lr 9.8907e-03 eta 0:31:06
epoch [3/30] batch [100/204] time 0.320 (0.329) data 0.000 (0.008) loss 2.2949 (1.0145) lr 9.8907e-03 eta 0:30:45
epoch [3/30] batch [120/204] time 0.323 (0.327) data 0.000 (0.007) loss 0.9365 (0.9852) lr 9.8907e-03 eta 0:30:27
epoch [3/30] batch [140/204] time 0.317 (0.326) data 0.000 (0.006) loss 0.5674 (0.9698) lr 9.8907e-03 eta 0:30:15
epoch [3/30] batch [160/204] time 0.327 (0.325) data 0.000 (0.005) loss 0.2959 (0.9709) lr 9.8907e-03 eta 0:30:04
epoch [3/30] batch [180/204] time 0.295 (0.323) data 0.000 (0.005) loss 0.4871 (0.9719) lr 9.8907e-03 eta 0:29:47
epoch [3/30] batch [200/204] time 0.302 (0.320) data 0.000 (0.004) loss 0.4639 (0.9691) lr 9.8907e-03 eta 0:29:26
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.20s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.01s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.61it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.26it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.93it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.56it/s]100%|██████████| 7/7 [00:03<00:00,  4.16it/s]100%|██████████| 7/7 [00:03<00:00,  2.12it/s]=> result
* total: 696
* correct: 543
* accuracy: 78.0%
* error: 22.0%
* macro_f1: 74.1%

epoch [4/30] batch [20/204] time 0.318 (0.367) data 0.000 (0.039) loss 1.1055 (0.8315) lr 9.7553e-03 eta 0:33:33
epoch [4/30] batch [40/204] time 0.318 (0.342) data 0.000 (0.019) loss 0.2668 (0.7258) lr 9.7553e-03 eta 0:31:12
epoch [4/30] batch [60/204] time 0.327 (0.335) data 0.000 (0.013) loss 0.1052 (0.8786) lr 9.7553e-03 eta 0:30:25
epoch [4/30] batch [80/204] time 0.321 (0.331) data 0.000 (0.010) loss 0.7939 (0.8735) lr 9.7553e-03 eta 0:29:59
epoch [4/30] batch [100/204] time 0.319 (0.329) data 0.000 (0.008) loss 0.1665 (0.8770) lr 9.7553e-03 eta 0:29:38
epoch [4/30] batch [120/204] time 0.314 (0.327) data 0.000 (0.007) loss 1.6729 (0.8762) lr 9.7553e-03 eta 0:29:20
epoch [4/30] batch [140/204] time 0.412 (0.326) data 0.000 (0.006) loss 0.4111 (0.8689) lr 9.7553e-03 eta 0:29:11
epoch [4/30] batch [160/204] time 0.325 (0.325) data 0.000 (0.005) loss 2.9473 (0.8642) lr 9.7553e-03 eta 0:28:59
epoch [4/30] batch [180/204] time 0.297 (0.324) data 0.000 (0.005) loss 1.3789 (0.8692) lr 9.7553e-03 eta 0:28:44
epoch [4/30] batch [200/204] time 0.297 (0.321) data 0.000 (0.004) loss 1.2822 (0.8587) lr 9.7553e-03 eta 0:28:22
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.18s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.63it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.28it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.92it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.55it/s]100%|██████████| 7/7 [00:03<00:00,  4.14it/s]100%|██████████| 7/7 [00:03<00:00,  2.12it/s]=> result
* total: 696
* correct: 557
* accuracy: 80.0%
* error: 20.0%
* macro_f1: 76.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [5/30] batch [20/204] time 0.317 (0.366) data 0.000 (0.044) loss 0.7778 (0.9552) lr 9.5677e-03 eta 0:32:12
epoch [5/30] batch [40/204] time 0.315 (0.345) data 0.000 (0.022) loss 0.0927 (0.8416) lr 9.5677e-03 eta 0:30:13
epoch [5/30] batch [60/204] time 0.316 (0.336) data 0.000 (0.015) loss 1.1494 (0.8123) lr 9.5677e-03 eta 0:29:21
epoch [5/30] batch [80/204] time 0.327 (0.332) data 0.000 (0.011) loss 1.0029 (0.8435) lr 9.5677e-03 eta 0:28:52
epoch [5/30] batch [100/204] time 0.325 (0.329) data 0.000 (0.009) loss 0.9238 (0.8467) lr 9.5677e-03 eta 0:28:30
epoch [5/30] batch [120/204] time 0.319 (0.327) data 0.000 (0.008) loss 0.2839 (0.8188) lr 9.5677e-03 eta 0:28:13
epoch [5/30] batch [140/204] time 0.312 (0.325) data 0.000 (0.006) loss 0.8037 (0.8180) lr 9.5677e-03 eta 0:27:59
epoch [5/30] batch [160/204] time 0.322 (0.325) data 0.000 (0.006) loss 0.9302 (0.8271) lr 9.5677e-03 eta 0:27:50
epoch [5/30] batch [180/204] time 0.296 (0.323) data 0.000 (0.005) loss 1.3330 (0.8394) lr 9.5677e-03 eta 0:27:37
epoch [5/30] batch [200/204] time 0.297 (0.321) data 0.000 (0.005) loss 0.3716 (0.8091) lr 9.5677e-03 eta 0:27:17
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.17s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.63it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.30it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.97it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.60it/s]100%|██████████| 7/7 [00:03<00:00,  4.19it/s]100%|██████████| 7/7 [00:03<00:00,  2.14it/s]=> result
* total: 696
* correct: 565
* accuracy: 81.2%
* error: 18.8%
* macro_f1: 77.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [6/30] batch [20/204] time 0.311 (0.361) data 0.000 (0.039) loss 1.0879 (0.6914) lr 9.3301e-03 eta 0:30:31
epoch [6/30] batch [40/204] time 0.325 (0.341) data 0.000 (0.019) loss 0.9004 (0.6934) lr 9.3301e-03 eta 0:28:47
epoch [6/30] batch [60/204] time 0.316 (0.336) data 0.000 (0.013) loss 0.9165 (0.7118) lr 9.3301e-03 eta 0:28:13
epoch [6/30] batch [80/204] time 0.322 (0.332) data 0.000 (0.010) loss 0.6016 (0.7302) lr 9.3301e-03 eta 0:27:44
epoch [6/30] batch [100/204] time 0.320 (0.329) data 0.000 (0.008) loss 0.0504 (0.7566) lr 9.3301e-03 eta 0:27:25
epoch [6/30] batch [120/204] time 0.319 (0.328) data 0.000 (0.007) loss 0.8955 (0.7557) lr 9.3301e-03 eta 0:27:11
epoch [6/30] batch [140/204] time 0.321 (0.327) data 0.000 (0.006) loss 0.8213 (0.7499) lr 9.3301e-03 eta 0:26:59
epoch [6/30] batch [160/204] time 0.319 (0.326) data 0.000 (0.005) loss 0.1968 (0.7202) lr 9.3301e-03 eta 0:26:49
epoch [6/30] batch [180/204] time 0.296 (0.324) data 0.000 (0.005) loss 0.0059 (0.6972) lr 9.3301e-03 eta 0:26:36
epoch [6/30] batch [200/204] time 0.296 (0.322) data 0.000 (0.004) loss 0.3286 (0.6961) lr 9.3301e-03 eta 0:26:17
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.64it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.31it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.97it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.60it/s]100%|██████████| 7/7 [00:03<00:00,  4.19it/s]100%|██████████| 7/7 [00:03<00:00,  2.15it/s]=> result
* total: 696
* correct: 576
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 79.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [7/30] batch [20/204] time 0.319 (0.364) data 0.000 (0.045) loss 0.4834 (0.7167) lr 9.0451e-03 eta 0:29:35
epoch [7/30] batch [40/204] time 0.315 (0.342) data 0.000 (0.023) loss 0.9355 (0.6300) lr 9.0451e-03 eta 0:27:42
epoch [7/30] batch [60/204] time 0.317 (0.334) data 0.000 (0.015) loss 0.1841 (0.6174) lr 9.0451e-03 eta 0:26:56
epoch [7/30] batch [80/204] time 0.315 (0.331) data 0.000 (0.011) loss 0.1614 (0.6056) lr 9.0451e-03 eta 0:26:36
epoch [7/30] batch [100/204] time 0.316 (0.328) data 0.000 (0.009) loss 0.2122 (0.6003) lr 9.0451e-03 eta 0:26:15
epoch [7/30] batch [120/204] time 0.323 (0.327) data 0.000 (0.008) loss 0.4248 (0.6269) lr 9.0451e-03 eta 0:26:02
epoch [7/30] batch [140/204] time 0.318 (0.326) data 0.000 (0.007) loss 0.0133 (0.6044) lr 9.0451e-03 eta 0:25:51
epoch [7/30] batch [160/204] time 0.319 (0.325) data 0.000 (0.006) loss 0.6528 (0.6473) lr 9.0451e-03 eta 0:25:41
epoch [7/30] batch [180/204] time 0.300 (0.324) data 0.000 (0.005) loss 0.5723 (0.6654) lr 9.0451e-03 eta 0:25:26
epoch [7/30] batch [200/204] time 0.296 (0.321) data 0.000 (0.005) loss 0.2805 (0.6554) lr 9.0451e-03 eta 0:25:07
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.18s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.00it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.62it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.28it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.95it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.58it/s]100%|██████████| 7/7 [00:03<00:00,  4.17it/s]100%|██████████| 7/7 [00:03<00:00,  2.13it/s]=> result
* total: 696
* correct: 595
* accuracy: 85.5%
* error: 14.5%
* macro_f1: 83.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [8/30] batch [20/204] time 0.314 (0.363) data 0.000 (0.042) loss 0.8760 (0.5383) lr 8.7157e-03 eta 0:28:17
epoch [8/30] batch [40/204] time 0.312 (0.341) data 0.000 (0.021) loss 0.2798 (0.5693) lr 8.7157e-03 eta 0:26:27
epoch [8/30] batch [60/204] time 0.316 (0.334) data 0.000 (0.014) loss 0.3450 (0.5551) lr 8.7157e-03 eta 0:25:46
epoch [8/30] batch [80/204] time 0.318 (0.330) data 0.000 (0.011) loss 0.1028 (0.5074) lr 8.7157e-03 eta 0:25:20
epoch [8/30] batch [100/204] time 0.309 (0.329) data 0.000 (0.009) loss 0.6987 (0.5026) lr 8.7157e-03 eta 0:25:08
epoch [8/30] batch [120/204] time 0.327 (0.327) data 0.000 (0.007) loss 0.3103 (0.5325) lr 8.7157e-03 eta 0:24:54
epoch [8/30] batch [140/204] time 0.318 (0.326) data 0.000 (0.006) loss 0.8740 (0.5427) lr 8.7157e-03 eta 0:24:42
epoch [8/30] batch [160/204] time 0.317 (0.325) data 0.000 (0.006) loss 1.9580 (0.5577) lr 8.7157e-03 eta 0:24:33
epoch [8/30] batch [180/204] time 0.298 (0.323) data 0.000 (0.005) loss 0.0610 (0.5659) lr 8.7157e-03 eta 0:24:19
epoch [8/30] batch [200/204] time 0.303 (0.321) data 0.000 (0.004) loss 0.2954 (0.5631) lr 8.7157e-03 eta 0:24:01
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.64it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.31it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.98it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.20it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 598
* accuracy: 85.9%
* error: 14.1%
* macro_f1: 83.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [9/30] batch [20/204] time 0.312 (0.361) data 0.000 (0.040) loss -0.0204 (0.4060) lr 8.3457e-03 eta 0:26:52
epoch [9/30] batch [40/204] time 0.312 (0.342) data 0.000 (0.020) loss 0.9409 (0.5057) lr 8.3457e-03 eta 0:25:21
epoch [9/30] batch [60/204] time 0.322 (0.334) data 0.000 (0.013) loss 0.8989 (0.4947) lr 8.3457e-03 eta 0:24:37
epoch [9/30] batch [80/204] time 0.321 (0.330) data 0.000 (0.010) loss 0.5410 (0.5018) lr 8.3457e-03 eta 0:24:13
epoch [9/30] batch [100/204] time 0.310 (0.328) data 0.000 (0.008) loss 0.6475 (0.5522) lr 8.3457e-03 eta 0:23:57
epoch [9/30] batch [120/204] time 0.321 (0.326) data 0.000 (0.007) loss 0.7773 (0.5430) lr 8.3457e-03 eta 0:23:44
epoch [9/30] batch [140/204] time 0.320 (0.326) data 0.000 (0.006) loss 0.5845 (0.5442) lr 8.3457e-03 eta 0:23:36
epoch [9/30] batch [160/204] time 0.322 (0.325) data 0.000 (0.005) loss 1.1768 (0.5400) lr 8.3457e-03 eta 0:23:25
epoch [9/30] batch [180/204] time 0.294 (0.323) data 0.000 (0.005) loss 0.9214 (0.5350) lr 8.3457e-03 eta 0:23:11
epoch [9/30] batch [200/204] time 0.297 (0.320) data 0.000 (0.004) loss 1.3887 (0.5162) lr 8.3457e-03 eta 0:22:53
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.31it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.98it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.60it/s]100%|██████████| 7/7 [00:03<00:00,  4.19it/s]100%|██████████| 7/7 [00:03<00:00,  2.15it/s]=> result
* total: 696
* correct: 623
* accuracy: 89.5%
* error: 10.5%
* macro_f1: 87.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [10/30] batch [20/204] time 0.318 (0.362) data 0.000 (0.041) loss 0.9214 (0.4535) lr 7.9389e-03 eta 0:25:45
epoch [10/30] batch [40/204] time 0.312 (0.340) data 0.000 (0.021) loss 0.5176 (0.5335) lr 7.9389e-03 eta 0:24:01
epoch [10/30] batch [60/204] time 0.316 (0.332) data 0.000 (0.014) loss 0.6318 (0.5186) lr 7.9389e-03 eta 0:23:23
epoch [10/30] batch [80/204] time 0.327 (0.328) data 0.000 (0.010) loss 0.4990 (0.5096) lr 7.9389e-03 eta 0:23:00
epoch [10/30] batch [100/204] time 0.309 (0.326) data 0.000 (0.008) loss 0.3857 (0.5083) lr 7.9389e-03 eta 0:22:44
epoch [10/30] batch [120/204] time 0.318 (0.325) data 0.000 (0.007) loss 0.4341 (0.5057) lr 7.9389e-03 eta 0:22:33
epoch [10/30] batch [140/204] time 0.320 (0.324) data 0.000 (0.006) loss 0.1986 (0.4917) lr 7.9389e-03 eta 0:22:22
epoch [10/30] batch [160/204] time 0.337 (0.324) data 0.000 (0.005) loss 0.6807 (0.4951) lr 7.9389e-03 eta 0:22:16
epoch [10/30] batch [180/204] time 0.295 (0.322) data 0.000 (0.005) loss 0.2969 (0.4842) lr 7.9389e-03 eta 0:22:03
epoch [10/30] batch [200/204] time 0.293 (0.320) data 0.000 (0.004) loss 0.4285 (0.4941) lr 7.9389e-03 eta 0:21:45
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:16,  2.74s/it] 29%|██▊       | 2/7 [00:02<00:06,  1.22s/it] 43%|████▎     | 3/7 [00:03<00:02,  1.35it/s] 57%|█████▋    | 4/7 [00:03<00:01,  1.95it/s] 71%|███████▏  | 5/7 [00:03<00:00,  2.59it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.22it/s]100%|██████████| 7/7 [00:03<00:00,  3.84it/s]100%|██████████| 7/7 [00:03<00:00,  1.82it/s]=> result
* total: 696
* correct: 621
* accuracy: 89.2%
* error: 10.8%
* macro_f1: 86.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model.pth.tar-10

epoch [11/30] batch [20/204] time 0.322 (0.367) data 0.000 (0.041) loss 0.3010 (0.4574) lr 7.5000e-03 eta 0:24:50
epoch [11/30] batch [40/204] time 0.322 (0.343) data 0.000 (0.021) loss 0.1597 (0.4651) lr 7.5000e-03 eta 0:23:04
epoch [11/30] batch [60/204] time 0.319 (0.335) data 0.000 (0.014) loss 0.4302 (0.5045) lr 7.5000e-03 eta 0:22:25
epoch [11/30] batch [80/204] time 0.313 (0.331) data 0.000 (0.010) loss 0.9541 (0.4986) lr 7.5000e-03 eta 0:22:05
epoch [11/30] batch [100/204] time 0.318 (0.329) data 0.000 (0.008) loss 0.1473 (0.4769) lr 7.5000e-03 eta 0:21:49
epoch [11/30] batch [120/204] time 0.321 (0.327) data 0.000 (0.007) loss 0.1868 (0.4583) lr 7.5000e-03 eta 0:21:34
epoch [11/30] batch [140/204] time 0.318 (0.326) data 0.000 (0.006) loss 0.4888 (0.4427) lr 7.5000e-03 eta 0:21:23
epoch [11/30] batch [160/204] time 0.317 (0.325) data 0.000 (0.005) loss 0.0410 (0.4496) lr 7.5000e-03 eta 0:21:13
epoch [11/30] batch [180/204] time 0.296 (0.324) data 0.000 (0.005) loss 3.5586 (0.4617) lr 7.5000e-03 eta 0:21:02
epoch [11/30] batch [200/204] time 0.297 (0.321) data 0.000 (0.004) loss 0.1549 (0.4581) lr 7.5000e-03 eta 0:20:45
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.28s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.04s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.57it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.22it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.88it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.51it/s]100%|██████████| 7/7 [00:03<00:00,  4.11it/s]100%|██████████| 7/7 [00:03<00:00,  2.07it/s]=> result
* total: 696
* correct: 629
* accuracy: 90.4%
* error: 9.6%
* macro_f1: 88.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [12/30] batch [20/204] time 0.324 (0.361) data 0.000 (0.043) loss 0.0579 (0.4247) lr 7.0337e-03 eta 0:23:12
epoch [12/30] batch [40/204] time 0.315 (0.339) data 0.000 (0.021) loss 0.3337 (0.3890) lr 7.0337e-03 eta 0:21:42
epoch [12/30] batch [60/204] time 0.313 (0.331) data 0.000 (0.014) loss 0.3901 (0.4257) lr 7.0337e-03 eta 0:21:04
epoch [12/30] batch [80/204] time 0.319 (0.328) data 0.000 (0.011) loss 0.0888 (0.4173) lr 7.0337e-03 eta 0:20:44
epoch [12/30] batch [100/204] time 0.324 (0.326) data 0.000 (0.009) loss 0.1545 (0.4012) lr 7.0337e-03 eta 0:20:30
epoch [12/30] batch [120/204] time 0.317 (0.324) data 0.000 (0.007) loss 0.0241 (0.3931) lr 7.0337e-03 eta 0:20:18
epoch [12/30] batch [140/204] time 0.312 (0.323) data 0.000 (0.006) loss 0.0271 (0.3774) lr 7.0337e-03 eta 0:20:06
epoch [12/30] batch [160/204] time 0.317 (0.322) data 0.000 (0.006) loss 0.2168 (0.3882) lr 7.0337e-03 eta 0:19:56
epoch [12/30] batch [180/204] time 0.301 (0.321) data 0.000 (0.005) loss 0.9346 (0.3977) lr 7.0337e-03 eta 0:19:45
epoch [12/30] batch [200/204] time 0.295 (0.319) data 0.000 (0.005) loss 0.1512 (0.3948) lr 7.0337e-03 eta 0:19:31
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.18s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.00it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.62it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.28it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.93it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.56it/s]100%|██████████| 7/7 [00:03<00:00,  4.15it/s]100%|██████████| 7/7 [00:03<00:00,  2.12it/s]=> result
* total: 696
* correct: 638
* accuracy: 91.7%
* error: 8.3%
* macro_f1: 89.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [13/30] batch [20/204] time 0.318 (0.362) data 0.000 (0.042) loss 0.1488 (0.3906) lr 6.5451e-03 eta 0:22:02
epoch [13/30] batch [40/204] time 0.320 (0.340) data 0.000 (0.021) loss 0.3950 (0.3618) lr 6.5451e-03 eta 0:20:35
epoch [13/30] batch [60/204] time 0.310 (0.333) data 0.000 (0.014) loss 1.0391 (0.4217) lr 6.5451e-03 eta 0:20:03
epoch [13/30] batch [80/204] time 0.311 (0.329) data 0.000 (0.011) loss 0.3999 (0.3927) lr 6.5451e-03 eta 0:19:40
epoch [13/30] batch [100/204] time 0.315 (0.326) data 0.000 (0.009) loss 0.4854 (0.4069) lr 6.5451e-03 eta 0:19:25
epoch [13/30] batch [120/204] time 0.321 (0.326) data 0.000 (0.007) loss 1.3271 (0.4318) lr 6.5451e-03 eta 0:19:18
epoch [13/30] batch [140/204] time 0.320 (0.325) data 0.000 (0.006) loss 0.1002 (0.4104) lr 6.5451e-03 eta 0:19:08
epoch [13/30] batch [160/204] time 0.325 (0.324) data 0.000 (0.005) loss 0.0577 (0.4119) lr 6.5451e-03 eta 0:18:59
epoch [13/30] batch [180/204] time 0.294 (0.323) data 0.000 (0.005) loss 0.0459 (0.4251) lr 6.5451e-03 eta 0:18:46
epoch [13/30] batch [200/204] time 0.303 (0.320) data 0.000 (0.004) loss 0.0906 (0.4036) lr 6.5451e-03 eta 0:18:31
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.21s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.01s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.60it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.26it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.90it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.52it/s]100%|██████████| 7/7 [00:03<00:00,  4.10it/s]100%|██████████| 7/7 [00:03<00:00,  2.10it/s]=> result
* total: 696
* correct: 645
* accuracy: 92.7%
* error: 7.3%
* macro_f1: 90.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [14/30] batch [20/204] time 0.317 (0.365) data 0.000 (0.041) loss 0.0464 (0.2613) lr 6.0396e-03 eta 0:20:57
epoch [14/30] batch [40/204] time 0.319 (0.341) data 0.000 (0.021) loss 0.9458 (0.3158) lr 6.0396e-03 eta 0:19:30
epoch [14/30] batch [60/204] time 0.317 (0.333) data 0.000 (0.014) loss 0.1434 (0.3292) lr 6.0396e-03 eta 0:18:56
epoch [14/30] batch [80/204] time 0.314 (0.330) data 0.000 (0.010) loss 0.5781 (0.3492) lr 6.0396e-03 eta 0:18:38
epoch [14/30] batch [100/204] time 0.316 (0.328) data 0.000 (0.008) loss 0.3430 (0.3547) lr 6.0396e-03 eta 0:18:23
epoch [14/30] batch [120/204] time 0.318 (0.326) data 0.000 (0.007) loss 1.4990 (0.3512) lr 6.0396e-03 eta 0:18:10
epoch [14/30] batch [140/204] time 0.315 (0.324) data 0.000 (0.006) loss 0.3689 (0.3746) lr 6.0396e-03 eta 0:17:59
epoch [14/30] batch [160/204] time 0.323 (0.324) data 0.000 (0.005) loss 0.5913 (0.3769) lr 6.0396e-03 eta 0:17:50
epoch [14/30] batch [180/204] time 0.295 (0.322) data 0.000 (0.005) loss 0.0329 (0.3968) lr 6.0396e-03 eta 0:17:37
epoch [14/30] batch [200/204] time 0.293 (0.319) data 0.000 (0.004) loss 0.0430 (0.3914) lr 6.0396e-03 eta 0:17:23
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.64it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.30it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.96it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.59it/s]100%|██████████| 7/7 [00:03<00:00,  4.18it/s]100%|██████████| 7/7 [00:03<00:00,  2.14it/s]=> result
* total: 696
* correct: 643
* accuracy: 92.4%
* error: 7.6%
* macro_f1: 90.0%

epoch [15/30] batch [20/204] time 0.314 (0.361) data 0.000 (0.041) loss 0.2917 (0.2172) lr 5.5226e-03 eta 0:19:31
epoch [15/30] batch [40/204] time 0.329 (0.343) data 0.000 (0.021) loss 0.8613 (0.2905) lr 5.5226e-03 eta 0:18:25
epoch [15/30] batch [60/204] time 0.322 (0.335) data 0.000 (0.014) loss 0.3914 (0.3211) lr 5.5226e-03 eta 0:17:52
epoch [15/30] batch [80/204] time 0.324 (0.332) data 0.000 (0.010) loss 0.2747 (0.3265) lr 5.5226e-03 eta 0:17:35
epoch [15/30] batch [100/204] time 0.319 (0.329) data 0.000 (0.008) loss 0.1924 (0.3374) lr 5.5226e-03 eta 0:17:20
epoch [15/30] batch [120/204] time 0.321 (0.328) data 0.000 (0.007) loss 0.1404 (0.3275) lr 5.5226e-03 eta 0:17:09
epoch [15/30] batch [140/204] time 0.326 (0.326) data 0.000 (0.006) loss 0.6875 (0.3342) lr 5.5226e-03 eta 0:16:59
epoch [15/30] batch [160/204] time 0.316 (0.325) data 0.000 (0.005) loss 0.1230 (0.3225) lr 5.5226e-03 eta 0:16:49
epoch [15/30] batch [180/204] time 0.296 (0.324) data 0.000 (0.005) loss 0.0515 (0.3216) lr 5.5226e-03 eta 0:16:39
epoch [15/30] batch [200/204] time 0.295 (0.321) data 0.000 (0.004) loss 0.1929 (0.3375) lr 5.5226e-03 eta 0:16:24
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.15s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.64it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.31it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.98it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.20it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 648
* accuracy: 93.1%
* error: 6.9%
* macro_f1: 91.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [16/30] batch [20/204] time 0.318 (0.362) data 0.000 (0.039) loss 0.0124 (0.1583) lr 5.0000e-03 eta 0:18:20
epoch [16/30] batch [40/204] time 0.317 (0.340) data 0.000 (0.020) loss 0.0132 (0.3255) lr 5.0000e-03 eta 0:17:06
epoch [16/30] batch [60/204] time 0.318 (0.332) data 0.000 (0.013) loss 1.4473 (0.3714) lr 5.0000e-03 eta 0:16:36
epoch [16/30] batch [80/204] time 0.311 (0.329) data 0.000 (0.010) loss 0.6343 (0.3758) lr 5.0000e-03 eta 0:16:21
epoch [16/30] batch [100/204] time 0.319 (0.327) data 0.000 (0.008) loss 0.5518 (0.3723) lr 5.0000e-03 eta 0:16:08
epoch [16/30] batch [120/204] time 0.321 (0.326) data 0.000 (0.007) loss 0.2400 (0.3483) lr 5.0000e-03 eta 0:15:59
epoch [16/30] batch [140/204] time 0.313 (0.325) data 0.000 (0.006) loss 1.3848 (0.3443) lr 5.0000e-03 eta 0:15:49
epoch [16/30] batch [160/204] time 0.319 (0.324) data 0.000 (0.005) loss 0.1426 (0.3423) lr 5.0000e-03 eta 0:15:40
epoch [16/30] batch [180/204] time 0.298 (0.323) data 0.000 (0.005) loss 1.3008 (0.3294) lr 5.0000e-03 eta 0:15:30
epoch [16/30] batch [200/204] time 0.296 (0.320) data 0.000 (0.004) loss 0.0613 (0.3321) lr 5.0000e-03 eta 0:15:15
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.17s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.63it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.30it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.96it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.59it/s]100%|██████████| 7/7 [00:03<00:00,  4.18it/s]100%|██████████| 7/7 [00:03<00:00,  2.14it/s]=> result
* total: 696
* correct: 647
* accuracy: 93.0%
* error: 7.0%
* macro_f1: 91.1%

epoch [17/30] batch [20/204] time 0.323 (0.364) data 0.000 (0.039) loss 0.0349 (0.2795) lr 4.4774e-03 eta 0:17:11
epoch [17/30] batch [40/204] time 0.313 (0.342) data 0.000 (0.020) loss 0.1316 (0.2921) lr 4.4774e-03 eta 0:16:03
epoch [17/30] batch [60/204] time 0.317 (0.335) data 0.000 (0.013) loss 0.2004 (0.3027) lr 4.4774e-03 eta 0:15:35
epoch [17/30] batch [80/204] time 0.318 (0.331) data 0.000 (0.010) loss 0.3694 (0.2957) lr 4.4774e-03 eta 0:15:17
epoch [17/30] batch [100/204] time 0.324 (0.329) data 0.000 (0.008) loss 0.0912 (0.3144) lr 4.4774e-03 eta 0:15:07
epoch [17/30] batch [120/204] time 0.319 (0.328) data 0.000 (0.007) loss 1.4395 (0.3097) lr 4.4774e-03 eta 0:14:56
epoch [17/30] batch [140/204] time 0.314 (0.327) data 0.000 (0.006) loss 0.7070 (0.3061) lr 4.4774e-03 eta 0:14:47
epoch [17/30] batch [160/204] time 0.318 (0.326) data 0.000 (0.005) loss 0.2703 (0.3048) lr 4.4774e-03 eta 0:14:38
epoch [17/30] batch [180/204] time 0.297 (0.324) data 0.000 (0.005) loss 0.4768 (0.3077) lr 4.4774e-03 eta 0:14:27
epoch [17/30] batch [200/204] time 0.295 (0.321) data 0.000 (0.004) loss 0.1165 (0.3124) lr 4.4774e-03 eta 0:14:13
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.17s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.63it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.29it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.96it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.59it/s]100%|██████████| 7/7 [00:03<00:00,  4.18it/s]100%|██████████| 7/7 [00:03<00:00,  2.14it/s]=> result
* total: 696
* correct: 648
* accuracy: 93.1%
* error: 6.9%
* macro_f1: 91.3%

epoch [18/30] batch [20/204] time 0.322 (0.368) data 0.000 (0.040) loss 0.0562 (0.3161) lr 3.9604e-03 eta 0:16:08
epoch [18/30] batch [40/204] time 0.324 (0.343) data 0.000 (0.020) loss 0.0558 (0.2817) lr 3.9604e-03 eta 0:14:54
epoch [18/30] batch [60/204] time 0.317 (0.335) data 0.000 (0.014) loss 0.3191 (0.3290) lr 3.9604e-03 eta 0:14:28
epoch [18/30] batch [80/204] time 0.318 (0.331) data 0.000 (0.010) loss 0.2695 (0.3111) lr 3.9604e-03 eta 0:14:11
epoch [18/30] batch [100/204] time 0.326 (0.328) data 0.000 (0.008) loss 0.5195 (0.3241) lr 3.9604e-03 eta 0:13:58
epoch [18/30] batch [120/204] time 0.326 (0.327) data 0.000 (0.007) loss 0.3188 (0.3369) lr 3.9604e-03 eta 0:13:49
epoch [18/30] batch [140/204] time 0.318 (0.326) data 0.000 (0.006) loss -0.0005 (0.3420) lr 3.9604e-03 eta 0:13:40
epoch [18/30] batch [160/204] time 0.313 (0.325) data 0.000 (0.005) loss 0.7505 (0.3395) lr 3.9604e-03 eta 0:13:30
epoch [18/30] batch [180/204] time 0.300 (0.323) data 0.000 (0.005) loss 0.6514 (0.3443) lr 3.9604e-03 eta 0:13:19
epoch [18/30] batch [200/204] time 0.301 (0.321) data 0.000 (0.004) loss 0.2488 (0.3354) lr 3.9604e-03 eta 0:13:06
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.64it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.31it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.98it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.20it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 655
* accuracy: 94.1%
* error: 5.9%
* macro_f1: 92.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [19/30] batch [20/204] time 0.319 (0.362) data 0.000 (0.042) loss 0.2466 (0.1892) lr 3.4549e-03 eta 0:14:39
epoch [19/30] batch [40/204] time 0.316 (0.340) data 0.000 (0.021) loss 0.0886 (0.2214) lr 3.4549e-03 eta 0:13:39
epoch [19/30] batch [60/204] time 0.331 (0.334) data 0.000 (0.014) loss 0.6826 (0.2318) lr 3.4549e-03 eta 0:13:17
epoch [19/30] batch [80/204] time 0.319 (0.330) data 0.000 (0.011) loss 0.0338 (0.2957) lr 3.4549e-03 eta 0:13:01
epoch [19/30] batch [100/204] time 0.325 (0.328) data 0.000 (0.009) loss 0.9302 (0.2941) lr 3.4549e-03 eta 0:12:49
epoch [19/30] batch [120/204] time 0.320 (0.326) data 0.000 (0.007) loss 0.4751 (0.3091) lr 3.4549e-03 eta 0:12:39
epoch [19/30] batch [140/204] time 0.312 (0.326) data 0.000 (0.006) loss 0.1243 (0.3183) lr 3.4549e-03 eta 0:12:32
epoch [19/30] batch [160/204] time 0.314 (0.325) data 0.000 (0.005) loss 0.1986 (0.3090) lr 3.4549e-03 eta 0:12:24
epoch [19/30] batch [180/204] time 0.296 (0.323) data 0.000 (0.005) loss 0.6152 (0.3082) lr 3.4549e-03 eta 0:12:13
epoch [19/30] batch [200/204] time 0.295 (0.321) data 0.000 (0.004) loss -0.0132 (0.2939) lr 3.4549e-03 eta 0:12:00
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:14,  2.46s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.11s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.47it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.10it/s] 71%|███████▏  | 5/7 [00:03<00:00,  2.75it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.38it/s]100%|██████████| 7/7 [00:03<00:00,  3.98it/s]100%|██████████| 7/7 [00:03<00:00,  1.96it/s]=> result
* total: 696
* correct: 654
* accuracy: 94.0%
* error: 6.0%
* macro_f1: 92.1%

epoch [20/30] batch [20/204] time 0.325 (0.365) data 0.000 (0.042) loss 0.0276 (0.5688) lr 2.9663e-03 eta 0:13:31
epoch [20/30] batch [40/204] time 0.308 (0.341) data 0.000 (0.021) loss 0.0327 (0.3919) lr 2.9663e-03 eta 0:12:32
epoch [20/30] batch [60/204] time 0.316 (0.334) data 0.000 (0.014) loss 0.5757 (0.3503) lr 2.9663e-03 eta 0:12:10
epoch [20/30] batch [80/204] time 0.316 (0.331) data 0.000 (0.011) loss 0.0456 (0.3427) lr 2.9663e-03 eta 0:11:55
epoch [20/30] batch [100/204] time 0.317 (0.328) data 0.000 (0.009) loss 2.2637 (0.3550) lr 2.9663e-03 eta 0:11:43
epoch [20/30] batch [120/204] time 0.317 (0.327) data 0.000 (0.007) loss 0.0076 (0.3459) lr 2.9663e-03 eta 0:11:33
epoch [20/30] batch [140/204] time 0.319 (0.326) data 0.000 (0.006) loss -0.0227 (0.3463) lr 2.9663e-03 eta 0:11:25
epoch [20/30] batch [160/204] time 0.318 (0.326) data 0.000 (0.005) loss 0.0656 (0.3308) lr 2.9663e-03 eta 0:11:18
epoch [20/30] batch [180/204] time 0.291 (0.324) data 0.000 (0.005) loss 0.8076 (0.3304) lr 2.9663e-03 eta 0:11:08
epoch [20/30] batch [200/204] time 0.293 (0.321) data 0.000 (0.004) loss 0.0026 (0.3094) lr 2.9663e-03 eta 0:10:57
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.18s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.63it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.29it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.96it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.59it/s]100%|██████████| 7/7 [00:03<00:00,  4.16it/s]100%|██████████| 7/7 [00:03<00:00,  2.13it/s]=> result
* total: 696
* correct: 657
* accuracy: 94.4%
* error: 5.6%
* macro_f1: 92.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model.pth.tar-20

epoch [21/30] batch [20/204] time 0.313 (0.366) data 0.000 (0.043) loss 0.6045 (0.2219) lr 2.5000e-03 eta 0:12:19
epoch [21/30] batch [40/204] time 0.314 (0.341) data 0.000 (0.022) loss 0.0206 (0.2576) lr 2.5000e-03 eta 0:11:22
epoch [21/30] batch [60/204] time 0.318 (0.333) data 0.000 (0.014) loss 0.4883 (0.2494) lr 2.5000e-03 eta 0:10:59
epoch [21/30] batch [80/204] time 0.318 (0.329) data 0.000 (0.011) loss 0.1626 (0.2558) lr 2.5000e-03 eta 0:10:45
epoch [21/30] batch [100/204] time 0.321 (0.327) data 0.000 (0.009) loss 0.0108 (0.2337) lr 2.5000e-03 eta 0:10:34
epoch [21/30] batch [120/204] time 0.329 (0.326) data 0.000 (0.007) loss 0.1620 (0.2622) lr 2.5000e-03 eta 0:10:25
epoch [21/30] batch [140/204] time 0.318 (0.324) data 0.000 (0.006) loss 0.7573 (0.2552) lr 2.5000e-03 eta 0:10:16
epoch [21/30] batch [160/204] time 0.321 (0.324) data 0.000 (0.006) loss 0.3232 (0.2559) lr 2.5000e-03 eta 0:10:08
epoch [21/30] batch [180/204] time 0.295 (0.323) data 0.000 (0.005) loss 0.0285 (0.2480) lr 2.5000e-03 eta 0:10:00
epoch [21/30] batch [200/204] time 0.297 (0.320) data 0.000 (0.005) loss 0.0137 (0.2478) lr 2.5000e-03 eta 0:09:49
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.31it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.95it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.58it/s]100%|██████████| 7/7 [00:03<00:00,  4.17it/s]100%|██████████| 7/7 [00:03<00:00,  2.15it/s]=> result
* total: 696
* correct: 664
* accuracy: 95.4%
* error: 4.6%
* macro_f1: 94.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

epoch [22/30] batch [20/204] time 0.316 (0.360) data 0.000 (0.040) loss 0.2690 (0.3131) lr 2.0611e-03 eta 0:10:54
epoch [22/30] batch [40/204] time 0.318 (0.340) data 0.000 (0.020) loss -0.0023 (0.2632) lr 2.0611e-03 eta 0:10:10
epoch [22/30] batch [60/204] time 0.321 (0.332) data 0.000 (0.014) loss 0.2395 (0.2409) lr 2.0611e-03 eta 0:09:50
epoch [22/30] batch [80/204] time 0.314 (0.329) data 0.000 (0.010) loss 0.0900 (0.2086) lr 2.0611e-03 eta 0:09:37
epoch [22/30] batch [100/204] time 0.322 (0.327) data 0.000 (0.008) loss 0.3313 (0.2120) lr 2.0611e-03 eta 0:09:26
epoch [22/30] batch [120/204] time 0.319 (0.326) data 0.000 (0.007) loss 0.3298 (0.2171) lr 2.0611e-03 eta 0:09:19
epoch [22/30] batch [140/204] time 0.320 (0.325) data 0.000 (0.006) loss 0.5352 (0.2351) lr 2.0611e-03 eta 0:09:11
epoch [22/30] batch [160/204] time 0.314 (0.324) data 0.000 (0.005) loss 0.2878 (0.2424) lr 2.0611e-03 eta 0:09:03
epoch [22/30] batch [180/204] time 0.294 (0.322) data 0.000 (0.005) loss 0.2159 (0.2510) lr 2.0611e-03 eta 0:08:53
epoch [22/30] batch [200/204] time 0.298 (0.320) data 0.000 (0.004) loss 0.3564 (0.2597) lr 2.0611e-03 eta 0:08:43
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.13s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.03it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.66it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.31it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.96it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.59it/s]100%|██████████| 7/7 [00:03<00:00,  4.10it/s]100%|██████████| 7/7 [00:03<00:00,  2.15it/s]=> result
* total: 696
* correct: 658
* accuracy: 94.5%
* error: 5.5%
* macro_f1: 93.1%

epoch [23/30] batch [20/204] time 0.315 (0.361) data 0.000 (0.041) loss 0.0302 (0.2048) lr 1.6543e-03 eta 0:09:42
epoch [23/30] batch [40/204] time 0.326 (0.340) data 0.000 (0.021) loss 0.1954 (0.2382) lr 1.6543e-03 eta 0:09:01
epoch [23/30] batch [60/204] time 0.310 (0.334) data 0.000 (0.014) loss 0.1276 (0.2757) lr 1.6543e-03 eta 0:08:44
epoch [23/30] batch [80/204] time 0.314 (0.330) data 0.000 (0.010) loss 0.1664 (0.3312) lr 1.6543e-03 eta 0:08:31
epoch [23/30] batch [100/204] time 0.321 (0.327) data 0.000 (0.008) loss 0.0991 (0.3469) lr 1.6543e-03 eta 0:08:21
epoch [23/30] batch [120/204] time 0.320 (0.326) data 0.000 (0.007) loss 0.0807 (0.3390) lr 1.6543e-03 eta 0:08:12
epoch [23/30] batch [140/204] time 0.313 (0.325) data 0.000 (0.006) loss 0.0220 (0.3049) lr 1.6543e-03 eta 0:08:05
epoch [23/30] batch [160/204] time 0.327 (0.324) data 0.000 (0.005) loss 0.0831 (0.3017) lr 1.6543e-03 eta 0:07:57
epoch [23/30] batch [180/204] time 0.296 (0.323) data 0.000 (0.005) loss 0.0174 (0.2928) lr 1.6543e-03 eta 0:07:48
epoch [23/30] batch [200/204] time 0.293 (0.320) data 0.000 (0.004) loss 0.1273 (0.2891) lr 1.6543e-03 eta 0:07:38
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.20s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.01s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.61it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.27it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.93it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.56it/s]100%|██████████| 7/7 [00:03<00:00,  4.16it/s]100%|██████████| 7/7 [00:03<00:00,  2.12it/s]=> result
* total: 696
* correct: 657
* accuracy: 94.4%
* error: 5.6%
* macro_f1: 93.0%

epoch [24/30] batch [20/204] time 0.318 (0.368) data 0.000 (0.042) loss 0.3997 (0.2511) lr 1.2843e-03 eta 0:08:37
epoch [24/30] batch [40/204] time 0.318 (0.344) data 0.000 (0.021) loss 0.0417 (0.2521) lr 1.2843e-03 eta 0:07:57
epoch [24/30] batch [60/204] time 0.315 (0.335) data 0.000 (0.014) loss 0.0208 (0.2715) lr 1.2843e-03 eta 0:07:38
epoch [24/30] batch [80/204] time 0.317 (0.331) data 0.000 (0.011) loss 0.0675 (0.2470) lr 1.2843e-03 eta 0:07:26
epoch [24/30] batch [100/204] time 0.325 (0.329) data 0.000 (0.009) loss -0.0227 (0.2405) lr 1.2843e-03 eta 0:07:17
epoch [24/30] batch [120/204] time 0.316 (0.327) data 0.000 (0.007) loss 0.0045 (0.2298) lr 1.2843e-03 eta 0:07:07
epoch [24/30] batch [140/204] time 0.326 (0.326) data 0.000 (0.006) loss -0.0132 (0.2343) lr 1.2843e-03 eta 0:06:59
epoch [24/30] batch [160/204] time 0.319 (0.326) data 0.000 (0.005) loss 0.3809 (0.2395) lr 1.2843e-03 eta 0:06:53
epoch [24/30] batch [180/204] time 0.293 (0.324) data 0.000 (0.005) loss 0.0562 (0.2404) lr 1.2843e-03 eta 0:06:44
epoch [24/30] batch [200/204] time 0.294 (0.321) data 0.000 (0.004) loss 0.1470 (0.2516) lr 1.2843e-03 eta 0:06:34
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.16s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.64it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.30it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.97it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.60it/s]100%|██████████| 7/7 [00:03<00:00,  4.19it/s]100%|██████████| 7/7 [00:03<00:00,  2.15it/s]=> result
* total: 696
* correct: 661
* accuracy: 95.0%
* error: 5.0%
* macro_f1: 93.7%

epoch [25/30] batch [20/204] time 0.327 (0.363) data 0.000 (0.042) loss 0.2561 (0.2950) lr 9.5492e-04 eta 0:07:16
epoch [25/30] batch [40/204] time 0.312 (0.341) data 0.000 (0.021) loss 0.0182 (0.2617) lr 9.5492e-04 eta 0:06:43
epoch [25/30] batch [60/204] time 0.316 (0.335) data 0.000 (0.014) loss 0.7798 (0.2582) lr 9.5492e-04 eta 0:06:30
epoch [25/30] batch [80/204] time 0.321 (0.332) data 0.000 (0.011) loss 0.2386 (0.2449) lr 9.5492e-04 eta 0:06:19
epoch [25/30] batch [100/204] time 0.317 (0.329) data 0.000 (0.009) loss 0.0719 (0.2555) lr 9.5492e-04 eta 0:06:10
epoch [25/30] batch [120/204] time 0.315 (0.327) data 0.000 (0.007) loss 0.0426 (0.2317) lr 9.5492e-04 eta 0:06:01
epoch [25/30] batch [140/204] time 0.321 (0.326) data 0.000 (0.006) loss 0.0449 (0.2226) lr 9.5492e-04 eta 0:05:53
epoch [25/30] batch [160/204] time 0.319 (0.325) data 0.000 (0.005) loss 0.0240 (0.2327) lr 9.5492e-04 eta 0:05:45
epoch [25/30] batch [180/204] time 0.294 (0.323) data 0.000 (0.005) loss 1.1143 (0.2378) lr 9.5492e-04 eta 0:05:37
epoch [25/30] batch [200/204] time 0.293 (0.320) data 0.000 (0.004) loss 0.0506 (0.2335) lr 9.5492e-04 eta 0:05:28
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.15s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.64it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.30it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.96it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.59it/s]100%|██████████| 7/7 [00:03<00:00,  4.18it/s]100%|██████████| 7/7 [00:03<00:00,  2.15it/s]=> result
* total: 696
* correct: 659
* accuracy: 94.7%
* error: 5.3%
* macro_f1: 93.2%

epoch [26/30] batch [20/204] time 0.327 (0.363) data 0.000 (0.039) loss 0.4043 (0.2339) lr 6.6987e-04 eta 0:06:02
epoch [26/30] batch [40/204] time 0.317 (0.341) data 0.000 (0.020) loss -0.0029 (0.2338) lr 6.6987e-04 eta 0:05:34
epoch [26/30] batch [60/204] time 0.421 (0.336) data 0.000 (0.013) loss 0.0605 (0.2273) lr 6.6987e-04 eta 0:05:22
epoch [26/30] batch [80/204] time 0.319 (0.332) data 0.000 (0.010) loss 0.0372 (0.2018) lr 6.6987e-04 eta 0:05:11
epoch [26/30] batch [100/204] time 0.309 (0.329) data 0.000 (0.008) loss 0.2441 (0.2281) lr 6.6987e-04 eta 0:05:02
epoch [26/30] batch [120/204] time 0.315 (0.327) data 0.000 (0.007) loss 1.0859 (0.2335) lr 6.6987e-04 eta 0:04:54
epoch [26/30] batch [140/204] time 0.313 (0.326) data 0.000 (0.006) loss 0.3982 (0.2628) lr 6.6987e-04 eta 0:04:46
epoch [26/30] batch [160/204] time 0.316 (0.325) data 0.000 (0.005) loss 0.4395 (0.2725) lr 6.6987e-04 eta 0:04:39
epoch [26/30] batch [180/204] time 0.289 (0.323) data 0.000 (0.005) loss 0.3330 (0.2577) lr 6.6987e-04 eta 0:04:31
epoch [26/30] batch [200/204] time 0.286 (0.320) data 0.000 (0.004) loss 0.0115 (0.2548) lr 6.6987e-04 eta 0:04:22
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.15s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.64it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.31it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.98it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.20it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 660
* accuracy: 94.8%
* error: 5.2%
* macro_f1: 93.3%

epoch [27/30] batch [20/204] time 0.317 (0.361) data 0.000 (0.040) loss 0.1323 (0.1824) lr 4.3227e-04 eta 0:04:47
epoch [27/30] batch [40/204] time 0.317 (0.339) data 0.000 (0.020) loss 0.0010 (0.2280) lr 4.3227e-04 eta 0:04:23
epoch [27/30] batch [60/204] time 0.317 (0.333) data 0.000 (0.013) loss 0.0809 (0.2588) lr 4.3227e-04 eta 0:04:11
epoch [27/30] batch [80/204] time 0.317 (0.329) data 0.000 (0.010) loss 0.0668 (0.2409) lr 4.3227e-04 eta 0:04:02
epoch [27/30] batch [100/204] time 0.315 (0.328) data 0.000 (0.008) loss 0.0412 (0.2401) lr 4.3227e-04 eta 0:03:54
epoch [27/30] batch [120/204] time 0.318 (0.326) data 0.000 (0.007) loss 1.3154 (0.2510) lr 4.3227e-04 eta 0:03:47
epoch [27/30] batch [140/204] time 0.311 (0.325) data 0.000 (0.006) loss 0.0612 (0.2404) lr 4.3227e-04 eta 0:03:39
epoch [27/30] batch [160/204] time 0.314 (0.325) data 0.000 (0.005) loss 0.1340 (0.2450) lr 4.3227e-04 eta 0:03:32
epoch [27/30] batch [180/204] time 0.296 (0.323) data 0.000 (0.005) loss 0.4172 (0.2486) lr 4.3227e-04 eta 0:03:25
epoch [27/30] batch [200/204] time 0.298 (0.321) data 0.000 (0.004) loss 0.0623 (0.2361) lr 4.3227e-04 eta 0:03:17
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.33s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.06s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.54it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.18it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.83it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.46it/s]100%|██████████| 7/7 [00:03<00:00,  4.07it/s]100%|██████████| 7/7 [00:03<00:00,  2.04it/s]=> result
* total: 696
* correct: 663
* accuracy: 95.3%
* error: 4.7%
* macro_f1: 93.9%

epoch [28/30] batch [20/204] time 0.314 (0.361) data 0.000 (0.039) loss 0.0210 (0.2716) lr 2.4472e-04 eta 0:03:33
epoch [28/30] batch [40/204] time 0.320 (0.340) data 0.000 (0.020) loss -0.0162 (0.2709) lr 2.4472e-04 eta 0:03:14
epoch [28/30] batch [60/204] time 0.317 (0.333) data 0.000 (0.013) loss 0.1444 (0.2520) lr 2.4472e-04 eta 0:03:03
epoch [28/30] batch [80/204] time 0.319 (0.329) data 0.000 (0.010) loss 0.1973 (0.2555) lr 2.4472e-04 eta 0:02:55
epoch [28/30] batch [100/204] time 0.322 (0.327) data 0.000 (0.008) loss 0.1077 (0.2323) lr 2.4472e-04 eta 0:02:47
epoch [28/30] batch [120/204] time 0.317 (0.327) data 0.000 (0.007) loss 0.3022 (0.2228) lr 2.4472e-04 eta 0:02:40
epoch [28/30] batch [140/204] time 0.323 (0.325) data 0.000 (0.006) loss 0.1598 (0.2289) lr 2.4472e-04 eta 0:02:33
epoch [28/30] batch [160/204] time 0.314 (0.324) data 0.000 (0.005) loss 0.1075 (0.2277) lr 2.4472e-04 eta 0:02:26
epoch [28/30] batch [180/204] time 0.297 (0.323) data 0.000 (0.005) loss 0.0215 (0.2379) lr 2.4472e-04 eta 0:02:19
epoch [28/30] batch [200/204] time 0.293 (0.320) data 0.000 (0.004) loss 0.5693 (0.2488) lr 2.4472e-04 eta 0:02:11
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.25s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.03s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.58it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.23it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.90it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.53it/s]100%|██████████| 7/7 [00:03<00:00,  4.13it/s]100%|██████████| 7/7 [00:03<00:00,  2.09it/s]=> result
* total: 696
* correct: 663
* accuracy: 95.3%
* error: 4.7%
* macro_f1: 93.8%

epoch [29/30] batch [20/204] time 0.315 (0.361) data 0.000 (0.041) loss 0.3262 (0.1763) lr 1.0926e-04 eta 0:02:19
epoch [29/30] batch [40/204] time 0.319 (0.343) data 0.000 (0.021) loss 0.4780 (0.1899) lr 1.0926e-04 eta 0:02:06
epoch [29/30] batch [60/204] time 0.313 (0.334) data 0.000 (0.014) loss 0.0258 (0.2138) lr 1.0926e-04 eta 0:01:56
epoch [29/30] batch [80/204] time 0.313 (0.331) data 0.000 (0.010) loss 0.6304 (0.2018) lr 1.0926e-04 eta 0:01:48
epoch [29/30] batch [100/204] time 0.320 (0.328) data 0.000 (0.008) loss 0.3047 (0.1870) lr 1.0926e-04 eta 0:01:41
epoch [29/30] batch [120/204] time 0.326 (0.327) data 0.000 (0.007) loss 0.2710 (0.2035) lr 1.0926e-04 eta 0:01:34
epoch [29/30] batch [140/204] time 0.321 (0.326) data 0.000 (0.006) loss 0.1810 (0.2163) lr 1.0926e-04 eta 0:01:27
epoch [29/30] batch [160/204] time 0.316 (0.325) data 0.000 (0.005) loss 4.0586 (0.2474) lr 1.0926e-04 eta 0:01:20
epoch [29/30] batch [180/204] time 0.295 (0.324) data 0.000 (0.005) loss 0.0296 (0.2475) lr 1.0926e-04 eta 0:01:13
epoch [29/30] batch [200/204] time 0.295 (0.321) data 0.000 (0.004) loss 0.1426 (0.2415) lr 1.0926e-04 eta 0:01:06
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.18s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.00it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.62it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.29it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.95it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.58it/s]100%|██████████| 7/7 [00:03<00:00,  4.18it/s]100%|██████████| 7/7 [00:03<00:00,  2.14it/s]=> result
* total: 696
* correct: 663
* accuracy: 95.3%
* error: 4.7%
* macro_f1: 93.8%

epoch [30/30] batch [20/204] time 0.320 (0.362) data 0.000 (0.039) loss 0.0990 (0.1647) lr 2.7391e-05 eta 0:01:06
epoch [30/30] batch [40/204] time 0.317 (0.339) data 0.000 (0.020) loss 0.3303 (0.1976) lr 2.7391e-05 eta 0:00:55
epoch [30/30] batch [60/204] time 0.311 (0.332) data 0.000 (0.013) loss 0.1763 (0.2065) lr 2.7391e-05 eta 0:00:47
epoch [30/30] batch [80/204] time 0.315 (0.328) data 0.000 (0.010) loss 0.0385 (0.2374) lr 2.7391e-05 eta 0:00:40
epoch [30/30] batch [100/204] time 0.311 (0.326) data 0.000 (0.008) loss 0.0081 (0.2355) lr 2.7391e-05 eta 0:00:33
epoch [30/30] batch [120/204] time 0.321 (0.325) data 0.000 (0.007) loss 0.0870 (0.2263) lr 2.7391e-05 eta 0:00:27
epoch [30/30] batch [140/204] time 0.320 (0.324) data 0.000 (0.006) loss 0.0043 (0.2241) lr 2.7391e-05 eta 0:00:20
epoch [30/30] batch [160/204] time 0.334 (0.324) data 0.000 (0.005) loss 0.2964 (0.2196) lr 2.7391e-05 eta 0:00:14
epoch [30/30] batch [180/204] time 0.294 (0.323) data 0.000 (0.005) loss 0.0845 (0.2183) lr 2.7391e-05 eta 0:00:07
epoch [30/30] batch [200/204] time 0.295 (0.320) data 0.000 (0.004) loss -0.0062 (0.2168) lr 2.7391e-05 eta 0:00:01
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.32it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.98it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.21it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]
=> result
* total: 696
* correct: 663
* accuracy: 95.3%
* error: 4.7%
* macro_f1: 93.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar" (epoch = 21)
Evaluate on the *test* set
  0%|          | 0/11 [00:00<?, ?it/s]  9%|▉         | 1/11 [00:02<00:27,  2.75s/it] 18%|█▊        | 2/11 [00:02<00:11,  1.23s/it] 27%|██▋       | 3/11 [00:03<00:05,  1.35it/s] 36%|███▋      | 4/11 [00:03<00:03,  1.94it/s] 45%|████▌     | 5/11 [00:03<00:02,  2.57it/s] 55%|█████▍    | 6/11 [00:03<00:01,  3.20it/s] 64%|██████▎   | 7/11 [00:03<00:01,  3.79it/s] 73%|███████▎  | 8/11 [00:03<00:00,  4.31it/s] 82%|████████▏ | 9/11 [00:04<00:00,  4.75it/s] 91%|█████████ | 10/11 [00:04<00:00,  5.10it/s]100%|██████████| 11/11 [00:04<00:00,  5.97it/s]100%|██████████| 11/11 [00:04<00:00,  2.47it/s]
=> result
* total: 1,053
* correct: 1,018
* accuracy: 96.7%
* error: 3.3%
* macro_f1: 96.5%
Elapsed: 0:34:31
+ sh scripts/rpo_prime/base2new_test_sdl.sh oxford_flowers 1 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
816 937 1410
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      937
# test     1,410
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar" (epoch = 21)
Evaluate on the *test* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:04<01:08,  4.90s/it] 13%|█▎        | 2/15 [00:05<00:27,  2.11s/it] 20%|██        | 3/15 [00:05<00:14,  1.22s/it] 27%|██▋       | 4/15 [00:05<00:08,  1.24it/s] 33%|███▎      | 5/15 [00:05<00:05,  1.74it/s] 40%|████      | 6/15 [00:05<00:03,  2.30it/s] 47%|████▋     | 7/15 [00:05<00:02,  2.89it/s] 53%|█████▎    | 8/15 [00:06<00:02,  3.47it/s] 60%|██████    | 9/15 [00:06<00:01,  4.01it/s] 67%|██████▋   | 10/15 [00:06<00:01,  4.48it/s] 73%|███████▎  | 11/15 [00:06<00:00,  4.88it/s] 80%|████████  | 12/15 [00:06<00:00,  5.20it/s] 87%|████████▋ | 13/15 [00:06<00:00,  5.44it/s] 93%|█████████▎| 14/15 [00:07<00:00,  5.63it/s]100%|██████████| 15/15 [00:07<00:00,  2.09it/s]
=> result
* total: 1,410
* correct: 1,075
* accuracy: 76.2%
* error: 23.8%
* macro_f1: 70.8%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh oxford_flowers 2 0 main_tmp1_0.1sdl 16
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
816 696 1053
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      696
# test     1,053
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/30] batch [20/204] time 0.317 (0.456) data 0.000 (0.063) loss 0.5435 (2.0656) lr 1.0000e-02 eta 0:46:20
epoch [1/30] batch [40/204] time 0.327 (0.388) data 0.000 (0.032) loss 0.7559 (1.5969) lr 1.0000e-02 eta 0:39:17
epoch [1/30] batch [60/204] time 0.319 (0.365) data 0.000 (0.021) loss 1.3564 (1.4950) lr 1.0000e-02 eta 0:36:50
epoch [1/30] batch [80/204] time 0.316 (0.353) data 0.000 (0.016) loss 0.4285 (1.4348) lr 1.0000e-02 eta 0:35:33
epoch [1/30] batch [100/204] time 0.313 (0.346) data 0.000 (0.013) loss 0.8530 (1.4160) lr 1.0000e-02 eta 0:34:43
epoch [1/30] batch [120/204] time 0.313 (0.341) data 0.000 (0.011) loss 0.9473 (1.3910) lr 1.0000e-02 eta 0:34:07
epoch [1/30] batch [140/204] time 0.322 (0.338) data 0.000 (0.009) loss 0.8828 (1.3617) lr 1.0000e-02 eta 0:33:41
epoch [1/30] batch [160/204] time 0.313 (0.336) data 0.000 (0.008) loss 1.7930 (1.3558) lr 1.0000e-02 eta 0:33:19
epoch [1/30] batch [180/204] time 0.294 (0.333) data 0.000 (0.007) loss 0.4114 (1.3259) lr 1.0000e-02 eta 0:32:58
epoch [1/30] batch [200/204] time 0.294 (0.329) data 0.000 (0.007) loss 1.3887 (1.2945) lr 1.0000e-02 eta 0:32:30
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.32s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.05s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.53it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.18it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.83it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.47it/s]100%|██████████| 7/7 [00:03<00:00,  4.07it/s]100%|██████████| 7/7 [00:03<00:00,  2.04it/s]=> result
* total: 696
* correct: 509
* accuracy: 73.1%
* error: 26.9%
* macro_f1: 68.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [2/30] batch [20/204] time 0.316 (0.359) data 0.000 (0.040) loss 2.0234 (1.0314) lr 9.9726e-03 eta 0:35:17
epoch [2/30] batch [40/204] time 0.316 (0.338) data 0.000 (0.020) loss 0.2888 (1.0013) lr 9.9726e-03 eta 0:33:05
epoch [2/30] batch [60/204] time 0.327 (0.332) data 0.000 (0.014) loss 0.8037 (0.9459) lr 9.9726e-03 eta 0:32:23
epoch [2/30] batch [80/204] time 0.312 (0.328) data 0.000 (0.010) loss 1.9443 (0.9493) lr 9.9726e-03 eta 0:31:55
epoch [2/30] batch [100/204] time 0.320 (0.327) data 0.000 (0.008) loss 0.4214 (1.0081) lr 9.9726e-03 eta 0:31:40
epoch [2/30] batch [120/204] time 0.324 (0.325) data 0.000 (0.007) loss 1.1094 (1.0162) lr 9.9726e-03 eta 0:31:25
epoch [2/30] batch [140/204] time 0.308 (0.324) data 0.000 (0.006) loss 0.6904 (1.0107) lr 9.9726e-03 eta 0:31:11
epoch [2/30] batch [160/204] time 0.314 (0.323) data 0.000 (0.005) loss 0.5679 (1.0374) lr 9.9726e-03 eta 0:30:59
epoch [2/30] batch [180/204] time 0.292 (0.321) data 0.000 (0.005) loss 0.9360 (1.0059) lr 9.9726e-03 eta 0:30:43
epoch [2/30] batch [200/204] time 0.291 (0.319) data 0.000 (0.004) loss 0.5215 (1.0129) lr 9.9726e-03 eta 0:30:22
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.15s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.64it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.30it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.96it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.59it/s]100%|██████████| 7/7 [00:03<00:00,  4.18it/s]100%|██████████| 7/7 [00:03<00:00,  2.15it/s]=> result
* total: 696
* correct: 523
* accuracy: 75.1%
* error: 24.9%
* macro_f1: 70.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [3/30] batch [20/204] time 0.321 (0.362) data 0.000 (0.043) loss 2.5195 (1.0083) lr 9.8907e-03 eta 0:34:17
epoch [3/30] batch [40/204] time 0.319 (0.340) data 0.000 (0.022) loss 0.6914 (1.0678) lr 9.8907e-03 eta 0:32:08
epoch [3/30] batch [60/204] time 0.312 (0.332) data 0.000 (0.014) loss 1.2109 (0.9842) lr 9.8907e-03 eta 0:31:16
epoch [3/30] batch [80/204] time 0.315 (0.328) data 0.000 (0.011) loss 0.3850 (0.9689) lr 9.8907e-03 eta 0:30:49
epoch [3/30] batch [100/204] time 0.327 (0.326) data 0.000 (0.009) loss 1.9688 (0.9719) lr 9.8907e-03 eta 0:30:29
epoch [3/30] batch [120/204] time 0.316 (0.325) data 0.000 (0.007) loss 1.4971 (0.9577) lr 9.8907e-03 eta 0:30:14
epoch [3/30] batch [140/204] time 0.317 (0.323) data 0.000 (0.006) loss 0.2537 (0.9535) lr 9.8907e-03 eta 0:30:02
epoch [3/30] batch [160/204] time 0.317 (0.323) data 0.000 (0.006) loss 0.6616 (0.9421) lr 9.8907e-03 eta 0:29:52
epoch [3/30] batch [180/204] time 0.297 (0.321) data 0.000 (0.005) loss 0.9355 (0.9412) lr 9.8907e-03 eta 0:29:37
epoch [3/30] batch [200/204] time 0.294 (0.319) data 0.000 (0.005) loss 2.1602 (0.9338) lr 9.8907e-03 eta 0:29:16
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.13s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.30it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.97it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.58it/s]100%|██████████| 7/7 [00:03<00:00,  4.18it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 536
* accuracy: 77.0%
* error: 23.0%
* macro_f1: 73.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [4/30] batch [20/204] time 0.314 (0.367) data 0.000 (0.041) loss 1.3545 (1.0288) lr 9.7553e-03 eta 0:33:34
epoch [4/30] batch [40/204] time 0.309 (0.343) data 0.000 (0.021) loss 1.1709 (0.8815) lr 9.7553e-03 eta 0:31:13
epoch [4/30] batch [60/204] time 0.315 (0.335) data 0.000 (0.014) loss 1.6738 (0.9388) lr 9.7553e-03 eta 0:30:22
epoch [4/30] batch [80/204] time 0.317 (0.331) data 0.000 (0.010) loss 1.1045 (0.9322) lr 9.7553e-03 eta 0:29:54
epoch [4/30] batch [100/204] time 0.318 (0.329) data 0.000 (0.008) loss 0.1736 (0.8900) lr 9.7553e-03 eta 0:29:36
epoch [4/30] batch [120/204] time 0.317 (0.327) data 0.000 (0.007) loss 0.0966 (0.8499) lr 9.7553e-03 eta 0:29:22
epoch [4/30] batch [140/204] time 0.397 (0.326) data 0.000 (0.006) loss 1.1260 (0.8628) lr 9.7553e-03 eta 0:29:12
epoch [4/30] batch [160/204] time 0.319 (0.325) data 0.000 (0.005) loss 1.3955 (0.8577) lr 9.7553e-03 eta 0:28:59
epoch [4/30] batch [180/204] time 0.294 (0.324) data 0.000 (0.005) loss 0.6328 (0.8572) lr 9.7553e-03 eta 0:28:43
epoch [4/30] batch [200/204] time 0.292 (0.321) data 0.000 (0.004) loss 1.3086 (0.8553) lr 9.7553e-03 eta 0:28:21
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.32it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.98it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.60it/s]100%|██████████| 7/7 [00:03<00:00,  4.19it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 554
* accuracy: 79.6%
* error: 20.4%
* macro_f1: 76.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [5/30] batch [20/204] time 0.311 (0.361) data 0.000 (0.043) loss 0.0151 (0.7167) lr 9.5677e-03 eta 0:31:45
epoch [5/30] batch [40/204] time 0.312 (0.342) data 0.000 (0.022) loss 0.1943 (0.7025) lr 9.5677e-03 eta 0:29:58
epoch [5/30] batch [60/204] time 0.317 (0.333) data 0.000 (0.014) loss 0.9385 (0.7552) lr 9.5677e-03 eta 0:29:05
epoch [5/30] batch [80/204] time 0.313 (0.329) data 0.000 (0.011) loss 1.6084 (0.7811) lr 9.5677e-03 eta 0:28:37
epoch [5/30] batch [100/204] time 0.322 (0.326) data 0.000 (0.009) loss 1.3760 (0.7917) lr 9.5677e-03 eta 0:28:18
epoch [5/30] batch [120/204] time 0.325 (0.325) data 0.000 (0.007) loss 0.4082 (0.7646) lr 9.5677e-03 eta 0:28:06
epoch [5/30] batch [140/204] time 0.317 (0.324) data 0.000 (0.006) loss 1.1768 (0.7596) lr 9.5677e-03 eta 0:27:54
epoch [5/30] batch [160/204] time 0.315 (0.323) data 0.000 (0.006) loss 1.5498 (0.7883) lr 9.5677e-03 eta 0:27:42
epoch [5/30] batch [180/204] time 0.292 (0.322) data 0.000 (0.005) loss 0.3643 (0.7807) lr 9.5677e-03 eta 0:27:27
epoch [5/30] batch [200/204] time 0.293 (0.319) data 0.000 (0.005) loss 0.2861 (0.7637) lr 9.5677e-03 eta 0:27:07
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.13s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.31it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.98it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.20it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 576
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 79.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [6/30] batch [20/204] time 0.323 (0.362) data 0.000 (0.040) loss 2.0762 (0.9308) lr 9.3301e-03 eta 0:30:36
epoch [6/30] batch [40/204] time 0.306 (0.337) data 0.000 (0.020) loss 1.3340 (0.7618) lr 9.3301e-03 eta 0:28:22
epoch [6/30] batch [60/204] time 0.317 (0.333) data 0.000 (0.013) loss 0.3252 (0.7157) lr 9.3301e-03 eta 0:27:55
epoch [6/30] batch [80/204] time 0.319 (0.329) data 0.000 (0.010) loss 0.2568 (0.7175) lr 9.3301e-03 eta 0:27:30
epoch [6/30] batch [100/204] time 0.317 (0.326) data 0.000 (0.008) loss 1.6992 (0.7261) lr 9.3301e-03 eta 0:27:12
epoch [6/30] batch [120/204] time 0.316 (0.325) data 0.000 (0.007) loss 0.5747 (0.7278) lr 9.3301e-03 eta 0:26:58
epoch [6/30] batch [140/204] time 0.317 (0.324) data 0.000 (0.006) loss 0.8730 (0.7364) lr 9.3301e-03 eta 0:26:47
epoch [6/30] batch [160/204] time 0.322 (0.323) data 0.000 (0.005) loss 0.9648 (0.7171) lr 9.3301e-03 eta 0:26:37
epoch [6/30] batch [180/204] time 0.292 (0.322) data 0.000 (0.005) loss 1.7090 (0.7209) lr 9.3301e-03 eta 0:26:22
epoch [6/30] batch [200/204] time 0.292 (0.319) data 0.000 (0.004) loss 0.3582 (0.7135) lr 9.3301e-03 eta 0:26:05
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.20s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.01s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.61it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.27it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.93it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.56it/s]100%|██████████| 7/7 [00:03<00:00,  4.16it/s]100%|██████████| 7/7 [00:03<00:00,  2.12it/s]=> result
* total: 696
* correct: 591
* accuracy: 84.9%
* error: 15.1%
* macro_f1: 81.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [7/30] batch [20/204] time 0.310 (0.361) data 0.000 (0.039) loss 0.6880 (0.6880) lr 9.0451e-03 eta 0:29:20
epoch [7/30] batch [40/204] time 0.323 (0.340) data 0.000 (0.020) loss 0.0118 (0.6717) lr 9.0451e-03 eta 0:27:28
epoch [7/30] batch [60/204] time 0.323 (0.332) data 0.000 (0.013) loss 1.5264 (0.6277) lr 9.0451e-03 eta 0:26:46
epoch [7/30] batch [80/204] time 0.319 (0.329) data 0.000 (0.010) loss 0.7036 (0.6250) lr 9.0451e-03 eta 0:26:23
epoch [7/30] batch [100/204] time 0.315 (0.327) data 0.000 (0.008) loss 1.2148 (0.6372) lr 9.0451e-03 eta 0:26:09
epoch [7/30] batch [120/204] time 0.316 (0.326) data 0.000 (0.007) loss 1.1318 (0.6393) lr 9.0451e-03 eta 0:25:54
epoch [7/30] batch [140/204] time 0.328 (0.325) data 0.000 (0.006) loss 0.7925 (0.6262) lr 9.0451e-03 eta 0:25:43
epoch [7/30] batch [160/204] time 0.314 (0.324) data 0.000 (0.005) loss 0.1595 (0.6284) lr 9.0451e-03 eta 0:25:34
epoch [7/30] batch [180/204] time 0.293 (0.322) data 0.000 (0.005) loss 0.6670 (0.6185) lr 9.0451e-03 eta 0:25:18
epoch [7/30] batch [200/204] time 0.293 (0.319) data 0.000 (0.004) loss 0.7490 (0.6015) lr 9.0451e-03 eta 0:24:58
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.18s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.63it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.29it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.97it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.59it/s]100%|██████████| 7/7 [00:03<00:00,  4.19it/s]100%|██████████| 7/7 [00:03<00:00,  2.14it/s]=> result
* total: 696
* correct: 588
* accuracy: 84.5%
* error: 15.5%
* macro_f1: 80.9%

epoch [8/30] batch [20/204] time 0.322 (0.361) data 0.000 (0.038) loss 0.6240 (0.5916) lr 8.7157e-03 eta 0:28:04
epoch [8/30] batch [40/204] time 0.318 (0.341) data 0.000 (0.019) loss 0.1561 (0.6516) lr 8.7157e-03 eta 0:26:24
epoch [8/30] batch [60/204] time 0.320 (0.333) data 0.000 (0.013) loss 0.5010 (0.6330) lr 8.7157e-03 eta 0:25:42
epoch [8/30] batch [80/204] time 0.325 (0.329) data 0.000 (0.010) loss 0.2113 (0.6367) lr 8.7157e-03 eta 0:25:16
epoch [8/30] batch [100/204] time 0.315 (0.327) data 0.000 (0.008) loss 0.9111 (0.6171) lr 8.7157e-03 eta 0:25:00
epoch [8/30] batch [120/204] time 0.318 (0.326) data 0.000 (0.007) loss 0.0133 (0.6137) lr 8.7157e-03 eta 0:24:51
epoch [8/30] batch [140/204] time 0.328 (0.325) data 0.000 (0.006) loss 0.5938 (0.6031) lr 8.7157e-03 eta 0:24:38
epoch [8/30] batch [160/204] time 0.316 (0.324) data 0.000 (0.005) loss 1.6953 (0.6039) lr 8.7157e-03 eta 0:24:28
epoch [8/30] batch [180/204] time 0.295 (0.322) data 0.000 (0.004) loss 0.1859 (0.6001) lr 8.7157e-03 eta 0:24:14
epoch [8/30] batch [200/204] time 0.294 (0.320) data 0.000 (0.004) loss 0.0950 (0.5924) lr 8.7157e-03 eta 0:23:55
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.23s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.02s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.59it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.25it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.91it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.54it/s]100%|██████████| 7/7 [00:03<00:00,  4.14it/s]100%|██████████| 7/7 [00:03<00:00,  2.10it/s]=> result
* total: 696
* correct: 589
* accuracy: 84.6%
* error: 15.4%
* macro_f1: 83.0%

epoch [9/30] batch [20/204] time 0.322 (0.362) data 0.000 (0.042) loss 1.0801 (0.6506) lr 8.3457e-03 eta 0:26:55
epoch [9/30] batch [40/204] time 0.320 (0.343) data 0.000 (0.021) loss 0.2788 (0.6078) lr 8.3457e-03 eta 0:25:23
epoch [9/30] batch [60/204] time 0.316 (0.334) data 0.000 (0.014) loss 0.6968 (0.5576) lr 8.3457e-03 eta 0:24:36
epoch [9/30] batch [80/204] time 0.316 (0.330) data 0.000 (0.011) loss 0.8560 (0.5882) lr 8.3457e-03 eta 0:24:12
epoch [9/30] batch [100/204] time 0.316 (0.327) data 0.000 (0.009) loss 0.4392 (0.5744) lr 8.3457e-03 eta 0:23:56
epoch [9/30] batch [120/204] time 0.325 (0.326) data 0.000 (0.007) loss 0.5181 (0.5387) lr 8.3457e-03 eta 0:23:41
epoch [9/30] batch [140/204] time 0.316 (0.325) data 0.000 (0.006) loss 0.6904 (0.5332) lr 8.3457e-03 eta 0:23:31
epoch [9/30] batch [160/204] time 0.316 (0.324) data 0.000 (0.005) loss 0.3843 (0.5536) lr 8.3457e-03 eta 0:23:21
epoch [9/30] batch [180/204] time 0.298 (0.322) data 0.000 (0.005) loss 1.0635 (0.5746) lr 8.3457e-03 eta 0:23:08
epoch [9/30] batch [200/204] time 0.293 (0.319) data 0.000 (0.004) loss 0.5391 (0.5667) lr 8.3457e-03 eta 0:22:49
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.13s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.32it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.99it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.59it/s]100%|██████████| 7/7 [00:03<00:00,  4.18it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 614
* accuracy: 88.2%
* error: 11.8%
* macro_f1: 86.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [10/30] batch [20/204] time 0.318 (0.361) data 0.000 (0.041) loss 0.8418 (0.4421) lr 7.9389e-03 eta 0:25:37
epoch [10/30] batch [40/204] time 0.320 (0.338) data 0.000 (0.021) loss 0.4172 (0.4775) lr 7.9389e-03 eta 0:23:56
epoch [10/30] batch [60/204] time 0.314 (0.331) data 0.000 (0.014) loss 0.9150 (0.4939) lr 7.9389e-03 eta 0:23:19
epoch [10/30] batch [80/204] time 0.314 (0.328) data 0.000 (0.010) loss 1.7246 (0.5483) lr 7.9389e-03 eta 0:22:58
epoch [10/30] batch [100/204] time 0.315 (0.326) data 0.000 (0.008) loss 0.0456 (0.5361) lr 7.9389e-03 eta 0:22:43
epoch [10/30] batch [120/204] time 0.317 (0.325) data 0.000 (0.007) loss 0.1059 (0.5217) lr 7.9389e-03 eta 0:22:32
epoch [10/30] batch [140/204] time 0.316 (0.323) data 0.000 (0.006) loss 0.6943 (0.5008) lr 7.9389e-03 eta 0:22:20
epoch [10/30] batch [160/204] time 0.321 (0.323) data 0.000 (0.005) loss 0.4778 (0.4992) lr 7.9389e-03 eta 0:22:11
epoch [10/30] batch [180/204] time 0.297 (0.321) data 0.000 (0.005) loss 0.9019 (0.4807) lr 7.9389e-03 eta 0:21:58
epoch [10/30] batch [200/204] time 0.295 (0.319) data 0.000 (0.004) loss 0.8364 (0.4861) lr 7.9389e-03 eta 0:21:41
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.31it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.98it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.20it/s]100%|██████████| 7/7 [00:03<00:00,  2.17it/s]=> result
* total: 696
* correct: 624
* accuracy: 89.7%
* error: 10.3%
* macro_f1: 87.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-10

epoch [11/30] batch [20/204] time 0.320 (0.361) data 0.000 (0.040) loss 0.2119 (0.3792) lr 7.5000e-03 eta 0:24:26
epoch [11/30] batch [40/204] time 0.318 (0.339) data 0.000 (0.020) loss 0.7661 (0.4094) lr 7.5000e-03 eta 0:22:50
epoch [11/30] batch [60/204] time 0.314 (0.332) data 0.000 (0.014) loss 0.1306 (0.4782) lr 7.5000e-03 eta 0:22:14
epoch [11/30] batch [80/204] time 0.418 (0.329) data 0.000 (0.010) loss 0.6504 (0.4804) lr 7.5000e-03 eta 0:21:57
epoch [11/30] batch [100/204] time 0.318 (0.327) data 0.000 (0.008) loss 0.8267 (0.5135) lr 7.5000e-03 eta 0:21:42
epoch [11/30] batch [120/204] time 0.327 (0.326) data 0.000 (0.007) loss 0.7119 (0.5557) lr 7.5000e-03 eta 0:21:30
epoch [11/30] batch [140/204] time 0.316 (0.325) data 0.000 (0.006) loss 0.2900 (0.5414) lr 7.5000e-03 eta 0:21:20
epoch [11/30] batch [160/204] time 0.318 (0.324) data 0.000 (0.005) loss 0.2581 (0.5320) lr 7.5000e-03 eta 0:21:10
epoch [11/30] batch [180/204] time 0.294 (0.323) data 0.000 (0.005) loss 0.4983 (0.5338) lr 7.5000e-03 eta 0:20:58
epoch [11/30] batch [200/204] time 0.294 (0.320) data 0.000 (0.004) loss 0.1353 (0.5190) lr 7.5000e-03 eta 0:20:41
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.31it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.98it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.20it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 633
* accuracy: 90.9%
* error: 9.1%
* macro_f1: 89.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [12/30] batch [20/204] time 0.317 (0.358) data 0.000 (0.041) loss 0.0708 (0.4632) lr 7.0337e-03 eta 0:23:02
epoch [12/30] batch [40/204] time 0.320 (0.337) data 0.000 (0.021) loss 0.1232 (0.4941) lr 7.0337e-03 eta 0:21:32
epoch [12/30] batch [60/204] time 0.315 (0.329) data 0.000 (0.014) loss 0.6558 (0.4259) lr 7.0337e-03 eta 0:20:56
epoch [12/30] batch [80/204] time 0.320 (0.326) data 0.000 (0.010) loss 0.2617 (0.4031) lr 7.0337e-03 eta 0:20:37
epoch [12/30] batch [100/204] time 0.325 (0.324) data 0.000 (0.008) loss -0.0227 (0.4282) lr 7.0337e-03 eta 0:20:24
epoch [12/30] batch [120/204] time 0.313 (0.323) data 0.000 (0.007) loss 0.5068 (0.4106) lr 7.0337e-03 eta 0:20:12
epoch [12/30] batch [140/204] time 0.319 (0.322) data 0.000 (0.006) loss 0.9165 (0.4211) lr 7.0337e-03 eta 0:20:02
epoch [12/30] batch [160/204] time 0.314 (0.321) data 0.000 (0.005) loss 0.0919 (0.4365) lr 7.0337e-03 eta 0:19:53
epoch [12/30] batch [180/204] time 0.291 (0.320) data 0.000 (0.005) loss 0.2998 (0.4191) lr 7.0337e-03 eta 0:19:42
epoch [12/30] batch [200/204] time 0.392 (0.318) data 0.000 (0.004) loss 0.0435 (0.4353) lr 7.0337e-03 eta 0:19:27
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.32it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.99it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.62it/s]100%|██████████| 7/7 [00:03<00:00,  4.21it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 628
* accuracy: 90.2%
* error: 9.8%
* macro_f1: 88.8%

epoch [13/30] batch [20/204] time 0.323 (0.362) data 0.000 (0.043) loss 0.0321 (0.3254) lr 6.5451e-03 eta 0:22:03
epoch [13/30] batch [40/204] time 0.315 (0.338) data 0.000 (0.021) loss 0.0553 (0.2912) lr 6.5451e-03 eta 0:20:29
epoch [13/30] batch [60/204] time 0.320 (0.332) data 0.000 (0.014) loss 0.3171 (0.3208) lr 6.5451e-03 eta 0:19:59
epoch [13/30] batch [80/204] time 0.318 (0.329) data 0.000 (0.011) loss 0.2488 (0.3866) lr 6.5451e-03 eta 0:19:40
epoch [13/30] batch [100/204] time 0.310 (0.326) data 0.000 (0.009) loss 1.2217 (0.4115) lr 6.5451e-03 eta 0:19:25
epoch [13/30] batch [120/204] time 0.324 (0.325) data 0.000 (0.007) loss -0.0027 (0.3951) lr 6.5451e-03 eta 0:19:13
epoch [13/30] batch [140/204] time 0.318 (0.324) data 0.000 (0.006) loss 0.2510 (0.3854) lr 6.5451e-03 eta 0:19:05
epoch [13/30] batch [160/204] time 0.314 (0.324) data 0.000 (0.006) loss 0.6362 (0.4058) lr 6.5451e-03 eta 0:18:56
epoch [13/30] batch [180/204] time 0.294 (0.322) data 0.000 (0.005) loss 0.1689 (0.3955) lr 6.5451e-03 eta 0:18:44
epoch [13/30] batch [200/204] time 0.301 (0.319) data 0.000 (0.005) loss 0.4331 (0.4017) lr 6.5451e-03 eta 0:18:28
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.13s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.32it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.94it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.57it/s]100%|██████████| 7/7 [00:03<00:00,  4.16it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 647
* accuracy: 93.0%
* error: 7.0%
* macro_f1: 91.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [14/30] batch [20/204] time 0.314 (0.364) data 0.000 (0.040) loss 0.1248 (0.2716) lr 6.0396e-03 eta 0:20:56
epoch [14/30] batch [40/204] time 0.317 (0.341) data 0.000 (0.020) loss 0.1100 (0.2806) lr 6.0396e-03 eta 0:19:27
epoch [14/30] batch [60/204] time 0.320 (0.333) data 0.000 (0.013) loss 0.1639 (0.2791) lr 6.0396e-03 eta 0:18:53
epoch [14/30] batch [80/204] time 0.329 (0.328) data 0.000 (0.010) loss 0.4788 (0.2872) lr 6.0396e-03 eta 0:18:32
epoch [14/30] batch [100/204] time 0.323 (0.326) data 0.000 (0.008) loss 1.0088 (0.3523) lr 6.0396e-03 eta 0:18:17
epoch [14/30] batch [120/204] time 0.319 (0.325) data 0.000 (0.007) loss 0.0976 (0.3629) lr 6.0396e-03 eta 0:18:06
epoch [14/30] batch [140/204] time 0.314 (0.324) data 0.000 (0.006) loss 0.5088 (0.4002) lr 6.0396e-03 eta 0:17:57
epoch [14/30] batch [160/204] time 0.314 (0.322) data 0.000 (0.005) loss 1.0146 (0.4020) lr 6.0396e-03 eta 0:17:46
epoch [14/30] batch [180/204] time 0.301 (0.321) data 0.000 (0.005) loss 0.3542 (0.3925) lr 6.0396e-03 eta 0:17:34
epoch [14/30] batch [200/204] time 0.294 (0.318) data 0.000 (0.004) loss 0.0520 (0.3824) lr 6.0396e-03 eta 0:17:19
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.32it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.99it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.60it/s]100%|██████████| 7/7 [00:03<00:00,  4.19it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 642
* accuracy: 92.2%
* error: 7.8%
* macro_f1: 90.2%

epoch [15/30] batch [20/204] time 0.330 (0.360) data 0.000 (0.040) loss 0.1202 (0.2253) lr 5.5226e-03 eta 0:19:26
epoch [15/30] batch [40/204] time 0.319 (0.338) data 0.000 (0.020) loss 0.0914 (0.2049) lr 5.5226e-03 eta 0:18:09
epoch [15/30] batch [60/204] time 0.315 (0.333) data 0.000 (0.013) loss 0.0348 (0.2406) lr 5.5226e-03 eta 0:17:46
epoch [15/30] batch [80/204] time 0.320 (0.329) data 0.000 (0.010) loss 0.0426 (0.2739) lr 5.5226e-03 eta 0:17:28
epoch [15/30] batch [100/204] time 0.311 (0.327) data 0.000 (0.008) loss 1.0850 (0.2792) lr 5.5226e-03 eta 0:17:14
epoch [15/30] batch [120/204] time 0.317 (0.325) data 0.000 (0.007) loss 0.2654 (0.2796) lr 5.5226e-03 eta 0:17:03
epoch [15/30] batch [140/204] time 0.318 (0.325) data 0.000 (0.006) loss 0.2751 (0.2997) lr 5.5226e-03 eta 0:16:53
epoch [15/30] batch [160/204] time 0.316 (0.324) data 0.000 (0.005) loss 0.0447 (0.3283) lr 5.5226e-03 eta 0:16:45
epoch [15/30] batch [180/204] time 0.293 (0.323) data 0.000 (0.005) loss 0.9507 (0.3268) lr 5.5226e-03 eta 0:16:34
epoch [15/30] batch [200/204] time 0.294 (0.320) data 0.000 (0.004) loss 1.7090 (0.3402) lr 5.5226e-03 eta 0:16:19
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.16s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.64it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.30it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.97it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.60it/s]100%|██████████| 7/7 [00:03<00:00,  4.19it/s]100%|██████████| 7/7 [00:03<00:00,  2.15it/s]=> result
* total: 696
* correct: 649
* accuracy: 93.2%
* error: 6.8%
* macro_f1: 92.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [16/30] batch [20/204] time 0.314 (0.364) data 0.000 (0.044) loss 0.0717 (0.2561) lr 5.0000e-03 eta 0:18:25
epoch [16/30] batch [40/204] time 0.320 (0.340) data 0.000 (0.022) loss -0.0038 (0.2745) lr 5.0000e-03 eta 0:17:07
epoch [16/30] batch [60/204] time 0.321 (0.333) data 0.000 (0.015) loss 0.1968 (0.2714) lr 5.0000e-03 eta 0:16:38
epoch [16/30] batch [80/204] time 0.315 (0.330) data 0.000 (0.011) loss 0.0427 (0.2766) lr 5.0000e-03 eta 0:16:24
epoch [16/30] batch [100/204] time 0.318 (0.328) data 0.000 (0.009) loss 1.1016 (0.3091) lr 5.0000e-03 eta 0:16:10
epoch [16/30] batch [120/204] time 0.319 (0.326) data 0.000 (0.008) loss 0.4932 (0.3279) lr 5.0000e-03 eta 0:15:58
epoch [16/30] batch [140/204] time 0.312 (0.325) data 0.000 (0.007) loss 0.0298 (0.3370) lr 5.0000e-03 eta 0:15:48
epoch [16/30] batch [160/204] time 0.309 (0.324) data 0.000 (0.006) loss -0.0154 (0.3215) lr 5.0000e-03 eta 0:15:38
epoch [16/30] batch [180/204] time 0.292 (0.322) data 0.000 (0.005) loss 0.3162 (0.3200) lr 5.0000e-03 eta 0:15:27
epoch [16/30] batch [200/204] time 0.293 (0.319) data 0.000 (0.005) loss 0.2360 (0.3213) lr 5.0000e-03 eta 0:15:12
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.17s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.63it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.29it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.96it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.59it/s]100%|██████████| 7/7 [00:03<00:00,  4.18it/s]100%|██████████| 7/7 [00:03<00:00,  2.14it/s]=> result
* total: 696
* correct: 646
* accuracy: 92.8%
* error: 7.2%
* macro_f1: 91.6%

epoch [17/30] batch [20/204] time 0.327 (0.360) data 0.000 (0.041) loss 0.1703 (0.4422) lr 4.4774e-03 eta 0:17:02
epoch [17/30] batch [40/204] time 0.316 (0.338) data 0.000 (0.021) loss 0.0422 (0.3819) lr 4.4774e-03 eta 0:15:51
epoch [17/30] batch [60/204] time 0.328 (0.331) data 0.000 (0.014) loss 0.0172 (0.3328) lr 4.4774e-03 eta 0:15:25
epoch [17/30] batch [80/204] time 0.317 (0.328) data 0.000 (0.010) loss 0.8955 (0.3027) lr 4.4774e-03 eta 0:15:10
epoch [17/30] batch [100/204] time 0.323 (0.326) data 0.000 (0.008) loss 0.0648 (0.3149) lr 4.4774e-03 eta 0:14:59
epoch [17/30] batch [120/204] time 0.314 (0.325) data 0.000 (0.007) loss 0.1570 (0.3102) lr 4.4774e-03 eta 0:14:48
epoch [17/30] batch [140/204] time 0.315 (0.323) data 0.000 (0.006) loss 0.0303 (0.3360) lr 4.4774e-03 eta 0:14:38
epoch [17/30] batch [160/204] time 0.318 (0.323) data 0.000 (0.005) loss 0.8203 (0.3380) lr 4.4774e-03 eta 0:14:30
epoch [17/30] batch [180/204] time 0.299 (0.322) data 0.000 (0.005) loss 0.3943 (0.3303) lr 4.4774e-03 eta 0:14:21
epoch [17/30] batch [200/204] time 0.296 (0.319) data 0.000 (0.004) loss 0.5317 (0.3297) lr 4.4774e-03 eta 0:14:08
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.30s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.05s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.55it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.20it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.86it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.49it/s]100%|██████████| 7/7 [00:03<00:00,  4.08it/s]100%|██████████| 7/7 [00:03<00:00,  2.05it/s]=> result
* total: 696
* correct: 647
* accuracy: 93.0%
* error: 7.0%
* macro_f1: 92.0%

epoch [18/30] batch [20/204] time 0.427 (0.386) data 0.000 (0.044) loss 0.0707 (0.1538) lr 3.9604e-03 eta 0:16:55
epoch [18/30] batch [40/204] time 0.315 (0.351) data 0.000 (0.022) loss 0.4363 (0.2953) lr 3.9604e-03 eta 0:15:17
epoch [18/30] batch [60/204] time 0.320 (0.340) data 0.000 (0.015) loss 0.2603 (0.3419) lr 3.9604e-03 eta 0:14:42
epoch [18/30] batch [80/204] time 0.319 (0.335) data 0.000 (0.011) loss 0.8857 (0.3207) lr 3.9604e-03 eta 0:14:21
epoch [18/30] batch [100/204] time 0.314 (0.332) data 0.000 (0.009) loss 0.2190 (0.3105) lr 3.9604e-03 eta 0:14:06
epoch [18/30] batch [120/204] time 0.312 (0.330) data 0.000 (0.008) loss 0.4817 (0.2979) lr 3.9604e-03 eta 0:13:55
epoch [18/30] batch [140/204] time 0.313 (0.328) data 0.000 (0.007) loss 0.3472 (0.2954) lr 3.9604e-03 eta 0:13:43
epoch [18/30] batch [160/204] time 0.315 (0.326) data 0.000 (0.006) loss 0.0224 (0.2923) lr 3.9604e-03 eta 0:13:33
epoch [18/30] batch [180/204] time 0.293 (0.325) data 0.000 (0.005) loss 0.0362 (0.2954) lr 3.9604e-03 eta 0:13:22
epoch [18/30] batch [200/204] time 0.291 (0.321) data 0.000 (0.005) loss 0.2239 (0.3065) lr 3.9604e-03 eta 0:13:07
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.13s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.03it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.32it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.99it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.60it/s]100%|██████████| 7/7 [00:03<00:00,  4.19it/s]100%|██████████| 7/7 [00:03<00:00,  2.17it/s]=> result
* total: 696
* correct: 640
* accuracy: 92.0%
* error: 8.0%
* macro_f1: 90.7%

epoch [19/30] batch [20/204] time 0.316 (0.359) data 0.000 (0.040) loss 0.1166 (0.1679) lr 3.4549e-03 eta 0:14:31
epoch [19/30] batch [40/204] time 0.321 (0.339) data 0.000 (0.020) loss 0.1553 (0.2738) lr 3.4549e-03 eta 0:13:35
epoch [19/30] batch [60/204] time 0.321 (0.332) data 0.000 (0.014) loss 0.6924 (0.2951) lr 3.4549e-03 eta 0:13:13
epoch [19/30] batch [80/204] time 0.311 (0.328) data 0.000 (0.010) loss -0.0340 (0.2659) lr 3.4549e-03 eta 0:12:57
epoch [19/30] batch [100/204] time 0.316 (0.325) data 0.000 (0.008) loss -0.0045 (0.2762) lr 3.4549e-03 eta 0:12:44
epoch [19/30] batch [120/204] time 0.315 (0.324) data 0.000 (0.007) loss 0.6748 (0.2885) lr 3.4549e-03 eta 0:12:34
epoch [19/30] batch [140/204] time 0.309 (0.324) data 0.000 (0.006) loss -0.0308 (0.2861) lr 3.4549e-03 eta 0:12:26
epoch [19/30] batch [160/204] time 0.316 (0.323) data 0.000 (0.005) loss 0.9434 (0.2878) lr 3.4549e-03 eta 0:12:18
epoch [19/30] batch [180/204] time 0.293 (0.321) data 0.000 (0.005) loss 0.0302 (0.2687) lr 3.4549e-03 eta 0:12:08
epoch [19/30] batch [200/204] time 0.292 (0.319) data 0.000 (0.004) loss 0.6797 (0.2735) lr 3.4549e-03 eta 0:11:56
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.15s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.64it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.31it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.98it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.20it/s]100%|██████████| 7/7 [00:03<00:00,  2.15it/s]=> result
* total: 696
* correct: 656
* accuracy: 94.3%
* error: 5.7%
* macro_f1: 93.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [20/30] batch [20/204] time 0.312 (0.361) data 0.000 (0.043) loss 0.6318 (0.3404) lr 2.9663e-03 eta 0:13:21
epoch [20/30] batch [40/204] time 0.317 (0.337) data 0.000 (0.022) loss 0.2766 (0.2217) lr 2.9663e-03 eta 0:12:23
epoch [20/30] batch [60/204] time 0.316 (0.332) data 0.000 (0.015) loss 0.1594 (0.2173) lr 2.9663e-03 eta 0:12:04
epoch [20/30] batch [80/204] time 0.319 (0.329) data 0.000 (0.011) loss 0.0245 (0.2403) lr 2.9663e-03 eta 0:11:51
epoch [20/30] batch [100/204] time 0.311 (0.326) data 0.000 (0.009) loss 0.0752 (0.2368) lr 2.9663e-03 eta 0:11:39
epoch [20/30] batch [120/204] time 0.327 (0.325) data 0.000 (0.007) loss 0.0702 (0.2367) lr 2.9663e-03 eta 0:11:29
epoch [20/30] batch [140/204] time 0.326 (0.324) data 0.000 (0.006) loss 0.7627 (0.2599) lr 2.9663e-03 eta 0:11:21
epoch [20/30] batch [160/204] time 0.406 (0.324) data 0.000 (0.006) loss 0.6602 (0.2554) lr 2.9663e-03 eta 0:11:14
epoch [20/30] batch [180/204] time 0.295 (0.322) data 0.000 (0.005) loss -0.0466 (0.2468) lr 2.9663e-03 eta 0:11:04
epoch [20/30] batch [200/204] time 0.294 (0.319) data 0.000 (0.005) loss 0.1013 (0.2514) lr 2.9663e-03 eta 0:10:52
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.13s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.32it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.99it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.20it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 653
* accuracy: 93.8%
* error: 6.2%
* macro_f1: 92.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-20

epoch [21/30] batch [20/204] time 0.313 (0.363) data 0.000 (0.043) loss 0.1733 (0.2717) lr 2.5000e-03 eta 0:12:13
epoch [21/30] batch [40/204] time 0.308 (0.341) data 0.000 (0.022) loss 0.2457 (0.2495) lr 2.5000e-03 eta 0:11:21
epoch [21/30] batch [60/204] time 0.321 (0.332) data 0.000 (0.015) loss 0.0178 (0.2661) lr 2.5000e-03 eta 0:10:57
epoch [21/30] batch [80/204] time 0.325 (0.329) data 0.001 (0.011) loss 0.0425 (0.2503) lr 2.5000e-03 eta 0:10:43
epoch [21/30] batch [100/204] time 0.315 (0.326) data 0.000 (0.009) loss 0.1830 (0.2302) lr 2.5000e-03 eta 0:10:32
epoch [21/30] batch [120/204] time 0.318 (0.324) data 0.000 (0.007) loss 0.6338 (0.2668) lr 2.5000e-03 eta 0:10:22
epoch [21/30] batch [140/204] time 0.317 (0.323) data 0.000 (0.006) loss 0.2213 (0.2620) lr 2.5000e-03 eta 0:10:14
epoch [21/30] batch [160/204] time 0.314 (0.323) data 0.000 (0.006) loss 1.1396 (0.2847) lr 2.5000e-03 eta 0:10:07
epoch [21/30] batch [180/204] time 0.296 (0.322) data 0.000 (0.005) loss 0.3584 (0.2816) lr 2.5000e-03 eta 0:09:58
epoch [21/30] batch [200/204] time 0.298 (0.320) data 0.000 (0.005) loss 0.6084 (0.2791) lr 2.5000e-03 eta 0:09:47
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.12s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.31it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.98it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.20it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 652
* accuracy: 93.7%
* error: 6.3%
* macro_f1: 92.6%

epoch [22/30] batch [20/204] time 0.321 (0.361) data 0.000 (0.040) loss 0.0016 (0.2055) lr 2.0611e-03 eta 0:10:54
epoch [22/30] batch [40/204] time 0.321 (0.339) data 0.000 (0.020) loss 0.3250 (0.2408) lr 2.0611e-03 eta 0:10:09
epoch [22/30] batch [60/204] time 0.315 (0.332) data 0.000 (0.014) loss 0.0685 (0.2481) lr 2.0611e-03 eta 0:09:49
epoch [22/30] batch [80/204] time 0.315 (0.328) data 0.000 (0.010) loss 0.8940 (0.2936) lr 2.0611e-03 eta 0:09:36
epoch [22/30] batch [100/204] time 0.318 (0.326) data 0.000 (0.008) loss 1.0840 (0.2938) lr 2.0611e-03 eta 0:09:26
epoch [22/30] batch [120/204] time 0.316 (0.326) data 0.000 (0.007) loss -0.0252 (0.3049) lr 2.0611e-03 eta 0:09:19
epoch [22/30] batch [140/204] time 0.318 (0.325) data 0.000 (0.006) loss 0.4099 (0.2971) lr 2.0611e-03 eta 0:09:10
epoch [22/30] batch [160/204] time 0.318 (0.324) data 0.000 (0.005) loss 0.0070 (0.3016) lr 2.0611e-03 eta 0:09:03
epoch [22/30] batch [180/204] time 0.295 (0.323) data 0.000 (0.005) loss 0.0240 (0.2918) lr 2.0611e-03 eta 0:08:54
epoch [22/30] batch [200/204] time 0.295 (0.320) data 0.000 (0.004) loss 0.5527 (0.3039) lr 2.0611e-03 eta 0:08:43
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.15s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.64it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.30it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.96it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.59it/s]100%|██████████| 7/7 [00:03<00:00,  4.18it/s]100%|██████████| 7/7 [00:03<00:00,  2.14it/s]=> result
* total: 696
* correct: 663
* accuracy: 95.3%
* error: 4.7%
* macro_f1: 94.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [23/30] batch [20/204] time 0.317 (0.364) data 0.000 (0.041) loss 0.0452 (0.3808) lr 1.6543e-03 eta 0:09:47
epoch [23/30] batch [40/204] time 0.329 (0.341) data 0.000 (0.021) loss 0.6060 (0.3339) lr 1.6543e-03 eta 0:09:03
epoch [23/30] batch [60/204] time 0.316 (0.333) data 0.000 (0.014) loss 0.2651 (0.2933) lr 1.6543e-03 eta 0:08:43
epoch [23/30] batch [80/204] time 0.314 (0.329) data 0.000 (0.010) loss 0.5205 (0.3126) lr 1.6543e-03 eta 0:08:30
epoch [23/30] batch [100/204] time 0.317 (0.326) data 0.000 (0.008) loss 0.0784 (0.3089) lr 1.6543e-03 eta 0:08:20
epoch [23/30] batch [120/204] time 0.316 (0.325) data 0.000 (0.007) loss 0.2469 (0.2866) lr 1.6543e-03 eta 0:08:11
epoch [23/30] batch [140/204] time 0.317 (0.324) data 0.000 (0.006) loss 0.0360 (0.3166) lr 1.6543e-03 eta 0:08:04
epoch [23/30] batch [160/204] time 0.315 (0.323) data 0.000 (0.005) loss 0.0895 (0.3065) lr 1.6543e-03 eta 0:07:55
epoch [23/30] batch [180/204] time 0.288 (0.321) data 0.000 (0.005) loss 0.1390 (0.3015) lr 1.6543e-03 eta 0:07:46
epoch [23/30] batch [200/204] time 0.288 (0.318) data 0.000 (0.004) loss 1.0742 (0.3215) lr 1.6543e-03 eta 0:07:35
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.15s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.64it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.30it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.97it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.60it/s]100%|██████████| 7/7 [00:03<00:00,  4.19it/s]100%|██████████| 7/7 [00:03<00:00,  2.15it/s]=> result
* total: 696
* correct: 661
* accuracy: 95.0%
* error: 5.0%
* macro_f1: 93.9%

epoch [24/30] batch [20/204] time 0.333 (0.362) data 0.000 (0.040) loss 0.3916 (0.2886) lr 1.2843e-03 eta 0:08:29
epoch [24/30] batch [40/204] time 0.317 (0.342) data 0.000 (0.020) loss 0.0174 (0.2646) lr 1.2843e-03 eta 0:07:55
epoch [24/30] batch [60/204] time 0.322 (0.335) data 0.000 (0.014) loss 0.1558 (0.2732) lr 1.2843e-03 eta 0:07:37
epoch [24/30] batch [80/204] time 0.320 (0.331) data 0.000 (0.010) loss 0.0159 (0.2545) lr 1.2843e-03 eta 0:07:25
epoch [24/30] batch [100/204] time 0.325 (0.328) data 0.000 (0.008) loss -0.0031 (0.2744) lr 1.2843e-03 eta 0:07:15
epoch [24/30] batch [120/204] time 0.311 (0.326) data 0.000 (0.007) loss 0.0399 (0.2850) lr 1.2843e-03 eta 0:07:06
epoch [24/30] batch [140/204] time 0.312 (0.325) data 0.000 (0.006) loss 0.0131 (0.2892) lr 1.2843e-03 eta 0:06:59
epoch [24/30] batch [160/204] time 0.323 (0.325) data 0.000 (0.005) loss 0.0825 (0.2884) lr 1.2843e-03 eta 0:06:51
epoch [24/30] batch [180/204] time 0.293 (0.323) data 0.000 (0.005) loss -0.0175 (0.2829) lr 1.2843e-03 eta 0:06:43
epoch [24/30] batch [200/204] time 0.294 (0.320) data 0.000 (0.004) loss 0.1736 (0.2874) lr 1.2843e-03 eta 0:06:33
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.13s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.03it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.32it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.99it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.13it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 664
* accuracy: 95.4%
* error: 4.6%
* macro_f1: 94.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [25/30] batch [20/204] time 0.314 (0.359) data 0.000 (0.040) loss 0.1069 (0.1703) lr 9.5492e-04 eta 0:07:12
epoch [25/30] batch [40/204] time 0.313 (0.339) data 0.000 (0.020) loss 1.1670 (0.1819) lr 9.5492e-04 eta 0:06:40
epoch [25/30] batch [60/204] time 0.317 (0.333) data 0.000 (0.013) loss 1.1631 (0.2033) lr 9.5492e-04 eta 0:06:27
epoch [25/30] batch [80/204] time 0.316 (0.329) data 0.000 (0.010) loss -0.0003 (0.2122) lr 9.5492e-04 eta 0:06:16
epoch [25/30] batch [100/204] time 0.319 (0.327) data 0.000 (0.008) loss 0.1667 (0.2074) lr 9.5492e-04 eta 0:06:07
epoch [25/30] batch [120/204] time 0.325 (0.325) data 0.000 (0.007) loss -0.0243 (0.1906) lr 9.5492e-04 eta 0:05:59
epoch [25/30] batch [140/204] time 0.325 (0.324) data 0.000 (0.006) loss 0.2190 (0.1919) lr 9.5492e-04 eta 0:05:51
epoch [25/30] batch [160/204] time 0.320 (0.323) data 0.000 (0.005) loss 0.0446 (0.2071) lr 9.5492e-04 eta 0:05:44
epoch [25/30] batch [180/204] time 0.294 (0.322) data 0.000 (0.005) loss 0.4917 (0.2002) lr 9.5492e-04 eta 0:05:35
epoch [25/30] batch [200/204] time 0.292 (0.319) data 0.000 (0.004) loss 0.2371 (0.1975) lr 9.5492e-04 eta 0:05:26
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.16s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.63it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.29it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.96it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.59it/s]100%|██████████| 7/7 [00:03<00:00,  4.19it/s]100%|██████████| 7/7 [00:03<00:00,  2.14it/s]=> result
* total: 696
* correct: 656
* accuracy: 94.3%
* error: 5.7%
* macro_f1: 93.0%

epoch [26/30] batch [20/204] time 0.317 (0.362) data 0.000 (0.043) loss 0.0080 (0.2810) lr 6.6987e-04 eta 0:06:02
epoch [26/30] batch [40/204] time 0.311 (0.340) data 0.000 (0.022) loss -0.0120 (0.2731) lr 6.6987e-04 eta 0:05:32
epoch [26/30] batch [60/204] time 0.323 (0.332) data 0.000 (0.015) loss 0.0964 (0.2560) lr 6.6987e-04 eta 0:05:18
epoch [26/30] batch [80/204] time 0.319 (0.329) data 0.000 (0.011) loss 0.6523 (0.2437) lr 6.6987e-04 eta 0:05:09
epoch [26/30] batch [100/204] time 0.316 (0.327) data 0.000 (0.009) loss -0.0030 (0.2464) lr 6.6987e-04 eta 0:05:01
epoch [26/30] batch [120/204] time 0.317 (0.326) data 0.000 (0.007) loss 0.0023 (0.2625) lr 6.6987e-04 eta 0:04:53
epoch [26/30] batch [140/204] time 0.313 (0.324) data 0.000 (0.006) loss 0.0768 (0.2458) lr 6.6987e-04 eta 0:04:45
epoch [26/30] batch [160/204] time 0.308 (0.324) data 0.000 (0.006) loss 0.1116 (0.2407) lr 6.6987e-04 eta 0:04:38
epoch [26/30] batch [180/204] time 0.296 (0.322) data 0.000 (0.005) loss 0.6362 (0.2450) lr 6.6987e-04 eta 0:04:30
epoch [26/30] batch [200/204] time 0.293 (0.319) data 0.000 (0.005) loss 0.6113 (0.2492) lr 6.6987e-04 eta 0:04:21
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.27s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.03s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.57it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.23it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.89it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.49it/s]100%|██████████| 7/7 [00:03<00:00,  4.09it/s]100%|██████████| 7/7 [00:03<00:00,  2.08it/s]=> result
* total: 696
* correct: 656
* accuracy: 94.3%
* error: 5.7%
* macro_f1: 93.2%

epoch [27/30] batch [20/204] time 0.311 (0.360) data 0.000 (0.038) loss -0.0087 (0.2659) lr 4.3227e-04 eta 0:04:46
epoch [27/30] batch [40/204] time 0.310 (0.340) data 0.000 (0.019) loss -0.0090 (0.2138) lr 4.3227e-04 eta 0:04:23
epoch [27/30] batch [60/204] time 0.320 (0.333) data 0.000 (0.013) loss 0.2095 (0.2863) lr 4.3227e-04 eta 0:04:11
epoch [27/30] batch [80/204] time 0.323 (0.329) data 0.000 (0.010) loss 0.0043 (0.2613) lr 4.3227e-04 eta 0:04:01
epoch [27/30] batch [100/204] time 0.318 (0.328) data 0.000 (0.008) loss 0.2451 (0.2378) lr 4.3227e-04 eta 0:03:54
epoch [27/30] batch [120/204] time 0.313 (0.326) data 0.000 (0.007) loss 0.2253 (0.2432) lr 4.3227e-04 eta 0:03:46
epoch [27/30] batch [140/204] time 0.315 (0.325) data 0.000 (0.006) loss 0.2445 (0.2335) lr 4.3227e-04 eta 0:03:39
epoch [27/30] batch [160/204] time 0.320 (0.324) data 0.000 (0.005) loss 0.3420 (0.2255) lr 4.3227e-04 eta 0:03:32
epoch [27/30] batch [180/204] time 0.294 (0.322) data 0.000 (0.005) loss 0.0095 (0.2196) lr 4.3227e-04 eta 0:03:24
epoch [27/30] batch [200/204] time 0.297 (0.320) data 0.000 (0.004) loss 0.0007 (0.2082) lr 4.3227e-04 eta 0:03:16
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.20s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.00s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.61it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.27it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.93it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.56it/s]100%|██████████| 7/7 [00:03<00:00,  4.15it/s]100%|██████████| 7/7 [00:03<00:00,  2.12it/s]=> result
* total: 696
* correct: 662
* accuracy: 95.1%
* error: 4.9%
* macro_f1: 94.2%

epoch [28/30] batch [20/204] time 0.311 (0.362) data 0.000 (0.038) loss 0.0009 (0.1799) lr 2.4472e-04 eta 0:03:34
epoch [28/30] batch [40/204] time 0.320 (0.340) data 0.000 (0.019) loss 0.2057 (0.1852) lr 2.4472e-04 eta 0:03:14
epoch [28/30] batch [60/204] time 0.311 (0.332) data 0.000 (0.013) loss 0.0159 (0.1922) lr 2.4472e-04 eta 0:03:03
epoch [28/30] batch [80/204] time 0.320 (0.328) data 0.000 (0.010) loss 0.0846 (0.1800) lr 2.4472e-04 eta 0:02:54
epoch [28/30] batch [100/204] time 0.313 (0.326) data 0.000 (0.008) loss 1.1416 (0.2086) lr 2.4472e-04 eta 0:02:46
epoch [28/30] batch [120/204] time 0.311 (0.325) data 0.000 (0.007) loss 0.3350 (0.2229) lr 2.4472e-04 eta 0:02:39
epoch [28/30] batch [140/204] time 0.323 (0.324) data 0.000 (0.006) loss 1.1152 (0.2208) lr 2.4472e-04 eta 0:02:32
epoch [28/30] batch [160/204] time 0.313 (0.323) data 0.000 (0.005) loss 0.1464 (0.2263) lr 2.4472e-04 eta 0:02:25
epoch [28/30] batch [180/204] time 0.294 (0.321) data 0.000 (0.005) loss 0.5122 (0.2243) lr 2.4472e-04 eta 0:02:18
epoch [28/30] batch [200/204] time 0.296 (0.319) data 0.000 (0.004) loss 0.1581 (0.2167) lr 2.4472e-04 eta 0:02:11
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.24s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.02s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.59it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.24it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.89it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.52it/s]100%|██████████| 7/7 [00:03<00:00,  4.12it/s]100%|██████████| 7/7 [00:03<00:00,  2.10it/s]=> result
* total: 696
* correct: 661
* accuracy: 95.0%
* error: 5.0%
* macro_f1: 94.1%

epoch [29/30] batch [20/204] time 0.316 (0.360) data 0.000 (0.041) loss -0.0325 (0.1828) lr 1.0926e-04 eta 0:02:19
epoch [29/30] batch [40/204] time 0.311 (0.338) data 0.000 (0.021) loss 0.1179 (0.1664) lr 1.0926e-04 eta 0:02:04
epoch [29/30] batch [60/204] time 0.314 (0.333) data 0.000 (0.014) loss 0.0515 (0.2283) lr 1.0926e-04 eta 0:01:55
epoch [29/30] batch [80/204] time 0.315 (0.329) data 0.000 (0.011) loss 0.3962 (0.2353) lr 1.0926e-04 eta 0:01:48
epoch [29/30] batch [100/204] time 0.317 (0.327) data 0.000 (0.008) loss 0.0599 (0.2417) lr 1.0926e-04 eta 0:01:40
epoch [29/30] batch [120/204] time 0.320 (0.326) data 0.000 (0.007) loss 1.1230 (0.2403) lr 1.0926e-04 eta 0:01:33
epoch [29/30] batch [140/204] time 0.320 (0.325) data 0.000 (0.006) loss -0.0308 (0.2300) lr 1.0926e-04 eta 0:01:27
epoch [29/30] batch [160/204] time 0.320 (0.324) data 0.000 (0.005) loss -0.0438 (0.2265) lr 1.0926e-04 eta 0:01:20
epoch [29/30] batch [180/204] time 0.293 (0.323) data 0.000 (0.005) loss -0.0382 (0.2195) lr 1.0926e-04 eta 0:01:13
epoch [29/30] batch [200/204] time 0.298 (0.320) data 0.000 (0.004) loss -0.0073 (0.2136) lr 1.0926e-04 eta 0:01:06
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.16s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.64it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.30it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.97it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.59it/s]100%|██████████| 7/7 [00:03<00:00,  4.18it/s]100%|██████████| 7/7 [00:03<00:00,  2.15it/s]=> result
* total: 696
* correct: 661
* accuracy: 95.0%
* error: 5.0%
* macro_f1: 94.1%

epoch [30/30] batch [20/204] time 0.314 (0.360) data 0.000 (0.042) loss -0.0168 (0.1641) lr 2.7391e-05 eta 0:01:06
epoch [30/30] batch [40/204] time 0.317 (0.339) data 0.000 (0.021) loss 0.0011 (0.1779) lr 2.7391e-05 eta 0:00:55
epoch [30/30] batch [60/204] time 0.317 (0.332) data 0.000 (0.014) loss 0.0222 (0.1959) lr 2.7391e-05 eta 0:00:47
epoch [30/30] batch [80/204] time 0.323 (0.328) data 0.000 (0.011) loss -0.0246 (0.1955) lr 2.7391e-05 eta 0:00:40
epoch [30/30] batch [100/204] time 0.319 (0.326) data 0.000 (0.009) loss 0.2927 (0.1994) lr 2.7391e-05 eta 0:00:33
epoch [30/30] batch [120/204] time 0.321 (0.325) data 0.000 (0.007) loss 0.2223 (0.1861) lr 2.7391e-05 eta 0:00:27
epoch [30/30] batch [140/204] time 0.321 (0.323) data 0.000 (0.006) loss 0.0784 (0.2064) lr 2.7391e-05 eta 0:00:20
epoch [30/30] batch [160/204] time 0.312 (0.323) data 0.000 (0.005) loss 0.6235 (0.2173) lr 2.7391e-05 eta 0:00:14
epoch [30/30] batch [180/204] time 0.295 (0.322) data 0.000 (0.005) loss 0.0279 (0.2167) lr 2.7391e-05 eta 0:00:07
epoch [30/30] batch [200/204] time 0.294 (0.319) data 0.000 (0.004) loss 0.5918 (0.2197) lr 2.7391e-05 eta 0:00:01
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.18s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.63it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.29it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.95it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.58it/s]100%|██████████| 7/7 [00:03<00:00,  4.10it/s]100%|██████████| 7/7 [00:03<00:00,  2.12it/s]
=> result
* total: 696
* correct: 661
* accuracy: 95.0%
* error: 5.0%
* macro_f1: 94.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 24)
Evaluate on the *test* set
  0%|          | 0/11 [00:00<?, ?it/s]  9%|▉         | 1/11 [00:02<00:26,  2.66s/it] 18%|█▊        | 2/11 [00:02<00:10,  1.20s/it] 27%|██▋       | 3/11 [00:02<00:05,  1.38it/s] 36%|███▋      | 4/11 [00:03<00:03,  1.99it/s] 45%|████▌     | 5/11 [00:03<00:02,  2.63it/s] 55%|█████▍    | 6/11 [00:03<00:01,  3.26it/s] 64%|██████▎   | 7/11 [00:03<00:01,  3.84it/s] 73%|███████▎  | 8/11 [00:03<00:00,  4.36it/s] 82%|████████▏ | 9/11 [00:03<00:00,  4.78it/s] 91%|█████████ | 10/11 [00:04<00:00,  5.13it/s]100%|██████████| 11/11 [00:04<00:00,  6.00it/s]100%|██████████| 11/11 [00:04<00:00,  2.52it/s]
=> result
* total: 1,053
* correct: 1,022
* accuracy: 97.1%
* error: 2.9%
* macro_f1: 96.9%
Elapsed: 0:34:21
+ sh scripts/rpo_prime/base2new_test_sdl.sh oxford_flowers 2 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
816 937 1410
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      937
# test     1,410
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 24)
Evaluate on the *test* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:04<01:09,  4.98s/it] 13%|█▎        | 2/15 [00:05<00:28,  2.16s/it] 20%|██        | 3/15 [00:05<00:14,  1.25s/it] 27%|██▋       | 4/15 [00:05<00:09,  1.22it/s] 33%|███▎      | 5/15 [00:05<00:05,  1.72it/s] 40%|████      | 6/15 [00:05<00:03,  2.27it/s] 47%|████▋     | 7/15 [00:05<00:02,  2.86it/s] 53%|█████▎    | 8/15 [00:06<00:02,  3.44it/s] 60%|██████    | 9/15 [00:06<00:01,  3.98it/s] 67%|██████▋   | 10/15 [00:06<00:01,  4.45it/s] 73%|███████▎  | 11/15 [00:06<00:00,  4.86it/s] 80%|████████  | 12/15 [00:06<00:00,  5.18it/s] 87%|████████▋ | 13/15 [00:06<00:00,  5.43it/s] 93%|█████████▎| 14/15 [00:07<00:00,  5.62it/s]100%|██████████| 15/15 [00:07<00:00,  2.06it/s]
=> result
* total: 1,410
* correct: 1,075
* accuracy: 76.2%
* error: 23.8%
* macro_f1: 70.5%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh oxford_flowers 3 0 main_tmp1_0.1sdl 16
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
816 696 1053
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      696
# test     1,053
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/30] batch [20/204] time 0.318 (0.454) data 0.000 (0.063) loss 1.1699 (1.4275) lr 1.0000e-02 eta 0:46:11
epoch [1/30] batch [40/204] time 0.325 (0.386) data 0.000 (0.032) loss 2.7188 (1.4305) lr 1.0000e-02 eta 0:39:08
epoch [1/30] batch [60/204] time 0.309 (0.363) data 0.000 (0.021) loss 0.5728 (1.3679) lr 1.0000e-02 eta 0:36:40
epoch [1/30] batch [80/204] time 0.315 (0.351) data 0.000 (0.016) loss 0.3643 (1.3666) lr 1.0000e-02 eta 0:35:22
epoch [1/30] batch [100/204] time 0.322 (0.345) data 0.000 (0.013) loss 2.2539 (1.4040) lr 1.0000e-02 eta 0:34:35
epoch [1/30] batch [120/204] time 0.321 (0.340) data 0.000 (0.011) loss 1.4873 (1.3647) lr 1.0000e-02 eta 0:34:02
epoch [1/30] batch [140/204] time 0.317 (0.337) data 0.000 (0.009) loss 0.4846 (1.3277) lr 1.0000e-02 eta 0:33:36
epoch [1/30] batch [160/204] time 0.320 (0.335) data 0.000 (0.008) loss 1.3535 (1.2827) lr 1.0000e-02 eta 0:33:16
epoch [1/30] batch [180/204] time 0.289 (0.332) data 0.000 (0.007) loss 0.6201 (1.2493) lr 1.0000e-02 eta 0:32:54
epoch [1/30] batch [200/204] time 0.287 (0.328) data 0.000 (0.007) loss 0.7363 (1.2455) lr 1.0000e-02 eta 0:32:21
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.32s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.05s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.55it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.19it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.85it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.48it/s]100%|██████████| 7/7 [00:03<00:00,  4.08it/s]100%|██████████| 7/7 [00:03<00:00,  2.05it/s]=> result
* total: 696
* correct: 507
* accuracy: 72.8%
* error: 27.2%
* macro_f1: 67.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [2/30] batch [20/204] time 0.314 (0.364) data 0.000 (0.039) loss 0.5391 (0.9453) lr 9.9726e-03 eta 0:35:46
epoch [2/30] batch [40/204] time 0.313 (0.339) data 0.000 (0.019) loss 0.5137 (0.9998) lr 9.9726e-03 eta 0:33:12
epoch [2/30] batch [60/204] time 0.309 (0.333) data 0.000 (0.013) loss 1.4531 (1.0744) lr 9.9726e-03 eta 0:32:28
epoch [2/30] batch [80/204] time 0.312 (0.329) data 0.000 (0.010) loss 1.8740 (1.0594) lr 9.9726e-03 eta 0:31:58
epoch [2/30] batch [100/204] time 0.315 (0.328) data 0.000 (0.008) loss 1.4834 (1.0661) lr 9.9726e-03 eta 0:31:45
epoch [2/30] batch [120/204] time 0.318 (0.326) data 0.000 (0.007) loss 0.6323 (1.0788) lr 9.9726e-03 eta 0:31:28
epoch [2/30] batch [140/204] time 0.320 (0.325) data 0.000 (0.006) loss 1.3867 (1.0909) lr 9.9726e-03 eta 0:31:16
epoch [2/30] batch [160/204] time 0.316 (0.324) data 0.000 (0.005) loss 1.2168 (1.0915) lr 9.9726e-03 eta 0:31:04
epoch [2/30] batch [180/204] time 0.294 (0.322) data 0.000 (0.005) loss 0.8804 (1.0871) lr 9.9726e-03 eta 0:30:47
epoch [2/30] batch [200/204] time 0.297 (0.320) data 0.000 (0.004) loss 1.6523 (1.0956) lr 9.9726e-03 eta 0:30:27
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.12s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.03it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.66it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.33it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.99it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.60it/s]100%|██████████| 7/7 [00:03<00:00,  4.18it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 524
* accuracy: 75.3%
* error: 24.7%
* macro_f1: 70.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [3/30] batch [20/204] time 0.312 (0.360) data 0.000 (0.043) loss 0.9751 (0.8374) lr 9.8907e-03 eta 0:34:08
epoch [3/30] batch [40/204] time 0.317 (0.337) data 0.000 (0.022) loss 1.1348 (1.0565) lr 9.8907e-03 eta 0:31:53
epoch [3/30] batch [60/204] time 0.318 (0.331) data 0.000 (0.014) loss 1.0000 (1.0844) lr 9.8907e-03 eta 0:31:09
epoch [3/30] batch [80/204] time 0.322 (0.327) data 0.000 (0.011) loss 1.8418 (1.0538) lr 9.8907e-03 eta 0:30:43
epoch [3/30] batch [100/204] time 0.315 (0.325) data 0.000 (0.009) loss 0.5474 (1.0045) lr 9.8907e-03 eta 0:30:26
epoch [3/30] batch [120/204] time 0.315 (0.324) data 0.000 (0.007) loss 0.7798 (0.9821) lr 9.8907e-03 eta 0:30:11
epoch [3/30] batch [140/204] time 0.318 (0.323) data 0.000 (0.006) loss 0.5078 (0.9689) lr 9.8907e-03 eta 0:29:58
epoch [3/30] batch [160/204] time 0.321 (0.322) data 0.000 (0.006) loss 0.4033 (0.9739) lr 9.8907e-03 eta 0:29:48
epoch [3/30] batch [180/204] time 0.308 (0.321) data 0.000 (0.005) loss 1.3408 (0.9752) lr 9.8907e-03 eta 0:29:33
epoch [3/30] batch [200/204] time 0.298 (0.318) data 0.000 (0.005) loss 0.9648 (0.9631) lr 9.8907e-03 eta 0:29:13
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.11s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.03it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.66it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.33it/s] 71%|███████▏  | 5/7 [00:02<00:00,  3.00it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.63it/s]100%|██████████| 7/7 [00:03<00:00,  4.22it/s]100%|██████████| 7/7 [00:03<00:00,  2.18it/s]=> result
* total: 696
* correct: 544
* accuracy: 78.2%
* error: 21.8%
* macro_f1: 73.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [4/30] batch [20/204] time 0.314 (0.365) data 0.000 (0.040) loss 1.1494 (0.9062) lr 9.7553e-03 eta 0:33:24
epoch [4/30] batch [40/204] time 0.319 (0.341) data 0.000 (0.020) loss 0.6904 (0.9101) lr 9.7553e-03 eta 0:31:05
epoch [4/30] batch [60/204] time 0.318 (0.333) data 0.000 (0.013) loss 0.4578 (0.8334) lr 9.7553e-03 eta 0:30:12
epoch [4/30] batch [80/204] time 0.309 (0.329) data 0.000 (0.010) loss 1.4668 (0.8636) lr 9.7553e-03 eta 0:29:45
epoch [4/30] batch [100/204] time 0.314 (0.327) data 0.000 (0.008) loss 1.8398 (0.8585) lr 9.7553e-03 eta 0:29:26
epoch [4/30] batch [120/204] time 0.319 (0.325) data 0.000 (0.007) loss 0.2671 (0.8503) lr 9.7553e-03 eta 0:29:10
epoch [4/30] batch [140/204] time 0.402 (0.324) data 0.000 (0.006) loss 0.4607 (0.8310) lr 9.7553e-03 eta 0:29:01
epoch [4/30] batch [160/204] time 0.316 (0.323) data 0.000 (0.005) loss 1.7393 (0.8257) lr 9.7553e-03 eta 0:28:48
epoch [4/30] batch [180/204] time 0.296 (0.322) data 0.000 (0.005) loss 0.4189 (0.8193) lr 9.7553e-03 eta 0:28:33
epoch [4/30] batch [200/204] time 0.294 (0.319) data 0.000 (0.004) loss 1.5771 (0.8023) lr 9.7553e-03 eta 0:28:12
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.11s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.04it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.67it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.34it/s] 71%|███████▏  | 5/7 [00:02<00:00,  3.01it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.64it/s]100%|██████████| 7/7 [00:03<00:00,  4.23it/s]100%|██████████| 7/7 [00:03<00:00,  2.18it/s]=> result
* total: 696
* correct: 557
* accuracy: 80.0%
* error: 20.0%
* macro_f1: 75.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [5/30] batch [20/204] time 0.329 (0.361) data 0.000 (0.039) loss 0.4958 (0.7239) lr 9.5677e-03 eta 0:31:46
epoch [5/30] batch [40/204] time 0.313 (0.342) data 0.000 (0.020) loss 0.9429 (0.7265) lr 9.5677e-03 eta 0:29:59
epoch [5/30] batch [60/204] time 0.314 (0.333) data 0.000 (0.013) loss 0.4060 (0.7114) lr 9.5677e-03 eta 0:29:06
epoch [5/30] batch [80/204] time 0.317 (0.328) data 0.000 (0.010) loss 1.1943 (0.7428) lr 9.5677e-03 eta 0:28:35
epoch [5/30] batch [100/204] time 0.317 (0.326) data 0.000 (0.008) loss 1.3828 (0.7357) lr 9.5677e-03 eta 0:28:17
epoch [5/30] batch [120/204] time 0.314 (0.325) data 0.000 (0.007) loss 0.8403 (0.7464) lr 9.5677e-03 eta 0:28:04
epoch [5/30] batch [140/204] time 0.320 (0.324) data 0.000 (0.006) loss 0.9863 (0.7509) lr 9.5677e-03 eta 0:27:52
epoch [5/30] batch [160/204] time 0.311 (0.323) data 0.000 (0.005) loss 0.2173 (0.7757) lr 9.5677e-03 eta 0:27:40
epoch [5/30] batch [180/204] time 0.295 (0.321) data 0.000 (0.005) loss 1.5889 (0.7680) lr 9.5677e-03 eta 0:27:26
epoch [5/30] batch [200/204] time 0.291 (0.319) data 0.000 (0.004) loss 0.7993 (0.7663) lr 9.5677e-03 eta 0:27:07
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.13s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.03it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.32it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.99it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.62it/s]100%|██████████| 7/7 [00:03<00:00,  4.21it/s]100%|██████████| 7/7 [00:03<00:00,  2.17it/s]=> result
* total: 696
* correct: 580
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 80.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [6/30] batch [20/204] time 0.315 (0.363) data 0.000 (0.039) loss 0.1294 (0.5249) lr 9.3301e-03 eta 0:30:41
epoch [6/30] batch [40/204] time 0.318 (0.342) data 0.000 (0.020) loss 0.2646 (0.6187) lr 9.3301e-03 eta 0:28:48
epoch [6/30] batch [60/204] time 0.311 (0.336) data 0.000 (0.013) loss 0.4592 (0.6866) lr 9.3301e-03 eta 0:28:12
epoch [6/30] batch [80/204] time 0.327 (0.332) data 0.000 (0.010) loss 0.0933 (0.6593) lr 9.3301e-03 eta 0:27:44
epoch [6/30] batch [100/204] time 0.310 (0.329) data 0.000 (0.008) loss 0.2173 (0.6742) lr 9.3301e-03 eta 0:27:23
epoch [6/30] batch [120/204] time 0.314 (0.327) data 0.000 (0.007) loss 0.1479 (0.6659) lr 9.3301e-03 eta 0:27:07
epoch [6/30] batch [140/204] time 0.314 (0.326) data 0.000 (0.006) loss 0.3801 (0.6365) lr 9.3301e-03 eta 0:26:55
epoch [6/30] batch [160/204] time 0.322 (0.324) data 0.000 (0.005) loss 0.9473 (0.6360) lr 9.3301e-03 eta 0:26:42
epoch [6/30] batch [180/204] time 0.293 (0.323) data 0.000 (0.005) loss 0.1409 (0.6197) lr 9.3301e-03 eta 0:26:26
epoch [6/30] batch [200/204] time 0.297 (0.320) data 0.000 (0.004) loss 0.1915 (0.6144) lr 9.3301e-03 eta 0:26:08
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.19s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.00it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.62it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.29it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.95it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.58it/s]100%|██████████| 7/7 [00:03<00:00,  4.18it/s]100%|██████████| 7/7 [00:03<00:00,  2.14it/s]=> result
* total: 696
* correct: 604
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 83.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [7/30] batch [20/204] time 0.310 (0.360) data 0.000 (0.041) loss 1.0127 (0.6938) lr 9.0451e-03 eta 0:29:14
epoch [7/30] batch [40/204] time 0.318 (0.338) data 0.000 (0.021) loss 0.3252 (0.6443) lr 9.0451e-03 eta 0:27:20
epoch [7/30] batch [60/204] time 0.315 (0.331) data 0.000 (0.014) loss 0.4341 (0.7419) lr 9.0451e-03 eta 0:26:40
epoch [7/30] batch [80/204] time 0.314 (0.328) data 0.000 (0.010) loss 0.3262 (0.7136) lr 9.0451e-03 eta 0:26:18
epoch [7/30] batch [100/204] time 0.313 (0.326) data 0.000 (0.008) loss 2.0000 (0.7009) lr 9.0451e-03 eta 0:26:04
epoch [7/30] batch [120/204] time 0.318 (0.325) data 0.000 (0.007) loss 0.0721 (0.7006) lr 9.0451e-03 eta 0:25:50
epoch [7/30] batch [140/204] time 0.315 (0.324) data 0.000 (0.006) loss 1.0752 (0.7271) lr 9.0451e-03 eta 0:25:39
epoch [7/30] batch [160/204] time 0.316 (0.323) data 0.000 (0.005) loss 0.5024 (0.7051) lr 9.0451e-03 eta 0:25:29
epoch [7/30] batch [180/204] time 0.294 (0.321) data 0.000 (0.005) loss 0.1538 (0.6954) lr 9.0451e-03 eta 0:25:14
epoch [7/30] batch [200/204] time 0.292 (0.319) data 0.000 (0.004) loss 0.2466 (0.6730) lr 9.0451e-03 eta 0:24:55
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.22s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.01s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.60it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.26it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.92it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.55it/s]100%|██████████| 7/7 [00:03<00:00,  4.15it/s]100%|██████████| 7/7 [00:03<00:00,  2.11it/s]=> result
* total: 696
* correct: 606
* accuracy: 87.1%
* error: 12.9%
* macro_f1: 83.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [8/30] batch [20/204] time 0.318 (0.361) data 0.000 (0.038) loss 0.3521 (0.5335) lr 8.7157e-03 eta 0:28:06
epoch [8/30] batch [40/204] time 0.312 (0.338) data 0.000 (0.019) loss 1.3711 (0.5565) lr 8.7157e-03 eta 0:26:12
epoch [8/30] batch [60/204] time 0.319 (0.331) data 0.000 (0.013) loss 0.0559 (0.5345) lr 8.7157e-03 eta 0:25:33
epoch [8/30] batch [80/204] time 0.312 (0.328) data 0.000 (0.010) loss 1.4473 (0.5682) lr 8.7157e-03 eta 0:25:10
epoch [8/30] batch [100/204] time 0.319 (0.326) data 0.000 (0.008) loss 0.1461 (0.5358) lr 8.7157e-03 eta 0:24:55
epoch [8/30] batch [120/204] time 0.320 (0.325) data 0.000 (0.007) loss 0.8789 (0.5548) lr 8.7157e-03 eta 0:24:45
epoch [8/30] batch [140/204] time 0.319 (0.324) data 0.000 (0.006) loss 0.1807 (0.5782) lr 8.7157e-03 eta 0:24:33
epoch [8/30] batch [160/204] time 0.313 (0.323) data 0.000 (0.005) loss 0.1842 (0.5984) lr 8.7157e-03 eta 0:24:22
epoch [8/30] batch [180/204] time 0.295 (0.321) data 0.000 (0.004) loss 0.2537 (0.6114) lr 8.7157e-03 eta 0:24:09
epoch [8/30] batch [200/204] time 0.295 (0.319) data 0.000 (0.004) loss 0.1851 (0.6057) lr 8.7157e-03 eta 0:23:51
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.10s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.04it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.67it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.34it/s] 71%|███████▏  | 5/7 [00:02<00:00,  3.01it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.64it/s]100%|██████████| 7/7 [00:03<00:00,  4.23it/s]100%|██████████| 7/7 [00:03<00:00,  2.18it/s]=> result
* total: 696
* correct: 622
* accuracy: 89.4%
* error: 10.6%
* macro_f1: 86.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [9/30] batch [20/204] time 0.319 (0.360) data 0.000 (0.040) loss 0.1434 (0.5548) lr 8.3457e-03 eta 0:26:48
epoch [9/30] batch [40/204] time 0.325 (0.342) data 0.000 (0.020) loss 0.5132 (0.5589) lr 8.3457e-03 eta 0:25:22
epoch [9/30] batch [60/204] time 0.319 (0.334) data 0.000 (0.014) loss 0.4153 (0.5581) lr 8.3457e-03 eta 0:24:37
epoch [9/30] batch [80/204] time 0.322 (0.330) data 0.000 (0.010) loss 0.8145 (0.5576) lr 8.3457e-03 eta 0:24:14
epoch [9/30] batch [100/204] time 0.317 (0.327) data 0.000 (0.008) loss 0.3750 (0.5308) lr 8.3457e-03 eta 0:23:56
epoch [9/30] batch [120/204] time 0.322 (0.327) data 0.000 (0.007) loss 0.1069 (0.5233) lr 8.3457e-03 eta 0:23:46
epoch [9/30] batch [140/204] time 0.320 (0.326) data 0.000 (0.006) loss 0.0349 (0.5120) lr 8.3457e-03 eta 0:23:36
epoch [9/30] batch [160/204] time 0.318 (0.325) data 0.000 (0.005) loss 0.5708 (0.5207) lr 8.3457e-03 eta 0:23:25
epoch [9/30] batch [180/204] time 0.299 (0.323) data 0.000 (0.005) loss 0.0621 (0.5018) lr 8.3457e-03 eta 0:23:11
epoch [9/30] batch [200/204] time 0.297 (0.320) data 0.000 (0.004) loss 0.1547 (0.5000) lr 8.3457e-03 eta 0:22:53
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.10s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.04it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.67it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.34it/s] 71%|███████▏  | 5/7 [00:02<00:00,  3.01it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.64it/s]100%|██████████| 7/7 [00:03<00:00,  4.23it/s]100%|██████████| 7/7 [00:03<00:00,  2.19it/s]=> result
* total: 696
* correct: 628
* accuracy: 90.2%
* error: 9.8%
* macro_f1: 87.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [10/30] batch [20/204] time 0.319 (0.362) data 0.000 (0.042) loss 0.1211 (0.3611) lr 7.9389e-03 eta 0:25:41
epoch [10/30] batch [40/204] time 0.315 (0.339) data 0.000 (0.021) loss 0.0152 (0.4146) lr 7.9389e-03 eta 0:23:59
epoch [10/30] batch [60/204] time 0.319 (0.332) data 0.000 (0.014) loss 0.2690 (0.4418) lr 7.9389e-03 eta 0:23:24
epoch [10/30] batch [80/204] time 0.314 (0.329) data 0.000 (0.011) loss 0.2522 (0.4171) lr 7.9389e-03 eta 0:23:03
epoch [10/30] batch [100/204] time 0.315 (0.327) data 0.000 (0.009) loss 0.0222 (0.4353) lr 7.9389e-03 eta 0:22:48
epoch [10/30] batch [120/204] time 0.329 (0.326) data 0.000 (0.007) loss 0.3223 (0.4342) lr 7.9389e-03 eta 0:22:38
epoch [10/30] batch [140/204] time 0.316 (0.325) data 0.000 (0.006) loss 0.4033 (0.4555) lr 7.9389e-03 eta 0:22:27
epoch [10/30] batch [160/204] time 0.318 (0.325) data 0.000 (0.006) loss 0.7178 (0.4546) lr 7.9389e-03 eta 0:22:19
epoch [10/30] batch [180/204] time 0.296 (0.323) data 0.000 (0.005) loss 0.8398 (0.4565) lr 7.9389e-03 eta 0:22:05
epoch [10/30] batch [200/204] time 0.301 (0.320) data 0.000 (0.004) loss 0.5811 (0.4744) lr 7.9389e-03 eta 0:21:47
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.11s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.03it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.66it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.33it/s] 71%|███████▏  | 5/7 [00:02<00:00,  3.00it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.63it/s]100%|██████████| 7/7 [00:03<00:00,  4.22it/s]100%|██████████| 7/7 [00:03<00:00,  2.18it/s]=> result
* total: 696
* correct: 635
* accuracy: 91.2%
* error: 8.8%
* macro_f1: 89.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-10

epoch [11/30] batch [20/204] time 0.318 (0.356) data 0.000 (0.041) loss 0.2881 (0.4573) lr 7.5000e-03 eta 0:24:06
epoch [11/30] batch [40/204] time 0.314 (0.336) data 0.000 (0.021) loss 0.9438 (0.5146) lr 7.5000e-03 eta 0:22:36
epoch [11/30] batch [60/204] time 0.320 (0.329) data 0.000 (0.014) loss 0.9170 (0.5010) lr 7.5000e-03 eta 0:22:03
epoch [11/30] batch [80/204] time 0.419 (0.328) data 0.000 (0.010) loss 0.3699 (0.4805) lr 7.5000e-03 eta 0:21:50
epoch [11/30] batch [100/204] time 0.310 (0.326) data 0.000 (0.008) loss 0.0671 (0.4595) lr 7.5000e-03 eta 0:21:35
epoch [11/30] batch [120/204] time 0.320 (0.324) data 0.000 (0.007) loss 0.2124 (0.4661) lr 7.5000e-03 eta 0:21:22
epoch [11/30] batch [140/204] time 0.316 (0.323) data 0.000 (0.006) loss 0.1718 (0.4710) lr 7.5000e-03 eta 0:21:11
epoch [11/30] batch [160/204] time 0.306 (0.322) data 0.000 (0.005) loss 0.0712 (0.4564) lr 7.5000e-03 eta 0:21:01
epoch [11/30] batch [180/204] time 0.296 (0.321) data 0.000 (0.005) loss 0.1648 (0.4620) lr 7.5000e-03 eta 0:20:50
epoch [11/30] batch [200/204] time 0.292 (0.318) data 0.000 (0.004) loss 0.1823 (0.4528) lr 7.5000e-03 eta 0:20:34
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.17s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.63it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.30it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.97it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.56it/s]100%|██████████| 7/7 [00:03<00:00,  4.15it/s]100%|██████████| 7/7 [00:03<00:00,  2.15it/s]=> result
* total: 696
* correct: 634
* accuracy: 91.1%
* error: 8.9%
* macro_f1: 88.4%

epoch [12/30] batch [20/204] time 0.316 (0.365) data 0.000 (0.042) loss 0.4812 (0.5086) lr 7.0337e-03 eta 0:23:28
epoch [12/30] batch [40/204] time 0.309 (0.341) data 0.000 (0.021) loss 0.7773 (0.4598) lr 7.0337e-03 eta 0:21:48
epoch [12/30] batch [60/204] time 0.312 (0.333) data 0.000 (0.014) loss 1.6758 (0.5258) lr 7.0337e-03 eta 0:21:11
epoch [12/30] batch [80/204] time 0.322 (0.329) data 0.000 (0.011) loss 0.2406 (0.5042) lr 7.0337e-03 eta 0:20:50
epoch [12/30] batch [100/204] time 0.317 (0.327) data 0.000 (0.009) loss 0.5234 (0.4970) lr 7.0337e-03 eta 0:20:35
epoch [12/30] batch [120/204] time 0.318 (0.325) data 0.000 (0.007) loss 0.0467 (0.4913) lr 7.0337e-03 eta 0:20:21
epoch [12/30] batch [140/204] time 0.322 (0.324) data 0.000 (0.006) loss 0.1381 (0.4749) lr 7.0337e-03 eta 0:20:10
epoch [12/30] batch [160/204] time 0.319 (0.324) data 0.000 (0.005) loss 0.1786 (0.4730) lr 7.0337e-03 eta 0:20:02
epoch [12/30] batch [180/204] time 0.291 (0.322) data 0.000 (0.005) loss 0.2705 (0.4530) lr 7.0337e-03 eta 0:19:49
epoch [12/30] batch [200/204] time 0.390 (0.320) data 0.000 (0.004) loss 0.3054 (0.4382) lr 7.0337e-03 eta 0:19:34
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.11s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.03it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.66it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.33it/s] 71%|███████▏  | 5/7 [00:02<00:00,  3.00it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.62it/s]100%|██████████| 7/7 [00:03<00:00,  4.21it/s]100%|██████████| 7/7 [00:03<00:00,  2.18it/s]=> result
* total: 696
* correct: 643
* accuracy: 92.4%
* error: 7.6%
* macro_f1: 90.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [13/30] batch [20/204] time 0.327 (0.360) data 0.000 (0.040) loss 0.1134 (0.3325) lr 6.5451e-03 eta 0:21:53
epoch [13/30] batch [40/204] time 0.311 (0.339) data 0.000 (0.020) loss 0.0516 (0.3403) lr 6.5451e-03 eta 0:20:29
epoch [13/30] batch [60/204] time 0.322 (0.331) data 0.000 (0.013) loss 0.8032 (0.4128) lr 6.5451e-03 eta 0:19:55
epoch [13/30] batch [80/204] time 0.316 (0.328) data 0.000 (0.010) loss 0.2095 (0.3710) lr 6.5451e-03 eta 0:19:36
epoch [13/30] batch [100/204] time 0.317 (0.325) data 0.000 (0.008) loss 1.7900 (0.3919) lr 6.5451e-03 eta 0:19:21
epoch [13/30] batch [120/204] time 0.315 (0.324) data 0.000 (0.007) loss 0.0147 (0.3820) lr 6.5451e-03 eta 0:19:10
epoch [13/30] batch [140/204] time 0.320 (0.324) data 0.000 (0.006) loss 0.6406 (0.3811) lr 6.5451e-03 eta 0:19:02
epoch [13/30] batch [160/204] time 0.310 (0.322) data 0.000 (0.005) loss 1.0322 (0.3891) lr 6.5451e-03 eta 0:18:52
epoch [13/30] batch [180/204] time 0.293 (0.321) data 0.000 (0.005) loss 0.0094 (0.3889) lr 6.5451e-03 eta 0:18:40
epoch [13/30] batch [200/204] time 0.293 (0.318) data 0.000 (0.004) loss 0.3584 (0.3904) lr 6.5451e-03 eta 0:18:24
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.13s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.03it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.66it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.33it/s] 71%|███████▏  | 5/7 [00:02<00:00,  3.00it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.20it/s]100%|██████████| 7/7 [00:03<00:00,  2.18it/s]=> result
* total: 696
* correct: 638
* accuracy: 91.7%
* error: 8.3%
* macro_f1: 89.8%

epoch [14/30] batch [20/204] time 0.309 (0.363) data 0.000 (0.041) loss 0.1656 (0.3883) lr 6.0396e-03 eta 0:20:52
epoch [14/30] batch [40/204] time 0.314 (0.340) data 0.000 (0.020) loss 0.2098 (0.4500) lr 6.0396e-03 eta 0:19:26
epoch [14/30] batch [60/204] time 0.311 (0.332) data 0.000 (0.014) loss 1.0918 (0.4712) lr 6.0396e-03 eta 0:18:52
epoch [14/30] batch [80/204] time 0.321 (0.328) data 0.000 (0.010) loss 0.0749 (0.4154) lr 6.0396e-03 eta 0:18:31
epoch [14/30] batch [100/204] time 0.312 (0.326) data 0.000 (0.008) loss 1.5859 (0.4183) lr 6.0396e-03 eta 0:18:16
epoch [14/30] batch [120/204] time 0.324 (0.324) data 0.000 (0.007) loss 0.1888 (0.3952) lr 6.0396e-03 eta 0:18:06
epoch [14/30] batch [140/204] time 0.317 (0.323) data 0.000 (0.006) loss -0.0237 (0.3911) lr 6.0396e-03 eta 0:17:55
epoch [14/30] batch [160/204] time 0.314 (0.322) data 0.000 (0.005) loss 0.1553 (0.3830) lr 6.0396e-03 eta 0:17:46
epoch [14/30] batch [180/204] time 0.294 (0.321) data 0.000 (0.005) loss 0.7539 (0.3849) lr 6.0396e-03 eta 0:17:34
epoch [14/30] batch [200/204] time 0.293 (0.319) data 0.000 (0.004) loss 0.1895 (0.3948) lr 6.0396e-03 eta 0:17:21
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.32it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.99it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.19it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 644
* accuracy: 92.5%
* error: 7.5%
* macro_f1: 90.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [15/30] batch [20/204] time 0.320 (0.359) data 0.000 (0.040) loss 0.2576 (0.3313) lr 5.5226e-03 eta 0:19:24
epoch [15/30] batch [40/204] time 0.315 (0.337) data 0.000 (0.020) loss 0.4346 (0.3375) lr 5.5226e-03 eta 0:18:07
epoch [15/30] batch [60/204] time 0.315 (0.333) data 0.000 (0.013) loss 0.1240 (0.2846) lr 5.5226e-03 eta 0:17:45
epoch [15/30] batch [80/204] time 0.324 (0.329) data 0.000 (0.010) loss 0.2174 (0.2961) lr 5.5226e-03 eta 0:17:28
epoch [15/30] batch [100/204] time 0.312 (0.327) data 0.000 (0.008) loss 0.3418 (0.2960) lr 5.5226e-03 eta 0:17:15
epoch [15/30] batch [120/204] time 0.322 (0.325) data 0.000 (0.007) loss 0.3706 (0.3059) lr 5.5226e-03 eta 0:17:02
epoch [15/30] batch [140/204] time 0.322 (0.324) data 0.000 (0.006) loss 0.3853 (0.3021) lr 5.5226e-03 eta 0:16:53
epoch [15/30] batch [160/204] time 0.314 (0.324) data 0.000 (0.005) loss 0.0100 (0.2966) lr 5.5226e-03 eta 0:16:44
epoch [15/30] batch [180/204] time 0.295 (0.323) data 0.000 (0.005) loss 0.0575 (0.2830) lr 5.5226e-03 eta 0:16:34
epoch [15/30] batch [200/204] time 0.296 (0.320) data 0.000 (0.004) loss 0.2446 (0.3172) lr 5.5226e-03 eta 0:16:19
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.12s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.03it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.66it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.33it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.99it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.19it/s]100%|██████████| 7/7 [00:03<00:00,  2.18it/s]=> result
* total: 696
* correct: 650
* accuracy: 93.4%
* error: 6.6%
* macro_f1: 92.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [16/30] batch [20/204] time 0.319 (0.359) data 0.000 (0.039) loss 1.2783 (0.3376) lr 5.0000e-03 eta 0:18:10
epoch [16/30] batch [40/204] time 0.318 (0.337) data 0.000 (0.020) loss 0.0346 (0.2527) lr 5.0000e-03 eta 0:16:57
epoch [16/30] batch [60/204] time 0.318 (0.330) data 0.000 (0.013) loss 0.9702 (0.2696) lr 5.0000e-03 eta 0:16:31
epoch [16/30] batch [80/204] time 0.326 (0.329) data 0.000 (0.010) loss 0.0059 (0.2930) lr 5.0000e-03 eta 0:16:19
epoch [16/30] batch [100/204] time 0.317 (0.326) data 0.000 (0.008) loss 0.6543 (0.3041) lr 5.0000e-03 eta 0:16:05
epoch [16/30] batch [120/204] time 0.316 (0.324) data 0.000 (0.007) loss 0.5166 (0.3219) lr 5.0000e-03 eta 0:15:53
epoch [16/30] batch [140/204] time 0.316 (0.323) data 0.000 (0.006) loss 0.4568 (0.3126) lr 5.0000e-03 eta 0:15:43
epoch [16/30] batch [160/204] time 0.321 (0.323) data 0.000 (0.005) loss 0.1836 (0.3178) lr 5.0000e-03 eta 0:15:35
epoch [16/30] batch [180/204] time 0.294 (0.321) data 0.000 (0.005) loss 0.2189 (0.3269) lr 5.0000e-03 eta 0:15:24
epoch [16/30] batch [200/204] time 0.294 (0.318) data 0.000 (0.004) loss 0.2236 (0.3290) lr 5.0000e-03 eta 0:15:09
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.31s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.05s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.55it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.19it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.85it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.49it/s]100%|██████████| 7/7 [00:03<00:00,  4.09it/s]100%|██████████| 7/7 [00:03<00:00,  2.05it/s]=> result
* total: 696
* correct: 651
* accuracy: 93.5%
* error: 6.5%
* macro_f1: 92.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [17/30] batch [20/204] time 0.319 (0.364) data 0.000 (0.041) loss 1.4766 (0.3982) lr 4.4774e-03 eta 0:17:12
epoch [17/30] batch [40/204] time 0.316 (0.339) data 0.000 (0.021) loss 0.1501 (0.3869) lr 4.4774e-03 eta 0:15:55
epoch [17/30] batch [60/204] time 0.326 (0.332) data 0.000 (0.014) loss 0.2852 (0.3784) lr 4.4774e-03 eta 0:15:27
epoch [17/30] batch [80/204] time 0.311 (0.328) data 0.000 (0.011) loss 0.1367 (0.3498) lr 4.4774e-03 eta 0:15:09
epoch [17/30] batch [100/204] time 0.309 (0.327) data 0.000 (0.009) loss 0.1064 (0.3824) lr 4.4774e-03 eta 0:14:59
epoch [17/30] batch [120/204] time 0.311 (0.325) data 0.000 (0.007) loss 0.7593 (0.3757) lr 4.4774e-03 eta 0:14:48
epoch [17/30] batch [140/204] time 0.322 (0.324) data 0.000 (0.006) loss 0.3704 (0.3620) lr 4.4774e-03 eta 0:14:39
epoch [17/30] batch [160/204] time 0.317 (0.323) data 0.000 (0.005) loss -0.0169 (0.3503) lr 4.4774e-03 eta 0:14:30
epoch [17/30] batch [180/204] time 0.293 (0.321) data 0.000 (0.005) loss 0.2214 (0.3508) lr 4.4774e-03 eta 0:14:19
epoch [17/30] batch [200/204] time 0.291 (0.319) data 0.000 (0.004) loss 0.5049 (0.3527) lr 4.4774e-03 eta 0:14:06
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.16s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.63it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.30it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.96it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.59it/s]100%|██████████| 7/7 [00:03<00:00,  4.18it/s]100%|██████████| 7/7 [00:03<00:00,  2.14it/s]=> result
* total: 696
* correct: 636
* accuracy: 91.4%
* error: 8.6%
* macro_f1: 90.0%

epoch [18/30] batch [20/204] time 0.418 (0.363) data 0.000 (0.042) loss -0.0074 (0.3047) lr 3.9604e-03 eta 0:15:56
epoch [18/30] batch [40/204] time 0.321 (0.340) data 0.000 (0.021) loss 0.1689 (0.3049) lr 3.9604e-03 eta 0:14:47
epoch [18/30] batch [60/204] time 0.311 (0.332) data 0.000 (0.014) loss 0.3010 (0.2541) lr 3.9604e-03 eta 0:14:19
epoch [18/30] batch [80/204] time 0.320 (0.328) data 0.000 (0.011) loss 0.3503 (0.2837) lr 3.9604e-03 eta 0:14:03
epoch [18/30] batch [100/204] time 0.324 (0.326) data 0.000 (0.009) loss 0.1525 (0.2825) lr 3.9604e-03 eta 0:13:51
epoch [18/30] batch [120/204] time 0.313 (0.325) data 0.000 (0.007) loss 0.0803 (0.2853) lr 3.9604e-03 eta 0:13:43
epoch [18/30] batch [140/204] time 0.312 (0.324) data 0.000 (0.006) loss 0.0641 (0.2795) lr 3.9604e-03 eta 0:13:33
epoch [18/30] batch [160/204] time 0.314 (0.323) data 0.000 (0.006) loss 0.0562 (0.2715) lr 3.9604e-03 eta 0:13:24
epoch [18/30] batch [180/204] time 0.292 (0.321) data 0.000 (0.005) loss -0.0433 (0.2815) lr 3.9604e-03 eta 0:13:14
epoch [18/30] batch [200/204] time 0.290 (0.319) data 0.000 (0.004) loss 0.5518 (0.2783) lr 3.9604e-03 eta 0:13:00
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.12s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.03it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.66it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.33it/s] 71%|███████▏  | 5/7 [00:02<00:00,  3.01it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.56it/s]100%|██████████| 7/7 [00:03<00:00,  4.15it/s]100%|██████████| 7/7 [00:03<00:00,  2.17it/s]=> result
* total: 696
* correct: 656
* accuracy: 94.3%
* error: 5.7%
* macro_f1: 93.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [19/30] batch [20/204] time 0.321 (0.358) data 0.000 (0.039) loss 0.9980 (0.4363) lr 3.4549e-03 eta 0:14:29
epoch [19/30] batch [40/204] time 0.317 (0.337) data 0.000 (0.020) loss 0.1903 (0.3679) lr 3.4549e-03 eta 0:13:31
epoch [19/30] batch [60/204] time 0.320 (0.331) data 0.000 (0.013) loss 0.0947 (0.3377) lr 3.4549e-03 eta 0:13:11
epoch [19/30] batch [80/204] time 0.312 (0.328) data 0.000 (0.010) loss 0.0428 (0.3514) lr 3.4549e-03 eta 0:12:55
epoch [19/30] batch [100/204] time 0.312 (0.326) data 0.000 (0.008) loss 0.1917 (0.3355) lr 3.4549e-03 eta 0:12:44
epoch [19/30] batch [120/204] time 0.323 (0.324) data 0.000 (0.007) loss 0.9321 (0.3372) lr 3.4549e-03 eta 0:12:35
epoch [19/30] batch [140/204] time 0.319 (0.324) data 0.000 (0.006) loss 0.0167 (0.3350) lr 3.4549e-03 eta 0:12:28
epoch [19/30] batch [160/204] time 0.310 (0.323) data 0.000 (0.005) loss 0.2871 (0.3325) lr 3.4549e-03 eta 0:12:19
epoch [19/30] batch [180/204] time 0.295 (0.322) data 0.000 (0.005) loss 0.3047 (0.3261) lr 3.4549e-03 eta 0:12:09
epoch [19/30] batch [200/204] time 0.299 (0.319) data 0.000 (0.004) loss 0.0938 (0.3237) lr 3.4549e-03 eta 0:11:57
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.16s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.01it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.63it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.29it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.96it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.58it/s]100%|██████████| 7/7 [00:03<00:00,  4.18it/s]100%|██████████| 7/7 [00:03<00:00,  2.14it/s]=> result
* total: 696
* correct: 653
* accuracy: 93.8%
* error: 6.2%
* macro_f1: 92.4%

epoch [20/30] batch [20/204] time 0.320 (0.361) data 0.000 (0.040) loss 0.8647 (0.2163) lr 2.9663e-03 eta 0:13:23
epoch [20/30] batch [40/204] time 0.315 (0.339) data 0.000 (0.020) loss 0.3899 (0.2190) lr 2.9663e-03 eta 0:12:26
epoch [20/30] batch [60/204] time 0.311 (0.331) data 0.000 (0.014) loss 0.0640 (0.2105) lr 2.9663e-03 eta 0:12:03
epoch [20/30] batch [80/204] time 0.318 (0.329) data 0.000 (0.010) loss 0.2202 (0.1898) lr 2.9663e-03 eta 0:11:52
epoch [20/30] batch [100/204] time 0.314 (0.327) data 0.000 (0.008) loss 0.0471 (0.1977) lr 2.9663e-03 eta 0:11:40
epoch [20/30] batch [120/204] time 0.316 (0.325) data 0.000 (0.007) loss 0.0793 (0.2124) lr 2.9663e-03 eta 0:11:30
epoch [20/30] batch [140/204] time 0.309 (0.324) data 0.000 (0.006) loss 0.2737 (0.2306) lr 2.9663e-03 eta 0:11:21
epoch [20/30] batch [160/204] time 0.407 (0.324) data 0.000 (0.005) loss 0.8105 (0.2305) lr 2.9663e-03 eta 0:11:14
epoch [20/30] batch [180/204] time 0.297 (0.322) data 0.000 (0.005) loss 0.6680 (0.2365) lr 2.9663e-03 eta 0:11:04
epoch [20/30] batch [200/204] time 0.292 (0.319) data 0.000 (0.004) loss 1.0205 (0.2453) lr 2.9663e-03 eta 0:10:51
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.31it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.98it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.60it/s]100%|██████████| 7/7 [00:03<00:00,  4.19it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 652
* accuracy: 93.7%
* error: 6.3%
* macro_f1: 92.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-20

epoch [21/30] batch [20/204] time 0.318 (0.359) data 0.000 (0.039) loss 1.2627 (0.3620) lr 2.5000e-03 eta 0:12:05
epoch [21/30] batch [40/204] time 0.314 (0.338) data 0.000 (0.020) loss 0.4236 (0.2563) lr 2.5000e-03 eta 0:11:15
epoch [21/30] batch [60/204] time 0.317 (0.331) data 0.000 (0.013) loss 0.2888 (0.2665) lr 2.5000e-03 eta 0:10:54
epoch [21/30] batch [80/204] time 0.312 (0.327) data 0.000 (0.010) loss 1.2031 (0.2794) lr 2.5000e-03 eta 0:10:41
epoch [21/30] batch [100/204] time 0.310 (0.325) data 0.000 (0.008) loss -0.0285 (0.2620) lr 2.5000e-03 eta 0:10:31
epoch [21/30] batch [120/204] time 0.311 (0.324) data 0.000 (0.007) loss 0.5273 (0.2517) lr 2.5000e-03 eta 0:10:21
epoch [21/30] batch [140/204] time 0.318 (0.322) data 0.000 (0.006) loss 0.0328 (0.2732) lr 2.5000e-03 eta 0:10:12
epoch [21/30] batch [160/204] time 0.320 (0.322) data 0.000 (0.005) loss 0.7510 (0.2836) lr 2.5000e-03 eta 0:10:04
epoch [21/30] batch [180/204] time 0.293 (0.320) data 0.000 (0.005) loss 0.4167 (0.2765) lr 2.5000e-03 eta 0:09:56
epoch [21/30] batch [200/204] time 0.296 (0.319) data 0.000 (0.004) loss 0.0016 (0.2746) lr 2.5000e-03 eta 0:09:46
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.64it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.31it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.98it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.20it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 663
* accuracy: 95.3%
* error: 4.7%
* macro_f1: 93.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [22/30] batch [20/204] time 0.316 (0.361) data 0.000 (0.041) loss 0.6094 (0.2753) lr 2.0611e-03 eta 0:10:56
epoch [22/30] batch [40/204] time 0.316 (0.340) data 0.000 (0.021) loss 0.2242 (0.2415) lr 2.0611e-03 eta 0:10:10
epoch [22/30] batch [60/204] time 0.318 (0.333) data 0.000 (0.014) loss 0.0185 (0.2676) lr 2.0611e-03 eta 0:09:51
epoch [22/30] batch [80/204] time 0.312 (0.329) data 0.000 (0.010) loss 0.1497 (0.2737) lr 2.0611e-03 eta 0:09:37
epoch [22/30] batch [100/204] time 0.319 (0.327) data 0.000 (0.008) loss 0.1783 (0.2554) lr 2.0611e-03 eta 0:09:27
epoch [22/30] batch [120/204] time 0.318 (0.326) data 0.000 (0.007) loss 0.1656 (0.2500) lr 2.0611e-03 eta 0:09:19
epoch [22/30] batch [140/204] time 0.329 (0.325) data 0.000 (0.006) loss -0.0253 (0.2570) lr 2.0611e-03 eta 0:09:11
epoch [22/30] batch [160/204] time 0.314 (0.324) data 0.000 (0.005) loss 0.1990 (0.2575) lr 2.0611e-03 eta 0:09:03
epoch [22/30] batch [180/204] time 0.302 (0.322) data 0.000 (0.005) loss 0.0167 (0.2594) lr 2.0611e-03 eta 0:08:53
epoch [22/30] batch [200/204] time 0.293 (0.320) data 0.000 (0.004) loss 0.0142 (0.2495) lr 2.0611e-03 eta 0:08:42
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.11s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.04it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.67it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.34it/s] 71%|███████▏  | 5/7 [00:02<00:00,  3.02it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.64it/s]100%|██████████| 7/7 [00:03<00:00,  4.16it/s]100%|██████████| 7/7 [00:03<00:00,  2.17it/s]=> result
* total: 696
* correct: 665
* accuracy: 95.5%
* error: 4.5%
* macro_f1: 94.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [23/30] batch [20/204] time 0.315 (0.361) data 0.000 (0.040) loss 0.1667 (0.3086) lr 1.6543e-03 eta 0:09:42
epoch [23/30] batch [40/204] time 0.324 (0.339) data 0.000 (0.020) loss -0.0089 (0.2064) lr 1.6543e-03 eta 0:08:59
epoch [23/30] batch [60/204] time 0.316 (0.332) data 0.000 (0.013) loss 0.1965 (0.2213) lr 1.6543e-03 eta 0:08:41
epoch [23/30] batch [80/204] time 0.317 (0.328) data 0.000 (0.010) loss 0.0656 (0.2085) lr 1.6543e-03 eta 0:08:28
epoch [23/30] batch [100/204] time 0.316 (0.325) data 0.000 (0.008) loss 0.7046 (0.2199) lr 1.6543e-03 eta 0:08:18
epoch [23/30] batch [120/204] time 0.317 (0.324) data 0.000 (0.007) loss 0.0450 (0.2375) lr 1.6543e-03 eta 0:08:09
epoch [23/30] batch [140/204] time 0.323 (0.323) data 0.000 (0.006) loss 0.0092 (0.2321) lr 1.6543e-03 eta 0:08:02
epoch [23/30] batch [160/204] time 0.321 (0.322) data 0.000 (0.005) loss 0.4983 (0.2375) lr 1.6543e-03 eta 0:07:54
epoch [23/30] batch [180/204] time 0.292 (0.321) data 0.000 (0.005) loss 1.6377 (0.2562) lr 1.6543e-03 eta 0:07:45
epoch [23/30] batch [200/204] time 0.301 (0.318) data 0.000 (0.004) loss 1.0117 (0.2540) lr 1.6543e-03 eta 0:07:35
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.32it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.98it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.18it/s]100%|██████████| 7/7 [00:03<00:00,  2.15it/s]=> result
* total: 696
* correct: 666
* accuracy: 95.7%
* error: 4.3%
* macro_f1: 94.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [24/30] batch [20/204] time 0.321 (0.357) data 0.000 (0.038) loss 0.0626 (0.2721) lr 1.2843e-03 eta 0:08:22
epoch [24/30] batch [40/204] time 0.316 (0.340) data 0.000 (0.019) loss 0.0679 (0.2424) lr 1.2843e-03 eta 0:07:51
epoch [24/30] batch [60/204] time 0.321 (0.332) data 0.000 (0.013) loss 0.1382 (0.2090) lr 1.2843e-03 eta 0:07:34
epoch [24/30] batch [80/204] time 0.318 (0.328) data 0.000 (0.010) loss 0.2720 (0.2338) lr 1.2843e-03 eta 0:07:22
epoch [24/30] batch [100/204] time 0.318 (0.326) data 0.000 (0.008) loss -0.0015 (0.2176) lr 1.2843e-03 eta 0:07:13
epoch [24/30] batch [120/204] time 0.318 (0.325) data 0.000 (0.007) loss -0.0020 (0.2355) lr 1.2843e-03 eta 0:07:04
epoch [24/30] batch [140/204] time 0.323 (0.324) data 0.000 (0.006) loss 0.0002 (0.2284) lr 1.2843e-03 eta 0:06:57
epoch [24/30] batch [160/204] time 0.319 (0.324) data 0.000 (0.005) loss 0.0861 (0.2155) lr 1.2843e-03 eta 0:06:50
epoch [24/30] batch [180/204] time 0.293 (0.322) data 0.000 (0.004) loss 0.1492 (0.2320) lr 1.2843e-03 eta 0:06:41
epoch [24/30] batch [200/204] time 0.297 (0.319) data 0.000 (0.004) loss 0.7295 (0.2328) lr 1.2843e-03 eta 0:06:31
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.14s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.32it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.97it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.60it/s]100%|██████████| 7/7 [00:03<00:00,  4.19it/s]100%|██████████| 7/7 [00:03<00:00,  2.16it/s]=> result
* total: 696
* correct: 670
* accuracy: 96.3%
* error: 3.7%
* macro_f1: 95.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [25/30] batch [20/204] time 0.317 (0.361) data 0.000 (0.038) loss 0.0226 (0.2443) lr 9.5492e-04 eta 0:07:14
epoch [25/30] batch [40/204] time 0.316 (0.339) data 0.000 (0.019) loss -0.0049 (0.2513) lr 9.5492e-04 eta 0:06:41
epoch [25/30] batch [60/204] time 0.317 (0.333) data 0.000 (0.013) loss 0.0362 (0.2762) lr 9.5492e-04 eta 0:06:27
epoch [25/30] batch [80/204] time 0.325 (0.330) data 0.000 (0.010) loss 0.0396 (0.2355) lr 9.5492e-04 eta 0:06:17
epoch [25/30] batch [100/204] time 0.324 (0.327) data 0.000 (0.008) loss 1.4062 (0.2495) lr 9.5492e-04 eta 0:06:08
epoch [25/30] batch [120/204] time 0.311 (0.326) data 0.000 (0.007) loss 0.0561 (0.2410) lr 9.5492e-04 eta 0:05:59
epoch [25/30] batch [140/204] time 0.314 (0.324) data 0.000 (0.006) loss 0.5322 (0.2227) lr 9.5492e-04 eta 0:05:51
epoch [25/30] batch [160/204] time 0.315 (0.323) data 0.000 (0.005) loss 0.4697 (0.2278) lr 9.5492e-04 eta 0:05:43
epoch [25/30] batch [180/204] time 0.291 (0.321) data 0.000 (0.005) loss 1.4062 (0.2319) lr 9.5492e-04 eta 0:05:35
epoch [25/30] batch [200/204] time 0.292 (0.319) data 0.000 (0.004) loss 0.0621 (0.2333) lr 9.5492e-04 eta 0:05:26
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:13,  2.21s/it] 29%|██▊       | 2/7 [00:02<00:05,  1.00s/it] 43%|████▎     | 3/7 [00:02<00:02,  1.61it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.27it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.94it/s] 86%|████████▌ | 6/7 [00:03<00:00,  3.57it/s]100%|██████████| 7/7 [00:03<00:00,  4.16it/s]100%|██████████| 7/7 [00:03<00:00,  2.11it/s]=> result
* total: 696
* correct: 665
* accuracy: 95.5%
* error: 4.5%
* macro_f1: 94.5%

epoch [26/30] batch [20/204] time 0.324 (0.361) data 0.000 (0.041) loss 0.0404 (0.2570) lr 6.6987e-04 eta 0:06:00
epoch [26/30] batch [40/204] time 0.311 (0.340) data 0.000 (0.020) loss -0.0193 (0.2657) lr 6.6987e-04 eta 0:05:32
epoch [26/30] batch [60/204] time 0.314 (0.332) data 0.000 (0.014) loss 0.2734 (0.2356) lr 6.6987e-04 eta 0:05:18
epoch [26/30] batch [80/204] time 0.316 (0.330) data 0.000 (0.010) loss 0.0149 (0.1992) lr 6.6987e-04 eta 0:05:09
epoch [26/30] batch [100/204] time 0.318 (0.327) data 0.000 (0.008) loss -0.0317 (0.2211) lr 6.6987e-04 eta 0:05:00
epoch [26/30] batch [120/204] time 0.315 (0.325) data 0.000 (0.007) loss 0.2903 (0.2327) lr 6.6987e-04 eta 0:04:52
epoch [26/30] batch [140/204] time 0.321 (0.324) data 0.000 (0.006) loss 0.1110 (0.2293) lr 6.6987e-04 eta 0:04:45
epoch [26/30] batch [160/204] time 0.331 (0.323) data 0.000 (0.005) loss 0.0478 (0.2298) lr 6.6987e-04 eta 0:04:37
epoch [26/30] batch [180/204] time 0.296 (0.322) data 0.000 (0.005) loss 0.0826 (0.2208) lr 6.6987e-04 eta 0:04:30
epoch [26/30] batch [200/204] time 0.292 (0.319) data 0.000 (0.004) loss 0.0369 (0.2153) lr 6.6987e-04 eta 0:04:21
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.12s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.03it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.66it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.33it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.99it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.62it/s]100%|██████████| 7/7 [00:03<00:00,  4.21it/s]100%|██████████| 7/7 [00:03<00:00,  2.17it/s]=> result
* total: 696
* correct: 667
* accuracy: 95.8%
* error: 4.2%
* macro_f1: 94.8%

epoch [27/30] batch [20/204] time 0.320 (0.361) data 0.000 (0.041) loss 0.0057 (0.1728) lr 4.3227e-04 eta 0:04:47
epoch [27/30] batch [40/204] time 0.326 (0.339) data 0.000 (0.021) loss 0.0499 (0.1847) lr 4.3227e-04 eta 0:04:23
epoch [27/30] batch [60/204] time 0.316 (0.332) data 0.000 (0.014) loss -0.0130 (0.2064) lr 4.3227e-04 eta 0:04:10
epoch [27/30] batch [80/204] time 0.322 (0.328) data 0.000 (0.011) loss 0.0026 (0.1933) lr 4.3227e-04 eta 0:04:01
epoch [27/30] batch [100/204] time 0.304 (0.327) data 0.000 (0.008) loss 0.0281 (0.2143) lr 4.3227e-04 eta 0:03:53
epoch [27/30] batch [120/204] time 0.331 (0.325) data 0.000 (0.007) loss 0.0066 (0.2121) lr 4.3227e-04 eta 0:03:46
epoch [27/30] batch [140/204] time 0.314 (0.324) data 0.000 (0.006) loss 0.1084 (0.2008) lr 4.3227e-04 eta 0:03:39
epoch [27/30] batch [160/204] time 0.319 (0.323) data 0.000 (0.005) loss 0.4500 (0.2080) lr 4.3227e-04 eta 0:03:31
epoch [27/30] batch [180/204] time 0.294 (0.321) data 0.000 (0.005) loss 0.0025 (0.2100) lr 4.3227e-04 eta 0:03:24
epoch [27/30] batch [200/204] time 0.296 (0.319) data 0.000 (0.004) loss 0.0485 (0.2118) lr 4.3227e-04 eta 0:03:16
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.13s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.03it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.65it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.32it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.99it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.62it/s]100%|██████████| 7/7 [00:03<00:00,  4.21it/s]100%|██████████| 7/7 [00:03<00:00,  2.17it/s]=> result
* total: 696
* correct: 667
* accuracy: 95.8%
* error: 4.2%
* macro_f1: 94.9%

epoch [28/30] batch [20/204] time 0.329 (0.372) data 0.000 (0.039) loss 0.5439 (0.1845) lr 2.4472e-04 eta 0:03:40
epoch [28/30] batch [40/204] time 0.318 (0.346) data 0.000 (0.020) loss -0.0060 (0.1893) lr 2.4472e-04 eta 0:03:18
epoch [28/30] batch [60/204] time 0.320 (0.338) data 0.000 (0.013) loss 1.1699 (0.2236) lr 2.4472e-04 eta 0:03:06
epoch [28/30] batch [80/204] time 0.317 (0.333) data 0.000 (0.010) loss 1.3936 (0.2521) lr 2.4472e-04 eta 0:02:57
epoch [28/30] batch [100/204] time 0.314 (0.330) data 0.000 (0.008) loss 0.0175 (0.2404) lr 2.4472e-04 eta 0:02:48
epoch [28/30] batch [120/204] time 0.320 (0.328) data 0.000 (0.007) loss -0.0107 (0.2305) lr 2.4472e-04 eta 0:02:41
epoch [28/30] batch [140/204] time 0.316 (0.327) data 0.000 (0.006) loss -0.0066 (0.2211) lr 2.4472e-04 eta 0:02:34
epoch [28/30] batch [160/204] time 0.313 (0.326) data 0.000 (0.005) loss -0.0010 (0.2207) lr 2.4472e-04 eta 0:02:27
epoch [28/30] batch [180/204] time 0.294 (0.324) data 0.000 (0.005) loss 0.4131 (0.2279) lr 2.4472e-04 eta 0:02:19
epoch [28/30] batch [200/204] time 0.293 (0.321) data 0.000 (0.004) loss -0.0077 (0.2280) lr 2.4472e-04 eta 0:02:12
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.12s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.03it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.66it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.32it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.99it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.61it/s]100%|██████████| 7/7 [00:03<00:00,  4.19it/s]100%|██████████| 7/7 [00:03<00:00,  2.17it/s]=> result
* total: 696
* correct: 667
* accuracy: 95.8%
* error: 4.2%
* macro_f1: 94.6%

epoch [29/30] batch [20/204] time 0.313 (0.360) data 0.000 (0.039) loss 0.0557 (0.4084) lr 1.0926e-04 eta 0:02:19
epoch [29/30] batch [40/204] time 0.322 (0.339) data 0.000 (0.020) loss 0.8701 (0.3655) lr 1.0926e-04 eta 0:02:04
epoch [29/30] batch [60/204] time 0.308 (0.333) data 0.000 (0.013) loss -0.0466 (0.3253) lr 1.0926e-04 eta 0:01:56
epoch [29/30] batch [80/204] time 0.321 (0.329) data 0.000 (0.010) loss -0.0239 (0.2884) lr 1.0926e-04 eta 0:01:47
epoch [29/30] batch [100/204] time 0.321 (0.327) data 0.000 (0.008) loss 0.2664 (0.2579) lr 1.0926e-04 eta 0:01:40
epoch [29/30] batch [120/204] time 0.314 (0.325) data 0.000 (0.007) loss 0.2625 (0.2707) lr 1.0926e-04 eta 0:01:33
epoch [29/30] batch [140/204] time 0.316 (0.324) data 0.000 (0.006) loss -0.0182 (0.2828) lr 1.0926e-04 eta 0:01:26
epoch [29/30] batch [160/204] time 0.321 (0.324) data 0.000 (0.005) loss 0.0240 (0.2726) lr 1.0926e-04 eta 0:01:20
epoch [29/30] batch [180/204] time 0.287 (0.322) data 0.000 (0.005) loss 0.0452 (0.2531) lr 1.0926e-04 eta 0:01:13
epoch [29/30] batch [200/204] time 0.286 (0.318) data 0.000 (0.004) loss -0.0194 (0.2445) lr 1.0926e-04 eta 0:01:06
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.11s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.04it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.67it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.34it/s] 71%|███████▏  | 5/7 [00:02<00:00,  3.00it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.62it/s]100%|██████████| 7/7 [00:03<00:00,  4.21it/s]100%|██████████| 7/7 [00:03<00:00,  2.18it/s]=> result
* total: 696
* correct: 666
* accuracy: 95.7%
* error: 4.3%
* macro_f1: 94.5%

epoch [30/30] batch [20/204] time 0.308 (0.362) data 0.000 (0.044) loss 0.1765 (0.1297) lr 2.7391e-05 eta 0:01:06
epoch [30/30] batch [40/204] time 0.314 (0.339) data 0.000 (0.022) loss 0.0717 (0.2483) lr 2.7391e-05 eta 0:00:55
epoch [30/30] batch [60/204] time 0.307 (0.331) data 0.000 (0.015) loss -0.0142 (0.2183) lr 2.7391e-05 eta 0:00:47
epoch [30/30] batch [80/204] time 0.313 (0.327) data 0.000 (0.011) loss 0.1775 (0.2054) lr 2.7391e-05 eta 0:00:40
epoch [30/30] batch [100/204] time 0.314 (0.325) data 0.000 (0.009) loss 0.6475 (0.2133) lr 2.7391e-05 eta 0:00:33
epoch [30/30] batch [120/204] time 0.331 (0.324) data 0.000 (0.008) loss 0.0818 (0.2122) lr 2.7391e-05 eta 0:00:27
epoch [30/30] batch [140/204] time 0.316 (0.323) data 0.000 (0.007) loss 0.2734 (0.2059) lr 2.7391e-05 eta 0:00:20
epoch [30/30] batch [160/204] time 0.312 (0.322) data 0.000 (0.006) loss 0.1760 (0.2020) lr 2.7391e-05 eta 0:00:14
epoch [30/30] batch [180/204] time 0.287 (0.321) data 0.000 (0.005) loss 0.1017 (0.1932) lr 2.7391e-05 eta 0:00:07
epoch [30/30] batch [200/204] time 0.287 (0.318) data 0.000 (0.005) loss -0.0146 (0.1966) lr 2.7391e-05 eta 0:00:01
Evaluate on the *val* set
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:02<00:12,  2.15s/it] 29%|██▊       | 2/7 [00:02<00:04,  1.02it/s] 43%|████▎     | 3/7 [00:02<00:02,  1.64it/s] 57%|█████▋    | 4/7 [00:02<00:01,  2.31it/s] 71%|███████▏  | 5/7 [00:02<00:00,  2.94it/s] 86%|████████▌ | 6/7 [00:02<00:00,  3.57it/s]100%|██████████| 7/7 [00:03<00:00,  4.17it/s]100%|██████████| 7/7 [00:03<00:00,  2.15it/s]
=> result
* total: 696
* correct: 666
* accuracy: 95.7%
* error: 4.3%
* macro_f1: 94.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 24)
Evaluate on the *test* set
  0%|          | 0/11 [00:00<?, ?it/s]  9%|▉         | 1/11 [00:02<00:26,  2.62s/it] 18%|█▊        | 2/11 [00:02<00:10,  1.19s/it] 27%|██▋       | 3/11 [00:02<00:05,  1.39it/s] 36%|███▋      | 4/11 [00:03<00:03,  1.99it/s] 45%|████▌     | 5/11 [00:03<00:02,  2.63it/s] 55%|█████▍    | 6/11 [00:03<00:01,  3.27it/s] 64%|██████▎   | 7/11 [00:03<00:01,  3.86it/s] 73%|███████▎  | 8/11 [00:03<00:00,  4.37it/s] 82%|████████▏ | 9/11 [00:03<00:00,  4.80it/s] 91%|█████████ | 10/11 [00:04<00:00,  5.13it/s]100%|██████████| 11/11 [00:04<00:00,  6.01it/s]100%|██████████| 11/11 [00:04<00:00,  2.53it/s]
=> result
* total: 1,053
* correct: 1,022
* accuracy: 97.1%
* error: 2.9%
* macro_f1: 96.8%
Elapsed: 0:34:18
+ sh scripts/rpo_prime/base2new_test_sdl.sh oxford_flowers 3 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
816 937 1410
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      937
# test     1,410
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 24)
Evaluate on the *test* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:05<01:10,  5.02s/it] 13%|█▎        | 2/15 [00:05<00:28,  2.17s/it] 20%|██        | 3/15 [00:05<00:15,  1.25s/it] 27%|██▋       | 4/15 [00:05<00:09,  1.22it/s] 33%|███▎      | 5/15 [00:05<00:05,  1.71it/s] 40%|████      | 6/15 [00:05<00:03,  2.26it/s] 47%|████▋     | 7/15 [00:06<00:02,  2.85it/s] 53%|█████▎    | 8/15 [00:06<00:02,  3.43it/s] 60%|██████    | 9/15 [00:06<00:01,  3.97it/s] 67%|██████▋   | 10/15 [00:06<00:01,  4.45it/s] 73%|███████▎  | 11/15 [00:06<00:00,  4.85it/s] 80%|████████  | 12/15 [00:06<00:00,  5.18it/s] 87%|████████▋ | 13/15 [00:06<00:00,  5.43it/s] 93%|█████████▎| 14/15 [00:07<00:00,  5.62it/s]100%|██████████| 15/15 [00:07<00:00,  2.05it/s]
=> result
* total: 1,410
* correct: 1,032
* accuracy: 73.2%
* error: 26.8%
* macro_f1: 68.7%
