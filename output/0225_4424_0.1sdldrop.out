set -e
set -x

eval "$(conda shell.bash hook)"
++ conda shell.bash hook
+ eval 'export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
export CONDA_EXE='/home/s2/mjoolee/anaconda/bin/conda'
++ export CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
++ CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
export _CE_M=''
++ export _CE_M=
++ _CE_M=
export _CE_CONDA=''
++ export _CE_CONDA=
++ _CE_CONDA=
export CONDA_PYTHON_EXE='/home/s2/mjoolee/anaconda/bin/python'
++ export CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
++ CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We're not allowing PS1 to be unbound. It must at least be set.
    # However, we're not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi
++ '[' -z x ']'

conda activate base
++ conda activate base
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate base
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate base
+++ /home/s2/mjoolee/anaconda/bin/conda shell.posix activate base
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_PREFIX='\''/home/s2/mjoolee/anaconda'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/home/s2/mjoolee/anaconda/envs/dassl'\''
export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_PREFIX='\''/home/s2/mjoolee/anaconda'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/home/s2/mjoolee/anaconda/envs/dassl'\''
export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\'''
PS1='(base) '
+++ PS1='(base) '
export PATH='/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'
+++ export PATH=/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
+++ PATH=/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
export CONDA_PREFIX='/home/s2/mjoolee/anaconda'
+++ export CONDA_PREFIX=/home/s2/mjoolee/anaconda
+++ CONDA_PREFIX=/home/s2/mjoolee/anaconda
export CONDA_SHLVL='3'
+++ export CONDA_SHLVL=3
+++ CONDA_SHLVL=3
export CONDA_DEFAULT_ENV='base'
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
export CONDA_PROMPT_MODIFIER='(base) '
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
export CONDA_PREFIX_2='/home/s2/mjoolee/anaconda/envs/dassl'
+++ export CONDA_PREFIX_2=/home/s2/mjoolee/anaconda/envs/dassl
+++ CONDA_PREFIX_2=/home/s2/mjoolee/anaconda/envs/dassl
export CONDA_EXE='/home/s2/mjoolee/anaconda/bin/conda'
+++ export CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
+++ CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
export _CE_M=''
+++ export _CE_M=
+++ _CE_M=
export _CE_CONDA=''
+++ export _CE_CONDA=
+++ _CE_CONDA=
export CONDA_PYTHON_EXE='/home/s2/mjoolee/anaconda/bin/python'
+++ export CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
+++ CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
conda activate dassl
+ conda activate dassl
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate dassl
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate dassl
++ /home/s2/mjoolee/anaconda/bin/conda shell.posix activate dassl
+ ask_conda='PS1='\''(dassl) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_PREFIX='\''/home/s2/mjoolee/anaconda/envs/dassl'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''dassl'\''
export CONDA_PROMPT_MODIFIER='\''(dassl) '\''
export CONDA_PREFIX_3='\''/home/s2/mjoolee/anaconda'\''
export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\'''
+ eval 'PS1='\''(dassl) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_PREFIX='\''/home/s2/mjoolee/anaconda/envs/dassl'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''dassl'\''
export CONDA_PROMPT_MODIFIER='\''(dassl) '\''
export CONDA_PREFIX_3='\''/home/s2/mjoolee/anaconda'\''
export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\'''
PS1='(dassl) '
++ PS1='(dassl) '
export PATH='/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'
++ export PATH=/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
++ PATH=/home/s2/mjoolee/.vscode-server/bin/903b1e9d8990623e3d7da1df3d33db3e42d80eda/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
export CONDA_PREFIX='/home/s2/mjoolee/anaconda/envs/dassl'
++ export CONDA_PREFIX=/home/s2/mjoolee/anaconda/envs/dassl
++ CONDA_PREFIX=/home/s2/mjoolee/anaconda/envs/dassl
export CONDA_SHLVL='4'
++ export CONDA_SHLVL=4
++ CONDA_SHLVL=4
export CONDA_DEFAULT_ENV='dassl'
++ export CONDA_DEFAULT_ENV=dassl
++ CONDA_DEFAULT_ENV=dassl
export CONDA_PROMPT_MODIFIER='(dassl) '
++ export 'CONDA_PROMPT_MODIFIER=(dassl) '
++ CONDA_PROMPT_MODIFIER='(dassl) '
export CONDA_PREFIX_3='/home/s2/mjoolee/anaconda'
++ export CONDA_PREFIX_3=/home/s2/mjoolee/anaconda
++ CONDA_PREFIX_3=/home/s2/mjoolee/anaconda
export CONDA_EXE='/home/s2/mjoolee/anaconda/bin/conda'
++ export CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
++ CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
export _CE_M=''
++ export _CE_M=
++ _CE_M=
export _CE_CONDA=''
++ export _CE_CONDA=
++ _CE_CONDA=
export CONDA_PYTHON_EXE='/home/s2/mjoolee/anaconda/bin/python'
++ export CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
++ CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r

GPU=0
+ GPU=0
SHOT=16
+ SHOT=16

for dataset in eurosat dtd fgvc_aircraft oxford_flowers
do
    for seed in 1 2 3
    do
    sh scripts/rpo_prime/base2new_train_sdl.sh ${dataset} ${seed} ${GPU} main_tmp1_0.1sdl ${SHOT}
    #sh scripts/rpo_prime/base2new_test.sh ${dataset} ${seed} ${GPU} main_9_9 ${SHOT} base
    sh scripts/rpo_prime/base2new_test_sdl.sh ${dataset} ${seed} ${GPU} main_tmp1_0.1sdl ${SHOT} new
    done
done
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh eurosat 1 0 main_tmp1_0.1sdl 16
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
80 2800 4200
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      2,800
# test     4,200
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 1
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [2/20] time 0.301 (1.470) data 0.000 (0.433) loss 1.4885 (1.1385) lr 9.9901e-02 eta 0:14:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [4/20] time 0.280 (0.877) data 0.000 (0.217) loss 0.7895 (0.8850) lr 9.9901e-02 eta 0:08:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [6/20] time 0.286 (0.680) data 0.000 (0.145) loss 1.2869 (0.9108) lr 9.9901e-02 eta 0:06:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [8/20] time 0.395 (0.595) data 0.000 (0.109) loss 0.8071 (0.8827) lr 9.9901e-02 eta 0:05:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [10/20] time 0.289 (0.533) data 0.000 (0.087) loss 1.0038 (0.9198) lr 9.9901e-02 eta 0:05:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [12/20] time 0.287 (0.492) data 0.000 (0.072) loss 0.7715 (0.8742) lr 9.9901e-02 eta 0:04:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [14/20] time 0.285 (0.463) data 0.000 (0.062) loss 0.5716 (0.8608) lr 9.9901e-02 eta 0:04:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [16/20] time 0.289 (0.441) data 0.000 (0.054) loss 1.2285 (0.8849) lr 9.9901e-02 eta 0:04:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [18/20] time 0.286 (0.424) data 0.000 (0.048) loss 0.9958 (0.8855) lr 9.9901e-02 eta 0:03:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [20/20] time 0.288 (0.410) data 0.000 (0.044) loss 0.5827 (0.8569) lr 9.9606e-02 eta 0:03:49
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:02<00:41,  2.95s/it] 13%|█▎        | 2/15 [00:03<00:17,  1.38s/it] 20%|██        | 3/15 [00:03<00:10,  1.14it/s] 27%|██▋       | 4/15 [00:03<00:07,  1.56it/s] 33%|███▎      | 5/15 [00:04<00:05,  1.95it/s] 40%|████      | 6/15 [00:04<00:03,  2.30it/s] 47%|████▋     | 7/15 [00:04<00:03,  2.60it/s] 53%|█████▎    | 8/15 [00:04<00:02,  2.84it/s] 60%|██████    | 9/15 [00:05<00:01,  3.02it/s] 67%|██████▋   | 10/15 [00:05<00:01,  3.17it/s] 73%|███████▎  | 11/15 [00:05<00:01,  3.28it/s] 80%|████████  | 12/15 [00:06<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:06<00:00,  3.40it/s] 93%|█████████▎| 14/15 [00:06<00:00,  3.45it/s]100%|██████████| 15/15 [00:06<00:00,  2.19it/s]=> result
* total: 2,800
* correct: 1,796
* accuracy: 64.1%
* error: 35.9%
* macro_f1: 62.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [2/20] time 0.274 (0.580) data 0.000 (0.265) loss 0.6424 (0.8056) lr 9.9606e-02 eta 0:05:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [4/20] time 0.275 (0.427) data 0.000 (0.132) loss 0.6559 (0.7459) lr 9.9606e-02 eta 0:03:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [6/20] time 0.277 (0.377) data 0.000 (0.088) loss 0.6310 (0.7031) lr 9.9606e-02 eta 0:03:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [8/20] time 0.274 (0.352) data 0.000 (0.066) loss 0.5679 (0.6208) lr 9.9606e-02 eta 0:03:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [10/20] time 0.278 (0.337) data 0.000 (0.053) loss 0.7530 (0.6685) lr 9.9606e-02 eta 0:03:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [12/20] time 0.274 (0.326) data 0.000 (0.044) loss 0.8985 (0.6946) lr 9.9606e-02 eta 0:02:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [14/20] time 0.279 (0.320) data 0.000 (0.038) loss 0.5820 (0.6876) lr 9.9606e-02 eta 0:02:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [16/20] time 0.271 (0.314) data 0.000 (0.033) loss 0.5408 (0.6904) lr 9.9606e-02 eta 0:02:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [18/20] time 0.280 (0.310) data 0.000 (0.030) loss 0.3801 (0.6568) lr 9.9606e-02 eta 0:02:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [20/20] time 0.277 (0.307) data 0.000 (0.027) loss 0.1869 (0.6101) lr 9.9114e-02 eta 0:02:45
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:26,  1.92s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.05it/s] 20%|██        | 3/15 [00:02<00:07,  1.54it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.98it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.35it/s] 40%|████      | 6/15 [00:03<00:03,  2.65it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.89it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.07it/s] 60%|██████    | 9/15 [00:04<00:01,  3.20it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.30it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.36it/s] 80%|████████  | 12/15 [00:05<00:00,  3.41it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.45it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.47it/s]100%|██████████| 15/15 [00:05<00:00,  2.58it/s]=> result
* total: 2,800
* correct: 2,029
* accuracy: 72.5%
* error: 27.5%
* macro_f1: 71.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [2/20] time 0.275 (0.584) data 0.000 (0.274) loss 0.8879 (0.5331) lr 9.9114e-02 eta 0:05:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [4/20] time 0.282 (0.430) data 0.000 (0.137) loss 0.2313 (0.3515) lr 9.9114e-02 eta 0:03:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [6/20] time 0.280 (0.382) data 0.000 (0.092) loss 0.2196 (0.4086) lr 9.9114e-02 eta 0:03:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [8/20] time 0.286 (0.357) data 0.000 (0.069) loss 0.4599 (0.3701) lr 9.9114e-02 eta 0:03:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [10/20] time 0.276 (0.341) data 0.000 (0.055) loss 0.5853 (0.4298) lr 9.9114e-02 eta 0:03:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [12/20] time 0.281 (0.331) data 0.000 (0.046) loss 0.8064 (0.4555) lr 9.9114e-02 eta 0:02:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [14/20] time 0.278 (0.324) data 0.000 (0.039) loss 0.4166 (0.4778) lr 9.9114e-02 eta 0:02:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [16/20] time 0.278 (0.319) data 0.000 (0.034) loss 0.1046 (0.4841) lr 9.9114e-02 eta 0:02:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [18/20] time 0.281 (0.314) data 0.000 (0.031) loss 0.2445 (0.4388) lr 9.9114e-02 eta 0:02:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [20/20] time 0.278 (0.311) data 0.000 (0.028) loss 0.7937 (0.4582) lr 9.8429e-02 eta 0:02:41
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:26,  1.89s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.05it/s] 20%|██        | 3/15 [00:02<00:07,  1.55it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.99it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.36it/s] 40%|████      | 6/15 [00:03<00:03,  2.66it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.90it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.07it/s] 60%|██████    | 9/15 [00:04<00:01,  3.20it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.29it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.36it/s] 80%|████████  | 12/15 [00:05<00:00,  3.41it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.43it/s]100%|██████████| 15/15 [00:05<00:00,  2.57it/s]=> result
* total: 2,800
* correct: 2,034
* accuracy: 72.6%
* error: 27.4%
* macro_f1: 71.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [2/20] time 0.278 (0.591) data 0.000 (0.260) loss 0.7590 (0.4213) lr 9.8429e-02 eta 0:05:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [4/20] time 0.270 (0.431) data 0.000 (0.130) loss 0.4700 (0.4963) lr 9.8429e-02 eta 0:03:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [6/20] time 0.283 (0.382) data 0.000 (0.087) loss 0.3935 (0.4420) lr 9.8429e-02 eta 0:03:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [8/20] time 0.282 (0.356) data 0.000 (0.065) loss 0.7836 (0.4811) lr 9.8429e-02 eta 0:03:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [10/20] time 0.285 (0.341) data 0.000 (0.052) loss 0.5640 (0.4780) lr 9.8429e-02 eta 0:02:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [12/20] time 0.287 (0.332) data 0.000 (0.043) loss 0.6858 (0.4374) lr 9.8429e-02 eta 0:02:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [14/20] time 0.278 (0.324) data 0.000 (0.037) loss 0.0089 (0.3874) lr 9.8429e-02 eta 0:02:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [16/20] time 0.281 (0.319) data 0.000 (0.033) loss 0.1330 (0.4108) lr 9.8429e-02 eta 0:02:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [18/20] time 0.282 (0.315) data 0.000 (0.029) loss 0.3922 (0.3937) lr 9.8429e-02 eta 0:02:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [20/20] time 0.283 (0.311) data 0.000 (0.026) loss 0.3206 (0.4050) lr 9.7553e-02 eta 0:02:35
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:02<00:28,  2.02s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.00it/s] 20%|██        | 3/15 [00:02<00:08,  1.49it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.93it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.30it/s] 40%|████      | 6/15 [00:03<00:03,  2.59it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.83it/s] 53%|█████▎    | 8/15 [00:04<00:02,  3.00it/s] 60%|██████    | 9/15 [00:04<00:01,  3.13it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.21it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.29it/s] 80%|████████  | 12/15 [00:05<00:00,  3.34it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.37it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  4.23it/s]100%|██████████| 15/15 [00:05<00:00,  2.50it/s]=> result
* total: 2,800
* correct: 2,211
* accuracy: 79.0%
* error: 21.0%
* macro_f1: 78.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [2/20] time 0.301 (0.603) data 0.000 (0.274) loss 0.4993 (0.2600) lr 9.7553e-02 eta 0:05:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [4/20] time 0.291 (0.450) data 0.000 (0.137) loss 0.1462 (0.1492) lr 9.7553e-02 eta 0:03:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [6/20] time 0.289 (0.397) data 0.000 (0.091) loss 0.2319 (0.2564) lr 9.7553e-02 eta 0:03:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [8/20] time 0.285 (0.369) data 0.000 (0.069) loss -0.0249 (0.2443) lr 9.7553e-02 eta 0:03:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [10/20] time 0.293 (0.353) data 0.000 (0.055) loss 0.1484 (0.2011) lr 9.7553e-02 eta 0:02:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [12/20] time 0.289 (0.342) data 0.000 (0.046) loss 0.0933 (0.1908) lr 9.7553e-02 eta 0:02:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [14/20] time 0.286 (0.335) data 0.000 (0.039) loss 0.7538 (0.2403) lr 9.7553e-02 eta 0:02:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [16/20] time 0.288 (0.329) data 0.000 (0.034) loss -0.0711 (0.2082) lr 9.7553e-02 eta 0:02:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [18/20] time 0.288 (0.324) data 0.000 (0.031) loss 0.1364 (0.2256) lr 9.7553e-02 eta 0:02:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [20/20] time 0.289 (0.320) data 0.000 (0.028) loss 0.4630 (0.2340) lr 9.6489e-02 eta 0:02:33
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:22,  1.64s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.08it/s] 20%|██        | 3/15 [00:02<00:07,  1.57it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.01it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.38it/s] 40%|████      | 6/15 [00:03<00:03,  2.67it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.90it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.07it/s] 60%|██████    | 9/15 [00:04<00:01,  3.20it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.29it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.35it/s] 80%|████████  | 12/15 [00:04<00:00,  3.40it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.43it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.46it/s]100%|██████████| 15/15 [00:05<00:00,  2.62it/s]=> result
* total: 2,800
* correct: 2,148
* accuracy: 76.7%
* error: 23.3%
* macro_f1: 76.0%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [2/20] time 0.282 (0.579) data 0.000 (0.248) loss 0.1954 (0.1374) lr 9.6489e-02 eta 0:04:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [4/20] time 0.284 (0.432) data 0.000 (0.124) loss 0.1685 (0.1309) lr 9.6489e-02 eta 0:03:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [6/20] time 0.285 (0.384) data 0.000 (0.083) loss 0.3518 (0.1487) lr 9.6489e-02 eta 0:03:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [8/20] time 0.291 (0.360) data 0.000 (0.062) loss 0.2005 (0.1899) lr 9.6489e-02 eta 0:02:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [10/20] time 0.285 (0.345) data 0.000 (0.050) loss 0.0185 (0.2207) lr 9.6489e-02 eta 0:02:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [12/20] time 0.289 (0.336) data 0.000 (0.041) loss 0.3854 (0.2180) lr 9.6489e-02 eta 0:02:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [14/20] time 0.292 (0.329) data 0.000 (0.036) loss 0.5861 (0.2177) lr 9.6489e-02 eta 0:02:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [16/20] time 0.287 (0.324) data 0.000 (0.031) loss -0.1733 (0.1923) lr 9.6489e-02 eta 0:02:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [18/20] time 0.290 (0.320) data 0.000 (0.028) loss 0.0474 (0.1867) lr 9.6489e-02 eta 0:02:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [20/20] time 0.286 (0.317) data 0.000 (0.025) loss -0.0586 (0.1757) lr 9.5241e-02 eta 0:02:25
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:22,  1.62s/it] 13%|█▎        | 2/15 [00:02<00:11,  1.10it/s] 20%|██        | 3/15 [00:02<00:07,  1.58it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.01it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.38it/s] 40%|████      | 6/15 [00:03<00:03,  2.68it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.90it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.07it/s] 60%|██████    | 9/15 [00:04<00:01,  3.20it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.29it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.35it/s] 80%|████████  | 12/15 [00:04<00:00,  3.40it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.43it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.46it/s]100%|██████████| 15/15 [00:05<00:00,  2.63it/s]=> result
* total: 2,800
* correct: 2,278
* accuracy: 81.4%
* error: 18.6%
* macro_f1: 81.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [2/20] time 0.288 (0.564) data 0.000 (0.248) loss -0.0666 (-0.0292) lr 9.5241e-02 eta 0:04:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [4/20] time 0.280 (0.423) data 0.000 (0.124) loss 0.0191 (0.0842) lr 9.5241e-02 eta 0:03:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [6/20] time 0.302 (0.380) data 0.000 (0.083) loss 0.0752 (0.0781) lr 9.5241e-02 eta 0:02:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [8/20] time 0.285 (0.357) data 0.000 (0.062) loss 0.0311 (0.0862) lr 9.5241e-02 eta 0:02:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [10/20] time 0.287 (0.343) data 0.000 (0.050) loss 0.3625 (0.1144) lr 9.5241e-02 eta 0:02:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [12/20] time 0.289 (0.334) data 0.000 (0.042) loss 0.1574 (0.1249) lr 9.5241e-02 eta 0:02:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [14/20] time 0.288 (0.328) data 0.000 (0.036) loss 0.0811 (0.1406) lr 9.5241e-02 eta 0:02:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [16/20] time 0.290 (0.323) data 0.000 (0.031) loss 0.4332 (0.1512) lr 9.5241e-02 eta 0:02:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [18/20] time 0.289 (0.319) data 0.000 (0.028) loss -0.0630 (0.1483) lr 9.5241e-02 eta 0:02:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [20/20] time 0.290 (0.316) data 0.000 (0.025) loss 0.2893 (0.1465) lr 9.3815e-02 eta 0:02:18
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:26,  1.87s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.04it/s] 20%|██        | 3/15 [00:02<00:07,  1.53it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.97it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.34it/s] 40%|████      | 6/15 [00:03<00:03,  2.64it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.87it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.04it/s] 60%|██████    | 9/15 [00:04<00:01,  3.17it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.27it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.34it/s] 80%|████████  | 12/15 [00:05<00:00,  3.39it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.42it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.45it/s]100%|██████████| 15/15 [00:05<00:00,  2.56it/s]=> result
* total: 2,800
* correct: 2,369
* accuracy: 84.6%
* error: 15.4%
* macro_f1: 84.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [2/20] time 0.273 (0.591) data 0.000 (0.277) loss 0.2153 (0.0323) lr 9.3815e-02 eta 0:04:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [4/20] time 0.276 (0.434) data 0.000 (0.139) loss 0.0414 (0.0283) lr 9.3815e-02 eta 0:03:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [6/20] time 0.276 (0.381) data 0.000 (0.093) loss -0.0828 (0.0180) lr 9.3815e-02 eta 0:02:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [8/20] time 0.277 (0.355) data 0.000 (0.069) loss 0.4596 (0.1054) lr 9.3815e-02 eta 0:02:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [10/20] time 0.279 (0.340) data 0.000 (0.056) loss -0.1736 (0.0655) lr 9.3815e-02 eta 0:02:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [12/20] time 0.276 (0.329) data 0.000 (0.046) loss 0.1246 (0.0645) lr 9.3815e-02 eta 0:02:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [14/20] time 0.280 (0.322) data 0.000 (0.040) loss 0.0348 (0.0548) lr 9.3815e-02 eta 0:02:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [16/20] time 0.274 (0.316) data 0.000 (0.035) loss 0.0224 (0.0541) lr 9.3815e-02 eta 0:02:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [18/20] time 0.276 (0.312) data 0.000 (0.031) loss 0.2226 (0.0566) lr 9.3815e-02 eta 0:02:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [20/20] time 0.279 (0.309) data 0.000 (0.028) loss 0.1980 (0.0883) lr 9.2216e-02 eta 0:02:09
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:02<00:28,  2.03s/it] 13%|█▎        | 2/15 [00:02<00:13,  1.01s/it] 20%|██        | 3/15 [00:02<00:08,  1.47it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.90it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.27it/s] 40%|████      | 6/15 [00:03<00:03,  2.57it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.80it/s] 53%|█████▎    | 8/15 [00:04<00:02,  2.98it/s] 60%|██████    | 9/15 [00:04<00:01,  3.11it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.20it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.27it/s] 80%|████████  | 12/15 [00:05<00:00,  3.32it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.36it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.38it/s]100%|██████████| 15/15 [00:05<00:00,  4.20it/s]100%|██████████| 15/15 [00:06<00:00,  2.49it/s]=> result
* total: 2,800
* correct: 2,403
* accuracy: 85.8%
* error: 14.2%
* macro_f1: 85.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [2/20] time 0.281 (0.597) data 0.000 (0.255) loss 0.0592 (0.0186) lr 9.2216e-02 eta 0:04:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [4/20] time 0.292 (0.443) data 0.000 (0.128) loss -0.1453 (0.1538) lr 9.2216e-02 eta 0:03:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [6/20] time 0.288 (0.392) data 0.000 (0.085) loss 0.1635 (0.2061) lr 9.2216e-02 eta 0:02:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [8/20] time 0.290 (0.367) data 0.000 (0.064) loss -0.0273 (0.1590) lr 9.2216e-02 eta 0:02:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [10/20] time 0.292 (0.351) data 0.000 (0.051) loss 0.1328 (0.1453) lr 9.2216e-02 eta 0:02:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [12/20] time 0.327 (0.345) data 0.000 (0.043) loss 0.1059 (0.1447) lr 9.2216e-02 eta 0:02:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [14/20] time 0.290 (0.337) data 0.000 (0.037) loss 0.0279 (0.1327) lr 9.2216e-02 eta 0:02:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [16/20] time 0.286 (0.331) data 0.000 (0.032) loss 0.2802 (0.1498) lr 9.2216e-02 eta 0:02:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [18/20] time 0.289 (0.326) data 0.000 (0.029) loss 0.0856 (0.1642) lr 9.2216e-02 eta 0:02:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [20/20] time 0.280 (0.321) data 0.000 (0.026) loss -0.0433 (0.1507) lr 9.0451e-02 eta 0:02:08
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:21,  1.57s/it] 13%|█▎        | 2/15 [00:01<00:11,  1.13it/s] 20%|██        | 3/15 [00:02<00:07,  1.64it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.07it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.43it/s] 40%|████      | 6/15 [00:03<00:03,  2.72it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.93it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.09it/s] 60%|██████    | 9/15 [00:03<00:01,  3.21it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.30it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.35it/s] 80%|████████  | 12/15 [00:04<00:00,  3.40it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.43it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.45it/s]100%|██████████| 15/15 [00:05<00:00,  2.68it/s]=> result
* total: 2,800
* correct: 2,353
* accuracy: 84.0%
* error: 16.0%
* macro_f1: 83.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model.pth.tar-10

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [2/20] time 0.305 (0.591) data 0.000 (0.263) loss -0.0103 (0.1459) lr 9.0451e-02 eta 0:03:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [4/20] time 0.284 (0.436) data 0.000 (0.132) loss 0.7098 (0.2561) lr 9.0451e-02 eta 0:02:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [6/20] time 0.279 (0.385) data 0.000 (0.088) loss -0.0528 (0.1890) lr 9.0451e-02 eta 0:02:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [8/20] time 0.286 (0.359) data 0.000 (0.066) loss 0.1965 (0.1512) lr 9.0451e-02 eta 0:02:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [10/20] time 0.278 (0.343) data 0.000 (0.053) loss 0.1621 (0.1416) lr 9.0451e-02 eta 0:02:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [12/20] time 0.289 (0.342) data 0.000 (0.044) loss 0.0041 (0.0979) lr 9.0451e-02 eta 0:02:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [14/20] time 0.276 (0.333) data 0.000 (0.038) loss 1.1124 (0.1738) lr 9.0451e-02 eta 0:02:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [16/20] time 0.283 (0.326) data 0.000 (0.033) loss 0.0466 (0.1496) lr 9.0451e-02 eta 0:02:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [18/20] time 0.286 (0.322) data 0.000 (0.029) loss -0.2350 (0.1661) lr 9.0451e-02 eta 0:02:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [20/20] time 0.285 (0.318) data 0.000 (0.027) loss -0.1993 (0.1243) lr 8.8526e-02 eta 0:02:00
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:21,  1.57s/it] 13%|█▎        | 2/15 [00:01<00:11,  1.13it/s] 20%|██        | 3/15 [00:02<00:07,  1.64it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.07it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.43it/s] 40%|████      | 6/15 [00:03<00:03,  2.71it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.93it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.09it/s] 60%|██████    | 9/15 [00:03<00:01,  3.21it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.29it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.36it/s] 80%|████████  | 12/15 [00:04<00:00,  3.40it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.43it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.45it/s]100%|██████████| 15/15 [00:05<00:00,  2.66it/s]=> result
* total: 2,800
* correct: 2,452
* accuracy: 87.6%
* error: 12.4%
* macro_f1: 87.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [2/20] time 0.287 (0.583) data 0.000 (0.271) loss -0.2655 (-0.1948) lr 8.8526e-02 eta 0:03:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [4/20] time 0.283 (0.433) data 0.000 (0.136) loss -0.1980 (-0.1869) lr 8.8526e-02 eta 0:02:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [6/20] time 0.287 (0.384) data 0.000 (0.090) loss -0.2221 (-0.1893) lr 8.8526e-02 eta 0:02:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [8/20] time 0.281 (0.359) data 0.000 (0.068) loss -0.0464 (-0.1525) lr 8.8526e-02 eta 0:02:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [10/20] time 0.289 (0.344) data 0.000 (0.054) loss 0.1309 (-0.0711) lr 8.8526e-02 eta 0:02:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [12/20] time 0.280 (0.334) data 0.000 (0.045) loss 0.0631 (-0.0303) lr 8.8526e-02 eta 0:02:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [14/20] time 0.288 (0.327) data 0.000 (0.039) loss -0.1327 (-0.0465) lr 8.8526e-02 eta 0:01:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [16/20] time 0.281 (0.321) data 0.000 (0.034) loss 0.1132 (-0.0363) lr 8.8526e-02 eta 0:01:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [18/20] time 0.283 (0.317) data 0.000 (0.030) loss 0.0552 (-0.0366) lr 8.8526e-02 eta 0:01:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [20/20] time 0.290 (0.314) data 0.000 (0.027) loss 0.1923 (-0.0323) lr 8.6448e-02 eta 0:01:53
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:23,  1.65s/it] 13%|█▎        | 2/15 [00:02<00:11,  1.10it/s] 20%|██        | 3/15 [00:02<00:07,  1.60it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.04it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.40it/s] 40%|████      | 6/15 [00:03<00:03,  2.69it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.90it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.06it/s] 60%|██████    | 9/15 [00:04<00:01,  3.19it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.27it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.34it/s] 80%|████████  | 12/15 [00:04<00:00,  3.38it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.63it/s]=> result
* total: 2,800
* correct: 2,335
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 83.1%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [2/20] time 0.279 (0.563) data 0.000 (0.252) loss 0.3777 (0.0380) lr 8.6448e-02 eta 0:03:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [4/20] time 0.286 (0.423) data 0.000 (0.126) loss -0.2871 (-0.0659) lr 8.6448e-02 eta 0:02:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [6/20] time 0.281 (0.376) data 0.000 (0.084) loss 0.0427 (-0.0468) lr 8.6448e-02 eta 0:02:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [8/20] time 0.286 (0.354) data 0.000 (0.063) loss 0.5774 (0.0369) lr 8.6448e-02 eta 0:02:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [10/20] time 0.282 (0.340) data 0.000 (0.051) loss -0.2743 (-0.0125) lr 8.6448e-02 eta 0:01:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [12/20] time 0.285 (0.331) data 0.000 (0.042) loss 0.4019 (0.0156) lr 8.6448e-02 eta 0:01:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [14/20] time 0.284 (0.324) data 0.000 (0.036) loss -0.0920 (-0.0004) lr 8.6448e-02 eta 0:01:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [16/20] time 0.285 (0.319) data 0.000 (0.032) loss 0.4472 (0.0224) lr 8.6448e-02 eta 0:01:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [18/20] time 0.290 (0.316) data 0.000 (0.028) loss -0.2393 (-0.0083) lr 8.6448e-02 eta 0:01:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [20/20] time 0.282 (0.312) data 0.000 (0.025) loss -0.1089 (-0.0121) lr 8.4227e-02 eta 0:01:46
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:25,  1.83s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.04it/s] 20%|██        | 3/15 [00:02<00:07,  1.53it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.96it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.33it/s] 40%|████      | 6/15 [00:03<00:03,  2.63it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.86it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.04it/s] 60%|██████    | 9/15 [00:04<00:01,  3.17it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.26it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.33it/s] 80%|████████  | 12/15 [00:05<00:00,  3.37it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.41it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.43it/s]100%|██████████| 15/15 [00:05<00:00,  2.57it/s]=> result
* total: 2,800
* correct: 2,396
* accuracy: 85.6%
* error: 14.4%
* macro_f1: 85.5%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [2/20] time 0.285 (0.574) data 0.000 (0.269) loss -0.1870 (0.0804) lr 8.4227e-02 eta 0:03:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [4/20] time 0.283 (0.427) data 0.000 (0.135) loss 0.7886 (0.2133) lr 8.4227e-02 eta 0:02:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [6/20] time 0.284 (0.380) data 0.000 (0.090) loss -0.2452 (0.1268) lr 8.4227e-02 eta 0:02:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [8/20] time 0.285 (0.356) data 0.000 (0.067) loss -0.0939 (0.0657) lr 8.4227e-02 eta 0:01:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [10/20] time 0.283 (0.342) data 0.000 (0.054) loss 0.2942 (0.1131) lr 8.4227e-02 eta 0:01:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [12/20] time 0.285 (0.332) data 0.000 (0.045) loss 0.0854 (0.0830) lr 8.4227e-02 eta 0:01:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [14/20] time 0.281 (0.325) data 0.000 (0.039) loss -0.2113 (0.0447) lr 8.4227e-02 eta 0:01:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [16/20] time 0.286 (0.321) data 0.000 (0.034) loss 0.1272 (0.0296) lr 8.4227e-02 eta 0:01:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [18/20] time 0.284 (0.316) data 0.000 (0.030) loss -0.0940 (-0.0060) lr 8.4227e-02 eta 0:01:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [20/20] time 0.286 (0.313) data 0.000 (0.027) loss -0.1752 (-0.0143) lr 8.1871e-02 eta 0:01:40
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:27,  1.98s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.02it/s] 20%|██        | 3/15 [00:02<00:07,  1.50it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.94it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.31it/s] 40%|████      | 6/15 [00:03<00:03,  2.61it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.85it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.03it/s] 60%|██████    | 9/15 [00:04<00:01,  3.16it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.25it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.32it/s] 80%|████████  | 12/15 [00:05<00:00,  3.37it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.41it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.43it/s]100%|██████████| 15/15 [00:05<00:00,  2.54it/s]=> result
* total: 2,800
* correct: 2,451
* accuracy: 87.5%
* error: 12.5%
* macro_f1: 87.4%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [2/20] time 0.286 (0.584) data 0.000 (0.274) loss 0.0106 (-0.1194) lr 8.1871e-02 eta 0:03:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [4/20] time 0.282 (0.434) data 0.000 (0.137) loss -0.2895 (-0.2142) lr 8.1871e-02 eta 0:02:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [6/20] time 0.288 (0.386) data 0.000 (0.091) loss -0.2391 (-0.1237) lr 8.1871e-02 eta 0:02:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [8/20] time 0.285 (0.361) data 0.000 (0.069) loss 0.1874 (-0.1159) lr 8.1871e-02 eta 0:01:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [10/20] time 0.288 (0.346) data 0.000 (0.055) loss -0.4206 (-0.1713) lr 8.1871e-02 eta 0:01:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [12/20] time 0.286 (0.336) data 0.000 (0.046) loss 0.0383 (-0.1451) lr 8.1871e-02 eta 0:01:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [14/20] time 0.315 (0.331) data 0.000 (0.039) loss -0.0401 (-0.1530) lr 8.1871e-02 eta 0:01:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [16/20] time 0.275 (0.324) data 0.000 (0.034) loss -0.3343 (-0.1814) lr 8.1871e-02 eta 0:01:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [18/20] time 0.277 (0.319) data 0.000 (0.031) loss -0.2178 (-0.1811) lr 8.1871e-02 eta 0:01:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [20/20] time 0.277 (0.314) data 0.000 (0.028) loss 0.2736 (-0.1436) lr 7.9389e-02 eta 0:01:34
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:26,  1.92s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.04it/s] 20%|██        | 3/15 [00:02<00:07,  1.53it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.97it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.34it/s] 40%|████      | 6/15 [00:03<00:03,  2.63it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.86it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.04it/s] 60%|██████    | 9/15 [00:04<00:01,  3.16it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.26it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.33it/s] 80%|████████  | 12/15 [00:05<00:00,  3.37it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.41it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.43it/s]100%|██████████| 15/15 [00:05<00:00,  2.56it/s]=> result
* total: 2,800
* correct: 2,336
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 83.2%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [2/20] time 0.283 (0.571) data 0.000 (0.254) loss -0.3430 (0.1715) lr 7.9389e-02 eta 0:02:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [4/20] time 0.302 (0.433) data 0.000 (0.127) loss 0.6979 (0.3320) lr 7.9389e-02 eta 0:02:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [6/20] time 0.285 (0.388) data 0.000 (0.085) loss -0.3119 (0.0998) lr 7.9389e-02 eta 0:01:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [8/20] time 0.282 (0.362) data 0.000 (0.064) loss -0.1050 (0.0591) lr 7.9389e-02 eta 0:01:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [10/20] time 0.289 (0.346) data 0.000 (0.051) loss 0.0939 (0.0416) lr 7.9389e-02 eta 0:01:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [12/20] time 0.280 (0.336) data 0.000 (0.043) loss -0.3918 (-0.0091) lr 7.9389e-02 eta 0:01:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [14/20] time 0.286 (0.329) data 0.000 (0.036) loss -0.4039 (-0.0620) lr 7.9389e-02 eta 0:01:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [16/20] time 0.281 (0.323) data 0.000 (0.032) loss -0.3875 (-0.0689) lr 7.9389e-02 eta 0:01:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [18/20] time 0.277 (0.318) data 0.000 (0.028) loss -0.1650 (-0.0780) lr 7.9389e-02 eta 0:01:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [20/20] time 0.277 (0.314) data 0.000 (0.026) loss -0.2385 (-0.1018) lr 7.6791e-02 eta 0:01:27
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:27,  1.93s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.03it/s] 20%|██        | 3/15 [00:02<00:07,  1.52it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.95it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.32it/s] 40%|████      | 6/15 [00:03<00:03,  2.62it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.85it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.03it/s] 60%|██████    | 9/15 [00:04<00:01,  3.16it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.25it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.32it/s] 80%|████████  | 12/15 [00:05<00:00,  3.37it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.40it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.42it/s]100%|██████████| 15/15 [00:05<00:00,  4.27it/s]100%|██████████| 15/15 [00:05<00:00,  2.54it/s]=> result
* total: 2,800
* correct: 2,487
* accuracy: 88.8%
* error: 11.2%
* macro_f1: 88.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [2/20] time 0.283 (0.586) data 0.000 (0.273) loss -0.2935 (-0.3358) lr 7.6791e-02 eta 0:02:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [4/20] time 0.290 (0.437) data 0.000 (0.137) loss -0.3159 (-0.3429) lr 7.6791e-02 eta 0:02:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [6/20] time 0.283 (0.387) data 0.000 (0.091) loss -0.2621 (-0.2973) lr 7.6791e-02 eta 0:01:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [8/20] time 0.288 (0.363) data 0.000 (0.068) loss 0.4877 (-0.1206) lr 7.6791e-02 eta 0:01:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [10/20] time 0.284 (0.356) data 0.000 (0.055) loss -0.1859 (-0.1435) lr 7.6791e-02 eta 0:01:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [12/20] time 0.286 (0.345) data 0.000 (0.046) loss 0.0130 (-0.1543) lr 7.6791e-02 eta 0:01:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [14/20] time 0.285 (0.336) data 0.000 (0.039) loss -0.0727 (-0.1541) lr 7.6791e-02 eta 0:01:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [16/20] time 0.284 (0.330) data 0.000 (0.034) loss -0.2732 (-0.1572) lr 7.6791e-02 eta 0:01:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [18/20] time 0.291 (0.325) data 0.000 (0.031) loss -0.1935 (-0.1569) lr 7.6791e-02 eta 0:01:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [20/20] time 0.285 (0.321) data 0.000 (0.028) loss -0.0471 (-0.1471) lr 7.4088e-02 eta 0:01:23
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:23,  1.70s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.08it/s] 20%|██        | 3/15 [00:02<00:07,  1.58it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.01it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.38it/s] 40%|████      | 6/15 [00:03<00:03,  2.67it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.88it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.05it/s] 60%|██████    | 9/15 [00:04<00:01,  3.17it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.26it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.32it/s] 80%|████████  | 12/15 [00:04<00:00,  3.37it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.40it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.43it/s]100%|██████████| 15/15 [00:05<00:00,  2.60it/s]=> result
* total: 2,800
* correct: 2,379
* accuracy: 85.0%
* error: 15.0%
* macro_f1: 84.7%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [2/20] time 0.282 (0.572) data 0.000 (0.257) loss -0.2795 (-0.3599) lr 7.4088e-02 eta 0:02:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [4/20] time 0.288 (0.429) data 0.000 (0.129) loss 0.3343 (-0.1971) lr 7.4088e-02 eta 0:01:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [6/20] time 0.289 (0.383) data 0.000 (0.086) loss 0.1468 (-0.0340) lr 7.4088e-02 eta 0:01:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [8/20] time 0.292 (0.360) data 0.000 (0.065) loss -0.4526 (-0.0691) lr 7.4088e-02 eta 0:01:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [10/20] time 0.287 (0.345) data 0.000 (0.052) loss -0.0402 (-0.1118) lr 7.4088e-02 eta 0:01:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [12/20] time 0.289 (0.336) data 0.000 (0.043) loss 0.0539 (-0.1042) lr 7.4088e-02 eta 0:01:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [14/20] time 0.288 (0.329) data 0.000 (0.037) loss -0.2173 (-0.1046) lr 7.4088e-02 eta 0:01:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [16/20] time 0.292 (0.324) data 0.000 (0.032) loss -0.2688 (-0.1274) lr 7.4088e-02 eta 0:01:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [18/20] time 0.286 (0.320) data 0.000 (0.029) loss -0.3127 (-0.1524) lr 7.4088e-02 eta 0:01:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [20/20] time 0.290 (0.317) data 0.000 (0.026) loss -0.4244 (-0.1799) lr 7.1289e-02 eta 0:01:16
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:26,  1.87s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.06it/s] 20%|██        | 3/15 [00:02<00:07,  1.56it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.99it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.36it/s] 40%|████      | 6/15 [00:03<00:03,  2.65it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.87it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.04it/s] 60%|██████    | 9/15 [00:04<00:01,  3.16it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.25it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.32it/s] 80%|████████  | 12/15 [00:05<00:00,  3.37it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.40it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.42it/s]100%|██████████| 15/15 [00:05<00:00,  2.56it/s]=> result
* total: 2,800
* correct: 2,320
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 82.4%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [2/20] time 0.289 (0.585) data 0.000 (0.261) loss -0.2578 (-0.3333) lr 7.1289e-02 eta 0:02:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [4/20] time 0.279 (0.431) data 0.000 (0.130) loss -0.4033 (-0.3284) lr 7.1289e-02 eta 0:01:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [6/20] time 0.282 (0.382) data 0.000 (0.087) loss -0.0293 (-0.1878) lr 7.1289e-02 eta 0:01:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [8/20] time 0.279 (0.356) data 0.000 (0.065) loss -0.5512 (-0.2453) lr 7.1289e-02 eta 0:01:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [10/20] time 0.288 (0.343) data 0.000 (0.052) loss -0.1470 (-0.2260) lr 7.1289e-02 eta 0:01:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [12/20] time 0.281 (0.333) data 0.000 (0.044) loss 0.0853 (-0.2146) lr 7.1289e-02 eta 0:01:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [14/20] time 0.282 (0.326) data 0.000 (0.037) loss -0.3938 (-0.2222) lr 7.1289e-02 eta 0:01:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [16/20] time 0.284 (0.321) data 0.000 (0.033) loss -0.4608 (-0.2417) lr 7.1289e-02 eta 0:01:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [18/20] time 0.283 (0.317) data 0.000 (0.029) loss -0.2984 (-0.2451) lr 7.1289e-02 eta 0:01:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [20/20] time 0.285 (0.313) data 0.000 (0.026) loss 0.2361 (-0.1995) lr 6.8406e-02 eta 0:01:08
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:23,  1.65s/it] 13%|█▎        | 2/15 [00:02<00:11,  1.09it/s] 20%|██        | 3/15 [00:02<00:07,  1.59it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.03it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.39it/s] 40%|████      | 6/15 [00:03<00:03,  2.67it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.89it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.06it/s] 60%|██████    | 9/15 [00:04<00:01,  3.18it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.26it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.32it/s] 80%|████████  | 12/15 [00:04<00:00,  3.37it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.40it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.62it/s]=> result
* total: 2,800
* correct: 2,359
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 84.0%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [2/20] time 0.282 (0.571) data 0.000 (0.257) loss -0.0136 (-0.2219) lr 6.8406e-02 eta 0:02:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [4/20] time 0.294 (0.429) data 0.000 (0.128) loss -0.0867 (-0.1378) lr 6.8406e-02 eta 0:01:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [6/20] time 0.280 (0.381) data 0.000 (0.086) loss -0.4531 (-0.1854) lr 6.8406e-02 eta 0:01:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [8/20] time 0.287 (0.357) data 0.000 (0.064) loss 0.0362 (-0.1346) lr 6.8406e-02 eta 0:01:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [10/20] time 0.283 (0.342) data 0.000 (0.051) loss -0.0105 (-0.1168) lr 6.8406e-02 eta 0:01:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [12/20] time 0.289 (0.333) data 0.000 (0.043) loss -0.1088 (-0.0994) lr 6.8406e-02 eta 0:01:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [14/20] time 0.288 (0.326) data 0.000 (0.037) loss -0.1394 (-0.1047) lr 6.8406e-02 eta 0:01:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [16/20] time 0.286 (0.321) data 0.000 (0.032) loss -0.3287 (-0.1287) lr 6.8406e-02 eta 0:01:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [18/20] time 0.291 (0.318) data 0.000 (0.029) loss -0.4715 (-0.1697) lr 6.8406e-02 eta 0:01:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [20/20] time 0.286 (0.315) data 0.000 (0.026) loss 0.1371 (-0.1640) lr 6.5451e-02 eta 0:01:02
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:02<00:29,  2.08s/it] 13%|█▎        | 2/15 [00:02<00:13,  1.03s/it] 20%|██        | 3/15 [00:02<00:08,  1.45it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.88it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.26it/s] 40%|████      | 6/15 [00:03<00:03,  2.56it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.80it/s] 53%|█████▎    | 8/15 [00:04<00:02,  2.99it/s] 60%|██████    | 9/15 [00:04<00:01,  3.13it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:06<00:00,  2.47it/s]=> result
* total: 2,800
* correct: 2,474
* accuracy: 88.4%
* error: 11.6%
* macro_f1: 88.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model.pth.tar-20

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [2/20] time 0.272 (0.584) data 0.000 (0.267) loss -0.1481 (-0.0307) lr 6.5451e-02 eta 0:01:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [4/20] time 0.287 (0.434) data 0.000 (0.134) loss 0.0276 (-0.0170) lr 6.5451e-02 eta 0:01:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [6/20] time 0.275 (0.383) data 0.000 (0.089) loss -0.5791 (-0.1933) lr 6.5451e-02 eta 0:01:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [8/20] time 0.291 (0.358) data 0.000 (0.067) loss -0.6106 (-0.2279) lr 6.5451e-02 eta 0:01:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [10/20] time 0.271 (0.341) data 0.000 (0.054) loss -0.2521 (-0.2485) lr 6.5451e-02 eta 0:01:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [12/20] time 0.279 (0.331) data 0.000 (0.045) loss -0.3074 (-0.2624) lr 6.5451e-02 eta 0:01:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [14/20] time 0.280 (0.323) data 0.000 (0.038) loss -0.0585 (-0.2547) lr 6.5451e-02 eta 0:01:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [16/20] time 0.287 (0.319) data 0.000 (0.034) loss -0.0271 (-0.2291) lr 6.5451e-02 eta 0:00:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [18/20] time 0.397 (0.322) data 0.000 (0.030) loss -0.5504 (-0.2380) lr 6.5451e-02 eta 0:00:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [20/20] time 0.286 (0.319) data 0.000 (0.027) loss -0.3948 (-0.2349) lr 6.2434e-02 eta 0:00:57
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:22,  1.63s/it] 13%|█▎        | 2/15 [00:02<00:11,  1.10it/s] 20%|██        | 3/15 [00:02<00:07,  1.59it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.03it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.39it/s] 40%|████      | 6/15 [00:03<00:03,  2.67it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.89it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.05it/s] 60%|██████    | 9/15 [00:04<00:01,  3.18it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.26it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.32it/s] 80%|████████  | 12/15 [00:04<00:00,  3.37it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.40it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.42it/s]100%|██████████| 15/15 [00:05<00:00,  2.62it/s]=> result
* total: 2,800
* correct: 2,473
* accuracy: 88.3%
* error: 11.7%
* macro_f1: 88.3%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [2/20] time 0.286 (0.575) data 0.001 (0.255) loss -0.3296 (-0.2574) lr 6.2434e-02 eta 0:01:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [4/20] time 0.283 (0.428) data 0.000 (0.128) loss -0.2109 (-0.2103) lr 6.2434e-02 eta 0:01:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [6/20] time 0.287 (0.382) data 0.000 (0.085) loss -0.4127 (-0.2495) lr 6.2434e-02 eta 0:01:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [8/20] time 0.288 (0.358) data 0.000 (0.064) loss -0.3592 (-0.2572) lr 6.2434e-02 eta 0:01:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [10/20] time 0.287 (0.344) data 0.000 (0.051) loss -0.4057 (-0.2423) lr 6.2434e-02 eta 0:00:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [12/20] time 0.290 (0.335) data 0.000 (0.043) loss -0.0386 (-0.2328) lr 6.2434e-02 eta 0:00:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [14/20] time 0.284 (0.328) data 0.000 (0.037) loss -0.2664 (-0.2501) lr 6.2434e-02 eta 0:00:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [16/20] time 0.320 (0.325) data 0.000 (0.032) loss -0.2952 (-0.2526) lr 6.2434e-02 eta 0:00:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [18/20] time 0.283 (0.321) data 0.000 (0.029) loss -0.4024 (-0.2637) lr 6.2434e-02 eta 0:00:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [20/20] time 0.286 (0.317) data 0.000 (0.026) loss -0.2159 (-0.2749) lr 5.9369e-02 eta 0:00:50
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:21,  1.57s/it] 13%|█▎        | 2/15 [00:01<00:11,  1.13it/s] 20%|██        | 3/15 [00:02<00:07,  1.64it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.07it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.42it/s] 40%|████      | 6/15 [00:03<00:03,  2.70it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.91it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.07it/s] 60%|██████    | 9/15 [00:03<00:01,  3.18it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.27it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.33it/s] 80%|████████  | 12/15 [00:04<00:00,  3.37it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.40it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.42it/s]100%|██████████| 15/15 [00:05<00:00,  2.65it/s]=> result
* total: 2,800
* correct: 2,567
* accuracy: 91.7%
* error: 8.3%
* macro_f1: 91.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [2/20] time 0.266 (0.547) data 0.000 (0.251) loss -0.2316 (-0.1883) lr 5.9369e-02 eta 0:01:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [4/20] time 0.266 (0.407) data 0.000 (0.125) loss -0.2338 (-0.2614) lr 5.9369e-02 eta 0:01:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [6/20] time 0.273 (0.362) data 0.000 (0.084) loss -0.3488 (-0.2496) lr 5.9369e-02 eta 0:00:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [8/20] time 0.289 (0.343) data 0.000 (0.063) loss -0.4158 (-0.1768) lr 5.9369e-02 eta 0:00:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [10/20] time 0.289 (0.332) data 0.000 (0.050) loss -0.4999 (-0.2005) lr 5.9369e-02 eta 0:00:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [12/20] time 0.285 (0.324) data 0.000 (0.042) loss -0.6625 (-0.2434) lr 5.9369e-02 eta 0:00:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [14/20] time 0.296 (0.320) data 0.000 (0.036) loss -0.2532 (-0.2714) lr 5.9369e-02 eta 0:00:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [16/20] time 0.275 (0.315) data 0.000 (0.032) loss 0.0425 (-0.2498) lr 5.9369e-02 eta 0:00:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [18/20] time 0.291 (0.311) data 0.000 (0.028) loss -0.4120 (-0.2662) lr 5.9369e-02 eta 0:00:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [20/20] time 0.281 (0.309) data 0.000 (0.025) loss -0.4750 (-0.2853) lr 5.6267e-02 eta 0:00:43
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:25,  1.80s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.05it/s] 20%|██        | 3/15 [00:02<00:07,  1.53it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.97it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.33it/s] 40%|████      | 6/15 [00:03<00:03,  2.63it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.85it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.02it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.31it/s] 80%|████████  | 12/15 [00:05<00:00,  3.36it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.57it/s]=> result
* total: 2,800
* correct: 2,571
* accuracy: 91.8%
* error: 8.2%
* macro_f1: 91.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [2/20] time 0.283 (0.566) data 0.000 (0.247) loss -0.3175 (-0.1997) lr 5.6267e-02 eta 0:01:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [4/20] time 0.281 (0.423) data 0.000 (0.124) loss -0.4447 (-0.3440) lr 5.6267e-02 eta 0:00:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [6/20] time 0.285 (0.378) data 0.000 (0.083) loss -0.3886 (-0.3835) lr 5.6267e-02 eta 0:00:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [8/20] time 0.285 (0.354) data 0.000 (0.062) loss 0.0207 (-0.3576) lr 5.6267e-02 eta 0:00:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [10/20] time 0.280 (0.340) data 0.000 (0.050) loss -0.4072 (-0.3546) lr 5.6267e-02 eta 0:00:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [12/20] time 0.289 (0.331) data 0.000 (0.041) loss -0.4267 (-0.3686) lr 5.6267e-02 eta 0:00:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [14/20] time 0.284 (0.325) data 0.000 (0.036) loss -0.2381 (-0.3740) lr 5.6267e-02 eta 0:00:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [16/20] time 0.288 (0.320) data 0.000 (0.031) loss -0.5058 (-0.3975) lr 5.6267e-02 eta 0:00:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [18/20] time 0.286 (0.316) data 0.000 (0.028) loss -0.5332 (-0.4098) lr 5.6267e-02 eta 0:00:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [20/20] time 0.308 (0.314) data 0.000 (0.025) loss -0.3314 (-0.4089) lr 5.3140e-02 eta 0:00:37
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:21,  1.55s/it] 13%|█▎        | 2/15 [00:01<00:11,  1.14it/s] 20%|██        | 3/15 [00:02<00:07,  1.60it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.04it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.39it/s] 40%|████      | 6/15 [00:03<00:03,  2.68it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.89it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.05it/s] 60%|██████    | 9/15 [00:04<00:01,  3.17it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.26it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.32it/s] 80%|████████  | 12/15 [00:04<00:00,  3.37it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.40it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.42it/s]100%|██████████| 15/15 [00:05<00:00,  2.65it/s]=> result
* total: 2,800
* correct: 2,498
* accuracy: 89.2%
* error: 10.8%
* macro_f1: 88.8%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [2/20] time 0.311 (0.636) data 0.001 (0.281) loss -0.5463 (-0.5387) lr 5.3140e-02 eta 0:01:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [4/20] time 0.280 (0.458) data 0.000 (0.141) loss -0.5461 (-0.3139) lr 5.3140e-02 eta 0:00:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [6/20] time 0.285 (0.400) data 0.000 (0.094) loss -0.0165 (-0.2857) lr 5.3140e-02 eta 0:00:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [8/20] time 0.284 (0.371) data 0.000 (0.071) loss -0.1965 (-0.2415) lr 5.3140e-02 eta 0:00:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [10/20] time 0.287 (0.354) data 0.000 (0.057) loss -0.2895 (-0.2582) lr 5.3140e-02 eta 0:00:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [12/20] time 0.284 (0.342) data 0.000 (0.047) loss -0.2883 (-0.2716) lr 5.3140e-02 eta 0:00:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [14/20] time 0.283 (0.334) data 0.000 (0.040) loss -0.4706 (-0.3028) lr 5.3140e-02 eta 0:00:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [16/20] time 0.285 (0.327) data 0.000 (0.035) loss -0.3788 (-0.3134) lr 5.3140e-02 eta 0:00:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [18/20] time 0.283 (0.323) data 0.000 (0.031) loss -0.2588 (-0.2975) lr 5.3140e-02 eta 0:00:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [20/20] time 0.288 (0.319) data 0.000 (0.028) loss -0.1074 (-0.2637) lr 5.0000e-02 eta 0:00:31
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:24,  1.77s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.05it/s] 20%|██        | 3/15 [00:02<00:07,  1.54it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.97it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.34it/s] 40%|████      | 6/15 [00:03<00:03,  2.63it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.86it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.03it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.31it/s] 80%|████████  | 12/15 [00:05<00:00,  3.36it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.42it/s]100%|██████████| 15/15 [00:05<00:00,  2.58it/s]=> result
* total: 2,800
* correct: 2,558
* accuracy: 91.4%
* error: 8.6%
* macro_f1: 91.4%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [2/20] time 0.270 (0.575) data 0.000 (0.273) loss -0.3275 (0.0038) lr 5.0000e-02 eta 0:00:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [4/20] time 0.272 (0.422) data 0.000 (0.137) loss -0.3558 (-0.1327) lr 5.0000e-02 eta 0:00:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [6/20] time 0.294 (0.376) data 0.000 (0.091) loss -0.1294 (-0.1925) lr 5.0000e-02 eta 0:00:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [8/20] time 0.277 (0.354) data 0.000 (0.069) loss -0.5525 (-0.2506) lr 5.0000e-02 eta 0:00:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [10/20] time 0.277 (0.338) data 0.000 (0.055) loss -0.5004 (-0.2886) lr 5.0000e-02 eta 0:00:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [12/20] time 0.272 (0.328) data 0.000 (0.046) loss -0.6357 (-0.3372) lr 5.0000e-02 eta 0:00:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [14/20] time 0.278 (0.321) data 0.000 (0.039) loss -0.2982 (-0.3379) lr 5.0000e-02 eta 0:00:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [16/20] time 0.273 (0.315) data 0.000 (0.034) loss -0.4239 (-0.3502) lr 5.0000e-02 eta 0:00:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [18/20] time 0.283 (0.311) data 0.000 (0.031) loss -0.5421 (-0.3741) lr 5.0000e-02 eta 0:00:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [20/20] time 0.273 (0.308) data 0.000 (0.028) loss -0.6836 (-0.3921) lr 4.6860e-02 eta 0:00:24
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:25,  1.81s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.02it/s] 20%|██        | 3/15 [00:02<00:07,  1.50it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.94it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.30it/s] 40%|████      | 6/15 [00:03<00:03,  2.60it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.83it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.01it/s] 60%|██████    | 9/15 [00:04<00:01,  3.14it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.54it/s]=> result
* total: 2,800
* correct: 2,557
* accuracy: 91.3%
* error: 8.7%
* macro_f1: 91.3%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [2/20] time 0.266 (0.586) data 0.000 (0.278) loss -0.2634 (-0.1831) lr 4.6860e-02 eta 0:00:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [4/20] time 0.283 (0.431) data 0.000 (0.139) loss -0.3971 (-0.2808) lr 4.6860e-02 eta 0:00:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [6/20] time 0.273 (0.379) data 0.000 (0.093) loss -0.3964 (-0.3403) lr 4.6860e-02 eta 0:00:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [8/20] time 0.280 (0.354) data 0.000 (0.070) loss -0.5584 (-0.3661) lr 4.6860e-02 eta 0:00:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [10/20] time 0.273 (0.338) data 0.000 (0.056) loss -0.5456 (-0.4012) lr 4.6860e-02 eta 0:00:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [12/20] time 0.278 (0.328) data 0.000 (0.047) loss -0.1896 (-0.3916) lr 4.6860e-02 eta 0:00:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [14/20] time 0.275 (0.321) data 0.000 (0.040) loss -0.0129 (-0.3801) lr 4.6860e-02 eta 0:00:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [16/20] time 0.277 (0.315) data 0.000 (0.035) loss -0.6126 (-0.4021) lr 4.6860e-02 eta 0:00:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [18/20] time 0.277 (0.311) data 0.000 (0.031) loss -0.1133 (-0.3878) lr 4.6860e-02 eta 0:00:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [20/20] time 0.281 (0.308) data 0.000 (0.028) loss -0.6002 (-0.3990) lr 4.3733e-02 eta 0:00:18
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:22,  1.59s/it] 13%|█▎        | 2/15 [00:01<00:11,  1.17it/s] 20%|██        | 3/15 [00:02<00:07,  1.68it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.11it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.45it/s] 40%|████      | 6/15 [00:03<00:03,  2.73it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.93it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.09it/s] 60%|██████    | 9/15 [00:03<00:01,  3.18it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.27it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.33it/s] 80%|████████  | 12/15 [00:04<00:00,  3.37it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.40it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.42it/s]100%|██████████| 15/15 [00:05<00:00,  2.67it/s]=> result
* total: 2,800
* correct: 2,473
* accuracy: 88.3%
* error: 11.7%
* macro_f1: 88.0%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [2/20] time 0.284 (0.574) data 0.000 (0.251) loss -0.1002 (-0.2903) lr 4.3733e-02 eta 0:00:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [4/20] time 0.282 (0.428) data 0.000 (0.125) loss -0.5451 (-0.3538) lr 4.3733e-02 eta 0:00:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [6/20] time 0.284 (0.379) data 0.000 (0.084) loss -0.3187 (-0.4059) lr 4.3733e-02 eta 0:00:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [8/20] time 0.299 (0.357) data 0.000 (0.063) loss -0.6018 (-0.4291) lr 4.3733e-02 eta 0:00:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [10/20] time 0.287 (0.343) data 0.000 (0.050) loss -0.6882 (-0.4694) lr 4.3733e-02 eta 0:00:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [12/20] time 0.280 (0.333) data 0.000 (0.042) loss -0.4183 (-0.4607) lr 4.3733e-02 eta 0:00:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [14/20] time 0.286 (0.326) data 0.000 (0.036) loss -0.5910 (-0.4872) lr 4.3733e-02 eta 0:00:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [16/20] time 0.279 (0.320) data 0.000 (0.032) loss -0.4578 (-0.4794) lr 4.3733e-02 eta 0:00:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [18/20] time 0.285 (0.316) data 0.000 (0.028) loss -0.4590 (-0.4818) lr 4.3733e-02 eta 0:00:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [20/20] time 0.281 (0.313) data 0.000 (0.025) loss -0.2755 (-0.4730) lr 4.0631e-02 eta 0:00:12
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:27,  2.00s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.01it/s] 20%|██        | 3/15 [00:02<00:08,  1.48it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.90it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.28it/s] 40%|████      | 6/15 [00:03<00:03,  2.58it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.81it/s] 53%|█████▎    | 8/15 [00:04<00:02,  2.99it/s] 60%|██████    | 9/15 [00:04<00:01,  3.13it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.22it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.29it/s] 80%|████████  | 12/15 [00:05<00:00,  3.34it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.50it/s]=> result
* total: 2,800
* correct: 2,569
* accuracy: 91.8%
* error: 8.2%
* macro_f1: 91.7%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [2/20] time 0.282 (0.590) data 0.001 (0.258) loss -0.2733 (-0.3866) lr 4.0631e-02 eta 0:00:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [4/20] time 0.293 (0.440) data 0.000 (0.129) loss -0.3792 (-0.4135) lr 4.0631e-02 eta 0:00:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [6/20] time 0.302 (0.393) data 0.000 (0.086) loss -0.5686 (-0.4474) lr 4.0631e-02 eta 0:00:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [8/20] time 0.283 (0.366) data 0.000 (0.065) loss -0.3738 (-0.4081) lr 4.0631e-02 eta 0:00:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [10/20] time 0.286 (0.350) data 0.000 (0.052) loss -0.7488 (-0.4499) lr 4.0631e-02 eta 0:00:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [12/20] time 0.282 (0.339) data 0.000 (0.043) loss -0.5289 (-0.4361) lr 4.0631e-02 eta 0:00:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [14/20] time 0.288 (0.331) data 0.000 (0.037) loss -0.3907 (-0.4482) lr 4.0631e-02 eta 0:00:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [16/20] time 0.283 (0.325) data 0.000 (0.032) loss -0.5123 (-0.4446) lr 4.0631e-02 eta 0:00:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [18/20] time 0.289 (0.321) data 0.000 (0.029) loss -0.4117 (-0.4457) lr 4.0631e-02 eta 0:00:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [20/20] time 0.282 (0.317) data 0.000 (0.026) loss 0.0548 (-0.4282) lr 3.7566e-02 eta 0:00:06
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:22,  1.58s/it] 13%|█▎        | 2/15 [00:01<00:11,  1.12it/s] 20%|██        | 3/15 [00:02<00:07,  1.62it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.04it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.39it/s] 40%|████      | 6/15 [00:03<00:03,  2.66it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.87it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.02it/s] 60%|██████    | 9/15 [00:04<00:01,  3.13it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.21it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.25it/s] 80%|████████  | 12/15 [00:04<00:00,  3.29it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.34it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.37it/s]100%|██████████| 15/15 [00:05<00:00,  4.22it/s]100%|██████████| 15/15 [00:05<00:00,  2.62it/s]=> result
* total: 2,800
* correct: 2,532
* accuracy: 90.4%
* error: 9.6%
* macro_f1: 90.4%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [2/20] time 0.289 (0.575) data 0.000 (0.263) loss -0.2092 (-0.3533) lr 3.7566e-02 eta 0:00:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [4/20] time 0.287 (0.429) data 0.000 (0.132) loss -0.5158 (-0.4426) lr 3.7566e-02 eta 0:00:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [6/20] time 0.289 (0.383) data 0.000 (0.088) loss -0.1764 (-0.4514) lr 3.7566e-02 eta 0:00:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [8/20] time 0.291 (0.360) data 0.000 (0.066) loss -0.5374 (-0.4670) lr 3.7566e-02 eta 0:00:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [10/20] time 0.288 (0.346) data 0.000 (0.053) loss -0.2115 (-0.4540) lr 3.7566e-02 eta 0:00:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [12/20] time 0.288 (0.337) data 0.000 (0.044) loss -0.4350 (-0.4228) lr 3.7566e-02 eta 0:00:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [14/20] time 0.283 (0.329) data 0.000 (0.038) loss -0.6325 (-0.4326) lr 3.7566e-02 eta 0:00:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [16/20] time 0.287 (0.324) data 0.000 (0.033) loss -0.4258 (-0.4335) lr 3.7566e-02 eta 0:00:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [18/20] time 0.285 (0.320) data 0.000 (0.029) loss -0.5108 (-0.4461) lr 3.7566e-02 eta 0:00:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [20/20] time 0.292 (0.317) data 0.000 (0.026) loss -0.6087 (-0.4638) lr 3.4549e-02 eta 0:00:00
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:24,  1.72s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.06it/s] 20%|██        | 3/15 [00:02<00:07,  1.55it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.98it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.34it/s] 40%|████      | 6/15 [00:03<00:03,  2.63it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.86it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.03it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.31it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.57it/s]
=> result
* total: 2,800
* correct: 2,504
* accuracy: 89.4%
* error: 10.6%
* macro_f1: 89.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar" (epoch = 23)
Evaluate on the *test* set
  0%|          | 0/22 [00:00<?, ?it/s]  5%|▍         | 1/22 [00:01<00:41,  1.98s/it]  9%|▉         | 2/22 [00:02<00:24,  1.21s/it] 14%|█▎        | 3/22 [00:03<00:16,  1.14it/s] 18%|█▊        | 4/22 [00:03<00:11,  1.55it/s] 23%|██▎       | 5/22 [00:03<00:08,  1.93it/s] 27%|██▋       | 6/22 [00:04<00:07,  2.28it/s] 32%|███▏      | 7/22 [00:04<00:05,  2.56it/s] 36%|███▋      | 8/22 [00:04<00:05,  2.79it/s] 41%|████      | 9/22 [00:04<00:04,  2.97it/s] 45%|████▌     | 10/22 [00:05<00:03,  3.11it/s] 50%|█████     | 11/22 [00:05<00:03,  3.20it/s] 55%|█████▍    | 12/22 [00:05<00:03,  3.27it/s] 59%|█████▉    | 13/22 [00:06<00:02,  3.33it/s] 64%|██████▎   | 14/22 [00:06<00:02,  3.37it/s] 68%|██████▊   | 15/22 [00:06<00:02,  3.39it/s] 73%|███████▎  | 16/22 [00:06<00:01,  3.41it/s] 77%|███████▋  | 17/22 [00:07<00:01,  3.43it/s] 82%|████████▏ | 18/22 [00:07<00:01,  3.44it/s] 86%|████████▋ | 19/22 [00:07<00:00,  3.44it/s] 91%|█████████ | 20/22 [00:08<00:00,  3.45it/s] 95%|█████████▌| 21/22 [00:08<00:00,  3.45it/s]100%|██████████| 22/22 [00:08<00:00,  4.08it/s]100%|██████████| 22/22 [00:08<00:00,  2.56it/s]
=> result
* total: 4,200
* correct: 3,847
* accuracy: 91.6%
* error: 8.4%
* macro_f1: 91.4%
Elapsed: 0:06:06
+ sh scripts/rpo_prime/base2new_test_sdl.sh eurosat 1 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
80 2600 3900
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      2,600
# test     3,900
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar" (epoch = 23)
Evaluate on the *test* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:04<01:31,  4.80s/it] 10%|█         | 2/20 [00:05<00:38,  2.14s/it] 15%|█▌        | 3/20 [00:05<00:22,  1.30s/it] 20%|██        | 4/20 [00:05<00:14,  1.11it/s] 25%|██▌       | 5/20 [00:05<00:10,  1.47it/s] 30%|███       | 6/20 [00:06<00:07,  1.83it/s] 35%|███▌      | 7/20 [00:06<00:06,  2.17it/s] 40%|████      | 8/20 [00:06<00:04,  2.46it/s] 45%|████▌     | 9/20 [00:07<00:04,  2.70it/s] 50%|█████     | 10/20 [00:07<00:03,  2.89it/s] 55%|█████▌    | 11/20 [00:07<00:02,  3.05it/s] 60%|██████    | 12/20 [00:07<00:02,  3.17it/s] 65%|██████▌   | 13/20 [00:08<00:02,  3.26it/s] 70%|███████   | 14/20 [00:08<00:01,  3.32it/s] 75%|███████▌  | 15/20 [00:08<00:01,  3.36it/s] 80%|████████  | 16/20 [00:09<00:01,  3.40it/s] 85%|████████▌ | 17/20 [00:09<00:00,  3.42it/s] 90%|█████████ | 18/20 [00:09<00:00,  3.43it/s] 95%|█████████▌| 19/20 [00:09<00:00,  3.42it/s]100%|██████████| 20/20 [00:10<00:00,  3.53it/s]100%|██████████| 20/20 [00:10<00:00,  1.93it/s]
=> result
* total: 3,900
* correct: 2,455
* accuracy: 62.9%
* error: 37.1%
* macro_f1: 60.4%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh eurosat 2 0 main_tmp1_0.1sdl 16
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
80 2800 4200
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      2,800
# test     4,200
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [2/20] time 0.287 (1.487) data 0.000 (0.445) loss 0.6727 (0.8738) lr 1.0000e-02 eta 0:14:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [4/20] time 0.281 (0.885) data 0.000 (0.223) loss 1.4943 (1.0200) lr 1.0000e-02 eta 0:08:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [6/20] time 0.288 (0.685) data 0.000 (0.148) loss 0.8717 (1.0116) lr 1.0000e-02 eta 0:06:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [8/20] time 0.393 (0.598) data 0.000 (0.111) loss 0.6254 (0.9588) lr 1.0000e-02 eta 0:05:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [10/20] time 0.298 (0.537) data 0.000 (0.089) loss 1.2624 (0.9975) lr 1.0000e-02 eta 0:05:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [12/20] time 0.281 (0.498) data 0.000 (0.074) loss 1.0097 (1.0006) lr 1.0000e-02 eta 0:04:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [14/20] time 0.283 (0.467) data 0.000 (0.064) loss 1.2669 (1.0083) lr 1.0000e-02 eta 0:04:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [16/20] time 0.285 (0.444) data 0.000 (0.056) loss 1.1079 (1.0080) lr 1.0000e-02 eta 0:04:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [18/20] time 0.286 (0.427) data 0.000 (0.050) loss 1.2011 (1.0085) lr 1.0000e-02 eta 0:04:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [20/20] time 0.284 (0.412) data 0.000 (0.045) loss 0.8494 (0.9851) lr 9.9726e-03 eta 0:03:59
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:02<00:36,  2.59s/it] 13%|█▎        | 2/15 [00:02<00:16,  1.24s/it] 20%|██        | 3/15 [00:03<00:09,  1.24it/s] 27%|██▋       | 4/15 [00:03<00:06,  1.65it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.04it/s] 40%|████      | 6/15 [00:04<00:03,  2.37it/s] 47%|████▋     | 7/15 [00:04<00:03,  2.65it/s] 53%|█████▎    | 8/15 [00:04<00:02,  2.87it/s] 60%|██████    | 9/15 [00:04<00:01,  3.03it/s] 67%|██████▋   | 10/15 [00:05<00:01,  3.16it/s] 73%|███████▎  | 11/15 [00:05<00:01,  3.25it/s] 80%|████████  | 12/15 [00:05<00:00,  3.27it/s] 87%|████████▋ | 13/15 [00:06<00:00,  3.32it/s] 93%|█████████▎| 14/15 [00:06<00:00,  3.34it/s]100%|██████████| 15/15 [00:06<00:00,  4.16it/s]100%|██████████| 15/15 [00:06<00:00,  2.27it/s]=> result
* total: 2,800
* correct: 1,734
* accuracy: 61.9%
* error: 38.1%
* macro_f1: 62.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [2/20] time 0.306 (0.589) data 0.000 (0.251) loss 1.0182 (0.7564) lr 9.9726e-03 eta 0:05:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [4/20] time 0.284 (0.435) data 0.000 (0.126) loss 0.7486 (0.7114) lr 9.9726e-03 eta 0:04:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [6/20] time 0.274 (0.382) data 0.000 (0.084) loss 1.1656 (0.8415) lr 9.9726e-03 eta 0:03:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [8/20] time 0.284 (0.358) data 0.000 (0.063) loss 0.9669 (0.8718) lr 9.9726e-03 eta 0:03:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [10/20] time 0.279 (0.343) data 0.000 (0.050) loss 0.8580 (0.8740) lr 9.9726e-03 eta 0:03:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [12/20] time 0.281 (0.333) data 0.000 (0.042) loss 0.8049 (0.8426) lr 9.9726e-03 eta 0:03:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [14/20] time 0.278 (0.325) data 0.000 (0.036) loss 0.9599 (0.8473) lr 9.9726e-03 eta 0:03:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [16/20] time 0.284 (0.319) data 0.000 (0.032) loss 1.2739 (0.8397) lr 9.9726e-03 eta 0:03:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [18/20] time 0.283 (0.315) data 0.000 (0.028) loss 0.6829 (0.8565) lr 9.9726e-03 eta 0:02:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [20/20] time 0.290 (0.313) data 0.000 (0.025) loss 1.0902 (0.8594) lr 9.8907e-03 eta 0:02:55
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:25,  1.84s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.07it/s] 20%|██        | 3/15 [00:02<00:07,  1.54it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.97it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.33it/s] 40%|████      | 6/15 [00:03<00:03,  2.61it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.83it/s] 53%|█████▎    | 8/15 [00:03<00:02,  2.99it/s] 60%|██████    | 9/15 [00:04<00:01,  3.12it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.21it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.27it/s] 80%|████████  | 12/15 [00:05<00:00,  3.31it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.34it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.36it/s]100%|██████████| 15/15 [00:05<00:00,  4.18it/s]100%|██████████| 15/15 [00:05<00:00,  2.54it/s]=> result
* total: 2,800
* correct: 1,829
* accuracy: 65.3%
* error: 34.7%
* macro_f1: 63.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [2/20] time 0.319 (0.603) data 0.000 (0.262) loss 0.8511 (0.9653) lr 9.8907e-03 eta 0:05:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [4/20] time 0.281 (0.440) data 0.000 (0.131) loss 0.8044 (0.8529) lr 9.8907e-03 eta 0:04:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [6/20] time 0.272 (0.384) data 0.000 (0.088) loss 0.7361 (0.8426) lr 9.8907e-03 eta 0:03:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [8/20] time 0.273 (0.356) data 0.000 (0.066) loss 0.4991 (0.7908) lr 9.8907e-03 eta 0:03:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [10/20] time 0.268 (0.339) data 0.000 (0.053) loss 0.9422 (0.8798) lr 9.8907e-03 eta 0:03:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [12/20] time 0.277 (0.328) data 0.000 (0.044) loss 0.5851 (0.8542) lr 9.8907e-03 eta 0:02:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [14/20] time 0.268 (0.320) data 0.000 (0.038) loss 0.6698 (0.8126) lr 9.8907e-03 eta 0:02:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [16/20] time 0.276 (0.314) data 0.000 (0.033) loss 1.2384 (0.8304) lr 9.8907e-03 eta 0:02:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [18/20] time 0.274 (0.310) data 0.000 (0.029) loss 0.6839 (0.8149) lr 9.8907e-03 eta 0:02:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [20/20] time 0.272 (0.306) data 0.000 (0.026) loss 1.0682 (0.8284) lr 9.7553e-03 eta 0:02:45
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:27,  1.99s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.01it/s] 20%|██        | 3/15 [00:02<00:08,  1.49it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.92it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.29it/s] 40%|████      | 6/15 [00:03<00:03,  2.59it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.83it/s] 53%|█████▎    | 8/15 [00:04<00:02,  3.01it/s] 60%|██████    | 9/15 [00:04<00:01,  3.13it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.42it/s]100%|██████████| 15/15 [00:05<00:00,  2.51it/s]=> result
* total: 2,800
* correct: 1,919
* accuracy: 68.5%
* error: 31.5%
* macro_f1: 67.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [2/20] time 0.267 (0.575) data 0.000 (0.252) loss 0.7634 (0.6659) lr 9.7553e-03 eta 0:05:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [4/20] time 0.261 (0.419) data 0.000 (0.126) loss 1.2477 (0.8002) lr 9.7553e-03 eta 0:03:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [6/20] time 0.272 (0.370) data 0.000 (0.084) loss 0.5125 (0.7307) lr 9.7553e-03 eta 0:03:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [8/20] time 0.277 (0.347) data 0.000 (0.063) loss 0.5145 (0.7307) lr 9.7553e-03 eta 0:03:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [10/20] time 0.273 (0.332) data 0.000 (0.051) loss 0.7082 (0.7227) lr 9.7553e-03 eta 0:02:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [12/20] time 0.269 (0.322) data 0.000 (0.042) loss 0.5901 (0.7509) lr 9.7553e-03 eta 0:02:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [14/20] time 0.275 (0.315) data 0.000 (0.036) loss 0.8835 (0.7448) lr 9.7553e-03 eta 0:02:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [16/20] time 0.267 (0.309) data 0.000 (0.032) loss 0.6539 (0.7209) lr 9.7553e-03 eta 0:02:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [18/20] time 0.273 (0.305) data 0.000 (0.028) loss 1.2556 (0.7226) lr 9.7553e-03 eta 0:02:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [20/20] time 0.273 (0.302) data 0.000 (0.025) loss 0.6650 (0.7184) lr 9.5677e-03 eta 0:02:37
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:24,  1.78s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.03it/s] 20%|██        | 3/15 [00:02<00:07,  1.50it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.93it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.29it/s] 40%|████      | 6/15 [00:03<00:03,  2.58it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.80it/s] 53%|█████▎    | 8/15 [00:03<00:02,  2.96it/s] 60%|██████    | 9/15 [00:04<00:01,  3.09it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.18it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.25it/s] 80%|████████  | 12/15 [00:05<00:00,  3.30it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.33it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.35it/s]100%|██████████| 15/15 [00:05<00:00,  4.16it/s]100%|██████████| 15/15 [00:05<00:00,  2.53it/s]=> result
* total: 2,800
* correct: 1,951
* accuracy: 69.7%
* error: 30.3%
* macro_f1: 69.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [2/20] time 0.291 (0.600) data 0.000 (0.267) loss 0.5993 (0.5362) lr 9.5677e-03 eta 0:05:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [4/20] time 0.279 (0.437) data 0.000 (0.134) loss 0.7902 (0.7733) lr 9.5677e-03 eta 0:03:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [6/20] time 0.269 (0.382) data 0.000 (0.089) loss 0.4418 (0.6664) lr 9.5677e-03 eta 0:03:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [8/20] time 0.277 (0.356) data 0.000 (0.067) loss 0.9904 (0.7071) lr 9.5677e-03 eta 0:03:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [10/20] time 0.279 (0.339) data 0.000 (0.054) loss 0.1096 (0.6461) lr 9.5677e-03 eta 0:02:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [12/20] time 0.272 (0.328) data 0.000 (0.045) loss 0.9365 (0.6721) lr 9.5677e-03 eta 0:02:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [14/20] time 0.277 (0.320) data 0.000 (0.038) loss 1.0261 (0.6987) lr 9.5677e-03 eta 0:02:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [16/20] time 0.273 (0.315) data 0.000 (0.034) loss 0.3666 (0.6866) lr 9.5677e-03 eta 0:02:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [18/20] time 0.275 (0.310) data 0.000 (0.030) loss 0.3733 (0.6790) lr 9.5677e-03 eta 0:02:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [20/20] time 0.271 (0.306) data 0.000 (0.027) loss 0.7754 (0.6861) lr 9.3301e-03 eta 0:02:33
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:27,  1.95s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.02it/s] 20%|██        | 3/15 [00:02<00:07,  1.51it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.94it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.31it/s] 40%|████      | 6/15 [00:03<00:03,  2.60it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.83it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.00it/s] 60%|██████    | 9/15 [00:04<00:01,  3.13it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  4.24it/s]100%|██████████| 15/15 [00:05<00:00,  2.52it/s]=> result
* total: 2,800
* correct: 1,999
* accuracy: 71.4%
* error: 28.6%
* macro_f1: 70.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [2/20] time 0.286 (0.581) data 0.000 (0.260) loss 0.7261 (0.7633) lr 9.3301e-03 eta 0:04:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [4/20] time 0.270 (0.423) data 0.000 (0.130) loss 0.5554 (0.7988) lr 9.3301e-03 eta 0:03:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [6/20] time 0.274 (0.373) data 0.000 (0.087) loss 0.7996 (0.8144) lr 9.3301e-03 eta 0:03:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [8/20] time 0.274 (0.348) data 0.000 (0.065) loss 0.7726 (0.7953) lr 9.3301e-03 eta 0:02:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [10/20] time 0.271 (0.333) data 0.000 (0.052) loss 0.5575 (0.7453) lr 9.3301e-03 eta 0:02:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [12/20] time 0.276 (0.323) data 0.000 (0.044) loss 0.6016 (0.7705) lr 9.3301e-03 eta 0:02:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [14/20] time 0.268 (0.315) data 0.000 (0.037) loss 1.1482 (0.7854) lr 9.3301e-03 eta 0:02:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [16/20] time 0.274 (0.310) data 0.000 (0.033) loss 0.3862 (0.7415) lr 9.3301e-03 eta 0:02:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [18/20] time 0.275 (0.306) data 0.000 (0.029) loss 0.8002 (0.7396) lr 9.3301e-03 eta 0:02:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [20/20] time 0.277 (0.303) data 0.000 (0.026) loss 0.3704 (0.7008) lr 9.0451e-03 eta 0:02:25
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:25,  1.84s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.01it/s] 20%|██        | 3/15 [00:02<00:08,  1.48it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.92it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.29it/s] 40%|████      | 6/15 [00:03<00:03,  2.59it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.82it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.00it/s] 60%|██████    | 9/15 [00:04<00:01,  3.13it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.52it/s]=> result
* total: 2,800
* correct: 2,027
* accuracy: 72.4%
* error: 27.6%
* macro_f1: 70.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [2/20] time 0.288 (0.599) data 0.000 (0.293) loss 0.6749 (0.6324) lr 9.0451e-03 eta 0:04:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [4/20] time 0.287 (0.441) data 0.000 (0.146) loss 1.1891 (0.6983) lr 9.0451e-03 eta 0:03:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [6/20] time 0.291 (0.391) data 0.000 (0.098) loss 0.4645 (0.6871) lr 9.0451e-03 eta 0:03:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [8/20] time 0.263 (0.360) data 0.000 (0.073) loss 0.9995 (0.6617) lr 9.0451e-03 eta 0:02:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [10/20] time 0.268 (0.341) data 0.000 (0.059) loss 0.1918 (0.6078) lr 9.0451e-03 eta 0:02:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [12/20] time 0.267 (0.329) data 0.000 (0.049) loss 0.8840 (0.6383) lr 9.0451e-03 eta 0:02:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [14/20] time 0.272 (0.320) data 0.000 (0.042) loss 0.4262 (0.6028) lr 9.0451e-03 eta 0:02:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [16/20] time 0.265 (0.314) data 0.000 (0.037) loss 0.2221 (0.6258) lr 9.0451e-03 eta 0:02:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [18/20] time 0.269 (0.309) data 0.000 (0.033) loss 0.7149 (0.6377) lr 9.0451e-03 eta 0:02:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [20/20] time 0.262 (0.305) data 0.000 (0.029) loss 1.0661 (0.6484) lr 8.7157e-03 eta 0:02:20
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:24,  1.72s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.07it/s] 20%|██        | 3/15 [00:02<00:07,  1.55it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.98it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.34it/s] 40%|████      | 6/15 [00:03<00:03,  2.64it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.86it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.03it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.31it/s] 80%|████████  | 12/15 [00:05<00:00,  3.36it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.57it/s]=> result
* total: 2,800
* correct: 2,113
* accuracy: 75.5%
* error: 24.5%
* macro_f1: 75.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [2/20] time 0.279 (0.568) data 0.000 (0.251) loss 0.4218 (0.4809) lr 8.7157e-03 eta 0:04:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [4/20] time 0.285 (0.427) data 0.000 (0.126) loss 0.3846 (0.6633) lr 8.7157e-03 eta 0:03:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [6/20] time 0.284 (0.380) data 0.000 (0.084) loss 0.4627 (0.5952) lr 8.7157e-03 eta 0:02:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [8/20] time 0.287 (0.357) data 0.000 (0.063) loss 0.2752 (0.5851) lr 8.7157e-03 eta 0:02:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [10/20] time 0.284 (0.343) data 0.000 (0.050) loss 0.8627 (0.6158) lr 8.7157e-03 eta 0:02:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [12/20] time 0.285 (0.333) data 0.000 (0.042) loss 0.7694 (0.6236) lr 8.7157e-03 eta 0:02:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [14/20] time 0.284 (0.326) data 0.000 (0.036) loss 0.3789 (0.6333) lr 8.7157e-03 eta 0:02:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [16/20] time 0.288 (0.321) data 0.000 (0.032) loss 0.3861 (0.6333) lr 8.7157e-03 eta 0:02:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [18/20] time 0.284 (0.317) data 0.000 (0.028) loss 0.4650 (0.5993) lr 8.7157e-03 eta 0:02:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [20/20] time 0.285 (0.314) data 0.000 (0.025) loss 0.8863 (0.6118) lr 8.3457e-03 eta 0:02:18
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:26,  1.92s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.04it/s] 20%|██        | 3/15 [00:02<00:07,  1.53it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.96it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.32it/s] 40%|████      | 6/15 [00:03<00:03,  2.61it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.84it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.01it/s] 60%|██████    | 9/15 [00:04<00:01,  3.14it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.53it/s]=> result
* total: 2,800
* correct: 2,102
* accuracy: 75.1%
* error: 24.9%
* macro_f1: 74.3%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [2/20] time 0.282 (0.600) data 0.000 (0.279) loss 0.9393 (0.6044) lr 8.3457e-03 eta 0:04:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [4/20] time 0.282 (0.438) data 0.000 (0.139) loss 0.7009 (0.5719) lr 8.3457e-03 eta 0:03:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [6/20] time 0.278 (0.385) data 0.000 (0.093) loss 0.3011 (0.5536) lr 8.3457e-03 eta 0:02:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [8/20] time 0.283 (0.359) data 0.000 (0.070) loss 0.8424 (0.5962) lr 8.3457e-03 eta 0:02:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [10/20] time 0.280 (0.344) data 0.000 (0.056) loss 0.0968 (0.5256) lr 8.3457e-03 eta 0:02:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [12/20] time 0.282 (0.334) data 0.000 (0.047) loss 0.5294 (0.5249) lr 8.3457e-03 eta 0:02:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [14/20] time 0.279 (0.326) data 0.000 (0.040) loss 0.9994 (0.5692) lr 8.3457e-03 eta 0:02:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [16/20] time 0.286 (0.321) data 0.000 (0.035) loss 0.2898 (0.5229) lr 8.3457e-03 eta 0:02:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [18/20] time 0.283 (0.317) data 0.000 (0.031) loss 0.6539 (0.5562) lr 8.3457e-03 eta 0:02:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [20/20] time 0.290 (0.314) data 0.000 (0.028) loss 0.2967 (0.5365) lr 7.9389e-03 eta 0:02:11
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:21,  1.54s/it] 13%|█▎        | 2/15 [00:02<00:11,  1.09it/s] 20%|██        | 3/15 [00:02<00:07,  1.58it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.02it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.38it/s] 40%|████      | 6/15 [00:03<00:03,  2.66it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.88it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.05it/s] 60%|██████    | 9/15 [00:04<00:01,  3.17it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.25it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.32it/s] 80%|████████  | 12/15 [00:04<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.63it/s]=> result
* total: 2,800
* correct: 2,218
* accuracy: 79.2%
* error: 20.8%
* macro_f1: 79.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [2/20] time 0.282 (0.561) data 0.000 (0.255) loss 0.1939 (0.4689) lr 7.9389e-03 eta 0:03:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [4/20] time 0.283 (0.421) data 0.000 (0.128) loss 0.5338 (0.4674) lr 7.9389e-03 eta 0:02:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [6/20] time 0.285 (0.375) data 0.000 (0.085) loss 0.4309 (0.4308) lr 7.9389e-03 eta 0:02:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [8/20] time 0.284 (0.353) data 0.000 (0.064) loss 0.5629 (0.5392) lr 7.9389e-03 eta 0:02:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [10/20] time 0.285 (0.339) data 0.000 (0.051) loss 0.5690 (0.5703) lr 7.9389e-03 eta 0:02:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [12/20] time 0.282 (0.338) data 0.000 (0.043) loss 0.7106 (0.5511) lr 7.9389e-03 eta 0:02:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [14/20] time 0.285 (0.330) data 0.000 (0.037) loss 0.9260 (0.5594) lr 7.9389e-03 eta 0:02:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [16/20] time 0.282 (0.324) data 0.000 (0.032) loss 0.2811 (0.5616) lr 7.9389e-03 eta 0:02:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [18/20] time 0.284 (0.320) data 0.000 (0.029) loss 0.2145 (0.5292) lr 7.9389e-03 eta 0:02:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [20/20] time 0.284 (0.316) data 0.000 (0.026) loss 0.5957 (0.5386) lr 7.5000e-03 eta 0:02:06
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:25,  1.81s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.02it/s] 20%|██        | 3/15 [00:02<00:08,  1.49it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.93it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.30it/s] 40%|████      | 6/15 [00:03<00:03,  2.59it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.83it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.01it/s] 60%|██████    | 9/15 [00:04<00:01,  3.13it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.53it/s]=> result
* total: 2,800
* correct: 2,212
* accuracy: 79.0%
* error: 21.0%
* macro_f1: 79.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-10

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [2/20] time 0.283 (0.587) data 0.000 (0.265) loss 0.4933 (0.4847) lr 7.5000e-03 eta 0:03:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [4/20] time 0.290 (0.438) data 0.000 (0.132) loss 0.0808 (0.4327) lr 7.5000e-03 eta 0:02:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [6/20] time 0.302 (0.390) data 0.000 (0.088) loss 0.3062 (0.4334) lr 7.5000e-03 eta 0:02:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [8/20] time 0.291 (0.366) data 0.000 (0.066) loss 0.1784 (0.4226) lr 7.5000e-03 eta 0:02:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [10/20] time 0.297 (0.351) data 0.000 (0.053) loss 1.1009 (0.4838) lr 7.5000e-03 eta 0:02:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [12/20] time 0.291 (0.342) data 0.000 (0.044) loss 0.3888 (0.4769) lr 7.5000e-03 eta 0:02:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [14/20] time 0.299 (0.335) data 0.000 (0.038) loss 0.3684 (0.4666) lr 7.5000e-03 eta 0:02:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [16/20] time 0.288 (0.330) data 0.000 (0.033) loss 0.7008 (0.4829) lr 7.5000e-03 eta 0:02:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [18/20] time 0.293 (0.326) data 0.000 (0.030) loss 0.1738 (0.4400) lr 7.5000e-03 eta 0:02:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [20/20] time 0.291 (0.322) data 0.000 (0.027) loss 0.6328 (0.4256) lr 7.0337e-03 eta 0:02:02
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:25,  1.86s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.04it/s] 20%|██        | 3/15 [00:02<00:07,  1.53it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.97it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.33it/s] 40%|████      | 6/15 [00:03<00:03,  2.62it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.85it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.02it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  4.25it/s]100%|██████████| 15/15 [00:05<00:00,  2.55it/s]=> result
* total: 2,800
* correct: 2,084
* accuracy: 74.4%
* error: 25.6%
* macro_f1: 71.8%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [2/20] time 0.268 (0.597) data 0.000 (0.297) loss 0.6810 (0.6801) lr 7.0337e-03 eta 0:03:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [4/20] time 0.277 (0.436) data 0.000 (0.149) loss 0.2872 (0.6175) lr 7.0337e-03 eta 0:02:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [6/20] time 0.273 (0.383) data 0.000 (0.099) loss 0.3150 (0.5405) lr 7.0337e-03 eta 0:02:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [8/20] time 0.281 (0.357) data 0.000 (0.075) loss 0.8115 (0.5755) lr 7.0337e-03 eta 0:02:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [10/20] time 0.274 (0.341) data 0.000 (0.060) loss 0.5110 (0.5925) lr 7.0337e-03 eta 0:02:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [12/20] time 0.277 (0.330) data 0.000 (0.050) loss 0.5410 (0.5404) lr 7.0337e-03 eta 0:02:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [14/20] time 0.278 (0.322) data 0.000 (0.043) loss 0.1476 (0.5377) lr 7.0337e-03 eta 0:01:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [16/20] time 0.279 (0.317) data 0.000 (0.037) loss 0.2492 (0.5302) lr 7.0337e-03 eta 0:01:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [18/20] time 0.279 (0.313) data 0.000 (0.033) loss 0.3115 (0.5154) lr 7.0337e-03 eta 0:01:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [20/20] time 0.274 (0.309) data 0.000 (0.030) loss 0.3668 (0.5097) lr 6.5451e-03 eta 0:01:51
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:24,  1.75s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.04it/s] 20%|██        | 3/15 [00:02<00:07,  1.53it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.97it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.33it/s] 40%|████      | 6/15 [00:03<00:03,  2.62it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.85it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.02it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.56it/s]=> result
* total: 2,800
* correct: 2,342
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 83.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [2/20] time 0.279 (0.578) data 0.000 (0.256) loss 0.6195 (0.4503) lr 6.5451e-03 eta 0:03:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [4/20] time 0.278 (0.425) data 0.000 (0.128) loss 0.3733 (0.3674) lr 6.5451e-03 eta 0:02:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [6/20] time 0.279 (0.376) data 0.000 (0.086) loss 0.6005 (0.4449) lr 6.5451e-03 eta 0:02:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [8/20] time 0.275 (0.351) data 0.000 (0.064) loss 0.6695 (0.4653) lr 6.5451e-03 eta 0:02:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [10/20] time 0.277 (0.336) data 0.000 (0.051) loss 0.5976 (0.4583) lr 6.5451e-03 eta 0:01:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [12/20] time 0.278 (0.326) data 0.000 (0.043) loss 0.3763 (0.4113) lr 6.5451e-03 eta 0:01:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [14/20] time 0.275 (0.318) data 0.000 (0.037) loss 0.3699 (0.4018) lr 6.5451e-03 eta 0:01:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [16/20] time 0.279 (0.313) data 0.000 (0.032) loss 0.2589 (0.3689) lr 6.5451e-03 eta 0:01:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [18/20] time 0.277 (0.309) data 0.000 (0.029) loss 0.7391 (0.3833) lr 6.5451e-03 eta 0:01:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [20/20] time 0.276 (0.306) data 0.000 (0.026) loss 0.0026 (0.3737) lr 6.0396e-03 eta 0:01:43
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:25,  1.80s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.06it/s] 20%|██        | 3/15 [00:02<00:07,  1.55it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.97it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.33it/s] 40%|████      | 6/15 [00:03<00:03,  2.61it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.82it/s] 53%|█████▎    | 8/15 [00:03<00:02,  2.99it/s] 60%|██████    | 9/15 [00:04<00:01,  3.10it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.19it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.25it/s] 80%|████████  | 12/15 [00:05<00:00,  3.30it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.33it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.35it/s]100%|██████████| 15/15 [00:05<00:00,  4.18it/s]100%|██████████| 15/15 [00:05<00:00,  2.54it/s]=> result
* total: 2,800
* correct: 2,319
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 83.1%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [2/20] time 0.275 (0.577) data 0.000 (0.261) loss 0.8454 (0.7101) lr 6.0396e-03 eta 0:03:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [4/20] time 0.286 (0.431) data 0.000 (0.131) loss 0.5170 (0.6362) lr 6.0396e-03 eta 0:02:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [6/20] time 0.287 (0.383) data 0.000 (0.087) loss 0.4615 (0.5184) lr 6.0396e-03 eta 0:02:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [8/20] time 0.285 (0.359) data 0.000 (0.065) loss 0.6048 (0.5080) lr 6.0396e-03 eta 0:01:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [10/20] time 0.285 (0.344) data 0.000 (0.052) loss 0.6701 (0.5193) lr 6.0396e-03 eta 0:01:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [12/20] time 0.280 (0.334) data 0.000 (0.044) loss 0.8925 (0.5310) lr 6.0396e-03 eta 0:01:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [14/20] time 0.289 (0.327) data 0.000 (0.037) loss 0.1448 (0.4832) lr 6.0396e-03 eta 0:01:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [16/20] time 0.290 (0.323) data 0.000 (0.033) loss 0.0658 (0.4493) lr 6.0396e-03 eta 0:01:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [18/20] time 0.305 (0.320) data 0.000 (0.029) loss 0.3521 (0.4315) lr 6.0396e-03 eta 0:01:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [20/20] time 0.292 (0.318) data 0.000 (0.026) loss 0.2695 (0.4189) lr 5.5226e-03 eta 0:01:41
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:02<00:28,  2.03s/it] 13%|█▎        | 2/15 [00:02<00:13,  1.01s/it] 20%|██        | 3/15 [00:02<00:08,  1.47it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.90it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.27it/s] 40%|████      | 6/15 [00:03<00:03,  2.57it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.81it/s] 53%|█████▎    | 8/15 [00:04<00:02,  2.99it/s] 60%|██████    | 9/15 [00:04<00:01,  3.12it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.22it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.29it/s] 80%|████████  | 12/15 [00:05<00:00,  3.34it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:06<00:00,  2.50it/s]=> result
* total: 2,800
* correct: 2,280
* accuracy: 81.4%
* error: 18.6%
* macro_f1: 81.0%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [2/20] time 0.275 (0.566) data 0.000 (0.258) loss 0.4027 (0.5118) lr 5.5226e-03 eta 0:02:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [4/20] time 0.274 (0.420) data 0.000 (0.129) loss 0.3345 (0.4263) lr 5.5226e-03 eta 0:02:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [6/20] time 0.277 (0.373) data 0.000 (0.086) loss 0.2126 (0.4128) lr 5.5226e-03 eta 0:01:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [8/20] time 0.281 (0.354) data 0.000 (0.065) loss 0.2447 (0.3931) lr 5.5226e-03 eta 0:01:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [10/20] time 0.281 (0.339) data 0.000 (0.052) loss 0.4570 (0.3897) lr 5.5226e-03 eta 0:01:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [12/20] time 0.282 (0.330) data 0.000 (0.043) loss 0.1691 (0.3824) lr 5.5226e-03 eta 0:01:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [14/20] time 0.288 (0.323) data 0.000 (0.037) loss 0.5268 (0.4016) lr 5.5226e-03 eta 0:01:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [16/20] time 0.304 (0.319) data 0.000 (0.032) loss 0.1875 (0.3984) lr 5.5226e-03 eta 0:01:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [18/20] time 0.280 (0.315) data 0.000 (0.029) loss 0.1406 (0.3656) lr 5.5226e-03 eta 0:01:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [20/20] time 0.271 (0.311) data 0.000 (0.026) loss 0.3424 (0.3598) lr 5.0000e-03 eta 0:01:33
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:02<00:29,  2.10s/it] 13%|█▎        | 2/15 [00:02<00:13,  1.03s/it] 20%|██        | 3/15 [00:02<00:08,  1.44it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.87it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.25it/s] 40%|████      | 6/15 [00:03<00:03,  2.55it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.79it/s] 53%|█████▎    | 8/15 [00:04<00:02,  2.97it/s] 60%|██████    | 9/15 [00:04<00:01,  3.11it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.21it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.28it/s] 80%|████████  | 12/15 [00:05<00:00,  3.34it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.37it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:06<00:00,  2.46it/s]=> result
* total: 2,800
* correct: 2,407
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 86.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [2/20] time 0.276 (0.591) data 0.000 (0.261) loss 0.4154 (0.2616) lr 5.0000e-03 eta 0:02:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [4/20] time 0.296 (0.439) data 0.000 (0.130) loss 0.2533 (0.1932) lr 5.0000e-03 eta 0:02:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [6/20] time 0.280 (0.388) data 0.000 (0.087) loss 0.2795 (0.2712) lr 5.0000e-03 eta 0:01:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [8/20] time 0.284 (0.362) data 0.000 (0.065) loss 0.2787 (0.2795) lr 5.0000e-03 eta 0:01:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [10/20] time 0.284 (0.356) data 0.000 (0.052) loss 0.4353 (0.3064) lr 5.0000e-03 eta 0:01:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [12/20] time 0.280 (0.343) data 0.000 (0.044) loss 0.5905 (0.3147) lr 5.0000e-03 eta 0:01:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [14/20] time 0.284 (0.335) data 0.000 (0.037) loss 0.2689 (0.2936) lr 5.0000e-03 eta 0:01:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [16/20] time 0.270 (0.327) data 0.000 (0.033) loss 0.2223 (0.2889) lr 5.0000e-03 eta 0:01:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [18/20] time 0.282 (0.322) data 0.000 (0.029) loss 0.4459 (0.2864) lr 5.0000e-03 eta 0:01:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [20/20] time 0.276 (0.317) data 0.000 (0.026) loss 0.3054 (0.2941) lr 4.4774e-03 eta 0:01:28
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:25,  1.83s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.08it/s] 20%|██        | 3/15 [00:02<00:07,  1.57it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.00it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.36it/s] 40%|████      | 6/15 [00:03<00:03,  2.65it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.87it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.04it/s] 60%|██████    | 9/15 [00:04<00:01,  3.16it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.25it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.31it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  4.25it/s]100%|██████████| 15/15 [00:05<00:00,  2.57it/s]=> result
* total: 2,800
* correct: 2,429
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 87.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [2/20] time 0.269 (0.566) data 0.000 (0.241) loss 0.3164 (0.2094) lr 4.4774e-03 eta 0:02:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [4/20] time 0.269 (0.417) data 0.000 (0.121) loss 0.3160 (0.3278) lr 4.4774e-03 eta 0:01:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [6/20] time 0.278 (0.372) data 0.000 (0.081) loss 0.4270 (0.3669) lr 4.4774e-03 eta 0:01:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [8/20] time 0.276 (0.348) data 0.000 (0.060) loss -0.0462 (0.2999) lr 4.4774e-03 eta 0:01:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [10/20] time 0.283 (0.335) data 0.000 (0.048) loss 0.4537 (0.3535) lr 4.4774e-03 eta 0:01:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [12/20] time 0.281 (0.326) data 0.000 (0.040) loss 0.4094 (0.3577) lr 4.4774e-03 eta 0:01:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [14/20] time 0.290 (0.320) data 0.000 (0.035) loss 0.2175 (0.3395) lr 4.4774e-03 eta 0:01:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [16/20] time 0.286 (0.316) data 0.000 (0.030) loss 0.3792 (0.3349) lr 4.4774e-03 eta 0:01:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [18/20] time 0.284 (0.312) data 0.000 (0.027) loss 0.4132 (0.3448) lr 4.4774e-03 eta 0:01:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [20/20] time 0.287 (0.310) data 0.000 (0.024) loss 0.2880 (0.3407) lr 3.9604e-03 eta 0:01:20
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:23,  1.70s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.07it/s] 20%|██        | 3/15 [00:02<00:07,  1.56it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.99it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.35it/s] 40%|████      | 6/15 [00:03<00:03,  2.64it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.86it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.03it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:04<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.59it/s]=> result
* total: 2,800
* correct: 2,454
* accuracy: 87.6%
* error: 12.4%
* macro_f1: 87.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [2/20] time 0.277 (0.593) data 0.000 (0.267) loss 0.4207 (0.3503) lr 3.9604e-03 eta 0:02:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [4/20] time 0.286 (0.439) data 0.000 (0.134) loss 0.3716 (0.4400) lr 3.9604e-03 eta 0:01:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [6/20] time 0.285 (0.388) data 0.000 (0.089) loss 0.3402 (0.3774) lr 3.9604e-03 eta 0:01:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [8/20] time 0.283 (0.362) data 0.000 (0.067) loss 0.4963 (0.4369) lr 3.9604e-03 eta 0:01:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [10/20] time 0.286 (0.347) data 0.000 (0.054) loss 0.2698 (0.3878) lr 3.9604e-03 eta 0:01:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [12/20] time 0.282 (0.336) data 0.000 (0.045) loss 0.3977 (0.3484) lr 3.9604e-03 eta 0:01:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [14/20] time 0.284 (0.328) data 0.000 (0.038) loss 0.2570 (0.3121) lr 3.9604e-03 eta 0:01:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [16/20] time 0.286 (0.323) data 0.000 (0.034) loss 0.0735 (0.2878) lr 3.9604e-03 eta 0:01:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [18/20] time 0.284 (0.318) data 0.000 (0.030) loss 0.0347 (0.2709) lr 3.9604e-03 eta 0:01:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [20/20] time 0.280 (0.314) data 0.000 (0.027) loss 0.3645 (0.2803) lr 3.4549e-03 eta 0:01:15
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:21,  1.53s/it] 13%|█▎        | 2/15 [00:02<00:11,  1.09it/s] 20%|██        | 3/15 [00:02<00:07,  1.58it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.02it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.37it/s] 40%|████      | 6/15 [00:03<00:03,  2.66it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.88it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.04it/s] 60%|██████    | 9/15 [00:04<00:01,  3.16it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.25it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.31it/s] 80%|████████  | 12/15 [00:04<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.63it/s]=> result
* total: 2,800
* correct: 2,375
* accuracy: 84.8%
* error: 15.2%
* macro_f1: 84.6%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [2/20] time 0.270 (0.576) data 0.000 (0.254) loss 0.1613 (0.3765) lr 3.4549e-03 eta 0:02:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [4/20] time 0.266 (0.420) data 0.000 (0.127) loss 0.1989 (0.3557) lr 3.4549e-03 eta 0:01:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [6/20] time 0.280 (0.373) data 0.000 (0.085) loss 0.6852 (0.4427) lr 3.4549e-03 eta 0:01:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [8/20] time 0.269 (0.346) data 0.000 (0.064) loss 0.1454 (0.3794) lr 3.4549e-03 eta 0:01:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [10/20] time 0.267 (0.331) data 0.000 (0.051) loss 0.0356 (0.3045) lr 3.4549e-03 eta 0:01:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [12/20] time 0.269 (0.320) data 0.000 (0.042) loss 0.3498 (0.3112) lr 3.4549e-03 eta 0:01:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [14/20] time 0.270 (0.313) data 0.000 (0.036) loss 0.4015 (0.3067) lr 3.4549e-03 eta 0:01:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [16/20] time 0.278 (0.309) data 0.000 (0.032) loss 0.3970 (0.3070) lr 3.4549e-03 eta 0:01:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [18/20] time 0.270 (0.304) data 0.000 (0.028) loss 0.4671 (0.3071) lr 3.4549e-03 eta 0:01:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [20/20] time 0.277 (0.301) data 0.000 (0.026) loss -0.0134 (0.2870) lr 2.9663e-03 eta 0:01:06
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:21,  1.57s/it] 13%|█▎        | 2/15 [00:01<00:11,  1.13it/s] 20%|██        | 3/15 [00:02<00:07,  1.64it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.07it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.42it/s] 40%|████      | 6/15 [00:03<00:03,  2.69it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.90it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.06it/s] 60%|██████    | 9/15 [00:03<00:01,  3.18it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.26it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.31it/s] 80%|████████  | 12/15 [00:04<00:00,  3.36it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.64it/s]=> result
* total: 2,800
* correct: 2,442
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 87.4%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [2/20] time 0.276 (0.615) data 0.000 (0.288) loss 0.1110 (0.2077) lr 2.9663e-03 eta 0:02:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [4/20] time 0.286 (0.450) data 0.000 (0.144) loss 0.1664 (0.2534) lr 2.9663e-03 eta 0:01:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [6/20] time 0.282 (0.395) data 0.000 (0.096) loss 0.1523 (0.3691) lr 2.9663e-03 eta 0:01:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [8/20] time 0.285 (0.368) data 0.000 (0.072) loss 0.6627 (0.4076) lr 2.9663e-03 eta 0:01:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [10/20] time 0.282 (0.350) data 0.000 (0.058) loss -0.0088 (0.3663) lr 2.9663e-03 eta 0:01:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [12/20] time 0.279 (0.338) data 0.000 (0.048) loss 0.3698 (0.3221) lr 2.9663e-03 eta 0:01:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [14/20] time 0.279 (0.329) data 0.000 (0.041) loss 0.3292 (0.3393) lr 2.9663e-03 eta 0:01:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [16/20] time 0.275 (0.322) data 0.000 (0.036) loss 0.3667 (0.3467) lr 2.9663e-03 eta 0:01:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [18/20] time 0.379 (0.323) data 0.000 (0.032) loss 0.3451 (0.3657) lr 2.9663e-03 eta 0:01:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [20/20] time 0.274 (0.318) data 0.000 (0.029) loss 0.3072 (0.3890) lr 2.5000e-03 eta 0:01:03
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:02<00:28,  2.03s/it] 13%|█▎        | 2/15 [00:02<00:13,  1.00s/it] 20%|██        | 3/15 [00:02<00:08,  1.47it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.89it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.26it/s] 40%|████      | 6/15 [00:03<00:03,  2.57it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.80it/s] 53%|█████▎    | 8/15 [00:04<00:02,  2.98it/s] 60%|██████    | 9/15 [00:04<00:01,  3.12it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.22it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.29it/s] 80%|████████  | 12/15 [00:05<00:00,  3.34it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.37it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.38it/s]100%|██████████| 15/15 [00:06<00:00,  2.48it/s]=> result
* total: 2,800
* correct: 2,396
* accuracy: 85.6%
* error: 14.4%
* macro_f1: 85.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-20

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [2/20] time 0.277 (0.585) data 0.000 (0.257) loss 0.6049 (0.3609) lr 2.5000e-03 eta 0:01:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [4/20] time 0.281 (0.431) data 0.000 (0.129) loss 0.5570 (0.3847) lr 2.5000e-03 eta 0:01:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [6/20] time 0.274 (0.382) data 0.000 (0.086) loss 0.6967 (0.3957) lr 2.5000e-03 eta 0:01:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [8/20] time 0.278 (0.356) data 0.000 (0.064) loss -0.0432 (0.2975) lr 2.5000e-03 eta 0:01:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [10/20] time 0.272 (0.339) data 0.000 (0.052) loss 0.1314 (0.2964) lr 2.5000e-03 eta 0:01:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [12/20] time 0.279 (0.329) data 0.000 (0.043) loss 0.4308 (0.2965) lr 2.5000e-03 eta 0:01:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [14/20] time 0.273 (0.321) data 0.000 (0.037) loss 0.4125 (0.2886) lr 2.5000e-03 eta 0:00:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [16/20] time 0.274 (0.315) data 0.000 (0.032) loss 0.5551 (0.3034) lr 2.5000e-03 eta 0:00:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [18/20] time 0.274 (0.311) data 0.000 (0.029) loss 0.2191 (0.2908) lr 2.5000e-03 eta 0:00:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [20/20] time 0.284 (0.307) data 0.000 (0.026) loss 0.2517 (0.2966) lr 2.0611e-03 eta 0:00:55
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:22,  1.62s/it] 13%|█▎        | 2/15 [00:02<00:11,  1.11it/s] 20%|██        | 3/15 [00:02<00:07,  1.60it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.04it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.37it/s] 40%|████      | 6/15 [00:03<00:03,  2.65it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.86it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.03it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:04<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.61it/s]=> result
* total: 2,800
* correct: 2,498
* accuracy: 89.2%
* error: 10.8%
* macro_f1: 89.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [2/20] time 0.298 (0.604) data 0.000 (0.267) loss 0.1049 (0.0597) lr 2.0611e-03 eta 0:01:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [4/20] time 0.269 (0.435) data 0.000 (0.134) loss 0.1506 (0.1029) lr 2.0611e-03 eta 0:01:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [6/20] time 0.268 (0.379) data 0.000 (0.089) loss -0.0516 (0.1449) lr 2.0611e-03 eta 0:01:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [8/20] time 0.267 (0.351) data 0.000 (0.067) loss 0.0750 (0.1367) lr 2.0611e-03 eta 0:01:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [10/20] time 0.269 (0.334) data 0.000 (0.054) loss 0.1238 (0.1173) lr 2.0611e-03 eta 0:00:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [12/20] time 0.267 (0.323) data 0.000 (0.045) loss 0.1929 (0.1423) lr 2.0611e-03 eta 0:00:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [14/20] time 0.269 (0.315) data 0.000 (0.038) loss -0.0824 (0.1523) lr 2.0611e-03 eta 0:00:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [16/20] time 0.316 (0.313) data 0.000 (0.034) loss 0.0384 (0.1673) lr 2.0611e-03 eta 0:00:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [18/20] time 0.273 (0.309) data 0.000 (0.030) loss 0.1591 (0.1673) lr 2.0611e-03 eta 0:00:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [20/20] time 0.273 (0.305) data 0.000 (0.027) loss 0.8402 (0.2044) lr 1.6543e-03 eta 0:00:48
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:23,  1.68s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.08it/s] 20%|██        | 3/15 [00:02<00:07,  1.57it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.00it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.36it/s] 40%|████      | 6/15 [00:03<00:03,  2.65it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.87it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.03it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:04<00:00,  3.34it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.59it/s]=> result
* total: 2,800
* correct: 2,520
* accuracy: 90.0%
* error: 10.0%
* macro_f1: 90.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [2/20] time 0.293 (0.590) data 0.000 (0.261) loss 0.3792 (0.4015) lr 1.6543e-03 eta 0:01:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [4/20] time 0.296 (0.441) data 0.000 (0.130) loss -0.0198 (0.2355) lr 1.6543e-03 eta 0:01:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [6/20] time 0.291 (0.392) data 0.000 (0.087) loss 0.1140 (0.1741) lr 1.6543e-03 eta 0:01:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [8/20] time 0.295 (0.367) data 0.000 (0.065) loss 0.4093 (0.2448) lr 1.6543e-03 eta 0:00:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [10/20] time 0.288 (0.352) data 0.000 (0.052) loss 0.1562 (0.2175) lr 1.6543e-03 eta 0:00:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [12/20] time 0.298 (0.342) data 0.000 (0.044) loss 0.5835 (0.2428) lr 1.6543e-03 eta 0:00:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [14/20] time 0.285 (0.335) data 0.000 (0.037) loss 0.0277 (0.2345) lr 1.6543e-03 eta 0:00:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [16/20] time 0.297 (0.330) data 0.000 (0.033) loss -0.1372 (0.2262) lr 1.6543e-03 eta 0:00:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [18/20] time 0.293 (0.326) data 0.000 (0.029) loss -0.0462 (0.2153) lr 1.6543e-03 eta 0:00:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [20/20] time 0.294 (0.323) data 0.000 (0.026) loss 0.3142 (0.2197) lr 1.2843e-03 eta 0:00:45
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:22,  1.59s/it] 13%|█▎        | 2/15 [00:01<00:11,  1.12it/s] 20%|██        | 3/15 [00:02<00:07,  1.62it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.05it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.40it/s] 40%|████      | 6/15 [00:03<00:03,  2.68it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.89it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.05it/s] 60%|██████    | 9/15 [00:04<00:01,  3.17it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.25it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.31it/s] 80%|████████  | 12/15 [00:04<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.37it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.63it/s]=> result
* total: 2,800
* correct: 2,505
* accuracy: 89.5%
* error: 10.5%
* macro_f1: 89.6%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [2/20] time 0.281 (0.580) data 0.000 (0.271) loss -0.0003 (0.1228) lr 1.2843e-03 eta 0:01:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [4/20] time 0.281 (0.431) data 0.000 (0.136) loss -0.0441 (0.0645) lr 1.2843e-03 eta 0:00:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [6/20] time 0.286 (0.382) data 0.000 (0.090) loss 0.2266 (0.0750) lr 1.2843e-03 eta 0:00:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [8/20] time 0.282 (0.357) data 0.000 (0.068) loss 0.3382 (0.1934) lr 1.2843e-03 eta 0:00:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [10/20] time 0.286 (0.343) data 0.000 (0.054) loss 0.3357 (0.1912) lr 1.2843e-03 eta 0:00:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [12/20] time 0.280 (0.333) data 0.000 (0.045) loss 0.1766 (0.1941) lr 1.2843e-03 eta 0:00:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [14/20] time 0.284 (0.326) data 0.000 (0.039) loss -0.0193 (0.1925) lr 1.2843e-03 eta 0:00:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [16/20] time 0.284 (0.321) data 0.000 (0.034) loss 0.1255 (0.1944) lr 1.2843e-03 eta 0:00:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [18/20] time 0.288 (0.317) data 0.000 (0.030) loss 0.2195 (0.2043) lr 1.2843e-03 eta 0:00:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [20/20] time 0.287 (0.314) data 0.000 (0.027) loss 0.4307 (0.2201) lr 9.5492e-04 eta 0:00:37
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:25,  1.84s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.02it/s] 20%|██        | 3/15 [00:02<00:07,  1.50it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.93it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.30it/s] 40%|████      | 6/15 [00:03<00:03,  2.59it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.82it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.00it/s] 60%|██████    | 9/15 [00:04<00:01,  3.13it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.22it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.29it/s] 80%|████████  | 12/15 [00:05<00:00,  3.34it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.37it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.53it/s]=> result
* total: 2,800
* correct: 2,559
* accuracy: 91.4%
* error: 8.6%
* macro_f1: 91.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [2/20] time 0.287 (0.590) data 0.000 (0.268) loss 0.1718 (0.0925) lr 9.5492e-04 eta 0:01:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [4/20] time 0.296 (0.442) data 0.000 (0.134) loss 0.4026 (0.1538) lr 9.5492e-04 eta 0:00:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [6/20] time 0.297 (0.393) data 0.000 (0.090) loss 0.5810 (0.2564) lr 9.5492e-04 eta 0:00:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [8/20] time 0.295 (0.369) data 0.000 (0.067) loss 0.0855 (0.2294) lr 9.5492e-04 eta 0:00:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [10/20] time 0.298 (0.354) data 0.000 (0.054) loss 0.2698 (0.2534) lr 9.5492e-04 eta 0:00:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [12/20] time 0.291 (0.344) data 0.000 (0.045) loss 0.0636 (0.2147) lr 9.5492e-04 eta 0:00:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [14/20] time 0.296 (0.338) data 0.000 (0.039) loss -0.2232 (0.1970) lr 9.5492e-04 eta 0:00:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [16/20] time 0.292 (0.332) data 0.000 (0.034) loss 0.2435 (0.2170) lr 9.5492e-04 eta 0:00:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [18/20] time 0.297 (0.328) data 0.000 (0.030) loss 0.1502 (0.2171) lr 9.5492e-04 eta 0:00:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [20/20] time 0.285 (0.323) data 0.000 (0.027) loss 0.2417 (0.2250) lr 6.6987e-04 eta 0:00:32
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:23,  1.70s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.07it/s] 20%|██        | 3/15 [00:02<00:07,  1.56it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.00it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.36it/s] 40%|████      | 6/15 [00:03<00:03,  2.64it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.86it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.03it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:04<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.39it/s]100%|██████████| 15/15 [00:05<00:00,  2.59it/s]=> result
* total: 2,800
* correct: 2,449
* accuracy: 87.5%
* error: 12.5%
* macro_f1: 87.7%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [2/20] time 0.299 (0.594) data 0.000 (0.252) loss 0.3045 (0.1308) lr 6.6987e-04 eta 0:00:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [4/20] time 0.281 (0.437) data 0.000 (0.126) loss 0.1045 (0.1756) lr 6.6987e-04 eta 0:00:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [6/20] time 0.277 (0.384) data 0.000 (0.084) loss 0.0712 (0.1425) lr 6.6987e-04 eta 0:00:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [8/20] time 0.285 (0.359) data 0.000 (0.063) loss 0.0097 (0.1114) lr 6.6987e-04 eta 0:00:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [10/20] time 0.286 (0.344) data 0.000 (0.051) loss 0.1278 (0.1242) lr 6.6987e-04 eta 0:00:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [12/20] time 0.288 (0.334) data 0.000 (0.042) loss 0.2465 (0.1262) lr 6.6987e-04 eta 0:00:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [14/20] time 0.287 (0.328) data 0.000 (0.036) loss 0.2473 (0.1540) lr 6.6987e-04 eta 0:00:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [16/20] time 0.287 (0.323) data 0.000 (0.032) loss 0.0820 (0.1617) lr 6.6987e-04 eta 0:00:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [18/20] time 0.292 (0.319) data 0.000 (0.028) loss 0.3967 (0.1718) lr 6.6987e-04 eta 0:00:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [20/20] time 0.277 (0.315) data 0.000 (0.025) loss 0.2719 (0.1668) lr 4.3227e-04 eta 0:00:25
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:23,  1.71s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.06it/s] 20%|██        | 3/15 [00:02<00:07,  1.55it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.98it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.34it/s] 40%|████      | 6/15 [00:03<00:03,  2.63it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.86it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.02it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.39it/s]100%|██████████| 15/15 [00:05<00:00,  2.58it/s]=> result
* total: 2,800
* correct: 2,528
* accuracy: 90.3%
* error: 9.7%
* macro_f1: 90.4%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [2/20] time 0.331 (0.624) data 0.001 (0.262) loss 0.1827 (0.3864) lr 4.3227e-04 eta 0:00:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [4/20] time 0.294 (0.472) data 0.000 (0.131) loss 0.3459 (0.2823) lr 4.3227e-04 eta 0:00:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [6/20] time 0.294 (0.413) data 0.000 (0.087) loss 0.3197 (0.2468) lr 4.3227e-04 eta 0:00:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [8/20] time 0.302 (0.383) data 0.000 (0.066) loss -0.0521 (0.2050) lr 4.3227e-04 eta 0:00:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [10/20] time 0.287 (0.363) data 0.000 (0.053) loss 0.2133 (0.2128) lr 4.3227e-04 eta 0:00:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [12/20] time 0.286 (0.350) data 0.000 (0.044) loss 0.2978 (0.1888) lr 4.3227e-04 eta 0:00:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [14/20] time 0.291 (0.342) data 0.000 (0.038) loss -0.0997 (0.1763) lr 4.3227e-04 eta 0:00:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [16/20] time 0.295 (0.336) data 0.000 (0.033) loss 0.3382 (0.1802) lr 4.3227e-04 eta 0:00:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [18/20] time 0.295 (0.331) data 0.000 (0.029) loss 0.1872 (0.1790) lr 4.3227e-04 eta 0:00:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [20/20] time 0.292 (0.327) data 0.000 (0.026) loss -0.1875 (0.1565) lr 2.4472e-04 eta 0:00:19
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:24,  1.78s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.03it/s] 20%|██        | 3/15 [00:02<00:07,  1.52it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.95it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.30it/s] 40%|████      | 6/15 [00:03<00:03,  2.60it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.83it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.00it/s] 60%|██████    | 9/15 [00:04<00:01,  3.13it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.22it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.29it/s] 80%|████████  | 12/15 [00:05<00:00,  3.34it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.37it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.54it/s]=> result
* total: 2,800
* correct: 2,554
* accuracy: 91.2%
* error: 8.8%
* macro_f1: 91.3%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [2/20] time 0.283 (0.588) data 0.000 (0.259) loss -0.0994 (0.0126) lr 2.4472e-04 eta 0:00:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [4/20] time 0.281 (0.434) data 0.000 (0.130) loss 0.0592 (0.1118) lr 2.4472e-04 eta 0:00:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [6/20] time 0.287 (0.385) data 0.000 (0.087) loss -0.0489 (0.0638) lr 2.4472e-04 eta 0:00:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [8/20] time 0.286 (0.360) data 0.000 (0.065) loss -0.1208 (0.0712) lr 2.4472e-04 eta 0:00:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [10/20] time 0.285 (0.345) data 0.000 (0.052) loss 0.4842 (0.1169) lr 2.4472e-04 eta 0:00:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [12/20] time 0.285 (0.334) data 0.000 (0.043) loss 0.3341 (0.1514) lr 2.4472e-04 eta 0:00:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [14/20] time 0.281 (0.327) data 0.000 (0.037) loss -0.1025 (0.1232) lr 2.4472e-04 eta 0:00:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [16/20] time 0.282 (0.321) data 0.000 (0.033) loss 0.0585 (0.1293) lr 2.4472e-04 eta 0:00:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [18/20] time 0.279 (0.316) data 0.000 (0.029) loss -0.0319 (0.1228) lr 2.4472e-04 eta 0:00:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [20/20] time 0.280 (0.312) data 0.000 (0.026) loss 0.3007 (0.1229) lr 1.0926e-04 eta 0:00:12
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:22,  1.61s/it] 13%|█▎        | 2/15 [00:02<00:11,  1.11it/s] 20%|██        | 3/15 [00:02<00:07,  1.61it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.03it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.36it/s] 40%|████      | 6/15 [00:03<00:03,  2.63it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.85it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.02it/s] 60%|██████    | 9/15 [00:04<00:01,  3.14it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:04<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.61it/s]=> result
* total: 2,800
* correct: 2,554
* accuracy: 91.2%
* error: 8.8%
* macro_f1: 91.3%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [2/20] time 0.281 (0.592) data 0.000 (0.289) loss -0.0673 (-0.0536) lr 1.0926e-04 eta 0:00:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [4/20] time 0.287 (0.439) data 0.000 (0.144) loss 0.0450 (-0.0450) lr 1.0926e-04 eta 0:00:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [6/20] time 0.287 (0.388) data 0.000 (0.096) loss 0.1926 (0.0257) lr 1.0926e-04 eta 0:00:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [8/20] time 0.287 (0.362) data 0.000 (0.072) loss 0.1091 (0.0330) lr 1.0926e-04 eta 0:00:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [10/20] time 0.288 (0.347) data 0.000 (0.058) loss -0.0533 (0.0304) lr 1.0926e-04 eta 0:00:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [12/20] time 0.283 (0.337) data 0.000 (0.048) loss 0.1835 (0.0550) lr 1.0926e-04 eta 0:00:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [14/20] time 0.285 (0.337) data 0.000 (0.041) loss 0.3254 (0.0776) lr 1.0926e-04 eta 0:00:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [16/20] time 0.282 (0.330) data 0.000 (0.036) loss 0.3186 (0.1610) lr 1.0926e-04 eta 0:00:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [18/20] time 0.308 (0.326) data 0.000 (0.032) loss 0.5089 (0.1598) lr 1.0926e-04 eta 0:00:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [20/20] time 0.294 (0.323) data 0.000 (0.029) loss 0.2758 (0.2004) lr 2.7391e-05 eta 0:00:06
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:27,  1.98s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.01it/s] 20%|██        | 3/15 [00:02<00:08,  1.49it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.92it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.29it/s] 40%|████      | 6/15 [00:03<00:03,  2.58it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.82it/s] 53%|█████▎    | 8/15 [00:04<00:02,  2.99it/s] 60%|██████    | 9/15 [00:04<00:01,  3.12it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.22it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.29it/s] 80%|████████  | 12/15 [00:05<00:00,  3.34it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.37it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.52it/s]=> result
* total: 2,800
* correct: 2,561
* accuracy: 91.5%
* error: 8.5%
* macro_f1: 91.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [2/20] time 0.277 (0.579) data 0.000 (0.272) loss 0.2707 (0.6826) lr 2.7391e-05 eta 0:00:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [4/20] time 0.280 (0.429) data 0.000 (0.136) loss 0.1556 (0.4039) lr 2.7391e-05 eta 0:00:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [6/20] time 0.283 (0.379) data 0.000 (0.091) loss -0.0957 (0.2985) lr 2.7391e-05 eta 0:00:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [8/20] time 0.276 (0.354) data 0.000 (0.068) loss 0.3549 (0.2643) lr 2.7391e-05 eta 0:00:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [10/20] time 0.279 (0.339) data 0.000 (0.055) loss 0.1189 (0.2460) lr 2.7391e-05 eta 0:00:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [12/20] time 0.275 (0.328) data 0.000 (0.045) loss 0.1742 (0.2425) lr 2.7391e-05 eta 0:00:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [14/20] time 0.280 (0.321) data 0.000 (0.039) loss -0.1147 (0.2070) lr 2.7391e-05 eta 0:00:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [16/20] time 0.274 (0.315) data 0.000 (0.034) loss 0.3661 (0.1883) lr 2.7391e-05 eta 0:00:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [18/20] time 0.277 (0.311) data 0.000 (0.030) loss -0.1310 (0.1925) lr 2.7391e-05 eta 0:00:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [20/20] time 0.271 (0.307) data 0.000 (0.027) loss 0.1612 (0.1965) lr 0.0000e+00 eta 0:00:00
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:26,  1.87s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.05it/s] 20%|██        | 3/15 [00:02<00:07,  1.54it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.97it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.33it/s] 40%|████      | 6/15 [00:03<00:03,  2.62it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.85it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.02it/s] 60%|██████    | 9/15 [00:04<00:01,  3.14it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.55it/s]
=> result
* total: 2,800
* correct: 2,560
* accuracy: 91.4%
* error: 8.6%
* macro_f1: 91.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 29)
Evaluate on the *test* set
  0%|          | 0/22 [00:00<?, ?it/s]  5%|▍         | 1/22 [00:01<00:41,  1.95s/it]  9%|▉         | 2/22 [00:02<00:21,  1.06s/it] 14%|█▎        | 3/22 [00:02<00:16,  1.18it/s] 18%|█▊        | 4/22 [00:03<00:11,  1.60it/s] 23%|██▎       | 5/22 [00:03<00:08,  1.98it/s] 27%|██▋       | 6/22 [00:03<00:06,  2.32it/s] 32%|███▏      | 7/22 [00:04<00:05,  2.60it/s] 36%|███▋      | 8/22 [00:04<00:04,  2.82it/s] 41%|████      | 9/22 [00:04<00:04,  2.99it/s] 45%|████▌     | 10/22 [00:05<00:03,  3.12it/s] 50%|█████     | 11/22 [00:05<00:03,  3.21it/s] 55%|█████▍    | 12/22 [00:05<00:03,  3.28it/s] 59%|█████▉    | 13/22 [00:05<00:02,  3.34it/s] 64%|██████▎   | 14/22 [00:06<00:02,  3.37it/s] 68%|██████▊   | 15/22 [00:06<00:02,  3.39it/s] 73%|███████▎  | 16/22 [00:06<00:01,  3.41it/s] 77%|███████▋  | 17/22 [00:07<00:01,  3.43it/s] 82%|████████▏ | 18/22 [00:07<00:01,  3.43it/s] 86%|████████▋ | 19/22 [00:07<00:00,  3.43it/s] 91%|█████████ | 20/22 [00:07<00:00,  3.44it/s] 95%|█████████▌| 21/22 [00:08<00:00,  3.44it/s]100%|██████████| 22/22 [00:08<00:00,  4.09it/s]100%|██████████| 22/22 [00:08<00:00,  2.60it/s]
=> result
* total: 4,200
* correct: 3,844
* accuracy: 91.5%
* error: 8.5%
* macro_f1: 91.6%
Elapsed: 0:06:18
+ sh scripts/rpo_prime/base2new_test_sdl.sh eurosat 2 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
80 2600 3900
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      2,600
# test     3,900
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 29)
Evaluate on the *test* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:04<01:33,  4.92s/it] 10%|█         | 2/20 [00:05<00:39,  2.20s/it] 15%|█▌        | 3/20 [00:05<00:22,  1.32s/it] 20%|██        | 4/20 [00:05<00:14,  1.09it/s] 25%|██▌       | 5/20 [00:06<00:10,  1.45it/s] 30%|███       | 6/20 [00:06<00:07,  1.81it/s] 35%|███▌      | 7/20 [00:06<00:06,  2.14it/s] 40%|████      | 8/20 [00:06<00:04,  2.44it/s] 45%|████▌     | 9/20 [00:07<00:04,  2.69it/s] 50%|█████     | 10/20 [00:07<00:03,  2.89it/s] 55%|█████▌    | 11/20 [00:07<00:02,  3.04it/s] 60%|██████    | 12/20 [00:08<00:02,  3.16it/s] 65%|██████▌   | 13/20 [00:08<00:02,  3.25it/s] 70%|███████   | 14/20 [00:08<00:01,  3.31it/s] 75%|███████▌  | 15/20 [00:08<00:01,  3.36it/s] 80%|████████  | 16/20 [00:09<00:01,  3.39it/s] 85%|████████▌ | 17/20 [00:09<00:00,  3.41it/s] 90%|█████████ | 18/20 [00:09<00:00,  3.43it/s] 95%|█████████▌| 19/20 [00:10<00:00,  3.44it/s]100%|██████████| 20/20 [00:10<00:00,  3.54it/s]100%|██████████| 20/20 [00:10<00:00,  1.91it/s]
=> result
* total: 3,900
* correct: 2,345
* accuracy: 60.1%
* error: 39.9%
* macro_f1: 55.9%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh eurosat 3 0 main_tmp1_0.1sdl 16
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
80 2800 4200
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      2,800
# test     4,200
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [2/20] time 0.287 (1.565) data 0.000 (0.538) loss 1.2156 (1.0006) lr 1.0000e-02 eta 0:15:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [4/20] time 0.283 (0.925) data 0.000 (0.269) loss 1.2457 (1.2484) lr 1.0000e-02 eta 0:09:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [6/20] time 0.287 (0.713) data 0.000 (0.179) loss 1.4079 (1.2109) lr 1.0000e-02 eta 0:07:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [8/20] time 0.398 (0.620) data 0.000 (0.135) loss 0.9347 (1.1624) lr 1.0000e-02 eta 0:06:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [10/20] time 0.281 (0.553) data 0.000 (0.108) loss 1.1017 (1.1056) lr 1.0000e-02 eta 0:05:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [12/20] time 0.285 (0.508) data 0.000 (0.090) loss 0.8996 (1.0938) lr 1.0000e-02 eta 0:04:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [14/20] time 0.280 (0.476) data 0.000 (0.077) loss 0.7379 (1.0688) lr 1.0000e-02 eta 0:04:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [16/20] time 0.282 (0.451) data 0.000 (0.067) loss 1.1412 (1.0686) lr 1.0000e-02 eta 0:04:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [18/20] time 0.280 (0.433) data 0.000 (0.060) loss 0.9753 (1.0383) lr 1.0000e-02 eta 0:04:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [20/20] time 0.289 (0.418) data 0.000 (0.054) loss 0.9247 (1.0209) lr 9.9726e-03 eta 0:04:02
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:02<00:31,  2.24s/it] 13%|█▎        | 2/15 [00:02<00:15,  1.23s/it] 20%|██        | 3/15 [00:03<00:09,  1.25it/s] 27%|██▋       | 4/15 [00:03<00:06,  1.68it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.06it/s] 40%|████      | 6/15 [00:03<00:03,  2.39it/s] 47%|████▋     | 7/15 [00:04<00:03,  2.66it/s] 53%|█████▎    | 8/15 [00:04<00:02,  2.87it/s] 60%|██████    | 9/15 [00:04<00:01,  3.01it/s] 67%|██████▋   | 10/15 [00:05<00:01,  3.14it/s] 73%|███████▎  | 11/15 [00:05<00:01,  3.23it/s] 80%|████████  | 12/15 [00:05<00:00,  3.30it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.35it/s] 93%|█████████▎| 14/15 [00:06<00:00,  3.38it/s]100%|██████████| 15/15 [00:06<00:00,  4.20it/s]100%|██████████| 15/15 [00:06<00:00,  2.33it/s]=> result
* total: 2,800
* correct: 1,801
* accuracy: 64.3%
* error: 35.7%
* macro_f1: 64.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [2/20] time 0.285 (0.573) data 0.000 (0.266) loss 0.9566 (0.8696) lr 9.9726e-03 eta 0:05:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [4/20] time 0.294 (0.432) data 0.000 (0.133) loss 1.2044 (0.9789) lr 9.9726e-03 eta 0:04:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [6/20] time 0.280 (0.383) data 0.000 (0.089) loss 0.3599 (0.8580) lr 9.9726e-03 eta 0:03:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [8/20] time 0.281 (0.358) data 0.000 (0.067) loss 0.8185 (0.7574) lr 9.9726e-03 eta 0:03:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [10/20] time 0.281 (0.342) data 0.000 (0.053) loss 1.1982 (0.8382) lr 9.9726e-03 eta 0:03:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [12/20] time 0.282 (0.332) data 0.000 (0.045) loss 1.0705 (0.8515) lr 9.9726e-03 eta 0:03:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [14/20] time 0.285 (0.325) data 0.000 (0.038) loss 1.3355 (0.9123) lr 9.9726e-03 eta 0:03:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [16/20] time 0.279 (0.319) data 0.000 (0.033) loss 0.5963 (0.8762) lr 9.9726e-03 eta 0:03:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [18/20] time 0.283 (0.315) data 0.000 (0.030) loss 0.8000 (0.8712) lr 9.9726e-03 eta 0:02:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [20/20] time 0.280 (0.312) data 0.000 (0.027) loss 0.7606 (0.8770) lr 9.8907e-03 eta 0:02:54
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:23,  1.67s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.08it/s] 20%|██        | 3/15 [00:02<00:07,  1.57it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.98it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.34it/s] 40%|████      | 6/15 [00:03<00:03,  2.63it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.86it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.03it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:04<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.59it/s]=> result
* total: 2,800
* correct: 1,757
* accuracy: 62.8%
* error: 37.2%
* macro_f1: 60.4%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [2/20] time 0.296 (0.582) data 0.000 (0.263) loss 1.0313 (0.7711) lr 9.8907e-03 eta 0:05:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [4/20] time 0.289 (0.436) data 0.000 (0.131) loss 1.2452 (0.8771) lr 9.8907e-03 eta 0:04:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [6/20] time 0.301 (0.390) data 0.000 (0.088) loss 0.8237 (0.8426) lr 9.8907e-03 eta 0:03:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [8/20] time 0.294 (0.367) data 0.000 (0.066) loss 0.8504 (0.8399) lr 9.8907e-03 eta 0:03:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [10/20] time 0.292 (0.352) data 0.000 (0.053) loss 0.6013 (0.8084) lr 9.8907e-03 eta 0:03:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [12/20] time 0.296 (0.342) data 0.000 (0.044) loss 0.7659 (0.8210) lr 9.8907e-03 eta 0:03:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [14/20] time 0.297 (0.336) data 0.000 (0.038) loss 1.2722 (0.8262) lr 9.8907e-03 eta 0:03:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [16/20] time 0.297 (0.331) data 0.000 (0.033) loss 0.5900 (0.8077) lr 9.8907e-03 eta 0:02:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [18/20] time 0.294 (0.327) data 0.000 (0.029) loss 0.2561 (0.7652) lr 9.8907e-03 eta 0:02:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [20/20] time 0.296 (0.324) data 0.000 (0.027) loss 0.5340 (0.7382) lr 9.7553e-03 eta 0:02:54
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:26,  1.86s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.01it/s] 20%|██        | 3/15 [00:02<00:08,  1.49it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.92it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.29it/s] 40%|████      | 6/15 [00:03<00:03,  2.59it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.82it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.00it/s] 60%|██████    | 9/15 [00:04<00:01,  3.13it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.52it/s]=> result
* total: 2,800
* correct: 1,875
* accuracy: 67.0%
* error: 33.0%
* macro_f1: 64.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [2/20] time 0.268 (0.557) data 0.000 (0.261) loss 0.5170 (0.4902) lr 9.7553e-03 eta 0:04:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [4/20] time 0.262 (0.411) data 0.000 (0.131) loss 0.5854 (0.5174) lr 9.7553e-03 eta 0:03:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [6/20] time 0.280 (0.365) data 0.000 (0.087) loss 0.6961 (0.5446) lr 9.7553e-03 eta 0:03:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [8/20] time 0.270 (0.342) data 0.000 (0.065) loss 0.6482 (0.5161) lr 9.7553e-03 eta 0:03:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [10/20] time 0.274 (0.328) data 0.000 (0.052) loss 0.5058 (0.5433) lr 9.7553e-03 eta 0:02:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [12/20] time 0.268 (0.319) data 0.000 (0.044) loss 0.9870 (0.5733) lr 9.7553e-03 eta 0:02:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [14/20] time 0.273 (0.312) data 0.000 (0.037) loss 0.3133 (0.5693) lr 9.7553e-03 eta 0:02:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [16/20] time 0.268 (0.307) data 0.000 (0.033) loss 0.3670 (0.5677) lr 9.7553e-03 eta 0:02:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [18/20] time 0.274 (0.303) data 0.000 (0.029) loss 0.6526 (0.5601) lr 9.7553e-03 eta 0:02:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [20/20] time 0.268 (0.300) data 0.000 (0.026) loss 0.7859 (0.5568) lr 9.5677e-03 eta 0:02:35
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:27,  1.98s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.01it/s] 20%|██        | 3/15 [00:02<00:08,  1.50it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.93it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.30it/s] 40%|████      | 6/15 [00:03<00:03,  2.60it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.83it/s] 53%|█████▎    | 8/15 [00:04<00:02,  3.00it/s] 60%|██████    | 9/15 [00:04<00:01,  3.14it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.52it/s]=> result
* total: 2,800
* correct: 2,118
* accuracy: 75.6%
* error: 24.4%
* macro_f1: 76.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [2/20] time 0.273 (0.566) data 0.000 (0.258) loss 0.4363 (0.6876) lr 9.5677e-03 eta 0:04:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [4/20] time 0.283 (0.424) data 0.000 (0.129) loss 0.7895 (0.6462) lr 9.5677e-03 eta 0:03:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [6/20] time 0.279 (0.376) data 0.000 (0.086) loss 0.3443 (0.5657) lr 9.5677e-03 eta 0:03:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [8/20] time 0.283 (0.353) data 0.000 (0.065) loss 0.3979 (0.5156) lr 9.5677e-03 eta 0:03:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [10/20] time 0.277 (0.338) data 0.000 (0.052) loss 0.6154 (0.5179) lr 9.5677e-03 eta 0:02:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [12/20] time 0.281 (0.329) data 0.000 (0.043) loss 0.7410 (0.5823) lr 9.5677e-03 eta 0:02:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [14/20] time 0.278 (0.322) data 0.000 (0.037) loss 0.0818 (0.5030) lr 9.5677e-03 eta 0:02:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [16/20] time 0.291 (0.317) data 0.000 (0.032) loss 0.3635 (0.5246) lr 9.5677e-03 eta 0:02:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [18/20] time 0.291 (0.314) data 0.000 (0.029) loss 0.1887 (0.5075) lr 9.5677e-03 eta 0:02:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [20/20] time 0.290 (0.312) data 0.000 (0.026) loss 0.4294 (0.5251) lr 9.3301e-03 eta 0:02:36
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:26,  1.86s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.04it/s] 20%|██        | 3/15 [00:02<00:07,  1.53it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.96it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.32it/s] 40%|████      | 6/15 [00:03<00:03,  2.62it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.84it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.01it/s] 60%|██████    | 9/15 [00:04<00:01,  3.14it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.55it/s]=> result
* total: 2,800
* correct: 2,153
* accuracy: 76.9%
* error: 23.1%
* macro_f1: 77.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [2/20] time 0.289 (0.579) data 0.000 (0.257) loss 0.2584 (0.3727) lr 9.3301e-03 eta 0:04:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [4/20] time 0.287 (0.433) data 0.000 (0.129) loss 0.8563 (0.4345) lr 9.3301e-03 eta 0:03:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [6/20] time 0.290 (0.387) data 0.000 (0.086) loss 0.5268 (0.5416) lr 9.3301e-03 eta 0:03:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [8/20] time 0.290 (0.363) data 0.000 (0.065) loss 0.2055 (0.4861) lr 9.3301e-03 eta 0:02:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [10/20] time 0.291 (0.348) data 0.000 (0.052) loss 0.3221 (0.5160) lr 9.3301e-03 eta 0:02:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [12/20] time 0.296 (0.339) data 0.000 (0.043) loss 0.0234 (0.5004) lr 9.3301e-03 eta 0:02:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [14/20] time 0.286 (0.332) data 0.000 (0.037) loss 0.5608 (0.4851) lr 9.3301e-03 eta 0:02:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [16/20] time 0.294 (0.327) data 0.000 (0.032) loss 0.3776 (0.4722) lr 9.3301e-03 eta 0:02:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [18/20] time 0.285 (0.323) data 0.000 (0.029) loss 0.4298 (0.4507) lr 9.3301e-03 eta 0:02:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [20/20] time 0.289 (0.320) data 0.000 (0.026) loss 0.6223 (0.4619) lr 9.0451e-03 eta 0:02:33
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:22,  1.63s/it] 13%|█▎        | 2/15 [00:02<00:11,  1.10it/s] 20%|██        | 3/15 [00:02<00:07,  1.59it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.02it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.38it/s] 40%|████      | 6/15 [00:03<00:03,  2.66it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.88it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.05it/s] 60%|██████    | 9/15 [00:04<00:01,  3.17it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.25it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.32it/s] 80%|████████  | 12/15 [00:04<00:00,  3.36it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.61it/s]=> result
* total: 2,800
* correct: 2,124
* accuracy: 75.9%
* error: 24.1%
* macro_f1: 75.8%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [2/20] time 0.288 (0.578) data 0.000 (0.251) loss 0.4439 (0.5475) lr 9.0451e-03 eta 0:04:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [4/20] time 0.281 (0.432) data 0.000 (0.125) loss 0.3702 (0.4772) lr 9.0451e-03 eta 0:03:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [6/20] time 0.289 (0.386) data 0.000 (0.084) loss 0.7941 (0.6039) lr 9.0451e-03 eta 0:03:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [8/20] time 0.288 (0.361) data 0.000 (0.063) loss 0.4425 (0.5499) lr 9.0451e-03 eta 0:02:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [10/20] time 0.290 (0.347) data 0.000 (0.050) loss 0.7788 (0.5567) lr 9.0451e-03 eta 0:02:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [12/20] time 0.283 (0.337) data 0.000 (0.042) loss 0.3729 (0.5206) lr 9.0451e-03 eta 0:02:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [14/20] time 0.287 (0.330) data 0.000 (0.036) loss 0.4907 (0.5155) lr 9.0451e-03 eta 0:02:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [16/20] time 0.291 (0.325) data 0.000 (0.032) loss 0.6375 (0.5264) lr 9.0451e-03 eta 0:02:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [18/20] time 0.288 (0.321) data 0.000 (0.028) loss 0.6325 (0.5317) lr 9.0451e-03 eta 0:02:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [20/20] time 0.293 (0.318) data 0.000 (0.025) loss 0.1651 (0.5067) lr 8.7157e-03 eta 0:02:26
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:22,  1.60s/it] 13%|█▎        | 2/15 [00:02<00:11,  1.11it/s] 20%|██        | 3/15 [00:02<00:07,  1.58it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.01it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.37it/s] 40%|████      | 6/15 [00:03<00:03,  2.65it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.87it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.04it/s] 60%|██████    | 9/15 [00:04<00:01,  3.16it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:04<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.62it/s]=> result
* total: 2,800
* correct: 2,267
* accuracy: 81.0%
* error: 19.0%
* macro_f1: 80.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [2/20] time 0.281 (0.593) data 0.000 (0.267) loss 0.0794 (0.1437) lr 8.7157e-03 eta 0:04:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [4/20] time 0.290 (0.441) data 0.000 (0.133) loss 0.1654 (0.2323) lr 8.7157e-03 eta 0:03:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [6/20] time 0.285 (0.390) data 0.000 (0.089) loss 0.4026 (0.3993) lr 8.7157e-03 eta 0:02:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [8/20] time 0.286 (0.364) data 0.000 (0.067) loss 0.8902 (0.4588) lr 8.7157e-03 eta 0:02:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [10/20] time 0.288 (0.348) data 0.000 (0.054) loss 0.4053 (0.5157) lr 8.7157e-03 eta 0:02:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [12/20] time 0.289 (0.339) data 0.000 (0.045) loss 1.0970 (0.5643) lr 8.7157e-03 eta 0:02:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [14/20] time 0.288 (0.331) data 0.000 (0.038) loss 0.6259 (0.5821) lr 8.7157e-03 eta 0:02:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [16/20] time 0.281 (0.325) data 0.000 (0.034) loss 0.8898 (0.5646) lr 8.7157e-03 eta 0:02:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [18/20] time 0.287 (0.321) data 0.000 (0.030) loss 0.3798 (0.5336) lr 8.7157e-03 eta 0:02:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [20/20] time 0.283 (0.317) data 0.000 (0.027) loss 0.2346 (0.5125) lr 8.3457e-03 eta 0:02:19
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:25,  1.81s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.03it/s] 20%|██        | 3/15 [00:02<00:07,  1.51it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.94it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.31it/s] 40%|████      | 6/15 [00:03<00:03,  2.61it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.84it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.01it/s] 60%|██████    | 9/15 [00:04<00:01,  3.14it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.54it/s]=> result
* total: 2,800
* correct: 2,143
* accuracy: 76.5%
* error: 23.5%
* macro_f1: 76.5%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [2/20] time 0.271 (0.579) data 0.000 (0.268) loss 0.4226 (0.4248) lr 8.3457e-03 eta 0:04:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [4/20] time 0.280 (0.427) data 0.001 (0.134) loss 0.3460 (0.4755) lr 8.3457e-03 eta 0:03:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [6/20] time 0.275 (0.377) data 0.000 (0.090) loss -0.0590 (0.4324) lr 8.3457e-03 eta 0:02:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [8/20] time 0.283 (0.353) data 0.000 (0.067) loss 0.5478 (0.4229) lr 8.3457e-03 eta 0:02:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [10/20] time 0.276 (0.339) data 0.000 (0.054) loss 0.7040 (0.4524) lr 8.3457e-03 eta 0:02:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [12/20] time 0.285 (0.329) data 0.000 (0.045) loss 0.3211 (0.4099) lr 8.3457e-03 eta 0:02:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [14/20] time 0.279 (0.322) data 0.000 (0.038) loss 0.1684 (0.3898) lr 8.3457e-03 eta 0:02:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [16/20] time 0.278 (0.317) data 0.000 (0.034) loss 0.3894 (0.3930) lr 8.3457e-03 eta 0:02:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [18/20] time 0.278 (0.313) data 0.000 (0.030) loss 0.5199 (0.4383) lr 8.3457e-03 eta 0:02:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [20/20] time 0.280 (0.310) data 0.000 (0.027) loss 0.2188 (0.4077) lr 7.9389e-03 eta 0:02:10
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:23,  1.66s/it] 13%|█▎        | 2/15 [00:02<00:11,  1.11it/s] 20%|██        | 3/15 [00:02<00:07,  1.61it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.05it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.40it/s] 40%|████      | 6/15 [00:03<00:03,  2.68it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.89it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.05it/s] 60%|██████    | 9/15 [00:04<00:01,  3.17it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.26it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.32it/s] 80%|████████  | 12/15 [00:04<00:00,  3.36it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.63it/s]=> result
* total: 2,800
* correct: 2,252
* accuracy: 80.4%
* error: 19.6%
* macro_f1: 80.1%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [2/20] time 0.281 (0.583) data 0.000 (0.262) loss 0.5753 (0.4662) lr 7.9389e-03 eta 0:04:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [4/20] time 0.279 (0.431) data 0.000 (0.131) loss 0.1153 (0.3493) lr 7.9389e-03 eta 0:02:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [6/20] time 0.326 (0.389) data 0.000 (0.088) loss 0.7920 (0.3527) lr 7.9389e-03 eta 0:02:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [8/20] time 0.293 (0.366) data 0.000 (0.066) loss 0.5826 (0.3985) lr 7.9389e-03 eta 0:02:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [10/20] time 0.295 (0.352) data 0.000 (0.053) loss 0.0574 (0.3617) lr 7.9389e-03 eta 0:02:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [12/20] time 0.295 (0.350) data 0.000 (0.044) loss 0.2449 (0.3515) lr 7.9389e-03 eta 0:02:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [14/20] time 0.294 (0.342) data 0.000 (0.038) loss 0.0628 (0.3215) lr 7.9389e-03 eta 0:02:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [16/20] time 0.299 (0.336) data 0.000 (0.033) loss 0.0262 (0.3211) lr 7.9389e-03 eta 0:02:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [18/20] time 0.293 (0.332) data 0.000 (0.029) loss 0.3687 (0.3188) lr 7.9389e-03 eta 0:02:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [20/20] time 0.296 (0.328) data 0.000 (0.026) loss 0.4671 (0.3233) lr 7.5000e-03 eta 0:02:11
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:25,  1.80s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.08it/s] 20%|██        | 3/15 [00:02<00:07,  1.58it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.00it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.36it/s] 40%|████      | 6/15 [00:03<00:03,  2.64it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.87it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.03it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.31it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.58it/s]=> result
* total: 2,800
* correct: 2,364
* accuracy: 84.4%
* error: 15.6%
* macro_f1: 84.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-10

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [2/20] time 0.285 (0.575) data 0.000 (0.263) loss 0.3219 (0.3834) lr 7.5000e-03 eta 0:03:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [4/20] time 0.284 (0.432) data 0.000 (0.131) loss 0.3624 (0.4007) lr 7.5000e-03 eta 0:02:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [6/20] time 0.291 (0.384) data 0.000 (0.088) loss 0.2624 (0.3512) lr 7.5000e-03 eta 0:02:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [8/20] time 0.282 (0.359) data 0.000 (0.066) loss 0.1807 (0.3328) lr 7.5000e-03 eta 0:02:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [10/20] time 0.291 (0.344) data 0.000 (0.053) loss 0.4246 (0.2965) lr 7.5000e-03 eta 0:02:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [12/20] time 0.282 (0.334) data 0.000 (0.044) loss 0.8851 (0.3489) lr 7.5000e-03 eta 0:02:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [14/20] time 0.292 (0.328) data 0.000 (0.038) loss 0.6961 (0.3439) lr 7.5000e-03 eta 0:02:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [16/20] time 0.283 (0.322) data 0.000 (0.033) loss 0.8338 (0.3991) lr 7.5000e-03 eta 0:02:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [18/20] time 0.286 (0.318) data 0.000 (0.029) loss 0.2403 (0.3865) lr 7.5000e-03 eta 0:02:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [20/20] time 0.277 (0.314) data 0.000 (0.026) loss 0.8242 (0.4186) lr 7.0337e-03 eta 0:01:59
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:23,  1.69s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.07it/s] 20%|██        | 3/15 [00:02<00:07,  1.56it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.99it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.35it/s] 40%|████      | 6/15 [00:03<00:03,  2.64it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.87it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.03it/s] 60%|██████    | 9/15 [00:04<00:01,  3.16it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.31it/s] 80%|████████  | 12/15 [00:04<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  4.24it/s]100%|██████████| 15/15 [00:05<00:00,  2.58it/s]=> result
* total: 2,800
* correct: 2,256
* accuracy: 80.6%
* error: 19.4%
* macro_f1: 80.3%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [2/20] time 0.266 (0.575) data 0.001 (0.257) loss 0.4208 (0.2754) lr 7.0337e-03 eta 0:03:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [4/20] time 0.274 (0.423) data 0.000 (0.129) loss 0.7417 (0.2882) lr 7.0337e-03 eta 0:02:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [6/20] time 0.275 (0.375) data 0.000 (0.086) loss 0.2654 (0.2608) lr 7.0337e-03 eta 0:02:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [8/20] time 0.280 (0.351) data 0.000 (0.065) loss 0.3786 (0.3024) lr 7.0337e-03 eta 0:02:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [10/20] time 0.279 (0.336) data 0.000 (0.052) loss -0.0686 (0.2579) lr 7.0337e-03 eta 0:02:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [12/20] time 0.278 (0.327) data 0.000 (0.043) loss 0.3214 (0.3093) lr 7.0337e-03 eta 0:02:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [14/20] time 0.280 (0.320) data 0.000 (0.037) loss 0.4990 (0.3306) lr 7.0337e-03 eta 0:01:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [16/20] time 0.282 (0.315) data 0.000 (0.032) loss 0.3470 (0.3459) lr 7.0337e-03 eta 0:01:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [18/20] time 0.276 (0.311) data 0.000 (0.029) loss 0.9024 (0.3845) lr 7.0337e-03 eta 0:01:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [20/20] time 0.276 (0.307) data 0.000 (0.026) loss 0.2772 (0.3794) lr 6.5451e-03 eta 0:01:50
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:24,  1.78s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.08it/s] 20%|██        | 3/15 [00:02<00:07,  1.56it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.99it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.35it/s] 40%|████      | 6/15 [00:03<00:03,  2.63it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.86it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.03it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.31it/s] 80%|████████  | 12/15 [00:05<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.41it/s]100%|██████████| 15/15 [00:05<00:00,  2.57it/s]=> result
* total: 2,800
* correct: 2,388
* accuracy: 85.3%
* error: 14.7%
* macro_f1: 85.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [2/20] time 0.284 (0.580) data 0.000 (0.267) loss 0.2162 (0.1880) lr 6.5451e-03 eta 0:03:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [4/20] time 0.279 (0.429) data 0.000 (0.134) loss 0.4788 (0.2330) lr 6.5451e-03 eta 0:02:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [6/20] time 0.299 (0.384) data 0.000 (0.089) loss -0.0419 (0.2246) lr 6.5451e-03 eta 0:02:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [8/20] time 0.321 (0.370) data 0.000 (0.067) loss 0.5522 (0.2553) lr 6.5451e-03 eta 0:02:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [10/20] time 0.287 (0.353) data 0.000 (0.054) loss 0.3681 (0.2816) lr 6.5451e-03 eta 0:02:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [12/20] time 0.285 (0.342) data 0.000 (0.045) loss -0.0119 (0.2737) lr 6.5451e-03 eta 0:01:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [14/20] time 0.285 (0.334) data 0.000 (0.038) loss 0.4604 (0.3078) lr 6.5451e-03 eta 0:01:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [16/20] time 0.290 (0.328) data 0.000 (0.034) loss 0.2146 (0.3086) lr 6.5451e-03 eta 0:01:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [18/20] time 0.287 (0.323) data 0.000 (0.030) loss 0.2309 (0.2872) lr 6.5451e-03 eta 0:01:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [20/20] time 0.287 (0.319) data 0.000 (0.027) loss 0.8582 (0.3269) lr 6.0396e-03 eta 0:01:48
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:22,  1.63s/it] 13%|█▎        | 2/15 [00:02<00:11,  1.09it/s] 20%|██        | 3/15 [00:02<00:07,  1.58it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.00it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.36it/s] 40%|████      | 6/15 [00:03<00:03,  2.64it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.86it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.03it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:04<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.61it/s]=> result
* total: 2,800
* correct: 2,456
* accuracy: 87.7%
* error: 12.3%
* macro_f1: 87.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [2/20] time 0.280 (0.566) data 0.001 (0.254) loss 0.2694 (0.3255) lr 6.0396e-03 eta 0:03:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [4/20] time 0.282 (0.424) data 0.000 (0.127) loss 0.1886 (0.2635) lr 6.0396e-03 eta 0:02:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [6/20] time 0.285 (0.378) data 0.000 (0.085) loss 0.7113 (0.3548) lr 6.0396e-03 eta 0:02:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [8/20] time 0.287 (0.355) data 0.000 (0.064) loss 0.1674 (0.3049) lr 6.0396e-03 eta 0:01:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [10/20] time 0.286 (0.341) data 0.000 (0.051) loss 0.1609 (0.2685) lr 6.0396e-03 eta 0:01:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [12/20] time 0.285 (0.332) data 0.000 (0.043) loss 0.1536 (0.2963) lr 6.0396e-03 eta 0:01:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [14/20] time 0.287 (0.325) data 0.000 (0.037) loss 0.5951 (0.2811) lr 6.0396e-03 eta 0:01:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [16/20] time 0.282 (0.320) data 0.000 (0.032) loss 0.2109 (0.2825) lr 6.0396e-03 eta 0:01:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [18/20] time 0.287 (0.316) data 0.000 (0.028) loss 0.3513 (0.2996) lr 6.0396e-03 eta 0:01:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [20/20] time 0.280 (0.312) data 0.000 (0.026) loss -0.0207 (0.2628) lr 5.5226e-03 eta 0:01:39
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:26,  1.92s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.03it/s] 20%|██        | 3/15 [00:02<00:07,  1.51it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.95it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.31it/s] 40%|████      | 6/15 [00:03<00:03,  2.61it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.84it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.01it/s] 60%|██████    | 9/15 [00:04<00:01,  3.14it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.34it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.37it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.39it/s]100%|██████████| 15/15 [00:05<00:00,  2.53it/s]=> result
* total: 2,800
* correct: 2,491
* accuracy: 89.0%
* error: 11.0%
* macro_f1: 89.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [2/20] time 0.265 (0.549) data 0.000 (0.240) loss 0.1693 (0.1264) lr 5.5226e-03 eta 0:02:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [4/20] time 0.264 (0.407) data 0.000 (0.120) loss -0.0415 (0.1161) lr 5.5226e-03 eta 0:02:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [6/20] time 0.273 (0.362) data 0.000 (0.080) loss 0.3836 (0.1546) lr 5.5226e-03 eta 0:01:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [8/20] time 0.271 (0.339) data 0.000 (0.060) loss 0.0670 (0.1198) lr 5.5226e-03 eta 0:01:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [10/20] time 0.270 (0.325) data 0.000 (0.048) loss -0.0155 (0.1291) lr 5.5226e-03 eta 0:01:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [12/20] time 0.271 (0.316) data 0.000 (0.040) loss -0.0205 (0.1126) lr 5.5226e-03 eta 0:01:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [14/20] time 0.269 (0.309) data 0.000 (0.035) loss 0.8344 (0.1747) lr 5.5226e-03 eta 0:01:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [16/20] time 0.269 (0.304) data 0.000 (0.030) loss 0.3446 (0.1831) lr 5.5226e-03 eta 0:01:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [18/20] time 0.283 (0.301) data 0.000 (0.027) loss 0.3238 (0.1841) lr 5.5226e-03 eta 0:01:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [20/20] time 0.291 (0.300) data 0.000 (0.024) loss 0.1754 (0.1706) lr 5.0000e-03 eta 0:01:30
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:23,  1.69s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.08it/s] 20%|██        | 3/15 [00:02<00:07,  1.55it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.99it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.33it/s] 40%|████      | 6/15 [00:03<00:03,  2.62it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.85it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.02it/s] 60%|██████    | 9/15 [00:04<00:01,  3.14it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:04<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.59it/s]=> result
* total: 2,800
* correct: 2,520
* accuracy: 90.0%
* error: 10.0%
* macro_f1: 90.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [2/20] time 0.268 (0.564) data 0.000 (0.276) loss 0.3933 (0.3350) lr 5.0000e-03 eta 0:02:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [4/20] time 0.273 (0.419) data 0.000 (0.138) loss 0.5086 (0.4119) lr 5.0000e-03 eta 0:02:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [6/20] time 0.272 (0.370) data 0.000 (0.092) loss 0.1151 (0.2995) lr 5.0000e-03 eta 0:01:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [8/20] time 0.275 (0.346) data 0.000 (0.069) loss 0.0119 (0.2111) lr 5.0000e-03 eta 0:01:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [10/20] time 0.282 (0.342) data 0.000 (0.055) loss 0.1049 (0.1688) lr 5.0000e-03 eta 0:01:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [12/20] time 0.274 (0.331) data 0.000 (0.046) loss -0.0439 (0.1459) lr 5.0000e-03 eta 0:01:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [14/20] time 0.277 (0.323) data 0.000 (0.040) loss 0.7384 (0.2114) lr 5.0000e-03 eta 0:01:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [16/20] time 0.274 (0.317) data 0.000 (0.035) loss 0.5227 (0.2187) lr 5.0000e-03 eta 0:01:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [18/20] time 0.281 (0.312) data 0.000 (0.031) loss 0.4890 (0.2634) lr 5.0000e-03 eta 0:01:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [20/20] time 0.272 (0.309) data 0.000 (0.028) loss 0.2413 (0.2582) lr 4.4774e-03 eta 0:01:26
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:24,  1.78s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.03it/s] 20%|██        | 3/15 [00:02<00:07,  1.52it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.93it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.30it/s] 40%|████      | 6/15 [00:03<00:03,  2.60it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.83it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.00it/s] 60%|██████    | 9/15 [00:04<00:01,  3.13it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.22it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.29it/s] 80%|████████  | 12/15 [00:05<00:00,  3.34it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.37it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.55it/s]=> result
* total: 2,800
* correct: 2,536
* accuracy: 90.6%
* error: 9.4%
* macro_f1: 90.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [2/20] time 0.336 (0.622) data 0.000 (0.279) loss 0.4958 (0.3504) lr 4.4774e-03 eta 0:02:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [4/20] time 0.286 (0.460) data 0.000 (0.140) loss 0.3487 (0.3050) lr 4.4774e-03 eta 0:02:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [6/20] time 0.279 (0.401) data 0.000 (0.093) loss 0.0110 (0.2380) lr 4.4774e-03 eta 0:01:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [8/20] time 0.287 (0.372) data 0.000 (0.070) loss 0.6423 (0.2844) lr 4.4774e-03 eta 0:01:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [10/20] time 0.280 (0.354) data 0.000 (0.056) loss 0.2933 (0.2976) lr 4.4774e-03 eta 0:01:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [12/20] time 0.286 (0.343) data 0.000 (0.047) loss 0.0700 (0.2929) lr 4.4774e-03 eta 0:01:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [14/20] time 0.279 (0.334) data 0.000 (0.040) loss -0.0090 (0.2485) lr 4.4774e-03 eta 0:01:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [16/20] time 0.288 (0.328) data 0.000 (0.035) loss 0.0953 (0.2229) lr 4.4774e-03 eta 0:01:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [18/20] time 0.280 (0.323) data 0.000 (0.031) loss 0.2253 (0.2152) lr 4.4774e-03 eta 0:01:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [20/20] time 0.280 (0.318) data 0.000 (0.028) loss 0.1717 (0.1945) lr 3.9604e-03 eta 0:01:22
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:27,  1.97s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.02it/s] 20%|██        | 3/15 [00:02<00:07,  1.50it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.93it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.29it/s] 40%|████      | 6/15 [00:03<00:03,  2.58it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.81it/s] 53%|█████▎    | 8/15 [00:04<00:02,  2.99it/s] 60%|██████    | 9/15 [00:04<00:01,  3.12it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.22it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.29it/s] 80%|████████  | 12/15 [00:05<00:00,  3.33it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.37it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.52it/s]=> result
* total: 2,800
* correct: 2,514
* accuracy: 89.8%
* error: 10.2%
* macro_f1: 89.9%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [2/20] time 0.289 (0.619) data 0.000 (0.283) loss 0.8691 (0.5251) lr 3.9604e-03 eta 0:02:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [4/20] time 0.288 (0.452) data 0.000 (0.142) loss -0.0205 (0.2958) lr 3.9604e-03 eta 0:01:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [6/20] time 0.288 (0.398) data 0.000 (0.095) loss 0.3011 (0.2637) lr 3.9604e-03 eta 0:01:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [8/20] time 0.287 (0.369) data 0.000 (0.071) loss -0.0407 (0.2010) lr 3.9604e-03 eta 0:01:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [10/20] time 0.283 (0.353) data 0.000 (0.057) loss 0.5045 (0.2178) lr 3.9604e-03 eta 0:01:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [12/20] time 0.291 (0.342) data 0.000 (0.047) loss 0.1687 (0.2157) lr 3.9604e-03 eta 0:01:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [14/20] time 0.283 (0.334) data 0.000 (0.041) loss -0.2105 (0.1617) lr 3.9604e-03 eta 0:01:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [16/20] time 0.290 (0.328) data 0.000 (0.036) loss 0.4287 (0.1749) lr 3.9604e-03 eta 0:01:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [18/20] time 0.283 (0.323) data 0.000 (0.032) loss 0.4056 (0.1898) lr 3.9604e-03 eta 0:01:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [20/20] time 0.292 (0.320) data 0.000 (0.029) loss -0.0179 (0.1680) lr 3.4549e-03 eta 0:01:16
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:27,  1.93s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.03it/s] 20%|██        | 3/15 [00:02<00:07,  1.51it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.93it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.29it/s] 40%|████      | 6/15 [00:03<00:03,  2.57it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.80it/s] 53%|█████▎    | 8/15 [00:03<00:02,  2.96it/s] 60%|██████    | 9/15 [00:04<00:01,  3.08it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.17it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.23it/s] 80%|████████  | 12/15 [00:05<00:00,  3.27it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.32it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.36it/s]100%|██████████| 15/15 [00:05<00:00,  2.50it/s]=> result
* total: 2,800
* correct: 2,552
* accuracy: 91.1%
* error: 8.9%
* macro_f1: 91.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [2/20] time 0.269 (0.553) data 0.000 (0.252) loss -0.0027 (0.3031) lr 3.4549e-03 eta 0:02:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [4/20] time 0.265 (0.408) data 0.000 (0.126) loss 0.2102 (0.1587) lr 3.4549e-03 eta 0:01:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [6/20] time 0.280 (0.365) data 0.000 (0.084) loss -0.0723 (0.0861) lr 3.4549e-03 eta 0:01:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [8/20] time 0.274 (0.343) data 0.000 (0.063) loss -0.0716 (0.0964) lr 3.4549e-03 eta 0:01:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [10/20] time 0.283 (0.331) data 0.000 (0.051) loss -0.2099 (0.0827) lr 3.4549e-03 eta 0:01:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [12/20] time 0.282 (0.323) data 0.000 (0.042) loss -0.0487 (0.0930) lr 3.4549e-03 eta 0:01:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [14/20] time 0.289 (0.318) data 0.000 (0.036) loss -0.0769 (0.0718) lr 3.4549e-03 eta 0:01:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [16/20] time 0.285 (0.314) data 0.000 (0.032) loss 0.4703 (0.0953) lr 3.4549e-03 eta 0:01:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [18/20] time 0.294 (0.311) data 0.000 (0.028) loss 0.1792 (0.1174) lr 3.4549e-03 eta 0:01:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [20/20] time 0.301 (0.310) data 0.000 (0.025) loss 0.0937 (0.1306) lr 2.9663e-03 eta 0:01:08
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:27,  1.96s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.01it/s] 20%|██        | 3/15 [00:02<00:08,  1.49it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.92it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.29it/s] 40%|████      | 6/15 [00:03<00:03,  2.58it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.81it/s] 53%|█████▎    | 8/15 [00:04<00:02,  2.97it/s] 60%|██████    | 9/15 [00:04<00:01,  3.09it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.19it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.27it/s] 80%|████████  | 12/15 [00:05<00:00,  3.32it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.36it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.39it/s]100%|██████████| 15/15 [00:05<00:00,  2.51it/s]=> result
* total: 2,800
* correct: 2,555
* accuracy: 91.2%
* error: 8.8%
* macro_f1: 91.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [2/20] time 0.267 (0.576) data 0.000 (0.269) loss -0.1536 (-0.0332) lr 2.9663e-03 eta 0:02:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [4/20] time 0.260 (0.419) data 0.000 (0.134) loss -0.0142 (0.0872) lr 2.9663e-03 eta 0:01:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [6/20] time 0.290 (0.373) data 0.000 (0.090) loss 0.1508 (0.1000) lr 2.9663e-03 eta 0:01:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [8/20] time 0.273 (0.349) data 0.000 (0.067) loss -0.1015 (0.0792) lr 2.9663e-03 eta 0:01:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [10/20] time 0.272 (0.333) data 0.000 (0.054) loss 0.1288 (0.0744) lr 2.9663e-03 eta 0:01:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [12/20] time 0.272 (0.323) data 0.000 (0.045) loss 0.1283 (0.0737) lr 2.9663e-03 eta 0:01:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [14/20] time 0.271 (0.316) data 0.000 (0.039) loss 0.5141 (0.0891) lr 2.9663e-03 eta 0:01:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [16/20] time 0.273 (0.310) data 0.000 (0.034) loss -0.1297 (0.0926) lr 2.9663e-03 eta 0:01:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [18/20] time 0.371 (0.311) data 0.000 (0.030) loss -0.0454 (0.0935) lr 2.9663e-03 eta 0:01:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [20/20] time 0.278 (0.308) data 0.000 (0.027) loss 0.0130 (0.0959) lr 2.5000e-03 eta 0:01:01
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:22,  1.62s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.05it/s] 20%|██        | 3/15 [00:02<00:07,  1.54it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.98it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.34it/s] 40%|████      | 6/15 [00:03<00:03,  2.63it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.85it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.02it/s] 60%|██████    | 9/15 [00:04<00:01,  3.14it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:04<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.58it/s]=> result
* total: 2,800
* correct: 2,577
* accuracy: 92.0%
* error: 8.0%
* macro_f1: 92.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-20

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [2/20] time 0.276 (0.569) data 0.000 (0.259) loss 0.2572 (0.1451) lr 2.5000e-03 eta 0:01:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [4/20] time 0.280 (0.424) data 0.000 (0.130) loss 0.2857 (0.2293) lr 2.5000e-03 eta 0:01:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [6/20] time 0.266 (0.375) data 0.000 (0.087) loss 0.2882 (0.1605) lr 2.5000e-03 eta 0:01:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [8/20] time 0.270 (0.348) data 0.000 (0.065) loss 0.3523 (0.1818) lr 2.5000e-03 eta 0:01:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [10/20] time 0.263 (0.331) data 0.000 (0.052) loss 0.2459 (0.1847) lr 2.5000e-03 eta 0:01:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [12/20] time 0.269 (0.321) data 0.000 (0.043) loss -0.0787 (0.1473) lr 2.5000e-03 eta 0:01:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [14/20] time 0.262 (0.313) data 0.000 (0.037) loss 0.0075 (0.1386) lr 2.5000e-03 eta 0:00:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [16/20] time 0.268 (0.307) data 0.000 (0.033) loss 0.1525 (0.1456) lr 2.5000e-03 eta 0:00:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [18/20] time 0.263 (0.302) data 0.000 (0.029) loss 0.1430 (0.1670) lr 2.5000e-03 eta 0:00:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [20/20] time 0.269 (0.299) data 0.000 (0.026) loss 0.0735 (0.1972) lr 2.0611e-03 eta 0:00:53
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:26,  1.90s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.04it/s] 20%|██        | 3/15 [00:02<00:07,  1.52it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.95it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.30it/s] 40%|████      | 6/15 [00:03<00:03,  2.59it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.81it/s] 53%|█████▎    | 8/15 [00:03<00:02,  2.97it/s] 60%|██████    | 9/15 [00:04<00:01,  3.09it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.18it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.25it/s] 80%|████████  | 12/15 [00:05<00:00,  3.29it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.32it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.34it/s]100%|██████████| 15/15 [00:05<00:00,  4.16it/s]100%|██████████| 15/15 [00:05<00:00,  2.52it/s]=> result
* total: 2,800
* correct: 2,549
* accuracy: 91.0%
* error: 9.0%
* macro_f1: 91.1%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [2/20] time 0.269 (0.570) data 0.000 (0.257) loss 0.0676 (0.0065) lr 2.0611e-03 eta 0:01:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [4/20] time 0.267 (0.420) data 0.000 (0.129) loss -0.2187 (0.0170) lr 2.0611e-03 eta 0:01:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [6/20] time 0.278 (0.372) data 0.000 (0.086) loss -0.0156 (0.0359) lr 2.0611e-03 eta 0:01:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [8/20] time 0.275 (0.348) data 0.000 (0.064) loss 0.1021 (0.0613) lr 2.0611e-03 eta 0:00:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [10/20] time 0.279 (0.334) data 0.000 (0.052) loss -0.2117 (0.0338) lr 2.0611e-03 eta 0:00:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [12/20] time 0.276 (0.324) data 0.000 (0.043) loss 0.1577 (0.0362) lr 2.0611e-03 eta 0:00:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [14/20] time 0.277 (0.318) data 0.000 (0.037) loss 0.4267 (0.0560) lr 2.0611e-03 eta 0:00:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [16/20] time 0.279 (0.313) data 0.000 (0.032) loss 0.5963 (0.0867) lr 2.0611e-03 eta 0:00:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [18/20] time 0.282 (0.309) data 0.000 (0.029) loss 0.1067 (0.0783) lr 2.0611e-03 eta 0:00:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [20/20] time 0.285 (0.306) data 0.000 (0.026) loss -0.0308 (0.0775) lr 1.6543e-03 eta 0:00:49
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:23,  1.68s/it] 13%|█▎        | 2/15 [00:02<00:11,  1.08it/s] 20%|██        | 3/15 [00:02<00:07,  1.58it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.01it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.37it/s] 40%|████      | 6/15 [00:03<00:03,  2.65it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.85it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.02it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:04<00:00,  3.34it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  4.24it/s]100%|██████████| 15/15 [00:05<00:00,  2.59it/s]=> result
* total: 2,800
* correct: 2,571
* accuracy: 91.8%
* error: 8.2%
* macro_f1: 91.9%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [2/20] time 0.270 (0.567) data 0.000 (0.255) loss -0.0638 (0.2421) lr 1.6543e-03 eta 0:01:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [4/20] time 0.275 (0.420) data 0.000 (0.128) loss -0.0873 (0.1003) lr 1.6543e-03 eta 0:01:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [6/20] time 0.271 (0.371) data 0.000 (0.085) loss 0.0508 (0.1018) lr 1.6543e-03 eta 0:00:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [8/20] time 0.285 (0.349) data 0.000 (0.064) loss 0.1485 (0.1272) lr 1.6543e-03 eta 0:00:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [10/20] time 0.280 (0.336) data 0.000 (0.051) loss 0.0106 (0.1055) lr 1.6543e-03 eta 0:00:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [12/20] time 0.286 (0.327) data 0.000 (0.043) loss 0.1726 (0.1059) lr 1.6543e-03 eta 0:00:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [14/20] time 0.284 (0.321) data 0.000 (0.037) loss 0.0644 (0.0887) lr 1.6543e-03 eta 0:00:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [16/20] time 0.283 (0.316) data 0.000 (0.032) loss -0.2144 (0.0759) lr 1.6543e-03 eta 0:00:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [18/20] time 0.287 (0.313) data 0.000 (0.029) loss 0.0044 (0.0802) lr 1.6543e-03 eta 0:00:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [20/20] time 0.279 (0.309) data 0.000 (0.026) loss -0.2358 (0.0698) lr 1.2843e-03 eta 0:00:43
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:26,  1.86s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.07it/s] 20%|██        | 3/15 [00:02<00:07,  1.56it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.99it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.35it/s] 40%|████      | 6/15 [00:03<00:03,  2.64it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.86it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.02it/s] 60%|██████    | 9/15 [00:04<00:01,  3.15it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:05<00:00,  3.34it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.39it/s]100%|██████████| 15/15 [00:05<00:00,  2.55it/s]=> result
* total: 2,800
* correct: 2,587
* accuracy: 92.4%
* error: 7.6%
* macro_f1: 92.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [2/20] time 0.280 (0.555) data 0.000 (0.239) loss 0.1706 (0.2177) lr 1.2843e-03 eta 0:01:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [4/20] time 0.279 (0.416) data 0.000 (0.120) loss -0.0377 (0.1021) lr 1.2843e-03 eta 0:00:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [6/20] time 0.283 (0.372) data 0.000 (0.080) loss 0.0268 (0.0447) lr 1.2843e-03 eta 0:00:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [8/20] time 0.283 (0.349) data 0.000 (0.060) loss -0.0020 (0.0384) lr 1.2843e-03 eta 0:00:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [10/20] time 0.283 (0.336) data 0.000 (0.048) loss 0.1357 (0.0265) lr 1.2843e-03 eta 0:00:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [12/20] time 0.289 (0.327) data 0.000 (0.040) loss 0.1202 (0.0442) lr 1.2843e-03 eta 0:00:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [14/20] time 0.277 (0.320) data 0.000 (0.034) loss 0.1528 (0.0406) lr 1.2843e-03 eta 0:00:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [16/20] time 0.284 (0.315) data 0.000 (0.030) loss 0.3239 (0.0621) lr 1.2843e-03 eta 0:00:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [18/20] time 0.280 (0.312) data 0.000 (0.027) loss 0.0583 (0.0641) lr 1.2843e-03 eta 0:00:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [20/20] time 0.284 (0.309) data 0.000 (0.024) loss 0.8816 (0.1220) lr 9.5492e-04 eta 0:00:37
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:22,  1.64s/it] 13%|█▎        | 2/15 [00:02<00:11,  1.09it/s] 20%|██        | 3/15 [00:02<00:07,  1.59it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.02it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.38it/s] 40%|████      | 6/15 [00:03<00:03,  2.66it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.84it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.01it/s] 60%|██████    | 9/15 [00:04<00:01,  3.14it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.23it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.29it/s] 80%|████████  | 12/15 [00:04<00:00,  3.34it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.37it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.39it/s]100%|██████████| 15/15 [00:05<00:00,  2.60it/s]=> result
* total: 2,800
* correct: 2,565
* accuracy: 91.6%
* error: 8.4%
* macro_f1: 91.6%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [2/20] time 0.281 (0.579) data 0.000 (0.273) loss -0.0284 (-0.0076) lr 9.5492e-04 eta 0:01:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [4/20] time 0.284 (0.430) data 0.000 (0.137) loss 0.0383 (0.0287) lr 9.5492e-04 eta 0:00:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [6/20] time 0.285 (0.381) data 0.000 (0.091) loss 0.8216 (0.1532) lr 9.5492e-04 eta 0:00:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [8/20] time 0.282 (0.356) data 0.000 (0.069) loss 0.0278 (0.1066) lr 9.5492e-04 eta 0:00:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [10/20] time 0.285 (0.342) data 0.000 (0.055) loss -0.2329 (0.0720) lr 9.5492e-04 eta 0:00:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [12/20] time 0.280 (0.332) data 0.000 (0.046) loss -0.0440 (0.0539) lr 9.5492e-04 eta 0:00:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [14/20] time 0.282 (0.325) data 0.000 (0.039) loss 0.1064 (0.0821) lr 9.5492e-04 eta 0:00:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [16/20] time 0.280 (0.319) data 0.000 (0.034) loss 0.4740 (0.1153) lr 9.5492e-04 eta 0:00:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [18/20] time 0.286 (0.315) data 0.000 (0.031) loss 0.1168 (0.1018) lr 9.5492e-04 eta 0:00:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [20/20] time 0.283 (0.312) data 0.000 (0.028) loss -0.3368 (0.0685) lr 6.6987e-04 eta 0:00:31
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:22,  1.60s/it] 13%|█▎        | 2/15 [00:02<00:11,  1.12it/s] 20%|██        | 3/15 [00:02<00:07,  1.62it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.05it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.40it/s] 40%|████      | 6/15 [00:03<00:03,  2.68it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.89it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.05it/s] 60%|██████    | 9/15 [00:04<00:01,  3.17it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.25it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.31it/s] 80%|████████  | 12/15 [00:04<00:00,  3.36it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.39it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.63it/s]=> result
* total: 2,800
* correct: 2,564
* accuracy: 91.6%
* error: 8.4%
* macro_f1: 91.6%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [2/20] time 0.273 (0.561) data 0.000 (0.262) loss -0.1110 (-0.0024) lr 6.6987e-04 eta 0:00:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [4/20] time 0.280 (0.420) data 0.000 (0.131) loss -0.0790 (-0.0825) lr 6.6987e-04 eta 0:00:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [6/20] time 0.306 (0.378) data 0.000 (0.087) loss 0.2556 (0.0074) lr 6.6987e-04 eta 0:00:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [8/20] time 0.283 (0.354) data 0.000 (0.066) loss 0.2924 (0.0394) lr 6.6987e-04 eta 0:00:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [10/20] time 0.269 (0.337) data 0.000 (0.053) loss 0.0087 (0.0455) lr 6.6987e-04 eta 0:00:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [12/20] time 0.272 (0.327) data 0.000 (0.044) loss 0.0327 (0.0683) lr 6.6987e-04 eta 0:00:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [14/20] time 0.271 (0.319) data 0.000 (0.038) loss 0.3478 (0.0814) lr 6.6987e-04 eta 0:00:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [16/20] time 0.277 (0.314) data 0.000 (0.033) loss -0.0185 (0.0609) lr 6.6987e-04 eta 0:00:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [18/20] time 0.278 (0.310) data 0.000 (0.029) loss -0.1289 (0.0474) lr 6.6987e-04 eta 0:00:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [20/20] time 0.278 (0.307) data 0.000 (0.026) loss 0.0201 (0.0502) lr 4.3227e-04 eta 0:00:24
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:22,  1.58s/it] 13%|█▎        | 2/15 [00:01<00:11,  1.12it/s] 20%|██        | 3/15 [00:02<00:07,  1.61it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.04it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.40it/s] 40%|████      | 6/15 [00:03<00:03,  2.68it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.89it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.05it/s] 60%|██████    | 9/15 [00:04<00:01,  3.16it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.25it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.31it/s] 80%|████████  | 12/15 [00:04<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.38it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.63it/s]=> result
* total: 2,800
* correct: 2,585
* accuracy: 92.3%
* error: 7.7%
* macro_f1: 92.4%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [2/20] time 0.333 (0.646) data 0.001 (0.279) loss -0.1160 (-0.1082) lr 4.3227e-04 eta 0:00:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [4/20] time 0.286 (0.466) data 0.000 (0.140) loss 0.0861 (0.0164) lr 4.3227e-04 eta 0:00:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [6/20] time 0.288 (0.407) data 0.000 (0.093) loss 0.0467 (0.0510) lr 4.3227e-04 eta 0:00:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [8/20] time 0.285 (0.377) data 0.000 (0.070) loss 0.5545 (0.1104) lr 4.3227e-04 eta 0:00:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [10/20] time 0.288 (0.359) data 0.000 (0.056) loss -0.2104 (0.0942) lr 4.3227e-04 eta 0:00:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [12/20] time 0.344 (0.351) data 0.000 (0.047) loss 0.2474 (0.0997) lr 4.3227e-04 eta 0:00:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [14/20] time 0.293 (0.343) data 0.000 (0.040) loss -0.1152 (0.0779) lr 4.3227e-04 eta 0:00:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [16/20] time 0.299 (0.337) data 0.000 (0.035) loss 0.3258 (0.0843) lr 4.3227e-04 eta 0:00:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [18/20] time 0.294 (0.333) data 0.000 (0.031) loss 0.0582 (0.0903) lr 4.3227e-04 eta 0:00:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [20/20] time 0.300 (0.329) data 0.000 (0.028) loss -0.0225 (0.0819) lr 2.4472e-04 eta 0:00:19
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:21,  1.56s/it] 13%|█▎        | 2/15 [00:01<00:11,  1.14it/s] 20%|██        | 3/15 [00:02<00:07,  1.64it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.07it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.42it/s] 40%|████      | 6/15 [00:03<00:03,  2.69it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.90it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.05it/s] 60%|██████    | 9/15 [00:03<00:01,  3.16it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.24it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.30it/s] 80%|████████  | 12/15 [00:04<00:00,  3.35it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.37it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.39it/s]100%|██████████| 15/15 [00:05<00:00,  2.65it/s]=> result
* total: 2,800
* correct: 2,567
* accuracy: 91.7%
* error: 8.3%
* macro_f1: 91.7%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [2/20] time 0.278 (0.567) data 0.000 (0.250) loss 0.0205 (0.2743) lr 2.4472e-04 eta 0:00:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [4/20] time 0.285 (0.426) data 0.000 (0.125) loss -0.1677 (0.0729) lr 2.4472e-04 eta 0:00:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [6/20] time 0.287 (0.380) data 0.000 (0.083) loss 0.4796 (0.0970) lr 2.4472e-04 eta 0:00:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [8/20] time 0.288 (0.357) data 0.000 (0.063) loss -0.0888 (0.0808) lr 2.4472e-04 eta 0:00:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [10/20] time 0.286 (0.342) data 0.000 (0.050) loss 0.3499 (0.0838) lr 2.4472e-04 eta 0:00:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [12/20] time 0.284 (0.333) data 0.000 (0.042) loss 0.5016 (0.1236) lr 2.4472e-04 eta 0:00:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [14/20] time 0.285 (0.326) data 0.000 (0.036) loss 0.1594 (0.1218) lr 2.4472e-04 eta 0:00:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [16/20] time 0.284 (0.321) data 0.000 (0.031) loss 0.0064 (0.1016) lr 2.4472e-04 eta 0:00:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [18/20] time 0.287 (0.317) data 0.000 (0.028) loss 0.2330 (0.1423) lr 2.4472e-04 eta 0:00:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [20/20] time 0.279 (0.314) data 0.000 (0.025) loss -0.1443 (0.1188) lr 1.0926e-04 eta 0:00:12
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:22,  1.58s/it] 13%|█▎        | 2/15 [00:01<00:11,  1.12it/s] 20%|██        | 3/15 [00:02<00:07,  1.62it/s] 27%|██▋       | 4/15 [00:02<00:05,  2.05it/s] 33%|███▎      | 5/15 [00:02<00:04,  2.40it/s] 40%|████      | 6/15 [00:03<00:03,  2.68it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.90it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.05it/s] 60%|██████    | 9/15 [00:04<00:01,  3.16it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.25it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.31it/s] 80%|████████  | 12/15 [00:04<00:00,  3.34it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.37it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  4.24it/s]100%|██████████| 15/15 [00:05<00:00,  2.62it/s]=> result
* total: 2,800
* correct: 2,566
* accuracy: 91.6%
* error: 8.4%
* macro_f1: 91.7%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [2/20] time 0.289 (0.570) data 0.000 (0.258) loss -0.1249 (-0.0249) lr 1.0926e-04 eta 0:00:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [4/20] time 0.275 (0.424) data 0.000 (0.129) loss 0.1792 (0.0864) lr 1.0926e-04 eta 0:00:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [6/20] time 0.282 (0.377) data 0.000 (0.086) loss 0.3943 (0.1057) lr 1.0926e-04 eta 0:00:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [8/20] time 0.279 (0.353) data 0.000 (0.065) loss -0.0803 (0.0768) lr 1.0926e-04 eta 0:00:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [10/20] time 0.286 (0.339) data 0.000 (0.052) loss -0.1862 (0.0824) lr 1.0926e-04 eta 0:00:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [12/20] time 0.285 (0.329) data 0.000 (0.043) loss 0.0140 (0.0557) lr 1.0926e-04 eta 0:00:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [14/20] time 0.284 (0.323) data 0.000 (0.037) loss 0.2041 (0.0617) lr 1.0926e-04 eta 0:00:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [16/20] time 0.279 (0.317) data 0.000 (0.032) loss -0.1908 (0.0359) lr 1.0926e-04 eta 0:00:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [18/20] time 0.273 (0.312) data 0.000 (0.029) loss -0.1871 (0.0167) lr 1.0926e-04 eta 0:00:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [20/20] time 0.277 (0.309) data 0.000 (0.026) loss -0.1238 (0.0288) lr 2.7391e-05 eta 0:00:06
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:01<00:25,  1.80s/it] 13%|█▎        | 2/15 [00:02<00:12,  1.02it/s] 20%|██        | 3/15 [00:02<00:07,  1.51it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.94it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.30it/s] 40%|████      | 6/15 [00:03<00:03,  2.60it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.83it/s] 53%|█████▎    | 8/15 [00:03<00:02,  3.00it/s] 60%|██████    | 9/15 [00:04<00:01,  3.13it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.22it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.29it/s] 80%|████████  | 12/15 [00:05<00:00,  3.34it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.37it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.40it/s]100%|██████████| 15/15 [00:05<00:00,  2.52it/s]=> result
* total: 2,800
* correct: 2,566
* accuracy: 91.6%
* error: 8.4%
* macro_f1: 91.7%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [2/20] time 0.277 (0.708) data 0.000 (0.397) loss 0.1255 (0.0502) lr 2.7391e-05 eta 0:00:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [4/20] time 0.277 (0.491) data 0.000 (0.199) loss 0.1521 (0.0646) lr 2.7391e-05 eta 0:00:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [6/20] time 0.286 (0.423) data 0.000 (0.132) loss -0.0365 (0.0184) lr 2.7391e-05 eta 0:00:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [8/20] time 0.277 (0.386) data 0.000 (0.099) loss -0.1869 (-0.0116) lr 2.7391e-05 eta 0:00:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [10/20] time 0.282 (0.365) data 0.000 (0.080) loss -0.3090 (-0.0375) lr 2.7391e-05 eta 0:00:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [12/20] time 0.281 (0.351) data 0.000 (0.066) loss 0.2749 (-0.0183) lr 2.7391e-05 eta 0:00:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [14/20] time 0.282 (0.341) data 0.000 (0.057) loss -0.2525 (-0.0184) lr 2.7391e-05 eta 0:00:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [16/20] time 0.275 (0.333) data 0.000 (0.050) loss 0.0827 (-0.0175) lr 2.7391e-05 eta 0:00:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [18/20] time 0.274 (0.326) data 0.000 (0.044) loss 0.2754 (-0.0073) lr 2.7391e-05 eta 0:00:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [20/20] time 0.274 (0.321) data 0.000 (0.040) loss 0.0289 (-0.0143) lr 0.0000e+00 eta 0:00:00
Evaluate on the *val* set
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:02<00:28,  2.02s/it] 13%|█▎        | 2/15 [00:02<00:13,  1.00s/it] 20%|██        | 3/15 [00:02<00:08,  1.48it/s] 27%|██▋       | 4/15 [00:02<00:05,  1.91it/s] 33%|███▎      | 5/15 [00:03<00:04,  2.28it/s] 40%|████      | 6/15 [00:03<00:03,  2.58it/s] 47%|████▋     | 7/15 [00:03<00:02,  2.81it/s] 53%|█████▎    | 8/15 [00:04<00:02,  2.99it/s] 60%|██████    | 9/15 [00:04<00:01,  3.12it/s] 67%|██████▋   | 10/15 [00:04<00:01,  3.20it/s] 73%|███████▎  | 11/15 [00:04<00:01,  3.27it/s] 80%|████████  | 12/15 [00:05<00:00,  3.33it/s] 87%|████████▋ | 13/15 [00:05<00:00,  3.36it/s] 93%|█████████▎| 14/15 [00:05<00:00,  3.39it/s]100%|██████████| 15/15 [00:06<00:00,  2.49it/s]
=> result
* total: 2,800
* correct: 2,567
* accuracy: 91.7%
* error: 8.3%
* macro_f1: 91.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 23)
Evaluate on the *test* set
  0%|          | 0/22 [00:00<?, ?it/s]  5%|▍         | 1/22 [00:02<00:46,  2.20s/it]  9%|▉         | 2/22 [00:02<00:26,  1.32s/it] 14%|█▎        | 3/22 [00:03<00:16,  1.17it/s] 18%|█▊        | 4/22 [00:03<00:11,  1.59it/s] 23%|██▎       | 5/22 [00:03<00:08,  1.97it/s] 27%|██▋       | 6/22 [00:04<00:06,  2.31it/s] 32%|███▏      | 7/22 [00:04<00:05,  2.59it/s] 36%|███▋      | 8/22 [00:04<00:04,  2.81it/s] 41%|████      | 9/22 [00:04<00:04,  2.98it/s] 45%|████▌     | 10/22 [00:05<00:03,  3.11it/s] 50%|█████     | 11/22 [00:05<00:03,  3.21it/s] 55%|█████▍    | 12/22 [00:05<00:03,  3.28it/s] 59%|█████▉    | 13/22 [00:06<00:02,  3.33it/s] 64%|██████▎   | 14/22 [00:06<00:02,  3.37it/s] 68%|██████▊   | 15/22 [00:06<00:02,  3.39it/s] 73%|███████▎  | 16/22 [00:06<00:01,  3.41it/s] 77%|███████▋  | 17/22 [00:07<00:01,  3.42it/s] 82%|████████▏ | 18/22 [00:07<00:01,  3.43it/s] 86%|████████▋ | 19/22 [00:07<00:00,  3.43it/s] 91%|█████████ | 20/22 [00:08<00:00,  3.44it/s] 95%|█████████▌| 21/22 [00:08<00:00,  3.44it/s]100%|██████████| 22/22 [00:08<00:00,  4.09it/s]100%|██████████| 22/22 [00:08<00:00,  2.53it/s]
=> result
* total: 4,200
* correct: 3,880
* accuracy: 92.4%
* error: 7.6%
* macro_f1: 92.4%
Elapsed: 0:06:18
+ sh scripts/rpo_prime/base2new_test_sdl.sh eurosat 3 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:EuroSAT
Loading dataset: EuroSAT
Reading split from /shared/s2/lab01/dataset/clip/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/eurosat/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
80 2600 3900
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      2,600
# test     3,900
---------  -------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/eurosat/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 23)
Evaluate on the *test* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:04<01:29,  4.71s/it] 10%|█         | 2/20 [00:05<00:38,  2.11s/it] 15%|█▌        | 3/20 [00:05<00:21,  1.28s/it] 20%|██        | 4/20 [00:05<00:14,  1.13it/s] 25%|██▌       | 5/20 [00:05<00:10,  1.49it/s] 30%|███       | 6/20 [00:06<00:07,  1.85it/s] 35%|███▌      | 7/20 [00:06<00:05,  2.18it/s] 40%|████      | 8/20 [00:06<00:04,  2.47it/s] 45%|████▌     | 9/20 [00:07<00:04,  2.71it/s] 50%|█████     | 10/20 [00:07<00:03,  2.91it/s] 55%|█████▌    | 11/20 [00:07<00:02,  3.06it/s] 60%|██████    | 12/20 [00:07<00:02,  3.17it/s] 65%|██████▌   | 13/20 [00:08<00:02,  3.26it/s] 70%|███████   | 14/20 [00:08<00:01,  3.32it/s] 75%|███████▌  | 15/20 [00:08<00:01,  3.35it/s] 80%|████████  | 16/20 [00:09<00:01,  3.38it/s] 85%|████████▌ | 17/20 [00:09<00:00,  3.41it/s] 90%|█████████ | 18/20 [00:09<00:00,  3.43it/s] 95%|█████████▌| 19/20 [00:09<00:00,  3.44it/s]100%|██████████| 20/20 [00:10<00:00,  3.54it/s]100%|██████████| 20/20 [00:10<00:00,  1.94it/s]
=> result
* total: 3,900
* correct: 2,247
* accuracy: 57.6%
* error: 42.4%
* macro_f1: 54.5%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh dtd 1 0 main_tmp1_0.1sdl 16
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
384 576 864
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  24
# train_x  384
# val      576
# test     864
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [2/96] time 0.332 (1.646) data 0.000 (0.511) loss 1.9820 (2.4595) lr 1.0000e-02 eta 1:18:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [4/96] time 0.321 (0.984) data 0.000 (0.256) loss 1.8091 (2.2994) lr 1.0000e-02 eta 0:47:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [6/96] time 0.324 (0.767) data 0.000 (0.171) loss 1.5330 (2.1401) lr 1.0000e-02 eta 0:36:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [8/96] time 0.321 (0.656) data 0.000 (0.128) loss 1.1553 (1.9963) lr 1.0000e-02 eta 0:31:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [10/96] time 0.314 (0.588) data 0.000 (0.102) loss 1.5278 (1.9439) lr 1.0000e-02 eta 0:28:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [12/96] time 0.326 (0.545) data 0.000 (0.085) loss 1.3807 (1.8522) lr 1.0000e-02 eta 0:26:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [14/96] time 0.329 (0.513) data 0.000 (0.073) loss 0.9544 (1.8288) lr 1.0000e-02 eta 0:24:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [16/96] time 0.315 (0.489) data 0.000 (0.064) loss 2.0081 (1.8858) lr 1.0000e-02 eta 0:23:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [18/96] time 0.323 (0.470) data 0.000 (0.057) loss 0.9912 (1.8537) lr 1.0000e-02 eta 0:22:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [20/96] time 0.329 (0.456) data 0.000 (0.051) loss 1.4438 (1.7958) lr 1.0000e-02 eta 0:21:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [22/96] time 0.322 (0.444) data 0.000 (0.047) loss 1.3398 (1.7907) lr 1.0000e-02 eta 0:21:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [24/96] time 0.323 (0.434) data 0.000 (0.043) loss 1.7085 (1.7831) lr 1.0000e-02 eta 0:20:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [26/96] time 0.326 (0.425) data 0.000 (0.040) loss 2.4351 (1.8004) lr 1.0000e-02 eta 0:20:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [28/96] time 0.321 (0.418) data 0.000 (0.037) loss 3.0812 (1.8774) lr 1.0000e-02 eta 0:19:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [30/96] time 0.343 (0.412) data 0.000 (0.034) loss 2.0524 (1.9357) lr 1.0000e-02 eta 0:19:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [32/96] time 0.326 (0.407) data 0.001 (0.032) loss 2.7045 (1.9659) lr 1.0000e-02 eta 0:19:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [34/96] time 0.335 (0.403) data 0.000 (0.030) loss 2.1951 (1.9807) lr 1.0000e-02 eta 0:19:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [36/96] time 0.333 (0.399) data 0.000 (0.029) loss 1.3849 (1.9623) lr 1.0000e-02 eta 0:18:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [38/96] time 0.310 (0.395) data 0.000 (0.027) loss 1.4455 (1.9292) lr 1.0000e-02 eta 0:18:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [40/96] time 0.323 (0.392) data 0.000 (0.026) loss 0.8280 (1.8838) lr 1.0000e-02 eta 0:18:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [42/96] time 0.323 (0.388) data 0.000 (0.025) loss 2.1118 (1.8805) lr 1.0000e-02 eta 0:18:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [44/96] time 0.335 (0.386) data 0.000 (0.024) loss 1.7152 (1.8596) lr 1.0000e-02 eta 0:18:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [46/96] time 0.331 (0.384) data 0.000 (0.023) loss 2.2777 (1.8774) lr 1.0000e-02 eta 0:18:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [48/96] time 0.321 (0.381) data 0.000 (0.022) loss 2.3017 (1.8759) lr 1.0000e-02 eta 0:17:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [50/96] time 0.322 (0.379) data 0.000 (0.021) loss 1.0198 (1.8640) lr 1.0000e-02 eta 0:17:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [52/96] time 0.323 (0.377) data 0.000 (0.020) loss 1.4075 (1.8618) lr 1.0000e-02 eta 0:17:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [54/96] time 0.323 (0.375) data 0.000 (0.019) loss 1.3077 (1.8430) lr 1.0000e-02 eta 0:17:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [56/96] time 0.318 (0.373) data 0.000 (0.019) loss 1.7656 (1.8333) lr 1.0000e-02 eta 0:17:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [58/96] time 0.335 (0.372) data 0.000 (0.018) loss 1.3524 (1.8144) lr 1.0000e-02 eta 0:17:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [60/96] time 0.336 (0.370) data 0.000 (0.017) loss 2.0997 (1.8234) lr 1.0000e-02 eta 0:17:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [62/96] time 0.334 (0.369) data 0.000 (0.017) loss 2.7515 (1.8380) lr 1.0000e-02 eta 0:17:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [64/96] time 0.321 (0.368) data 0.000 (0.016) loss 1.5764 (1.8219) lr 1.0000e-02 eta 0:17:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [66/96] time 0.315 (0.366) data 0.000 (0.016) loss 1.2741 (1.8081) lr 1.0000e-02 eta 0:17:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [68/96] time 0.327 (0.365) data 0.000 (0.015) loss 1.6492 (1.8282) lr 1.0000e-02 eta 0:17:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [70/96] time 0.325 (0.364) data 0.000 (0.015) loss 2.1100 (1.8240) lr 1.0000e-02 eta 0:17:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [72/96] time 0.437 (0.364) data 0.001 (0.015) loss 1.4006 (1.8196) lr 1.0000e-02 eta 0:17:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [74/96] time 0.310 (0.363) data 0.000 (0.014) loss 1.9610 (1.8187) lr 1.0000e-02 eta 0:16:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [76/96] time 0.305 (0.361) data 0.000 (0.014) loss 1.8992 (1.8148) lr 1.0000e-02 eta 0:16:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [78/96] time 0.306 (0.360) data 0.000 (0.013) loss 1.9387 (1.8201) lr 1.0000e-02 eta 0:16:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [80/96] time 0.304 (0.358) data 0.000 (0.013) loss 2.2580 (1.8287) lr 1.0000e-02 eta 0:16:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [82/96] time 0.306 (0.357) data 0.000 (0.013) loss 2.5175 (1.8258) lr 1.0000e-02 eta 0:16:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [84/96] time 0.310 (0.356) data 0.000 (0.012) loss 2.4707 (1.8391) lr 1.0000e-02 eta 0:16:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [86/96] time 0.305 (0.355) data 0.000 (0.012) loss 1.1845 (1.8282) lr 1.0000e-02 eta 0:16:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [88/96] time 0.310 (0.354) data 0.000 (0.012) loss 1.7368 (1.8200) lr 1.0000e-02 eta 0:16:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [90/96] time 0.327 (0.353) data 0.000 (0.012) loss 0.9463 (1.8073) lr 1.0000e-02 eta 0:16:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [92/96] time 0.316 (0.352) data 0.000 (0.011) loss 2.4438 (1.8130) lr 1.0000e-02 eta 0:16:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [94/96] time 0.303 (0.351) data 0.000 (0.011) loss 1.9658 (1.8137) lr 1.0000e-02 eta 0:16:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [1/30] batch [96/96] time 0.310 (0.350) data 0.000 (0.011) loss 2.7991 (1.8175) lr 9.9726e-03 eta 0:16:15
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:03<00:06,  3.23s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.50s/it]100%|██████████| 3/3 [00:03<00:00,  1.06it/s]100%|██████████| 3/3 [00:03<00:00,  1.30s/it]=> result
* total: 576
* correct: 342
* accuracy: 59.4%
* error: 40.6%
* macro_f1: 56.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [2/96] time 0.338 (0.652) data 0.000 (0.281) loss 1.2475 (1.4940) lr 9.9726e-03 eta 0:30:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [4/96] time 0.343 (0.493) data 0.000 (0.141) loss 1.6668 (1.6495) lr 9.9726e-03 eta 0:22:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [6/96] time 0.346 (0.444) data 0.000 (0.094) loss 2.5965 (1.8342) lr 9.9726e-03 eta 0:20:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [8/96] time 0.335 (0.424) data 0.000 (0.070) loss 2.3739 (1.7868) lr 9.9726e-03 eta 0:19:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [10/96] time 0.322 (0.403) data 0.000 (0.056) loss 1.6432 (1.8660) lr 9.9726e-03 eta 0:18:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [12/96] time 0.315 (0.389) data 0.000 (0.047) loss 1.2040 (1.8572) lr 9.9726e-03 eta 0:17:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [14/96] time 0.332 (0.381) data 0.000 (0.040) loss 1.2776 (1.8161) lr 9.9726e-03 eta 0:17:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [16/96] time 0.327 (0.374) data 0.000 (0.035) loss 2.0117 (1.8011) lr 9.9726e-03 eta 0:17:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [18/96] time 0.327 (0.370) data 0.000 (0.031) loss 1.0406 (1.7466) lr 9.9726e-03 eta 0:17:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [20/96] time 0.338 (0.367) data 0.000 (0.028) loss 2.1909 (1.7376) lr 9.9726e-03 eta 0:16:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [22/96] time 0.327 (0.363) data 0.000 (0.026) loss 2.0495 (1.7620) lr 9.9726e-03 eta 0:16:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [24/96] time 0.324 (0.360) data 0.000 (0.024) loss 1.8691 (1.7713) lr 9.9726e-03 eta 0:16:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [26/96] time 0.314 (0.357) data 0.000 (0.022) loss 1.3043 (1.7423) lr 9.9726e-03 eta 0:16:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [28/96] time 0.330 (0.354) data 0.000 (0.020) loss 1.8015 (1.7614) lr 9.9726e-03 eta 0:16:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [30/96] time 0.324 (0.352) data 0.000 (0.019) loss 1.2481 (1.7087) lr 9.9726e-03 eta 0:16:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [32/96] time 0.321 (0.350) data 0.000 (0.018) loss 1.6928 (1.7047) lr 9.9726e-03 eta 0:16:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [34/96] time 0.313 (0.348) data 0.000 (0.017) loss 1.4809 (1.6971) lr 9.9726e-03 eta 0:15:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [36/96] time 0.328 (0.347) data 0.000 (0.016) loss 1.4167 (1.7089) lr 9.9726e-03 eta 0:15:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [38/96] time 0.322 (0.346) data 0.000 (0.015) loss 1.6611 (1.7065) lr 9.9726e-03 eta 0:15:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [40/96] time 0.324 (0.345) data 0.000 (0.014) loss 2.4087 (1.7298) lr 9.9726e-03 eta 0:15:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [42/96] time 0.349 (0.345) data 0.000 (0.014) loss 2.2006 (1.7289) lr 9.9726e-03 eta 0:15:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [44/96] time 0.333 (0.344) data 0.000 (0.013) loss 1.9366 (1.7400) lr 9.9726e-03 eta 0:15:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [46/96] time 0.325 (0.343) data 0.000 (0.013) loss 1.3038 (1.7261) lr 9.9726e-03 eta 0:15:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [48/96] time 0.337 (0.343) data 0.000 (0.012) loss 0.6980 (1.6920) lr 9.9726e-03 eta 0:15:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [50/96] time 0.319 (0.342) data 0.000 (0.012) loss 1.6148 (1.6857) lr 9.9726e-03 eta 0:15:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [52/96] time 0.329 (0.342) data 0.000 (0.011) loss 1.6052 (1.7039) lr 9.9726e-03 eta 0:15:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [54/96] time 0.328 (0.341) data 0.000 (0.011) loss 2.0241 (1.7081) lr 9.9726e-03 eta 0:15:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [56/96] time 0.333 (0.341) data 0.000 (0.010) loss 1.8678 (1.7049) lr 9.9726e-03 eta 0:15:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [58/96] time 0.337 (0.340) data 0.000 (0.010) loss 1.8660 (1.7006) lr 9.9726e-03 eta 0:15:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [60/96] time 0.338 (0.340) data 0.000 (0.010) loss 0.8493 (1.6856) lr 9.9726e-03 eta 0:15:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [62/96] time 0.318 (0.340) data 0.000 (0.009) loss 1.5795 (1.6790) lr 9.9726e-03 eta 0:15:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [64/96] time 0.332 (0.340) data 0.000 (0.009) loss 1.3644 (1.6700) lr 9.9726e-03 eta 0:15:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [66/96] time 0.336 (0.339) data 0.000 (0.009) loss 2.3504 (1.6754) lr 9.9726e-03 eta 0:15:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [68/96] time 0.335 (0.339) data 0.000 (0.009) loss 2.5104 (1.6815) lr 9.9726e-03 eta 0:15:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [70/96] time 0.342 (0.339) data 0.000 (0.008) loss 1.6137 (1.6789) lr 9.9726e-03 eta 0:15:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [72/96] time 0.320 (0.339) data 0.000 (0.008) loss 1.1121 (1.6687) lr 9.9726e-03 eta 0:15:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [74/96] time 0.309 (0.338) data 0.000 (0.008) loss 0.8330 (1.6452) lr 9.9726e-03 eta 0:15:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [76/96] time 0.311 (0.337) data 0.000 (0.008) loss 2.3247 (1.6504) lr 9.9726e-03 eta 0:15:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [78/96] time 0.303 (0.337) data 0.000 (0.008) loss 1.4277 (1.6413) lr 9.9726e-03 eta 0:15:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [80/96] time 0.306 (0.336) data 0.000 (0.007) loss 1.8001 (1.6328) lr 9.9726e-03 eta 0:15:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [82/96] time 0.306 (0.335) data 0.000 (0.007) loss 1.9463 (1.6332) lr 9.9726e-03 eta 0:15:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [84/96] time 0.305 (0.334) data 0.000 (0.007) loss 2.2119 (1.6309) lr 9.9726e-03 eta 0:15:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [86/96] time 0.307 (0.334) data 0.000 (0.007) loss 1.6996 (1.6362) lr 9.9726e-03 eta 0:15:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [88/96] time 0.306 (0.333) data 0.000 (0.007) loss 1.6040 (1.6385) lr 9.9726e-03 eta 0:14:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [90/96] time 0.309 (0.333) data 0.000 (0.007) loss 2.4432 (1.6576) lr 9.9726e-03 eta 0:14:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [92/96] time 0.309 (0.332) data 0.000 (0.006) loss 1.8259 (1.6595) lr 9.9726e-03 eta 0:14:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [94/96] time 0.307 (0.332) data 0.000 (0.006) loss 1.4555 (1.6529) lr 9.9726e-03 eta 0:14:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [2/30] batch [96/96] time 0.310 (0.331) data 0.000 (0.006) loss 1.5661 (1.6490) lr 9.8907e-03 eta 0:14:49
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.84s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.34s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 359
* accuracy: 62.3%
* error: 37.7%
* macro_f1: 59.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [2/96] time 0.341 (0.666) data 0.000 (0.281) loss 1.3126 (1.4250) lr 9.8907e-03 eta 0:29:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [4/96] time 0.354 (0.533) data 0.000 (0.141) loss 1.4295 (1.6403) lr 9.8907e-03 eta 0:23:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [6/96] time 0.336 (0.469) data 0.001 (0.094) loss 1.6059 (1.7200) lr 9.8907e-03 eta 0:20:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [8/96] time 0.334 (0.437) data 0.000 (0.071) loss 0.9162 (1.6061) lr 9.8907e-03 eta 0:19:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [10/96] time 0.332 (0.416) data 0.000 (0.057) loss 1.2787 (1.5600) lr 9.8907e-03 eta 0:18:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [12/96] time 0.335 (0.403) data 0.000 (0.047) loss 1.6096 (1.5855) lr 9.8907e-03 eta 0:17:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [14/96] time 0.335 (0.393) data 0.000 (0.041) loss 1.5559 (1.5969) lr 9.8907e-03 eta 0:17:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [16/96] time 0.344 (0.386) data 0.000 (0.036) loss 2.1295 (1.6576) lr 9.8907e-03 eta 0:17:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [18/96] time 0.340 (0.380) data 0.000 (0.032) loss 2.4267 (1.7176) lr 9.8907e-03 eta 0:16:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [20/96] time 0.337 (0.376) data 0.000 (0.029) loss 1.4294 (1.6981) lr 9.8907e-03 eta 0:16:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [22/96] time 0.339 (0.372) data 0.000 (0.026) loss 2.1964 (1.6859) lr 9.8907e-03 eta 0:16:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [24/96] time 0.340 (0.370) data 0.001 (0.024) loss 1.5394 (1.6679) lr 9.8907e-03 eta 0:16:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [26/96] time 0.324 (0.367) data 0.000 (0.022) loss 1.3693 (1.6365) lr 9.8907e-03 eta 0:16:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [28/96] time 0.332 (0.364) data 0.000 (0.020) loss 1.7743 (1.6255) lr 9.8907e-03 eta 0:16:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [30/96] time 0.330 (0.362) data 0.000 (0.019) loss 1.8381 (1.6231) lr 9.8907e-03 eta 0:16:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [32/96] time 0.329 (0.359) data 0.000 (0.018) loss 1.5080 (1.6145) lr 9.8907e-03 eta 0:15:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [34/96] time 0.322 (0.358) data 0.000 (0.017) loss 1.8710 (1.6168) lr 9.8907e-03 eta 0:15:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [36/96] time 0.335 (0.357) data 0.001 (0.016) loss 2.0147 (1.6201) lr 9.8907e-03 eta 0:15:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [38/96] time 0.325 (0.355) data 0.000 (0.015) loss 2.2369 (1.6569) lr 9.8907e-03 eta 0:15:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [40/96] time 0.328 (0.354) data 0.001 (0.014) loss 2.0670 (1.6674) lr 9.8907e-03 eta 0:15:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [42/96] time 0.322 (0.353) data 0.000 (0.014) loss 1.7272 (1.6469) lr 9.8907e-03 eta 0:15:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [44/96] time 0.323 (0.351) data 0.000 (0.013) loss 1.1341 (1.6362) lr 9.8907e-03 eta 0:15:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [46/96] time 0.327 (0.350) data 0.001 (0.013) loss 1.7796 (1.6390) lr 9.8907e-03 eta 0:15:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [48/96] time 0.327 (0.349) data 0.000 (0.012) loss 1.7525 (1.6357) lr 9.8907e-03 eta 0:15:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [50/96] time 0.328 (0.349) data 0.000 (0.012) loss 1.5693 (1.6408) lr 9.8907e-03 eta 0:15:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [52/96] time 0.323 (0.348) data 0.000 (0.011) loss 1.9860 (1.6384) lr 9.8907e-03 eta 0:15:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [54/96] time 0.324 (0.347) data 0.000 (0.011) loss 1.5528 (1.6537) lr 9.8907e-03 eta 0:15:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [56/96] time 0.321 (0.346) data 0.001 (0.010) loss 0.7492 (1.6294) lr 9.8907e-03 eta 0:15:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [58/96] time 0.335 (0.346) data 0.000 (0.010) loss 1.9264 (1.6210) lr 9.8907e-03 eta 0:15:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [60/96] time 0.346 (0.346) data 0.000 (0.010) loss 1.7898 (1.6070) lr 9.8907e-03 eta 0:15:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [62/96] time 0.343 (0.346) data 0.000 (0.009) loss 1.4891 (1.6124) lr 9.8907e-03 eta 0:15:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [64/96] time 0.330 (0.346) data 0.000 (0.009) loss 0.7513 (1.5967) lr 9.8907e-03 eta 0:15:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [66/96] time 0.335 (0.345) data 0.000 (0.009) loss 1.6854 (1.5993) lr 9.8907e-03 eta 0:15:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [68/96] time 0.339 (0.345) data 0.000 (0.009) loss 1.7340 (1.6136) lr 9.8907e-03 eta 0:15:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [70/96] time 0.319 (0.344) data 0.000 (0.008) loss 2.0215 (1.6270) lr 9.8907e-03 eta 0:15:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [72/96] time 0.339 (0.344) data 0.000 (0.008) loss 1.1188 (1.6134) lr 9.8907e-03 eta 0:14:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [74/96] time 0.307 (0.343) data 0.000 (0.008) loss 1.6471 (1.6232) lr 9.8907e-03 eta 0:14:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [76/96] time 0.309 (0.342) data 0.000 (0.008) loss 1.3630 (1.6176) lr 9.8907e-03 eta 0:14:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [78/96] time 0.310 (0.341) data 0.000 (0.008) loss 1.9732 (1.6248) lr 9.8907e-03 eta 0:14:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [80/96] time 0.311 (0.341) data 0.000 (0.007) loss 1.2494 (1.6146) lr 9.8907e-03 eta 0:14:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [82/96] time 0.311 (0.340) data 0.000 (0.007) loss 2.1435 (1.6193) lr 9.8907e-03 eta 0:14:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [84/96] time 0.311 (0.339) data 0.000 (0.007) loss 1.7723 (1.6170) lr 9.8907e-03 eta 0:14:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [86/96] time 0.310 (0.338) data 0.000 (0.007) loss 2.9208 (1.6272) lr 9.8907e-03 eta 0:14:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [88/96] time 0.311 (0.338) data 0.000 (0.007) loss 1.1507 (1.6106) lr 9.8907e-03 eta 0:14:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [90/96] time 0.316 (0.337) data 0.000 (0.007) loss 1.8604 (1.6095) lr 9.8907e-03 eta 0:14:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [92/96] time 0.319 (0.337) data 0.000 (0.006) loss 1.6007 (1.6136) lr 9.8907e-03 eta 0:14:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [94/96] time 0.311 (0.336) data 0.000 (0.006) loss 2.0844 (1.6252) lr 9.8907e-03 eta 0:14:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [3/30] batch [96/96] time 0.313 (0.336) data 0.000 (0.006) loss 2.3902 (1.6386) lr 9.7553e-03 eta 0:14:30
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.89s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 373
* accuracy: 64.8%
* error: 35.2%
* macro_f1: 60.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [2/96] time 0.330 (0.657) data 0.001 (0.285) loss 1.2536 (1.7782) lr 9.7553e-03 eta 0:28:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [4/96] time 0.337 (0.494) data 0.000 (0.143) loss 1.1066 (1.5865) lr 9.7553e-03 eta 0:21:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [6/96] time 0.336 (0.442) data 0.000 (0.095) loss 1.7454 (1.6897) lr 9.7553e-03 eta 0:19:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [8/96] time 0.326 (0.414) data 0.001 (0.072) loss 1.8734 (1.7051) lr 9.7553e-03 eta 0:17:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [10/96] time 0.329 (0.398) data 0.000 (0.057) loss 3.0725 (1.7798) lr 9.7553e-03 eta 0:17:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [12/96] time 0.334 (0.389) data 0.000 (0.048) loss 1.6527 (1.7295) lr 9.7553e-03 eta 0:16:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [14/96] time 0.325 (0.382) data 0.001 (0.041) loss 0.8443 (1.6492) lr 9.7553e-03 eta 0:16:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [16/96] time 0.340 (0.377) data 0.000 (0.036) loss 1.7338 (1.6566) lr 9.7553e-03 eta 0:16:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [18/96] time 0.320 (0.371) data 0.000 (0.032) loss 2.2934 (1.6549) lr 9.7553e-03 eta 0:15:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [20/96] time 0.340 (0.367) data 0.000 (0.029) loss 1.6292 (1.6463) lr 9.7553e-03 eta 0:15:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [22/96] time 0.330 (0.364) data 0.000 (0.026) loss 2.3628 (1.6648) lr 9.7553e-03 eta 0:15:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [24/96] time 0.325 (0.361) data 0.000 (0.024) loss 1.7078 (1.6357) lr 9.7553e-03 eta 0:15:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [26/96] time 0.328 (0.358) data 0.000 (0.022) loss 1.2667 (1.6675) lr 9.7553e-03 eta 0:15:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [28/96] time 0.328 (0.360) data 0.000 (0.021) loss 1.1403 (1.6677) lr 9.7553e-03 eta 0:15:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [30/96] time 0.323 (0.358) data 0.001 (0.019) loss 1.1916 (1.6563) lr 9.7553e-03 eta 0:15:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [32/96] time 0.331 (0.356) data 0.000 (0.018) loss 1.3229 (1.6395) lr 9.7553e-03 eta 0:15:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [34/96] time 0.327 (0.354) data 0.001 (0.017) loss 1.5480 (1.6268) lr 9.7553e-03 eta 0:15:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [36/96] time 0.332 (0.353) data 0.000 (0.016) loss 1.6353 (1.6317) lr 9.7553e-03 eta 0:15:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [38/96] time 0.331 (0.352) data 0.000 (0.015) loss 0.8238 (1.6087) lr 9.7553e-03 eta 0:14:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [40/96] time 0.329 (0.351) data 0.000 (0.015) loss 1.2017 (1.6044) lr 9.7553e-03 eta 0:14:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [42/96] time 0.323 (0.349) data 0.000 (0.014) loss 1.7433 (1.6079) lr 9.7553e-03 eta 0:14:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [44/96] time 0.339 (0.349) data 0.000 (0.013) loss 1.6840 (1.6001) lr 9.7553e-03 eta 0:14:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [46/96] time 0.352 (0.349) data 0.000 (0.013) loss 1.4011 (1.5900) lr 9.7553e-03 eta 0:14:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [48/96] time 0.346 (0.349) data 0.000 (0.012) loss 1.3578 (1.6011) lr 9.7553e-03 eta 0:14:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [50/96] time 0.338 (0.348) data 0.000 (0.012) loss 1.2756 (1.5976) lr 9.7553e-03 eta 0:14:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [52/96] time 0.346 (0.348) data 0.000 (0.011) loss 1.7949 (1.6079) lr 9.7553e-03 eta 0:14:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [54/96] time 0.343 (0.348) data 0.000 (0.011) loss 1.2441 (1.6060) lr 9.7553e-03 eta 0:14:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [56/96] time 0.336 (0.348) data 0.000 (0.011) loss 1.3605 (1.5897) lr 9.7553e-03 eta 0:14:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [58/96] time 0.350 (0.348) data 0.001 (0.010) loss 2.1870 (1.6078) lr 9.7553e-03 eta 0:14:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [60/96] time 0.350 (0.348) data 0.000 (0.010) loss 1.7205 (1.6117) lr 9.7553e-03 eta 0:14:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [62/96] time 0.337 (0.348) data 0.000 (0.010) loss 1.2020 (1.6076) lr 9.7553e-03 eta 0:14:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [64/96] time 0.338 (0.348) data 0.000 (0.009) loss 1.7726 (1.6045) lr 9.7553e-03 eta 0:14:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [66/96] time 0.341 (0.347) data 0.000 (0.009) loss 0.7564 (1.5886) lr 9.7553e-03 eta 0:14:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [68/96] time 0.353 (0.348) data 0.000 (0.009) loss 0.8375 (1.5748) lr 9.7553e-03 eta 0:14:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [70/96] time 0.352 (0.348) data 0.000 (0.008) loss 2.3349 (1.5881) lr 9.7553e-03 eta 0:14:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [72/96] time 0.360 (0.348) data 0.000 (0.008) loss 2.3166 (1.5951) lr 9.7553e-03 eta 0:14:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [74/96] time 0.325 (0.347) data 0.000 (0.008) loss 0.9743 (1.5910) lr 9.7553e-03 eta 0:14:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [76/96] time 0.324 (0.346) data 0.000 (0.008) loss 1.7440 (1.6053) lr 9.7553e-03 eta 0:14:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [78/96] time 0.323 (0.346) data 0.000 (0.008) loss 2.0300 (1.6071) lr 9.7553e-03 eta 0:14:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [80/96] time 0.325 (0.345) data 0.000 (0.007) loss 1.6364 (1.6078) lr 9.7553e-03 eta 0:14:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [82/96] time 0.323 (0.345) data 0.000 (0.007) loss 1.4106 (1.6115) lr 9.7553e-03 eta 0:14:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [84/96] time 0.324 (0.344) data 0.000 (0.007) loss 1.7080 (1.6063) lr 9.7553e-03 eta 0:14:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [86/96] time 0.325 (0.344) data 0.000 (0.007) loss 0.8454 (1.5937) lr 9.7553e-03 eta 0:14:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [88/96] time 0.324 (0.343) data 0.000 (0.007) loss 2.0627 (1.5920) lr 9.7553e-03 eta 0:14:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [90/96] time 0.326 (0.343) data 0.000 (0.007) loss 1.1954 (1.5926) lr 9.7553e-03 eta 0:14:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [92/96] time 0.333 (0.343) data 0.000 (0.007) loss 1.2810 (1.5917) lr 9.7553e-03 eta 0:14:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [94/96] time 0.323 (0.342) data 0.000 (0.006) loss 1.4884 (1.5849) lr 9.7553e-03 eta 0:14:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [4/30] batch [96/96] time 0.323 (0.342) data 0.000 (0.006) loss 1.4439 (1.5871) lr 9.5677e-03 eta 0:14:13
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.85s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 366
* accuracy: 63.5%
* error: 36.5%
* macro_f1: 60.6%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [2/96] time 0.348 (0.661) data 0.000 (0.257) loss 1.0779 (1.1808) lr 9.5677e-03 eta 0:27:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [4/96] time 0.347 (0.501) data 0.001 (0.129) loss 1.0665 (1.4318) lr 9.5677e-03 eta 0:20:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [6/96] time 0.343 (0.449) data 0.000 (0.086) loss 1.4154 (1.3319) lr 9.5677e-03 eta 0:18:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [8/96] time 0.352 (0.424) data 0.000 (0.065) loss 2.0682 (1.4061) lr 9.5677e-03 eta 0:17:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [10/96] time 0.330 (0.406) data 0.000 (0.052) loss 1.4996 (1.3357) lr 9.5677e-03 eta 0:16:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [12/96] time 0.336 (0.394) data 0.000 (0.043) loss 0.8132 (1.2885) lr 9.5677e-03 eta 0:16:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [14/96] time 0.338 (0.385) data 0.000 (0.037) loss 0.9970 (1.2888) lr 9.5677e-03 eta 0:15:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [16/96] time 0.325 (0.377) data 0.000 (0.032) loss 0.9527 (1.2964) lr 9.5677e-03 eta 0:15:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [18/96] time 0.319 (0.371) data 0.000 (0.029) loss 1.7300 (1.3556) lr 9.5677e-03 eta 0:15:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [20/96] time 0.332 (0.367) data 0.001 (0.026) loss 0.9962 (1.3550) lr 9.5677e-03 eta 0:15:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [22/96] time 0.333 (0.364) data 0.000 (0.024) loss 1.0733 (1.3786) lr 9.5677e-03 eta 0:15:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [24/96] time 0.331 (0.362) data 0.000 (0.022) loss 1.3660 (1.3872) lr 9.5677e-03 eta 0:14:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [26/96] time 0.345 (0.361) data 0.001 (0.020) loss 1.6923 (1.3986) lr 9.5677e-03 eta 0:14:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [28/96] time 0.327 (0.359) data 0.001 (0.019) loss 2.1366 (1.4327) lr 9.5677e-03 eta 0:14:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [30/96] time 0.322 (0.357) data 0.000 (0.018) loss 0.9451 (1.4552) lr 9.5677e-03 eta 0:14:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [32/96] time 0.327 (0.354) data 0.000 (0.016) loss 1.2694 (1.4559) lr 9.5677e-03 eta 0:14:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [34/96] time 0.317 (0.356) data 0.000 (0.015) loss 1.3442 (1.4697) lr 9.5677e-03 eta 0:14:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [36/96] time 0.347 (0.355) data 0.000 (0.015) loss 1.5961 (1.4748) lr 9.5677e-03 eta 0:14:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [38/96] time 0.326 (0.353) data 0.000 (0.014) loss 1.3873 (1.4905) lr 9.5677e-03 eta 0:14:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [40/96] time 0.329 (0.352) data 0.000 (0.013) loss 1.6424 (1.5051) lr 9.5677e-03 eta 0:14:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [42/96] time 0.324 (0.350) data 0.000 (0.013) loss 1.8082 (1.5119) lr 9.5677e-03 eta 0:14:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [44/96] time 0.325 (0.349) data 0.000 (0.012) loss 1.1090 (1.4976) lr 9.5677e-03 eta 0:14:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [46/96] time 0.323 (0.349) data 0.001 (0.012) loss 1.5706 (1.5159) lr 9.5677e-03 eta 0:14:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [48/96] time 0.336 (0.348) data 0.000 (0.011) loss 1.8278 (1.5141) lr 9.5677e-03 eta 0:14:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [50/96] time 0.324 (0.347) data 0.000 (0.011) loss 2.0102 (1.5363) lr 9.5677e-03 eta 0:14:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [52/96] time 0.314 (0.346) data 0.000 (0.010) loss 1.6031 (1.5329) lr 9.5677e-03 eta 0:14:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [54/96] time 0.335 (0.345) data 0.000 (0.010) loss 2.0660 (1.5355) lr 9.5677e-03 eta 0:14:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [56/96] time 0.325 (0.345) data 0.000 (0.010) loss 1.3726 (1.5328) lr 9.5677e-03 eta 0:14:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [58/96] time 0.328 (0.344) data 0.000 (0.009) loss 0.8105 (1.5302) lr 9.5677e-03 eta 0:13:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [60/96] time 0.322 (0.344) data 0.000 (0.009) loss 1.5146 (1.5430) lr 9.5677e-03 eta 0:13:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [62/96] time 0.330 (0.343) data 0.000 (0.009) loss 1.5029 (1.5388) lr 9.5677e-03 eta 0:13:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [64/96] time 0.328 (0.343) data 0.000 (0.008) loss 1.3864 (1.5414) lr 9.5677e-03 eta 0:13:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [66/96] time 0.324 (0.342) data 0.000 (0.008) loss 1.4493 (1.5413) lr 9.5677e-03 eta 0:13:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [68/96] time 0.327 (0.342) data 0.000 (0.008) loss 1.5564 (1.5529) lr 9.5677e-03 eta 0:13:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [70/96] time 0.338 (0.341) data 0.000 (0.008) loss 1.1677 (1.5486) lr 9.5677e-03 eta 0:13:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [72/96] time 0.348 (0.341) data 0.001 (0.008) loss 1.2243 (1.5458) lr 9.5677e-03 eta 0:13:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [74/96] time 0.325 (0.341) data 0.000 (0.007) loss 2.3946 (1.5589) lr 9.5677e-03 eta 0:13:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [76/96] time 0.319 (0.340) data 0.000 (0.007) loss 1.8709 (1.5520) lr 9.5677e-03 eta 0:13:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [78/96] time 0.321 (0.340) data 0.000 (0.007) loss 1.8327 (1.5527) lr 9.5677e-03 eta 0:13:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [80/96] time 0.321 (0.339) data 0.000 (0.007) loss 1.4450 (1.5471) lr 9.5677e-03 eta 0:13:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [82/96] time 0.318 (0.339) data 0.000 (0.007) loss 1.0927 (1.5339) lr 9.5677e-03 eta 0:13:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [84/96] time 0.319 (0.338) data 0.000 (0.006) loss 1.4650 (1.5324) lr 9.5677e-03 eta 0:13:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [86/96] time 0.321 (0.338) data 0.000 (0.006) loss 1.3709 (1.5299) lr 9.5677e-03 eta 0:13:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [88/96] time 0.322 (0.337) data 0.000 (0.006) loss 2.1203 (1.5327) lr 9.5677e-03 eta 0:13:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [90/96] time 0.319 (0.337) data 0.000 (0.006) loss 0.9165 (1.5257) lr 9.5677e-03 eta 0:13:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [92/96] time 0.319 (0.337) data 0.000 (0.006) loss 0.6690 (1.5123) lr 9.5677e-03 eta 0:13:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [94/96] time 0.321 (0.336) data 0.000 (0.006) loss 1.8328 (1.5221) lr 9.5677e-03 eta 0:13:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [5/30] batch [96/96] time 0.325 (0.336) data 0.000 (0.006) loss 2.0760 (1.5328) lr 9.3301e-03 eta 0:13:26
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.86s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.19s/it]=> result
* total: 576
* correct: 378
* accuracy: 65.6%
* error: 34.4%
* macro_f1: 63.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [2/96] time 0.316 (0.643) data 0.000 (0.282) loss 1.6070 (1.2263) lr 9.3301e-03 eta 0:25:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [4/96] time 0.331 (0.485) data 0.000 (0.141) loss 2.3792 (1.4970) lr 9.3301e-03 eta 0:19:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [6/96] time 0.337 (0.437) data 0.000 (0.094) loss 1.2807 (1.4420) lr 9.3301e-03 eta 0:17:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [8/96] time 0.327 (0.411) data 0.000 (0.071) loss 0.7048 (1.2821) lr 9.3301e-03 eta 0:16:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [10/96] time 0.319 (0.393) data 0.000 (0.057) loss 0.9637 (1.3625) lr 9.3301e-03 eta 0:15:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [12/96] time 0.331 (0.382) data 0.000 (0.047) loss 2.3963 (1.4218) lr 9.3301e-03 eta 0:15:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [14/96] time 0.335 (0.376) data 0.000 (0.041) loss 1.7441 (1.4767) lr 9.3301e-03 eta 0:14:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [16/96] time 0.345 (0.371) data 0.000 (0.036) loss 1.8733 (1.5141) lr 9.3301e-03 eta 0:14:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [18/96] time 0.321 (0.366) data 0.000 (0.032) loss 1.6012 (1.5200) lr 9.3301e-03 eta 0:14:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [20/96] time 0.318 (0.362) data 0.000 (0.029) loss 1.4391 (1.5406) lr 9.3301e-03 eta 0:14:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [22/96] time 0.329 (0.359) data 0.000 (0.026) loss 1.1466 (1.5182) lr 9.3301e-03 eta 0:14:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [24/96] time 0.327 (0.357) data 0.000 (0.024) loss 0.6949 (1.4740) lr 9.3301e-03 eta 0:14:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [26/96] time 0.340 (0.356) data 0.001 (0.022) loss 1.7924 (1.4902) lr 9.3301e-03 eta 0:14:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [28/96] time 0.328 (0.354) data 0.000 (0.020) loss 1.8502 (1.5065) lr 9.3301e-03 eta 0:13:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [30/96] time 0.331 (0.352) data 0.000 (0.019) loss 1.8424 (1.5313) lr 9.3301e-03 eta 0:13:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [32/96] time 0.325 (0.351) data 0.000 (0.018) loss 0.9043 (1.5158) lr 9.3301e-03 eta 0:13:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [34/96] time 0.322 (0.350) data 0.000 (0.017) loss 1.1510 (1.5031) lr 9.3301e-03 eta 0:13:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [36/96] time 0.446 (0.352) data 0.000 (0.016) loss 1.8020 (1.5042) lr 9.3301e-03 eta 0:13:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [38/96] time 0.329 (0.351) data 0.000 (0.015) loss 0.6816 (1.4838) lr 9.3301e-03 eta 0:13:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [40/96] time 0.321 (0.349) data 0.000 (0.014) loss 1.8678 (1.4815) lr 9.3301e-03 eta 0:13:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [42/96] time 0.327 (0.348) data 0.000 (0.014) loss 1.9026 (1.4902) lr 9.3301e-03 eta 0:13:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [44/96] time 0.322 (0.347) data 0.000 (0.013) loss 1.0214 (1.4755) lr 9.3301e-03 eta 0:13:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [46/96] time 0.324 (0.346) data 0.000 (0.013) loss 1.9594 (1.4875) lr 9.3301e-03 eta 0:13:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [48/96] time 0.330 (0.346) data 0.000 (0.012) loss 1.4910 (1.4841) lr 9.3301e-03 eta 0:13:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [50/96] time 0.326 (0.345) data 0.000 (0.012) loss 2.2323 (1.4975) lr 9.3301e-03 eta 0:13:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [52/96] time 0.327 (0.344) data 0.000 (0.011) loss 2.0981 (1.5121) lr 9.3301e-03 eta 0:13:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [54/96] time 0.341 (0.344) data 0.000 (0.011) loss 1.6263 (1.5198) lr 9.3301e-03 eta 0:13:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [56/96] time 0.337 (0.344) data 0.000 (0.010) loss 2.1192 (1.5300) lr 9.3301e-03 eta 0:13:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [58/96] time 0.326 (0.343) data 0.000 (0.010) loss 1.6279 (1.5352) lr 9.3301e-03 eta 0:13:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [60/96] time 0.321 (0.342) data 0.000 (0.010) loss 0.9602 (1.5185) lr 9.3301e-03 eta 0:13:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [62/96] time 0.342 (0.342) data 0.000 (0.009) loss 1.2378 (1.5335) lr 9.3301e-03 eta 0:13:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [64/96] time 0.335 (0.342) data 0.000 (0.009) loss 1.3461 (1.5330) lr 9.3301e-03 eta 0:13:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [66/96] time 0.323 (0.341) data 0.000 (0.009) loss 2.7987 (1.5403) lr 9.3301e-03 eta 0:13:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [68/96] time 0.322 (0.341) data 0.000 (0.009) loss 1.9120 (1.5548) lr 9.3301e-03 eta 0:13:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [70/96] time 0.343 (0.341) data 0.000 (0.008) loss 1.9244 (1.5585) lr 9.3301e-03 eta 0:13:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [72/96] time 0.329 (0.340) data 0.000 (0.008) loss 2.3102 (1.5621) lr 9.3301e-03 eta 0:13:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [74/96] time 0.319 (0.340) data 0.000 (0.008) loss 1.3626 (1.5528) lr 9.3301e-03 eta 0:13:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [76/96] time 0.323 (0.339) data 0.000 (0.008) loss 1.5280 (1.5618) lr 9.3301e-03 eta 0:13:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [78/96] time 0.322 (0.339) data 0.000 (0.008) loss 1.1111 (1.5488) lr 9.3301e-03 eta 0:13:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [80/96] time 0.320 (0.338) data 0.000 (0.007) loss 1.5523 (1.5484) lr 9.3301e-03 eta 0:13:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [82/96] time 0.323 (0.338) data 0.000 (0.007) loss 1.5685 (1.5459) lr 9.3301e-03 eta 0:13:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [84/96] time 0.321 (0.338) data 0.000 (0.007) loss 1.8490 (1.5472) lr 9.3301e-03 eta 0:13:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [86/96] time 0.323 (0.337) data 0.000 (0.007) loss 1.8607 (1.5468) lr 9.3301e-03 eta 0:13:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [88/96] time 0.319 (0.337) data 0.000 (0.007) loss 1.0783 (1.5348) lr 9.3301e-03 eta 0:12:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [90/96] time 0.324 (0.336) data 0.000 (0.007) loss 0.9755 (1.5294) lr 9.3301e-03 eta 0:12:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [92/96] time 0.326 (0.336) data 0.000 (0.006) loss 1.6877 (1.5291) lr 9.3301e-03 eta 0:12:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [94/96] time 0.327 (0.336) data 0.000 (0.006) loss 2.0697 (1.5394) lr 9.3301e-03 eta 0:12:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [6/30] batch [96/96] time 0.325 (0.336) data 0.000 (0.006) loss 1.3036 (1.5406) lr 9.0451e-03 eta 0:12:53
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.87s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 373
* accuracy: 64.8%
* error: 35.2%
* macro_f1: 62.4%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [2/96] time 0.321 (0.651) data 0.000 (0.276) loss 0.8310 (1.0282) lr 9.0451e-03 eta 0:24:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [4/96] time 0.333 (0.491) data 0.000 (0.138) loss 2.0216 (1.5488) lr 9.0451e-03 eta 0:18:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [6/96] time 0.325 (0.436) data 0.000 (0.092) loss 1.7320 (1.6079) lr 9.0451e-03 eta 0:16:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [8/96] time 0.335 (0.411) data 0.000 (0.069) loss 1.1799 (1.4592) lr 9.0451e-03 eta 0:15:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [10/96] time 0.328 (0.394) data 0.000 (0.056) loss 1.3122 (1.5002) lr 9.0451e-03 eta 0:15:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [12/96] time 0.319 (0.383) data 0.000 (0.046) loss 1.4170 (1.4716) lr 9.0451e-03 eta 0:14:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [14/96] time 0.323 (0.374) data 0.000 (0.040) loss 2.0960 (1.5454) lr 9.0451e-03 eta 0:14:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [16/96] time 0.334 (0.369) data 0.000 (0.035) loss 1.4769 (1.5351) lr 9.0451e-03 eta 0:14:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [18/96] time 0.333 (0.365) data 0.000 (0.031) loss 1.0376 (1.5388) lr 9.0451e-03 eta 0:13:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [20/96] time 0.320 (0.361) data 0.000 (0.028) loss 0.9440 (1.5151) lr 9.0451e-03 eta 0:13:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [22/96] time 0.328 (0.358) data 0.000 (0.025) loss 1.4734 (1.5018) lr 9.0451e-03 eta 0:13:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [24/96] time 0.326 (0.355) data 0.000 (0.023) loss 1.3671 (1.5045) lr 9.0451e-03 eta 0:13:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [26/96] time 0.334 (0.353) data 0.000 (0.022) loss 0.9466 (1.4976) lr 9.0451e-03 eta 0:13:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [28/96] time 0.326 (0.352) data 0.000 (0.020) loss 0.9284 (1.4692) lr 9.0451e-03 eta 0:13:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [30/96] time 0.324 (0.350) data 0.000 (0.019) loss 2.3301 (1.4881) lr 9.0451e-03 eta 0:13:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [32/96] time 0.328 (0.349) data 0.000 (0.018) loss 1.1293 (1.4783) lr 9.0451e-03 eta 0:13:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [34/96] time 0.330 (0.348) data 0.000 (0.017) loss 1.1192 (1.4519) lr 9.0451e-03 eta 0:13:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [36/96] time 0.433 (0.350) data 0.000 (0.016) loss 1.3050 (1.4339) lr 9.0451e-03 eta 0:13:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [38/96] time 0.328 (0.349) data 0.000 (0.015) loss 1.6623 (1.4303) lr 9.0451e-03 eta 0:13:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [40/96] time 0.334 (0.348) data 0.000 (0.014) loss 1.7534 (1.4498) lr 9.0451e-03 eta 0:13:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [42/96] time 0.332 (0.347) data 0.000 (0.013) loss 1.0867 (1.4259) lr 9.0451e-03 eta 0:13:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [44/96] time 0.332 (0.346) data 0.000 (0.013) loss 1.0026 (1.4058) lr 9.0451e-03 eta 0:13:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [46/96] time 0.335 (0.346) data 0.000 (0.012) loss 1.6429 (1.4158) lr 9.0451e-03 eta 0:13:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [48/96] time 0.323 (0.344) data 0.000 (0.012) loss 1.7358 (1.4349) lr 9.0451e-03 eta 0:12:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [50/96] time 0.336 (0.344) data 0.000 (0.011) loss 1.7183 (1.4378) lr 9.0451e-03 eta 0:12:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [52/96] time 0.319 (0.343) data 0.000 (0.011) loss 1.3292 (1.4416) lr 9.0451e-03 eta 0:12:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [54/96] time 0.326 (0.342) data 0.000 (0.011) loss 1.4471 (1.4467) lr 9.0451e-03 eta 0:12:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [56/96] time 0.319 (0.342) data 0.000 (0.010) loss 1.4811 (1.4598) lr 9.0451e-03 eta 0:12:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [58/96] time 0.324 (0.341) data 0.000 (0.010) loss 1.0602 (1.4453) lr 9.0451e-03 eta 0:12:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [60/96] time 0.333 (0.341) data 0.000 (0.010) loss 0.9985 (1.4399) lr 9.0451e-03 eta 0:12:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [62/96] time 0.324 (0.341) data 0.000 (0.009) loss 2.2782 (1.4539) lr 9.0451e-03 eta 0:12:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [64/96] time 0.342 (0.340) data 0.000 (0.009) loss 1.1564 (1.4490) lr 9.0451e-03 eta 0:12:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [66/96] time 0.328 (0.340) data 0.000 (0.009) loss 1.4213 (1.4444) lr 9.0451e-03 eta 0:12:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [68/96] time 0.332 (0.340) data 0.000 (0.008) loss 1.3304 (1.4423) lr 9.0451e-03 eta 0:12:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [70/96] time 0.340 (0.340) data 0.000 (0.008) loss 2.6015 (1.4572) lr 9.0451e-03 eta 0:12:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [72/96] time 0.323 (0.339) data 0.000 (0.008) loss 1.7831 (1.4588) lr 9.0451e-03 eta 0:12:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [74/96] time 0.315 (0.339) data 0.000 (0.008) loss 1.3351 (1.4567) lr 9.0451e-03 eta 0:12:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [76/96] time 0.314 (0.338) data 0.000 (0.008) loss 0.9666 (1.4466) lr 9.0451e-03 eta 0:12:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [78/96] time 0.314 (0.337) data 0.000 (0.007) loss 1.2719 (1.4411) lr 9.0451e-03 eta 0:12:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [80/96] time 0.311 (0.337) data 0.000 (0.007) loss 1.7162 (1.4438) lr 9.0451e-03 eta 0:12:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [82/96] time 0.313 (0.336) data 0.000 (0.007) loss 2.7378 (1.4662) lr 9.0451e-03 eta 0:12:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [84/96] time 0.308 (0.335) data 0.000 (0.007) loss 1.5249 (1.4730) lr 9.0451e-03 eta 0:12:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [86/96] time 0.310 (0.335) data 0.000 (0.007) loss 1.6694 (1.4750) lr 9.0451e-03 eta 0:12:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [88/96] time 0.311 (0.334) data 0.000 (0.007) loss 1.2349 (1.4753) lr 9.0451e-03 eta 0:12:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [90/96] time 0.313 (0.334) data 0.000 (0.006) loss 1.1161 (1.4811) lr 9.0451e-03 eta 0:12:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [92/96] time 0.312 (0.333) data 0.000 (0.006) loss 1.1552 (1.4763) lr 9.0451e-03 eta 0:12:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [94/96] time 0.312 (0.333) data 0.000 (0.006) loss 1.7905 (1.4755) lr 9.0451e-03 eta 0:12:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [7/30] batch [96/96] time 0.314 (0.332) data 0.000 (0.006) loss 1.6539 (1.4809) lr 8.7157e-03 eta 0:12:14
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.95s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.38s/it]100%|██████████| 3/3 [00:03<00:00,  1.14it/s]100%|██████████| 3/3 [00:03<00:00,  1.21s/it]=> result
* total: 576
* correct: 379
* accuracy: 65.8%
* error: 34.2%
* macro_f1: 64.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [2/96] time 0.330 (0.655) data 0.000 (0.270) loss 2.1874 (1.8594) lr 8.7157e-03 eta 0:24:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [4/96] time 0.321 (0.486) data 0.000 (0.135) loss 1.5022 (1.5836) lr 8.7157e-03 eta 0:17:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [6/96] time 0.315 (0.432) data 0.000 (0.090) loss 1.3138 (1.5375) lr 8.7157e-03 eta 0:15:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [8/96] time 0.330 (0.406) data 0.000 (0.068) loss 1.1118 (1.4779) lr 8.7157e-03 eta 0:14:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [10/96] time 0.333 (0.392) data 0.001 (0.054) loss 1.6685 (1.4491) lr 8.7157e-03 eta 0:14:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [12/96] time 0.319 (0.382) data 0.000 (0.045) loss 1.2457 (1.4353) lr 8.7157e-03 eta 0:13:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [14/96] time 0.329 (0.375) data 0.000 (0.039) loss 1.8119 (1.4423) lr 8.7157e-03 eta 0:13:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [16/96] time 0.327 (0.369) data 0.000 (0.034) loss 0.8213 (1.4080) lr 8.7157e-03 eta 0:13:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [18/96] time 0.327 (0.364) data 0.000 (0.030) loss 1.7535 (1.4298) lr 8.7157e-03 eta 0:13:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [20/96] time 0.320 (0.360) data 0.000 (0.027) loss 1.7357 (1.4634) lr 8.7157e-03 eta 0:13:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [22/96] time 0.325 (0.357) data 0.000 (0.025) loss 1.5056 (1.4672) lr 8.7157e-03 eta 0:13:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [24/96] time 0.332 (0.355) data 0.000 (0.023) loss 2.1227 (1.5177) lr 8.7157e-03 eta 0:12:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [26/96] time 0.345 (0.353) data 0.000 (0.021) loss 0.9740 (1.4821) lr 8.7157e-03 eta 0:12:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [28/96] time 0.343 (0.352) data 0.000 (0.020) loss 1.3915 (1.4777) lr 8.7157e-03 eta 0:12:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [30/96] time 0.333 (0.351) data 0.000 (0.018) loss 1.7511 (1.5039) lr 8.7157e-03 eta 0:12:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [32/96] time 0.324 (0.350) data 0.000 (0.017) loss 1.0385 (1.4796) lr 8.7157e-03 eta 0:12:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [34/96] time 0.329 (0.348) data 0.000 (0.016) loss 2.1562 (1.5023) lr 8.7157e-03 eta 0:12:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [36/96] time 0.432 (0.350) data 0.000 (0.015) loss 2.2127 (1.5183) lr 8.7157e-03 eta 0:12:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [38/96] time 0.330 (0.349) data 0.000 (0.015) loss 1.2059 (1.5332) lr 8.7157e-03 eta 0:12:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [40/96] time 0.327 (0.348) data 0.000 (0.014) loss 1.3586 (1.5512) lr 8.7157e-03 eta 0:12:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [42/96] time 0.332 (0.347) data 0.000 (0.013) loss 1.2040 (1.5523) lr 8.7157e-03 eta 0:12:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [44/96] time 0.332 (0.346) data 0.000 (0.013) loss 1.4472 (1.5351) lr 8.7157e-03 eta 0:12:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [46/96] time 0.339 (0.346) data 0.001 (0.012) loss 1.2689 (1.5375) lr 8.7157e-03 eta 0:12:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [48/96] time 0.338 (0.345) data 0.000 (0.012) loss 1.5459 (1.5432) lr 8.7157e-03 eta 0:12:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [50/96] time 0.336 (0.345) data 0.000 (0.011) loss 1.0739 (1.5349) lr 8.7157e-03 eta 0:12:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [52/96] time 0.316 (0.344) data 0.000 (0.011) loss 2.0498 (1.5459) lr 8.7157e-03 eta 0:12:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [54/96] time 0.327 (0.344) data 0.000 (0.010) loss 1.0593 (1.5325) lr 8.7157e-03 eta 0:12:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [56/96] time 0.325 (0.343) data 0.000 (0.010) loss 0.7492 (1.5199) lr 8.7157e-03 eta 0:12:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [58/96] time 0.330 (0.343) data 0.001 (0.010) loss 1.6568 (1.5226) lr 8.7157e-03 eta 0:12:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [60/96] time 0.326 (0.342) data 0.000 (0.009) loss 1.0165 (1.5133) lr 8.7157e-03 eta 0:12:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [62/96] time 0.325 (0.342) data 0.000 (0.009) loss 1.1984 (1.4969) lr 8.7157e-03 eta 0:12:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [64/96] time 0.329 (0.342) data 0.001 (0.009) loss 1.7521 (1.4936) lr 8.7157e-03 eta 0:12:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [66/96] time 0.321 (0.341) data 0.001 (0.009) loss 1.4947 (1.4872) lr 8.7157e-03 eta 0:12:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [68/96] time 0.324 (0.341) data 0.000 (0.008) loss 1.5758 (1.4886) lr 8.7157e-03 eta 0:12:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [70/96] time 0.330 (0.341) data 0.001 (0.008) loss 1.4938 (1.4878) lr 8.7157e-03 eta 0:12:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [72/96] time 0.352 (0.341) data 0.000 (0.008) loss 0.9927 (1.4741) lr 8.7157e-03 eta 0:12:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [74/96] time 0.313 (0.341) data 0.000 (0.008) loss 0.9763 (1.4652) lr 8.7157e-03 eta 0:12:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [76/96] time 0.312 (0.340) data 0.000 (0.007) loss 0.8926 (1.4489) lr 8.7157e-03 eta 0:12:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [78/96] time 0.314 (0.339) data 0.000 (0.007) loss 1.0631 (1.4453) lr 8.7157e-03 eta 0:12:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [80/96] time 0.310 (0.338) data 0.000 (0.007) loss 1.0019 (1.4571) lr 8.7157e-03 eta 0:11:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [82/96] time 0.310 (0.338) data 0.000 (0.007) loss 2.3542 (1.4671) lr 8.7157e-03 eta 0:11:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [84/96] time 0.308 (0.337) data 0.000 (0.007) loss 1.9295 (1.4704) lr 8.7157e-03 eta 0:11:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [86/96] time 0.310 (0.336) data 0.000 (0.007) loss 0.9397 (1.4667) lr 8.7157e-03 eta 0:11:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [88/96] time 0.313 (0.336) data 0.000 (0.006) loss 1.3287 (1.4595) lr 8.7157e-03 eta 0:11:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [90/96] time 0.308 (0.335) data 0.000 (0.006) loss 1.0267 (1.4664) lr 8.7157e-03 eta 0:11:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [92/96] time 0.311 (0.335) data 0.000 (0.006) loss 1.1800 (1.4586) lr 8.7157e-03 eta 0:11:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [94/96] time 0.315 (0.334) data 0.000 (0.006) loss 1.6178 (1.4644) lr 8.7157e-03 eta 0:11:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [8/30] batch [96/96] time 0.310 (0.334) data 0.000 (0.006) loss 0.7558 (1.4578) lr 8.3457e-03 eta 0:11:44
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.84s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.34s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 395
* accuracy: 68.6%
* error: 31.4%
* macro_f1: 65.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [2/96] time 0.330 (0.653) data 0.000 (0.281) loss 1.0590 (1.0836) lr 8.3457e-03 eta 0:22:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [4/96] time 0.345 (0.491) data 0.000 (0.141) loss 0.7926 (0.9853) lr 8.3457e-03 eta 0:17:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [6/96] time 0.344 (0.443) data 0.000 (0.094) loss 1.0293 (1.1067) lr 8.3457e-03 eta 0:15:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [8/96] time 0.342 (0.418) data 0.000 (0.071) loss 1.7325 (1.1810) lr 8.3457e-03 eta 0:14:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [10/96] time 0.333 (0.400) data 0.000 (0.057) loss 1.3350 (1.1943) lr 8.3457e-03 eta 0:14:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [12/96] time 0.337 (0.389) data 0.000 (0.047) loss 0.8826 (1.1565) lr 8.3457e-03 eta 0:13:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [14/96] time 0.339 (0.383) data 0.000 (0.040) loss 1.3444 (1.1751) lr 8.3457e-03 eta 0:13:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [16/96] time 0.336 (0.377) data 0.000 (0.035) loss 1.7302 (1.2148) lr 8.3457e-03 eta 0:13:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [18/96] time 0.338 (0.373) data 0.000 (0.032) loss 1.4092 (1.2510) lr 8.3457e-03 eta 0:13:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [20/96] time 0.334 (0.370) data 0.000 (0.028) loss 1.3843 (1.2494) lr 8.3457e-03 eta 0:12:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [22/96] time 0.341 (0.367) data 0.000 (0.026) loss 0.7042 (1.2281) lr 8.3457e-03 eta 0:12:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [24/96] time 0.327 (0.364) data 0.001 (0.024) loss 1.8785 (1.2906) lr 8.3457e-03 eta 0:12:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [26/96] time 0.337 (0.362) data 0.000 (0.022) loss 1.7639 (1.2961) lr 8.3457e-03 eta 0:12:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [28/96] time 0.322 (0.360) data 0.000 (0.020) loss 2.7431 (1.3423) lr 8.3457e-03 eta 0:12:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [30/96] time 0.327 (0.358) data 0.001 (0.019) loss 1.0589 (1.3336) lr 8.3457e-03 eta 0:12:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [32/96] time 0.322 (0.356) data 0.000 (0.018) loss 2.0652 (1.3776) lr 8.3457e-03 eta 0:12:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [34/96] time 0.325 (0.354) data 0.000 (0.017) loss 1.2167 (1.3871) lr 8.3457e-03 eta 0:12:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [36/96] time 0.428 (0.355) data 0.000 (0.016) loss 0.9045 (1.3880) lr 8.3457e-03 eta 0:12:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [38/96] time 0.329 (0.353) data 0.000 (0.015) loss 1.9712 (1.4219) lr 8.3457e-03 eta 0:12:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [40/96] time 0.323 (0.352) data 0.001 (0.014) loss 1.1161 (1.4128) lr 8.3457e-03 eta 0:12:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [42/96] time 0.327 (0.351) data 0.000 (0.014) loss 1.2951 (1.4142) lr 8.3457e-03 eta 0:12:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [44/96] time 0.330 (0.350) data 0.000 (0.013) loss 1.1560 (1.4024) lr 8.3457e-03 eta 0:12:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [46/96] time 0.331 (0.349) data 0.000 (0.013) loss 1.5746 (1.4111) lr 8.3457e-03 eta 0:12:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [48/96] time 0.312 (0.348) data 0.000 (0.012) loss 1.9402 (1.4287) lr 8.3457e-03 eta 0:11:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [50/96] time 0.330 (0.347) data 0.000 (0.012) loss 1.4512 (1.4303) lr 8.3457e-03 eta 0:11:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [52/96] time 0.342 (0.347) data 0.000 (0.011) loss 1.0910 (1.4184) lr 8.3457e-03 eta 0:11:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [54/96] time 0.324 (0.346) data 0.000 (0.011) loss 2.0102 (1.4245) lr 8.3457e-03 eta 0:11:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [56/96] time 0.326 (0.346) data 0.000 (0.010) loss 1.7240 (1.4293) lr 8.3457e-03 eta 0:11:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [58/96] time 0.318 (0.345) data 0.000 (0.010) loss 0.8414 (1.4076) lr 8.3457e-03 eta 0:11:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [60/96] time 0.316 (0.344) data 0.000 (0.010) loss 1.0490 (1.4121) lr 8.3457e-03 eta 0:11:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [62/96] time 0.336 (0.343) data 0.000 (0.009) loss 1.6397 (1.4179) lr 8.3457e-03 eta 0:11:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [64/96] time 0.336 (0.343) data 0.000 (0.009) loss 1.0055 (1.4071) lr 8.3457e-03 eta 0:11:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [66/96] time 0.331 (0.342) data 0.000 (0.009) loss 1.8383 (1.4061) lr 8.3457e-03 eta 0:11:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [68/96] time 0.343 (0.342) data 0.000 (0.009) loss 1.1029 (1.4099) lr 8.3457e-03 eta 0:11:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [70/96] time 0.324 (0.342) data 0.000 (0.008) loss 0.6867 (1.4071) lr 8.3457e-03 eta 0:11:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [72/96] time 0.321 (0.341) data 0.000 (0.008) loss 1.7176 (1.4144) lr 8.3457e-03 eta 0:11:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [74/96] time 0.300 (0.340) data 0.000 (0.008) loss 1.3899 (1.4203) lr 8.3457e-03 eta 0:11:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [76/96] time 0.299 (0.339) data 0.000 (0.008) loss 1.6548 (1.4161) lr 8.3457e-03 eta 0:11:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [78/96] time 0.298 (0.338) data 0.000 (0.008) loss 1.6616 (1.4305) lr 8.3457e-03 eta 0:11:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [80/96] time 0.299 (0.337) data 0.000 (0.007) loss 1.6501 (1.4328) lr 8.3457e-03 eta 0:11:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [82/96] time 0.299 (0.336) data 0.000 (0.007) loss 1.4210 (1.4385) lr 8.3457e-03 eta 0:11:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [84/96] time 0.298 (0.335) data 0.000 (0.007) loss 1.7860 (1.4436) lr 8.3457e-03 eta 0:11:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [86/96] time 0.298 (0.334) data 0.000 (0.007) loss 1.5102 (1.4499) lr 8.3457e-03 eta 0:11:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [88/96] time 0.297 (0.333) data 0.000 (0.007) loss 2.2434 (1.4572) lr 8.3457e-03 eta 0:11:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [90/96] time 0.299 (0.333) data 0.000 (0.007) loss 1.7950 (1.4577) lr 8.3457e-03 eta 0:11:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [92/96] time 0.298 (0.332) data 0.000 (0.006) loss 1.3013 (1.4540) lr 8.3457e-03 eta 0:11:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [94/96] time 0.299 (0.331) data 0.000 (0.006) loss 1.2562 (1.4437) lr 8.3457e-03 eta 0:11:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [9/30] batch [96/96] time 0.299 (0.330) data 0.000 (0.006) loss 1.4996 (1.4389) lr 7.9389e-03 eta 0:11:06
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.88s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 393
* accuracy: 68.2%
* error: 31.8%
* macro_f1: 66.2%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [2/96] time 0.322 (0.642) data 0.000 (0.266) loss 1.5570 (1.3466) lr 7.9389e-03 eta 0:21:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [4/96] time 0.322 (0.482) data 0.001 (0.133) loss 0.9043 (1.3133) lr 7.9389e-03 eta 0:16:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [6/96] time 0.328 (0.432) data 0.000 (0.089) loss 0.6611 (1.2206) lr 7.9389e-03 eta 0:14:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [8/96] time 0.323 (0.406) data 0.000 (0.067) loss 1.6440 (1.2416) lr 7.9389e-03 eta 0:13:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [10/96] time 0.329 (0.389) data 0.001 (0.053) loss 1.3308 (1.2553) lr 7.9389e-03 eta 0:13:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [12/96] time 0.327 (0.379) data 0.000 (0.045) loss 1.2661 (1.2113) lr 7.9389e-03 eta 0:12:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [14/96] time 0.327 (0.372) data 0.000 (0.038) loss 1.6614 (1.2751) lr 7.9389e-03 eta 0:12:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [16/96] time 0.347 (0.368) data 0.000 (0.033) loss 0.8793 (1.2570) lr 7.9389e-03 eta 0:12:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [18/96] time 0.321 (0.363) data 0.000 (0.030) loss 1.1715 (1.2584) lr 7.9389e-03 eta 0:12:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [20/96] time 0.326 (0.359) data 0.000 (0.027) loss 1.9571 (1.2901) lr 7.9389e-03 eta 0:11:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [22/96] time 0.325 (0.356) data 0.000 (0.024) loss 2.1908 (1.3459) lr 7.9389e-03 eta 0:11:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [24/96] time 0.334 (0.354) data 0.000 (0.022) loss 1.2421 (1.3351) lr 7.9389e-03 eta 0:11:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [26/96] time 0.322 (0.352) data 0.000 (0.021) loss 1.2441 (1.3112) lr 7.9389e-03 eta 0:11:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [28/96] time 0.324 (0.350) data 0.000 (0.019) loss 2.1513 (1.3340) lr 7.9389e-03 eta 0:11:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [30/96] time 0.326 (0.349) data 0.000 (0.018) loss 1.2122 (1.3384) lr 7.9389e-03 eta 0:11:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [32/96] time 0.322 (0.347) data 0.000 (0.017) loss 1.8904 (1.3447) lr 7.9389e-03 eta 0:11:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [34/96] time 0.323 (0.346) data 0.000 (0.016) loss 1.2740 (1.3667) lr 7.9389e-03 eta 0:11:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [36/96] time 0.418 (0.348) data 0.001 (0.015) loss 1.0932 (1.3663) lr 7.9389e-03 eta 0:11:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [38/96] time 0.328 (0.346) data 0.000 (0.014) loss 2.4677 (1.3986) lr 7.9389e-03 eta 0:11:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [40/96] time 0.321 (0.345) data 0.000 (0.014) loss 1.2442 (1.3862) lr 7.9389e-03 eta 0:11:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [42/96] time 0.337 (0.345) data 0.000 (0.013) loss 1.5747 (1.4113) lr 7.9389e-03 eta 0:11:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [44/96] time 0.372 (0.346) data 0.001 (0.012) loss 1.4209 (1.4240) lr 7.9389e-03 eta 0:11:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [46/96] time 0.343 (0.346) data 0.000 (0.012) loss 1.9837 (1.4256) lr 7.9389e-03 eta 0:11:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [48/96] time 0.331 (0.346) data 0.000 (0.011) loss 1.4145 (1.4262) lr 7.9389e-03 eta 0:11:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [50/96] time 0.329 (0.346) data 0.000 (0.011) loss 1.3159 (1.4310) lr 7.9389e-03 eta 0:11:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [52/96] time 0.343 (0.345) data 0.000 (0.011) loss 1.6860 (1.4332) lr 7.9389e-03 eta 0:11:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [54/96] time 0.337 (0.345) data 0.000 (0.010) loss 1.1525 (1.4341) lr 7.9389e-03 eta 0:11:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [56/96] time 0.331 (0.345) data 0.001 (0.010) loss 1.2023 (1.4224) lr 7.9389e-03 eta 0:11:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [58/96] time 0.346 (0.345) data 0.001 (0.009) loss 1.9981 (1.4413) lr 7.9389e-03 eta 0:11:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [60/96] time 0.342 (0.344) data 0.000 (0.009) loss 1.1509 (1.4394) lr 7.9389e-03 eta 0:11:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [62/96] time 0.335 (0.344) data 0.000 (0.009) loss 1.5467 (1.4522) lr 7.9389e-03 eta 0:11:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [64/96] time 0.340 (0.344) data 0.000 (0.009) loss 2.1626 (1.4577) lr 7.9389e-03 eta 0:11:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [66/96] time 0.344 (0.344) data 0.000 (0.008) loss 2.0737 (1.4642) lr 7.9389e-03 eta 0:11:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [68/96] time 0.337 (0.344) data 0.001 (0.008) loss 1.4957 (1.4728) lr 7.9389e-03 eta 0:11:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [70/96] time 0.319 (0.343) data 0.000 (0.008) loss 1.4089 (1.4636) lr 7.9389e-03 eta 0:11:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [72/96] time 0.328 (0.343) data 0.000 (0.008) loss 1.3386 (1.4558) lr 7.9389e-03 eta 0:11:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [74/96] time 0.312 (0.342) data 0.000 (0.008) loss 1.6809 (1.4574) lr 7.9389e-03 eta 0:11:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [76/96] time 0.307 (0.341) data 0.000 (0.007) loss 1.1277 (1.4461) lr 7.9389e-03 eta 0:11:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [78/96] time 0.310 (0.340) data 0.000 (0.007) loss 1.4082 (1.4513) lr 7.9389e-03 eta 0:10:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [80/96] time 0.309 (0.339) data 0.000 (0.007) loss 1.4517 (1.4656) lr 7.9389e-03 eta 0:10:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [82/96] time 0.306 (0.339) data 0.000 (0.007) loss 1.2211 (1.4650) lr 7.9389e-03 eta 0:10:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [84/96] time 0.305 (0.338) data 0.000 (0.007) loss 0.8684 (1.4537) lr 7.9389e-03 eta 0:10:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [86/96] time 0.307 (0.337) data 0.000 (0.007) loss 1.5405 (1.4484) lr 7.9389e-03 eta 0:10:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [88/96] time 0.307 (0.336) data 0.000 (0.006) loss 1.2626 (1.4502) lr 7.9389e-03 eta 0:10:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [90/96] time 0.301 (0.336) data 0.000 (0.006) loss 1.0853 (1.4422) lr 7.9389e-03 eta 0:10:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [92/96] time 0.308 (0.335) data 0.000 (0.006) loss 0.9166 (1.4381) lr 7.9389e-03 eta 0:10:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [94/96] time 0.307 (0.334) data 0.000 (0.006) loss 1.2703 (1.4361) lr 7.9389e-03 eta 0:10:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [10/30] batch [96/96] time 0.311 (0.334) data 0.000 (0.006) loss 0.8241 (1.4260) lr 7.5000e-03 eta 0:10:41
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.88s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 393
* accuracy: 68.2%
* error: 31.8%
* macro_f1: 66.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model.pth.tar-10

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [2/96] time 0.324 (0.647) data 0.000 (0.269) loss 2.5643 (1.9086) lr 7.5000e-03 eta 0:20:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [4/96] time 0.334 (0.487) data 0.000 (0.134) loss 1.3087 (1.5531) lr 7.5000e-03 eta 0:15:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [6/96] time 0.323 (0.433) data 0.000 (0.090) loss 1.2094 (1.4216) lr 7.5000e-03 eta 0:13:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [8/96] time 0.322 (0.408) data 0.000 (0.067) loss 1.6197 (1.4020) lr 7.5000e-03 eta 0:12:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [10/96] time 0.326 (0.391) data 0.000 (0.054) loss 1.2312 (1.4122) lr 7.5000e-03 eta 0:12:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [12/96] time 0.327 (0.382) data 0.000 (0.045) loss 0.8248 (1.3417) lr 7.5000e-03 eta 0:12:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [14/96] time 0.324 (0.374) data 0.000 (0.039) loss 1.7791 (1.3603) lr 7.5000e-03 eta 0:11:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [16/96] time 0.333 (0.369) data 0.000 (0.034) loss 0.9775 (1.3410) lr 7.5000e-03 eta 0:11:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [18/96] time 0.321 (0.364) data 0.000 (0.030) loss 1.5695 (1.3479) lr 7.5000e-03 eta 0:11:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [20/96] time 0.332 (0.362) data 0.001 (0.027) loss 1.7681 (1.3649) lr 7.5000e-03 eta 0:11:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [22/96] time 0.325 (0.359) data 0.001 (0.025) loss 1.2644 (1.3392) lr 7.5000e-03 eta 0:11:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [24/96] time 0.345 (0.357) data 0.000 (0.023) loss 0.8494 (1.3007) lr 7.5000e-03 eta 0:11:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [26/96] time 0.335 (0.356) data 0.000 (0.021) loss 1.4727 (1.3226) lr 7.5000e-03 eta 0:11:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [28/96] time 0.342 (0.354) data 0.000 (0.020) loss 0.8318 (1.3170) lr 7.5000e-03 eta 0:11:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [30/96] time 0.338 (0.353) data 0.000 (0.018) loss 1.6854 (1.3268) lr 7.5000e-03 eta 0:11:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [32/96] time 0.328 (0.352) data 0.000 (0.017) loss 1.0930 (1.3108) lr 7.5000e-03 eta 0:11:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [34/96] time 0.335 (0.351) data 0.000 (0.016) loss 1.4391 (1.3078) lr 7.5000e-03 eta 0:11:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [36/96] time 0.454 (0.354) data 0.001 (0.015) loss 0.9329 (1.2963) lr 7.5000e-03 eta 0:11:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [38/96] time 0.327 (0.353) data 0.000 (0.014) loss 2.3431 (1.3426) lr 7.5000e-03 eta 0:11:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [40/96] time 0.329 (0.351) data 0.000 (0.014) loss 1.5281 (1.3336) lr 7.5000e-03 eta 0:11:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [42/96] time 0.335 (0.351) data 0.000 (0.013) loss 1.8089 (1.3543) lr 7.5000e-03 eta 0:10:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [44/96] time 0.326 (0.349) data 0.000 (0.013) loss 1.2925 (1.3392) lr 7.5000e-03 eta 0:10:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [46/96] time 0.338 (0.349) data 0.001 (0.012) loss 0.7916 (1.3365) lr 7.5000e-03 eta 0:10:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [48/96] time 0.344 (0.348) data 0.000 (0.012) loss 0.9945 (1.3327) lr 7.5000e-03 eta 0:10:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [50/96] time 0.333 (0.348) data 0.000 (0.011) loss 1.1427 (1.3377) lr 7.5000e-03 eta 0:10:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [52/96] time 0.336 (0.347) data 0.000 (0.011) loss 1.1067 (1.3246) lr 7.5000e-03 eta 0:10:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [54/96] time 0.326 (0.346) data 0.001 (0.010) loss 1.3972 (1.3233) lr 7.5000e-03 eta 0:10:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [56/96] time 0.330 (0.345) data 0.000 (0.010) loss 1.0058 (1.3161) lr 7.5000e-03 eta 0:10:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [58/96] time 0.339 (0.345) data 0.001 (0.010) loss 2.2869 (1.3285) lr 7.5000e-03 eta 0:10:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [60/96] time 0.329 (0.344) data 0.000 (0.009) loss 1.4761 (1.3349) lr 7.5000e-03 eta 0:10:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [62/96] time 0.329 (0.344) data 0.000 (0.009) loss 0.7869 (1.3239) lr 7.5000e-03 eta 0:10:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [64/96] time 0.327 (0.343) data 0.000 (0.009) loss 1.6751 (1.3254) lr 7.5000e-03 eta 0:10:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [66/96] time 0.329 (0.343) data 0.000 (0.008) loss 1.7446 (1.3332) lr 7.5000e-03 eta 0:10:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [68/96] time 0.341 (0.343) data 0.000 (0.008) loss 0.8116 (1.3276) lr 7.5000e-03 eta 0:10:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [70/96] time 0.324 (0.342) data 0.000 (0.008) loss 1.1577 (1.3331) lr 7.5000e-03 eta 0:10:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [72/96] time 0.330 (0.342) data 0.000 (0.008) loss 1.9018 (1.3370) lr 7.5000e-03 eta 0:10:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [74/96] time 0.313 (0.341) data 0.000 (0.008) loss 1.6278 (1.3353) lr 7.5000e-03 eta 0:10:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [76/96] time 0.311 (0.340) data 0.000 (0.007) loss 1.2174 (1.3313) lr 7.5000e-03 eta 0:10:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [78/96] time 0.326 (0.340) data 0.000 (0.007) loss 2.2883 (1.3417) lr 7.5000e-03 eta 0:10:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [80/96] time 0.346 (0.340) data 0.000 (0.007) loss 1.7243 (1.3582) lr 7.5000e-03 eta 0:10:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [82/96] time 0.313 (0.339) data 0.000 (0.007) loss 1.3171 (1.3577) lr 7.5000e-03 eta 0:10:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [84/96] time 0.311 (0.338) data 0.000 (0.007) loss 1.1446 (1.3475) lr 7.5000e-03 eta 0:10:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [86/96] time 0.307 (0.338) data 0.000 (0.007) loss 1.8071 (1.3484) lr 7.5000e-03 eta 0:10:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [88/96] time 0.311 (0.337) data 0.000 (0.006) loss 0.8477 (1.3473) lr 7.5000e-03 eta 0:10:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [90/96] time 0.312 (0.336) data 0.000 (0.006) loss 2.3509 (1.3581) lr 7.5000e-03 eta 0:10:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [92/96] time 0.308 (0.336) data 0.000 (0.006) loss 1.5805 (1.3595) lr 7.5000e-03 eta 0:10:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [94/96] time 0.315 (0.335) data 0.000 (0.006) loss 1.2438 (1.3549) lr 7.5000e-03 eta 0:10:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [11/30] batch [96/96] time 0.307 (0.335) data 0.000 (0.006) loss 1.7940 (1.3536) lr 7.0337e-03 eta 0:10:10
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.89s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 405
* accuracy: 70.3%
* error: 29.7%
* macro_f1: 69.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [2/96] time 0.334 (0.665) data 0.000 (0.295) loss 0.9244 (1.6859) lr 7.0337e-03 eta 0:20:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [4/96] time 0.328 (0.497) data 0.000 (0.148) loss 1.3499 (1.5966) lr 7.0337e-03 eta 0:15:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [6/96] time 0.336 (0.441) data 0.000 (0.099) loss 0.8095 (1.4587) lr 7.0337e-03 eta 0:13:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [8/96] time 0.327 (0.413) data 0.000 (0.074) loss 1.1911 (1.4939) lr 7.0337e-03 eta 0:12:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [10/96] time 0.337 (0.397) data 0.000 (0.059) loss 1.4946 (1.5023) lr 7.0337e-03 eta 0:11:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [12/96] time 0.346 (0.387) data 0.000 (0.049) loss 1.1101 (1.4957) lr 7.0337e-03 eta 0:11:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [14/96] time 0.341 (0.381) data 0.001 (0.042) loss 1.3991 (1.4680) lr 7.0337e-03 eta 0:11:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [16/96] time 0.337 (0.375) data 0.000 (0.037) loss 1.5095 (1.4522) lr 7.0337e-03 eta 0:11:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [18/96] time 0.320 (0.370) data 0.001 (0.033) loss 1.4082 (1.4587) lr 7.0337e-03 eta 0:11:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [20/96] time 0.329 (0.366) data 0.000 (0.030) loss 1.1848 (1.4323) lr 7.0337e-03 eta 0:10:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [22/96] time 0.333 (0.363) data 0.000 (0.027) loss 1.0580 (1.3960) lr 7.0337e-03 eta 0:10:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [24/96] time 0.321 (0.360) data 0.000 (0.025) loss 1.3941 (1.3965) lr 7.0337e-03 eta 0:10:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [26/96] time 0.331 (0.358) data 0.000 (0.023) loss 1.4948 (1.3812) lr 7.0337e-03 eta 0:10:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [28/96] time 0.327 (0.356) data 0.000 (0.021) loss 1.1860 (1.3701) lr 7.0337e-03 eta 0:10:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [30/96] time 0.324 (0.354) data 0.000 (0.020) loss 1.5273 (1.3579) lr 7.0337e-03 eta 0:10:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [32/96] time 0.322 (0.352) data 0.000 (0.019) loss 1.4200 (1.3523) lr 7.0337e-03 eta 0:10:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [34/96] time 0.337 (0.351) data 0.000 (0.018) loss 1.0530 (1.3540) lr 7.0337e-03 eta 0:10:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [36/96] time 0.443 (0.353) data 0.000 (0.017) loss 2.2836 (1.3869) lr 7.0337e-03 eta 0:10:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [38/96] time 0.324 (0.352) data 0.000 (0.016) loss 1.3257 (1.3780) lr 7.0337e-03 eta 0:10:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [40/96] time 0.348 (0.351) data 0.000 (0.015) loss 1.0959 (1.3695) lr 7.0337e-03 eta 0:10:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [42/96] time 0.317 (0.350) data 0.000 (0.014) loss 2.0408 (1.3937) lr 7.0337e-03 eta 0:10:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [44/96] time 0.324 (0.349) data 0.000 (0.014) loss 0.8380 (1.3911) lr 7.0337e-03 eta 0:10:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [46/96] time 0.332 (0.348) data 0.000 (0.013) loss 1.6192 (1.4040) lr 7.0337e-03 eta 0:10:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [48/96] time 0.332 (0.347) data 0.000 (0.013) loss 0.8265 (1.3858) lr 7.0337e-03 eta 0:10:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [50/96] time 0.325 (0.346) data 0.000 (0.012) loss 1.3144 (1.3923) lr 7.0337e-03 eta 0:10:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [52/96] time 0.324 (0.345) data 0.000 (0.012) loss 0.8755 (1.3864) lr 7.0337e-03 eta 0:10:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [54/96] time 0.339 (0.345) data 0.001 (0.011) loss 0.9464 (1.3870) lr 7.0337e-03 eta 0:10:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [56/96] time 0.353 (0.345) data 0.000 (0.011) loss 0.7584 (1.3806) lr 7.0337e-03 eta 0:10:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [58/96] time 0.334 (0.345) data 0.000 (0.011) loss 1.5473 (1.3887) lr 7.0337e-03 eta 0:10:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [60/96] time 0.325 (0.344) data 0.000 (0.010) loss 0.8687 (1.3766) lr 7.0337e-03 eta 0:10:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [62/96] time 0.330 (0.344) data 0.000 (0.010) loss 1.9582 (1.3928) lr 7.0337e-03 eta 0:10:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [64/96] time 0.323 (0.344) data 0.000 (0.010) loss 1.1824 (1.3864) lr 7.0337e-03 eta 0:10:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [66/96] time 0.323 (0.343) data 0.000 (0.009) loss 1.6323 (1.3855) lr 7.0337e-03 eta 0:10:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [68/96] time 0.329 (0.343) data 0.000 (0.009) loss 1.6468 (1.3892) lr 7.0337e-03 eta 0:10:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [70/96] time 0.328 (0.342) data 0.000 (0.009) loss 1.2602 (1.3822) lr 7.0337e-03 eta 0:10:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [72/96] time 0.335 (0.342) data 0.000 (0.009) loss 1.0606 (1.3774) lr 7.0337e-03 eta 0:09:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [74/96] time 0.303 (0.341) data 0.000 (0.008) loss 0.7496 (1.3696) lr 7.0337e-03 eta 0:09:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [76/96] time 0.306 (0.340) data 0.000 (0.008) loss 1.1894 (1.3659) lr 7.0337e-03 eta 0:09:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [78/96] time 0.307 (0.339) data 0.000 (0.008) loss 1.1786 (1.3636) lr 7.0337e-03 eta 0:09:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [80/96] time 0.312 (0.338) data 0.000 (0.008) loss 1.1307 (1.3648) lr 7.0337e-03 eta 0:09:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [82/96] time 0.309 (0.338) data 0.000 (0.008) loss 1.5015 (1.3634) lr 7.0337e-03 eta 0:09:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [84/96] time 0.307 (0.337) data 0.000 (0.007) loss 1.2762 (1.3665) lr 7.0337e-03 eta 0:09:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [86/96] time 0.307 (0.336) data 0.000 (0.007) loss 2.2357 (1.3692) lr 7.0337e-03 eta 0:09:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [88/96] time 0.308 (0.336) data 0.000 (0.007) loss 1.3766 (1.3650) lr 7.0337e-03 eta 0:09:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [90/96] time 0.310 (0.335) data 0.000 (0.007) loss 1.9045 (1.3731) lr 7.0337e-03 eta 0:09:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [92/96] time 0.308 (0.334) data 0.000 (0.007) loss 0.9735 (1.3708) lr 7.0337e-03 eta 0:09:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [94/96] time 0.309 (0.334) data 0.000 (0.007) loss 1.4395 (1.3775) lr 7.0337e-03 eta 0:09:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [12/30] batch [96/96] time 0.308 (0.333) data 0.000 (0.006) loss 0.7965 (1.3730) lr 6.5451e-03 eta 0:09:36
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.89s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.37s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.19s/it]=> result
* total: 576
* correct: 411
* accuracy: 71.4%
* error: 28.6%
* macro_f1: 70.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [2/96] time 0.320 (0.668) data 0.000 (0.305) loss 1.4992 (1.2727) lr 6.5451e-03 eta 0:19:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [4/96] time 0.330 (0.497) data 0.000 (0.153) loss 0.9421 (1.3340) lr 6.5451e-03 eta 0:14:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [6/96] time 0.334 (0.442) data 0.000 (0.102) loss 1.7138 (1.3660) lr 6.5451e-03 eta 0:12:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [8/96] time 0.331 (0.413) data 0.000 (0.077) loss 1.1661 (1.3186) lr 6.5451e-03 eta 0:11:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [10/96] time 0.340 (0.398) data 0.000 (0.061) loss 0.8723 (1.2666) lr 6.5451e-03 eta 0:11:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [12/96] time 0.341 (0.387) data 0.000 (0.051) loss 0.9499 (1.2312) lr 6.5451e-03 eta 0:11:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [14/96] time 0.336 (0.380) data 0.000 (0.044) loss 1.6850 (1.2699) lr 6.5451e-03 eta 0:10:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [16/96] time 0.335 (0.374) data 0.000 (0.038) loss 1.8612 (1.3050) lr 6.5451e-03 eta 0:10:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [18/96] time 0.323 (0.368) data 0.000 (0.034) loss 1.1045 (1.2890) lr 6.5451e-03 eta 0:10:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [20/96] time 0.332 (0.364) data 0.000 (0.031) loss 1.3458 (1.2671) lr 6.5451e-03 eta 0:10:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [22/96] time 0.343 (0.362) data 0.000 (0.028) loss 1.7548 (1.2737) lr 6.5451e-03 eta 0:10:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [24/96] time 0.336 (0.359) data 0.000 (0.026) loss 1.1522 (1.2639) lr 6.5451e-03 eta 0:10:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [26/96] time 0.324 (0.357) data 0.000 (0.024) loss 0.7091 (1.2634) lr 6.5451e-03 eta 0:10:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [28/96] time 0.329 (0.355) data 0.000 (0.022) loss 1.7855 (1.2823) lr 6.5451e-03 eta 0:10:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [30/96] time 0.320 (0.353) data 0.000 (0.021) loss 0.8329 (1.2703) lr 6.5451e-03 eta 0:10:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [32/96] time 0.327 (0.352) data 0.000 (0.019) loss 1.3781 (1.2866) lr 6.5451e-03 eta 0:09:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [34/96] time 0.336 (0.351) data 0.000 (0.018) loss 0.8805 (1.2596) lr 6.5451e-03 eta 0:09:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [36/96] time 0.430 (0.353) data 0.000 (0.017) loss 1.3306 (1.2629) lr 6.5451e-03 eta 0:09:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [38/96] time 0.333 (0.351) data 0.000 (0.016) loss 1.8534 (1.2773) lr 6.5451e-03 eta 0:09:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [40/96] time 0.327 (0.350) data 0.000 (0.016) loss 1.2167 (1.2807) lr 6.5451e-03 eta 0:09:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [42/96] time 0.343 (0.350) data 0.000 (0.015) loss 0.9809 (1.2785) lr 6.5451e-03 eta 0:09:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [44/96] time 0.326 (0.349) data 0.001 (0.014) loss 1.5712 (1.2878) lr 6.5451e-03 eta 0:09:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [46/96] time 0.336 (0.348) data 0.001 (0.014) loss 0.8689 (1.2844) lr 6.5451e-03 eta 0:09:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [48/96] time 0.327 (0.347) data 0.000 (0.013) loss 1.5681 (1.2802) lr 6.5451e-03 eta 0:09:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [50/96] time 0.322 (0.347) data 0.001 (0.013) loss 1.1359 (1.2877) lr 6.5451e-03 eta 0:09:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [52/96] time 0.339 (0.346) data 0.000 (0.012) loss 1.0512 (1.2858) lr 6.5451e-03 eta 0:09:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [54/96] time 0.318 (0.345) data 0.000 (0.012) loss 1.3750 (1.2853) lr 6.5451e-03 eta 0:09:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [56/96] time 0.323 (0.344) data 0.000 (0.011) loss 1.5871 (1.3107) lr 6.5451e-03 eta 0:09:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [58/96] time 0.320 (0.343) data 0.000 (0.011) loss 2.6403 (1.3341) lr 6.5451e-03 eta 0:09:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [60/96] time 0.320 (0.342) data 0.000 (0.010) loss 1.4861 (1.3456) lr 6.5451e-03 eta 0:09:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [62/96] time 0.319 (0.342) data 0.000 (0.010) loss 0.9482 (1.3643) lr 6.5451e-03 eta 0:09:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [64/96] time 0.326 (0.341) data 0.000 (0.010) loss 0.9615 (1.3736) lr 6.5451e-03 eta 0:09:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [66/96] time 0.315 (0.340) data 0.000 (0.010) loss 2.2573 (1.3953) lr 6.5451e-03 eta 0:09:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [68/96] time 0.330 (0.340) data 0.000 (0.009) loss 1.1365 (1.3906) lr 6.5451e-03 eta 0:09:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [70/96] time 0.327 (0.340) data 0.000 (0.009) loss 1.2902 (1.3872) lr 6.5451e-03 eta 0:09:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [72/96] time 0.321 (0.339) data 0.000 (0.009) loss 1.8167 (1.3902) lr 6.5451e-03 eta 0:09:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [74/96] time 0.310 (0.338) data 0.000 (0.009) loss 1.7287 (1.3914) lr 6.5451e-03 eta 0:09:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [76/96] time 0.302 (0.337) data 0.000 (0.008) loss 1.1901 (1.3850) lr 6.5451e-03 eta 0:09:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [78/96] time 0.307 (0.337) data 0.000 (0.008) loss 0.8296 (1.3850) lr 6.5451e-03 eta 0:09:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [80/96] time 0.302 (0.336) data 0.000 (0.008) loss 1.6463 (1.3795) lr 6.5451e-03 eta 0:09:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [82/96] time 0.301 (0.335) data 0.000 (0.008) loss 1.3883 (1.3796) lr 6.5451e-03 eta 0:09:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [84/96] time 0.306 (0.334) data 0.000 (0.008) loss 0.8154 (1.3738) lr 6.5451e-03 eta 0:09:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [86/96] time 0.302 (0.334) data 0.000 (0.007) loss 1.5013 (1.3717) lr 6.5451e-03 eta 0:09:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [88/96] time 0.304 (0.333) data 0.000 (0.007) loss 1.3063 (1.3740) lr 6.5451e-03 eta 0:09:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [90/96] time 0.301 (0.332) data 0.000 (0.007) loss 1.7678 (1.3787) lr 6.5451e-03 eta 0:09:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [92/96] time 0.328 (0.332) data 0.000 (0.007) loss 1.3195 (1.3778) lr 6.5451e-03 eta 0:09:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [94/96] time 0.306 (0.332) data 0.000 (0.007) loss 1.0804 (1.3750) lr 6.5451e-03 eta 0:09:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [13/30] batch [96/96] time 0.305 (0.331) data 0.000 (0.007) loss 1.8333 (1.3744) lr 6.0396e-03 eta 0:09:00
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.83s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.33s/it]100%|██████████| 3/3 [00:03<00:00,  1.18it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 406
* accuracy: 70.5%
* error: 29.5%
* macro_f1: 69.5%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [2/96] time 0.333 (0.663) data 0.000 (0.287) loss 1.1616 (1.2330) lr 6.0396e-03 eta 0:18:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [4/96] time 0.320 (0.494) data 0.000 (0.144) loss 0.8848 (1.0414) lr 6.0396e-03 eta 0:13:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [6/96] time 0.326 (0.437) data 0.000 (0.096) loss 1.4323 (1.1393) lr 6.0396e-03 eta 0:11:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [8/96] time 0.319 (0.406) data 0.000 (0.072) loss 1.0653 (1.1514) lr 6.0396e-03 eta 0:10:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [10/96] time 0.328 (0.390) data 0.000 (0.058) loss 1.6454 (1.2601) lr 6.0396e-03 eta 0:10:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [12/96] time 0.338 (0.379) data 0.000 (0.048) loss 1.6754 (1.3042) lr 6.0396e-03 eta 0:10:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [14/96] time 0.335 (0.373) data 0.000 (0.041) loss 0.8700 (1.2660) lr 6.0396e-03 eta 0:10:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [16/96] time 0.316 (0.368) data 0.000 (0.036) loss 1.8244 (1.2885) lr 6.0396e-03 eta 0:09:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [18/96] time 0.315 (0.362) data 0.000 (0.032) loss 0.9660 (1.2581) lr 6.0396e-03 eta 0:09:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [20/96] time 0.315 (0.358) data 0.000 (0.029) loss 1.0516 (1.2470) lr 6.0396e-03 eta 0:09:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [22/96] time 0.338 (0.356) data 0.000 (0.026) loss 1.5801 (1.2582) lr 6.0396e-03 eta 0:09:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [24/96] time 0.333 (0.354) data 0.000 (0.024) loss 0.8580 (1.2517) lr 6.0396e-03 eta 0:09:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [26/96] time 0.320 (0.351) data 0.001 (0.022) loss 1.4121 (1.2903) lr 6.0396e-03 eta 0:09:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [28/96] time 0.343 (0.350) data 0.000 (0.021) loss 1.3739 (1.3057) lr 6.0396e-03 eta 0:09:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [30/96] time 0.329 (0.349) data 0.000 (0.019) loss 1.6533 (1.3133) lr 6.0396e-03 eta 0:09:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [32/96] time 0.319 (0.347) data 0.000 (0.018) loss 1.0521 (1.2888) lr 6.0396e-03 eta 0:09:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [34/96] time 0.325 (0.345) data 0.000 (0.017) loss 1.9431 (1.3046) lr 6.0396e-03 eta 0:09:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [36/96] time 0.411 (0.347) data 0.000 (0.016) loss 0.8920 (1.2914) lr 6.0396e-03 eta 0:09:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [38/96] time 0.324 (0.346) data 0.000 (0.015) loss 1.4008 (1.2793) lr 6.0396e-03 eta 0:09:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [40/96] time 0.321 (0.344) data 0.000 (0.015) loss 1.1350 (1.2622) lr 6.0396e-03 eta 0:09:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [42/96] time 0.333 (0.344) data 0.000 (0.014) loss 1.0646 (1.2718) lr 6.0396e-03 eta 0:09:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [44/96] time 0.329 (0.343) data 0.000 (0.013) loss 1.7132 (1.2726) lr 6.0396e-03 eta 0:09:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [46/96] time 0.319 (0.342) data 0.000 (0.013) loss 1.4328 (1.2667) lr 6.0396e-03 eta 0:09:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [48/96] time 0.321 (0.342) data 0.000 (0.012) loss 0.9521 (1.2552) lr 6.0396e-03 eta 0:09:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [50/96] time 0.331 (0.341) data 0.000 (0.012) loss 1.3344 (1.2534) lr 6.0396e-03 eta 0:08:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [52/96] time 0.320 (0.341) data 0.000 (0.011) loss 1.2087 (1.2469) lr 6.0396e-03 eta 0:08:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [54/96] time 0.327 (0.340) data 0.000 (0.011) loss 1.5151 (1.2452) lr 6.0396e-03 eta 0:08:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [56/96] time 0.333 (0.340) data 0.000 (0.011) loss 1.3127 (1.2642) lr 6.0396e-03 eta 0:08:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [58/96] time 0.321 (0.339) data 0.000 (0.010) loss 0.8937 (1.2509) lr 6.0396e-03 eta 0:08:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [60/96] time 0.323 (0.339) data 0.000 (0.010) loss 1.1175 (1.2605) lr 6.0396e-03 eta 0:08:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [62/96] time 0.318 (0.338) data 0.000 (0.010) loss 1.2187 (1.2624) lr 6.0396e-03 eta 0:08:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [64/96] time 0.313 (0.338) data 0.000 (0.009) loss 1.4137 (1.2592) lr 6.0396e-03 eta 0:08:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [66/96] time 0.330 (0.337) data 0.000 (0.009) loss 1.2074 (1.2697) lr 6.0396e-03 eta 0:08:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [68/96] time 0.313 (0.337) data 0.000 (0.009) loss 1.3390 (1.2724) lr 6.0396e-03 eta 0:08:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [70/96] time 0.317 (0.336) data 0.000 (0.009) loss 1.4170 (1.2731) lr 6.0396e-03 eta 0:08:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [72/96] time 0.319 (0.336) data 0.000 (0.008) loss 1.7140 (1.2939) lr 6.0396e-03 eta 0:08:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [74/96] time 0.300 (0.335) data 0.000 (0.008) loss 1.2492 (1.2904) lr 6.0396e-03 eta 0:08:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [76/96] time 0.304 (0.334) data 0.000 (0.008) loss 1.5780 (1.3042) lr 6.0396e-03 eta 0:08:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [78/96] time 0.299 (0.333) data 0.000 (0.008) loss 1.3352 (1.3046) lr 6.0396e-03 eta 0:08:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [80/96] time 0.307 (0.333) data 0.000 (0.008) loss 1.8954 (1.3169) lr 6.0396e-03 eta 0:08:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [82/96] time 0.305 (0.332) data 0.000 (0.007) loss 1.8575 (1.3202) lr 6.0396e-03 eta 0:08:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [84/96] time 0.302 (0.331) data 0.000 (0.007) loss 1.8770 (1.3244) lr 6.0396e-03 eta 0:08:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [86/96] time 0.304 (0.331) data 0.000 (0.007) loss 1.6764 (1.3368) lr 6.0396e-03 eta 0:08:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [88/96] time 0.302 (0.330) data 0.000 (0.007) loss 1.0005 (1.3302) lr 6.0396e-03 eta 0:08:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [90/96] time 0.307 (0.329) data 0.000 (0.007) loss 1.0844 (1.3250) lr 6.0396e-03 eta 0:08:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [92/96] time 0.303 (0.329) data 0.000 (0.007) loss 0.8816 (1.3190) lr 6.0396e-03 eta 0:08:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [94/96] time 0.305 (0.328) data 0.000 (0.006) loss 1.2956 (1.3170) lr 6.0396e-03 eta 0:08:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [14/30] batch [96/96] time 0.304 (0.328) data 0.000 (0.006) loss 1.1202 (1.3136) lr 5.5226e-03 eta 0:08:23
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.82s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.33s/it]100%|██████████| 3/3 [00:03<00:00,  1.18it/s]100%|██████████| 3/3 [00:03<00:00,  1.16s/it]=> result
* total: 576
* correct: 415
* accuracy: 72.0%
* error: 28.0%
* macro_f1: 70.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [2/96] time 0.326 (0.651) data 0.000 (0.283) loss 1.2936 (1.2279) lr 5.5226e-03 eta 0:16:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [4/96] time 0.328 (0.488) data 0.000 (0.142) loss 1.2406 (1.2652) lr 5.5226e-03 eta 0:12:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [6/96] time 0.335 (0.436) data 0.000 (0.095) loss 1.3110 (1.2552) lr 5.5226e-03 eta 0:11:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [8/96] time 0.332 (0.408) data 0.001 (0.071) loss 1.5154 (1.2889) lr 5.5226e-03 eta 0:10:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [10/96] time 0.325 (0.391) data 0.001 (0.057) loss 1.8077 (1.3062) lr 5.5226e-03 eta 0:09:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [12/96] time 0.322 (0.379) data 0.000 (0.048) loss 0.7666 (1.2338) lr 5.5226e-03 eta 0:09:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [14/96] time 0.324 (0.371) data 0.000 (0.041) loss 1.7630 (1.2957) lr 5.5226e-03 eta 0:09:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [16/96] time 0.319 (0.365) data 0.000 (0.036) loss 1.0017 (1.3213) lr 5.5226e-03 eta 0:09:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [18/96] time 0.327 (0.362) data 0.000 (0.032) loss 1.5341 (1.2980) lr 5.5226e-03 eta 0:09:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [20/96] time 0.339 (0.359) data 0.000 (0.029) loss 1.2329 (1.2770) lr 5.5226e-03 eta 0:09:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [22/96] time 0.325 (0.356) data 0.000 (0.026) loss 0.9959 (1.2684) lr 5.5226e-03 eta 0:08:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [24/96] time 0.322 (0.354) data 0.000 (0.024) loss 0.9655 (1.2654) lr 5.5226e-03 eta 0:08:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [26/96] time 0.317 (0.351) data 0.000 (0.022) loss 0.7519 (1.2385) lr 5.5226e-03 eta 0:08:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [28/96] time 0.327 (0.350) data 0.000 (0.021) loss 0.9569 (1.2544) lr 5.5226e-03 eta 0:08:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [30/96] time 0.329 (0.348) data 0.000 (0.019) loss 1.2533 (1.2650) lr 5.5226e-03 eta 0:08:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [32/96] time 0.324 (0.347) data 0.000 (0.018) loss 1.2254 (1.2644) lr 5.5226e-03 eta 0:08:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [34/96] time 0.313 (0.345) data 0.000 (0.017) loss 1.1967 (1.2504) lr 5.5226e-03 eta 0:08:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [36/96] time 0.419 (0.347) data 0.000 (0.016) loss 0.8506 (1.2429) lr 5.5226e-03 eta 0:08:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [38/96] time 0.317 (0.345) data 0.000 (0.015) loss 1.4619 (1.2443) lr 5.5226e-03 eta 0:08:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [40/96] time 0.317 (0.344) data 0.000 (0.014) loss 1.4577 (1.2410) lr 5.5226e-03 eta 0:08:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [42/96] time 0.325 (0.343) data 0.000 (0.014) loss 1.5767 (1.2582) lr 5.5226e-03 eta 0:08:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [44/96] time 0.325 (0.342) data 0.000 (0.013) loss 1.1088 (1.2744) lr 5.5226e-03 eta 0:08:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [46/96] time 0.323 (0.342) data 0.000 (0.013) loss 1.1737 (1.2768) lr 5.5226e-03 eta 0:08:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [48/96] time 0.351 (0.342) data 0.000 (0.012) loss 0.8241 (1.2714) lr 5.5226e-03 eta 0:08:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [50/96] time 0.347 (0.341) data 0.000 (0.012) loss 0.7207 (1.2548) lr 5.5226e-03 eta 0:08:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [52/96] time 0.313 (0.341) data 0.000 (0.011) loss 1.7963 (1.2585) lr 5.5226e-03 eta 0:08:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [54/96] time 0.330 (0.340) data 0.000 (0.011) loss 1.3513 (1.2643) lr 5.5226e-03 eta 0:08:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [56/96] time 0.319 (0.339) data 0.000 (0.010) loss 1.5672 (1.2656) lr 5.5226e-03 eta 0:08:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [58/96] time 0.325 (0.339) data 0.000 (0.010) loss 1.1989 (1.2790) lr 5.5226e-03 eta 0:08:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [60/96] time 0.326 (0.338) data 0.000 (0.010) loss 1.6102 (1.2842) lr 5.5226e-03 eta 0:08:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [62/96] time 0.317 (0.338) data 0.000 (0.009) loss 0.7666 (1.2724) lr 5.5226e-03 eta 0:08:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [64/96] time 0.360 (0.339) data 0.000 (0.009) loss 1.2345 (1.2783) lr 5.5226e-03 eta 0:08:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [66/96] time 0.351 (0.339) data 0.000 (0.009) loss 1.5867 (1.2828) lr 5.5226e-03 eta 0:08:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [68/96] time 0.342 (0.339) data 0.001 (0.009) loss 1.0590 (1.2745) lr 5.5226e-03 eta 0:08:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [70/96] time 0.341 (0.339) data 0.000 (0.008) loss 1.1188 (1.2720) lr 5.5226e-03 eta 0:08:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [72/96] time 0.336 (0.339) data 0.000 (0.008) loss 1.6780 (1.2794) lr 5.5226e-03 eta 0:08:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [74/96] time 0.325 (0.338) data 0.000 (0.008) loss 0.9253 (1.2821) lr 5.5226e-03 eta 0:08:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [76/96] time 0.323 (0.338) data 0.000 (0.008) loss 1.1447 (1.2864) lr 5.5226e-03 eta 0:08:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [78/96] time 0.321 (0.338) data 0.000 (0.008) loss 1.4053 (1.2850) lr 5.5226e-03 eta 0:08:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [80/96] time 0.323 (0.337) data 0.000 (0.007) loss 0.9437 (1.2775) lr 5.5226e-03 eta 0:08:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [82/96] time 0.319 (0.337) data 0.000 (0.007) loss 1.5085 (1.2845) lr 5.5226e-03 eta 0:08:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [84/96] time 0.326 (0.336) data 0.000 (0.007) loss 1.8631 (1.2888) lr 5.5226e-03 eta 0:08:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [86/96] time 0.326 (0.336) data 0.000 (0.007) loss 1.6128 (1.3011) lr 5.5226e-03 eta 0:08:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [88/96] time 0.318 (0.336) data 0.000 (0.007) loss 1.0456 (1.2925) lr 5.5226e-03 eta 0:08:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [90/96] time 0.330 (0.336) data 0.000 (0.007) loss 1.5358 (1.2932) lr 5.5226e-03 eta 0:08:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [92/96] time 0.308 (0.335) data 0.000 (0.006) loss 1.2473 (1.2971) lr 5.5226e-03 eta 0:08:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [94/96] time 0.306 (0.335) data 0.000 (0.006) loss 1.0239 (1.2940) lr 5.5226e-03 eta 0:08:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [15/30] batch [96/96] time 0.312 (0.334) data 0.000 (0.006) loss 1.3776 (1.2921) lr 5.0000e-03 eta 0:08:00
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.91s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.37s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.19s/it]=> result
* total: 576
* correct: 421
* accuracy: 73.1%
* error: 26.9%
* macro_f1: 71.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [2/96] time 0.313 (0.628) data 0.000 (0.265) loss 1.0430 (1.1574) lr 5.0000e-03 eta 0:15:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [4/96] time 0.326 (0.476) data 0.000 (0.133) loss 1.6063 (1.1414) lr 5.0000e-03 eta 0:11:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [6/96] time 0.330 (0.427) data 0.000 (0.088) loss 0.9881 (1.2278) lr 5.0000e-03 eta 0:10:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [8/96] time 0.314 (0.401) data 0.000 (0.066) loss 1.6794 (1.2891) lr 5.0000e-03 eta 0:09:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [10/96] time 0.323 (0.385) data 0.000 (0.053) loss 1.0142 (1.2128) lr 5.0000e-03 eta 0:09:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [12/96] time 0.332 (0.375) data 0.000 (0.044) loss 1.2535 (1.2034) lr 5.0000e-03 eta 0:08:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [14/96] time 0.332 (0.369) data 0.000 (0.038) loss 1.3434 (1.2323) lr 5.0000e-03 eta 0:08:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [16/96] time 0.328 (0.364) data 0.000 (0.033) loss 0.9500 (1.2167) lr 5.0000e-03 eta 0:08:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [18/96] time 0.321 (0.359) data 0.000 (0.030) loss 1.0287 (1.2283) lr 5.0000e-03 eta 0:08:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [20/96] time 0.322 (0.356) data 0.000 (0.027) loss 1.1969 (1.2511) lr 5.0000e-03 eta 0:08:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [22/96] time 0.336 (0.354) data 0.000 (0.024) loss 1.6542 (1.2650) lr 5.0000e-03 eta 0:08:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [24/96] time 0.313 (0.351) data 0.000 (0.022) loss 1.0142 (1.2444) lr 5.0000e-03 eta 0:08:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [26/96] time 0.332 (0.349) data 0.000 (0.021) loss 1.0798 (1.2710) lr 5.0000e-03 eta 0:08:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [28/96] time 0.322 (0.348) data 0.000 (0.019) loss 1.6233 (1.2794) lr 5.0000e-03 eta 0:08:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [30/96] time 0.328 (0.347) data 0.000 (0.018) loss 1.1425 (1.2653) lr 5.0000e-03 eta 0:08:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [32/96] time 0.331 (0.346) data 0.000 (0.017) loss 1.8343 (1.2829) lr 5.0000e-03 eta 0:08:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [34/96] time 0.322 (0.344) data 0.000 (0.016) loss 1.5732 (1.3039) lr 5.0000e-03 eta 0:08:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [36/96] time 0.430 (0.346) data 0.000 (0.015) loss 1.4215 (1.3167) lr 5.0000e-03 eta 0:08:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [38/96] time 0.331 (0.345) data 0.000 (0.014) loss 1.1241 (1.3042) lr 5.0000e-03 eta 0:08:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [40/96] time 0.331 (0.345) data 0.000 (0.014) loss 1.7045 (1.3222) lr 5.0000e-03 eta 0:08:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [42/96] time 0.322 (0.344) data 0.000 (0.013) loss 0.9221 (1.3276) lr 5.0000e-03 eta 0:08:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [44/96] time 0.327 (0.343) data 0.000 (0.012) loss 1.1502 (1.3221) lr 5.0000e-03 eta 0:07:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [46/96] time 0.320 (0.342) data 0.000 (0.012) loss 1.2853 (1.3220) lr 5.0000e-03 eta 0:07:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [48/96] time 0.344 (0.342) data 0.000 (0.011) loss 1.7156 (1.3368) lr 5.0000e-03 eta 0:07:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [50/96] time 0.336 (0.342) data 0.000 (0.011) loss 0.7985 (1.3302) lr 5.0000e-03 eta 0:07:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [52/96] time 0.339 (0.342) data 0.000 (0.011) loss 1.4805 (1.3302) lr 5.0000e-03 eta 0:07:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [54/96] time 0.350 (0.342) data 0.000 (0.010) loss 1.3747 (1.3238) lr 5.0000e-03 eta 0:07:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [56/96] time 0.328 (0.341) data 0.000 (0.010) loss 1.1172 (1.3190) lr 5.0000e-03 eta 0:07:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [58/96] time 0.335 (0.341) data 0.000 (0.009) loss 1.1524 (1.3137) lr 5.0000e-03 eta 0:07:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [60/96] time 0.325 (0.340) data 0.000 (0.009) loss 1.3412 (1.3248) lr 5.0000e-03 eta 0:07:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [62/96] time 0.322 (0.340) data 0.000 (0.009) loss 1.9385 (1.3336) lr 5.0000e-03 eta 0:07:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [64/96] time 0.335 (0.339) data 0.000 (0.009) loss 0.8541 (1.3172) lr 5.0000e-03 eta 0:07:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [66/96] time 0.345 (0.339) data 0.000 (0.008) loss 1.1525 (1.3112) lr 5.0000e-03 eta 0:07:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [68/96] time 0.341 (0.339) data 0.000 (0.008) loss 1.1659 (1.3158) lr 5.0000e-03 eta 0:07:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [70/96] time 0.342 (0.339) data 0.000 (0.008) loss 0.7736 (1.3199) lr 5.0000e-03 eta 0:07:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [72/96] time 0.330 (0.339) data 0.000 (0.008) loss 1.7978 (1.3280) lr 5.0000e-03 eta 0:07:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [74/96] time 0.320 (0.338) data 0.000 (0.007) loss 1.2869 (1.3408) lr 5.0000e-03 eta 0:07:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [76/96] time 0.323 (0.338) data 0.000 (0.007) loss 2.1349 (1.3614) lr 5.0000e-03 eta 0:07:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [78/96] time 0.318 (0.338) data 0.000 (0.007) loss 1.0986 (1.3574) lr 5.0000e-03 eta 0:07:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [80/96] time 0.322 (0.337) data 0.000 (0.007) loss 1.0006 (1.3534) lr 5.0000e-03 eta 0:07:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [82/96] time 0.323 (0.337) data 0.000 (0.007) loss 1.0037 (1.3428) lr 5.0000e-03 eta 0:07:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [84/96] time 0.318 (0.336) data 0.000 (0.007) loss 1.2390 (1.3423) lr 5.0000e-03 eta 0:07:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [86/96] time 0.320 (0.336) data 0.000 (0.006) loss 0.9127 (1.3330) lr 5.0000e-03 eta 0:07:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [88/96] time 0.323 (0.336) data 0.000 (0.006) loss 0.8904 (1.3252) lr 5.0000e-03 eta 0:07:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [90/96] time 0.320 (0.335) data 0.000 (0.006) loss 0.8188 (1.3149) lr 5.0000e-03 eta 0:07:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [92/96] time 0.322 (0.335) data 0.000 (0.006) loss 1.4952 (1.3118) lr 5.0000e-03 eta 0:07:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [94/96] time 0.314 (0.335) data 0.000 (0.006) loss 0.9739 (1.3062) lr 5.0000e-03 eta 0:07:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [16/30] batch [96/96] time 0.319 (0.334) data 0.000 (0.006) loss 0.7882 (1.2948) lr 4.4774e-03 eta 0:07:29
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.80s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.33s/it]100%|██████████| 3/3 [00:03<00:00,  1.18it/s]100%|██████████| 3/3 [00:03<00:00,  1.16s/it]=> result
* total: 576
* correct: 410
* accuracy: 71.2%
* error: 28.8%
* macro_f1: 70.0%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [2/96] time 0.344 (0.648) data 0.000 (0.270) loss 0.7603 (1.2747) lr 4.4774e-03 eta 0:14:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [4/96] time 0.332 (0.486) data 0.000 (0.135) loss 1.5303 (1.1897) lr 4.4774e-03 eta 0:10:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [6/96] time 0.329 (0.435) data 0.000 (0.090) loss 1.3424 (1.1455) lr 4.4774e-03 eta 0:09:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [8/96] time 0.331 (0.410) data 0.000 (0.068) loss 1.3503 (1.2347) lr 4.4774e-03 eta 0:09:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [10/96] time 0.324 (0.393) data 0.000 (0.054) loss 1.3731 (1.2871) lr 4.4774e-03 eta 0:08:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [12/96] time 0.318 (0.382) data 0.000 (0.045) loss 1.2005 (1.2374) lr 4.4774e-03 eta 0:08:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [14/96] time 0.327 (0.375) data 0.000 (0.039) loss 1.2694 (1.2151) lr 4.4774e-03 eta 0:08:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [16/96] time 0.327 (0.369) data 0.000 (0.034) loss 0.8707 (1.2125) lr 4.4774e-03 eta 0:08:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [18/96] time 0.334 (0.364) data 0.000 (0.030) loss 0.9502 (1.1978) lr 4.4774e-03 eta 0:08:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [20/96] time 0.325 (0.361) data 0.000 (0.027) loss 1.1372 (1.1949) lr 4.4774e-03 eta 0:07:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [22/96] time 0.329 (0.358) data 0.000 (0.025) loss 1.5847 (1.2005) lr 4.4774e-03 eta 0:07:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [24/96] time 0.325 (0.356) data 0.000 (0.023) loss 0.8911 (1.1923) lr 4.4774e-03 eta 0:07:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [26/96] time 0.336 (0.353) data 0.000 (0.021) loss 1.4347 (1.1951) lr 4.4774e-03 eta 0:07:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [28/96] time 0.330 (0.352) data 0.000 (0.020) loss 1.3076 (1.1960) lr 4.4774e-03 eta 0:07:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [30/96] time 0.337 (0.350) data 0.000 (0.018) loss 1.1916 (1.1919) lr 4.4774e-03 eta 0:07:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [32/96] time 0.347 (0.350) data 0.000 (0.017) loss 1.0130 (1.1767) lr 4.4774e-03 eta 0:07:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [34/96] time 0.322 (0.348) data 0.000 (0.016) loss 0.9255 (1.1637) lr 4.4774e-03 eta 0:07:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [36/96] time 0.421 (0.350) data 0.000 (0.015) loss 1.8491 (1.2283) lr 4.4774e-03 eta 0:07:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [38/96] time 0.329 (0.348) data 0.000 (0.015) loss 1.1655 (1.2241) lr 4.4774e-03 eta 0:07:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [40/96] time 0.329 (0.347) data 0.000 (0.014) loss 0.9410 (1.2216) lr 4.4774e-03 eta 0:07:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [42/96] time 0.345 (0.347) data 0.000 (0.013) loss 1.1460 (1.2123) lr 4.4774e-03 eta 0:07:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [44/96] time 0.330 (0.347) data 0.000 (0.013) loss 1.1728 (1.2117) lr 4.4774e-03 eta 0:07:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [46/96] time 0.344 (0.346) data 0.000 (0.012) loss 1.6110 (1.2310) lr 4.4774e-03 eta 0:07:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [48/96] time 0.337 (0.346) data 0.000 (0.012) loss 1.5060 (1.2253) lr 4.4774e-03 eta 0:07:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [50/96] time 0.340 (0.346) data 0.000 (0.011) loss 2.0665 (1.2498) lr 4.4774e-03 eta 0:07:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [52/96] time 0.337 (0.346) data 0.000 (0.011) loss 1.0273 (1.2436) lr 4.4774e-03 eta 0:07:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [54/96] time 0.323 (0.345) data 0.000 (0.010) loss 1.0879 (1.2420) lr 4.4774e-03 eta 0:07:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [56/96] time 0.327 (0.344) data 0.000 (0.010) loss 1.3764 (1.2497) lr 4.4774e-03 eta 0:07:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [58/96] time 0.331 (0.344) data 0.000 (0.010) loss 1.1466 (1.2509) lr 4.4774e-03 eta 0:07:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [60/96] time 0.327 (0.343) data 0.000 (0.009) loss 1.2115 (1.2468) lr 4.4774e-03 eta 0:07:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [62/96] time 0.328 (0.343) data 0.000 (0.009) loss 1.0341 (1.2606) lr 4.4774e-03 eta 0:07:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [64/96] time 0.323 (0.342) data 0.000 (0.009) loss 0.9452 (1.2535) lr 4.4774e-03 eta 0:07:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [66/96] time 0.321 (0.341) data 0.000 (0.009) loss 1.9724 (1.2645) lr 4.4774e-03 eta 0:07:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [68/96] time 0.325 (0.341) data 0.000 (0.008) loss 1.1126 (1.2644) lr 4.4774e-03 eta 0:07:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [70/96] time 0.337 (0.341) data 0.000 (0.008) loss 1.3158 (1.2625) lr 4.4774e-03 eta 0:07:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [72/96] time 0.323 (0.340) data 0.000 (0.008) loss 1.6820 (1.2677) lr 4.4774e-03 eta 0:07:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [74/96] time 0.307 (0.339) data 0.000 (0.008) loss 0.9150 (1.2603) lr 4.4774e-03 eta 0:07:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [76/96] time 0.311 (0.339) data 0.000 (0.007) loss 1.1110 (1.2570) lr 4.4774e-03 eta 0:07:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [78/96] time 0.309 (0.338) data 0.000 (0.007) loss 1.7025 (1.2572) lr 4.4774e-03 eta 0:07:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [80/96] time 0.306 (0.337) data 0.000 (0.007) loss 1.0831 (1.2576) lr 4.4774e-03 eta 0:07:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [82/96] time 0.312 (0.336) data 0.000 (0.007) loss 1.1992 (1.2527) lr 4.4774e-03 eta 0:07:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [84/96] time 0.304 (0.336) data 0.000 (0.007) loss 1.2576 (1.2521) lr 4.4774e-03 eta 0:07:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [86/96] time 0.310 (0.335) data 0.000 (0.007) loss 1.1418 (1.2520) lr 4.4774e-03 eta 0:07:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [88/96] time 0.308 (0.334) data 0.000 (0.006) loss 1.6127 (1.2539) lr 4.4774e-03 eta 0:06:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [90/96] time 0.308 (0.334) data 0.000 (0.006) loss 2.0824 (1.2608) lr 4.4774e-03 eta 0:06:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [92/96] time 0.306 (0.333) data 0.000 (0.006) loss 1.1986 (1.2586) lr 4.4774e-03 eta 0:06:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [94/96] time 0.304 (0.333) data 0.000 (0.006) loss 1.3634 (1.2655) lr 4.4774e-03 eta 0:06:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [17/30] batch [96/96] time 0.309 (0.332) data 0.000 (0.006) loss 1.3550 (1.2660) lr 3.9604e-03 eta 0:06:54
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.84s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 420
* accuracy: 72.9%
* error: 27.1%
* macro_f1: 71.6%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [2/96] time 0.330 (0.656) data 0.000 (0.283) loss 0.7747 (0.8171) lr 3.9604e-03 eta 0:13:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [4/96] time 0.334 (0.495) data 0.000 (0.142) loss 1.3985 (1.1179) lr 3.9604e-03 eta 0:10:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [6/96] time 0.343 (0.444) data 0.000 (0.095) loss 2.1868 (1.3699) lr 3.9604e-03 eta 0:09:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [8/96] time 0.332 (0.417) data 0.000 (0.071) loss 2.1880 (1.4121) lr 3.9604e-03 eta 0:08:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [10/96] time 0.338 (0.400) data 0.000 (0.057) loss 1.3489 (1.3660) lr 3.9604e-03 eta 0:08:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [12/96] time 0.331 (0.388) data 0.000 (0.048) loss 1.5024 (1.3620) lr 3.9604e-03 eta 0:07:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [14/96] time 0.317 (0.379) data 0.000 (0.041) loss 1.2603 (1.3298) lr 3.9604e-03 eta 0:07:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [16/96] time 0.322 (0.372) data 0.000 (0.036) loss 1.1557 (1.3312) lr 3.9604e-03 eta 0:07:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [18/96] time 0.323 (0.367) data 0.000 (0.032) loss 1.7661 (1.3662) lr 3.9604e-03 eta 0:07:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [20/96] time 0.335 (0.364) data 0.000 (0.029) loss 0.8335 (1.3156) lr 3.9604e-03 eta 0:07:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [22/96] time 0.317 (0.361) data 0.000 (0.026) loss 1.9918 (1.3271) lr 3.9604e-03 eta 0:07:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [24/96] time 0.328 (0.358) data 0.000 (0.024) loss 1.4453 (1.3194) lr 3.9604e-03 eta 0:07:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [26/96] time 0.332 (0.355) data 0.000 (0.022) loss 0.8759 (1.2913) lr 3.9604e-03 eta 0:07:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [28/96] time 0.321 (0.353) data 0.000 (0.021) loss 0.7852 (1.2621) lr 3.9604e-03 eta 0:07:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [30/96] time 0.328 (0.351) data 0.000 (0.019) loss 1.3847 (1.2596) lr 3.9604e-03 eta 0:07:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [32/96] time 0.337 (0.350) data 0.000 (0.018) loss 0.9908 (1.2549) lr 3.9604e-03 eta 0:07:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [34/96] time 0.322 (0.348) data 0.000 (0.017) loss 1.3669 (1.2478) lr 3.9604e-03 eta 0:07:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [36/96] time 0.414 (0.349) data 0.000 (0.016) loss 1.3138 (1.2386) lr 3.9604e-03 eta 0:07:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [38/96] time 0.325 (0.348) data 0.000 (0.015) loss 1.2651 (1.2406) lr 3.9604e-03 eta 0:07:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [40/96] time 0.340 (0.347) data 0.000 (0.014) loss 0.8393 (1.2311) lr 3.9604e-03 eta 0:06:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [42/96] time 0.334 (0.347) data 0.000 (0.014) loss 1.0703 (1.2221) lr 3.9604e-03 eta 0:06:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [44/96] time 0.326 (0.346) data 0.000 (0.013) loss 1.5402 (1.2406) lr 3.9604e-03 eta 0:06:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [46/96] time 0.316 (0.345) data 0.000 (0.013) loss 1.0824 (1.2390) lr 3.9604e-03 eta 0:06:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [48/96] time 0.333 (0.344) data 0.000 (0.012) loss 1.1047 (1.2407) lr 3.9604e-03 eta 0:06:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [50/96] time 0.338 (0.343) data 0.000 (0.012) loss 1.7521 (1.2591) lr 3.9604e-03 eta 0:06:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [52/96] time 0.334 (0.343) data 0.000 (0.011) loss 1.7409 (1.2691) lr 3.9604e-03 eta 0:06:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [54/96] time 0.331 (0.342) data 0.000 (0.011) loss 1.1731 (1.2628) lr 3.9604e-03 eta 0:06:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [56/96] time 0.324 (0.342) data 0.000 (0.010) loss 1.2503 (1.2618) lr 3.9604e-03 eta 0:06:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [58/96] time 0.329 (0.341) data 0.000 (0.010) loss 1.4603 (1.2667) lr 3.9604e-03 eta 0:06:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [60/96] time 0.333 (0.341) data 0.000 (0.010) loss 1.4510 (1.2721) lr 3.9604e-03 eta 0:06:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [62/96] time 0.315 (0.340) data 0.000 (0.009) loss 1.4340 (1.2874) lr 3.9604e-03 eta 0:06:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [64/96] time 0.327 (0.340) data 0.000 (0.009) loss 2.1457 (1.3035) lr 3.9604e-03 eta 0:06:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [66/96] time 0.335 (0.340) data 0.000 (0.009) loss 1.6417 (1.3038) lr 3.9604e-03 eta 0:06:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [68/96] time 0.316 (0.339) data 0.000 (0.009) loss 0.9806 (1.2935) lr 3.9604e-03 eta 0:06:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [70/96] time 0.317 (0.339) data 0.000 (0.008) loss 1.5082 (1.2881) lr 3.9604e-03 eta 0:06:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [72/96] time 0.329 (0.338) data 0.000 (0.008) loss 1.4072 (1.2879) lr 3.9604e-03 eta 0:06:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [74/96] time 0.304 (0.337) data 0.000 (0.008) loss 0.7824 (1.2761) lr 3.9604e-03 eta 0:06:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [76/96] time 0.308 (0.337) data 0.000 (0.008) loss 1.4928 (1.2802) lr 3.9604e-03 eta 0:06:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [78/96] time 0.307 (0.336) data 0.000 (0.008) loss 1.4442 (1.2790) lr 3.9604e-03 eta 0:06:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [80/96] time 0.304 (0.335) data 0.000 (0.007) loss 1.2993 (1.2733) lr 3.9604e-03 eta 0:06:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [82/96] time 0.305 (0.334) data 0.000 (0.007) loss 1.4631 (1.2735) lr 3.9604e-03 eta 0:06:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [84/96] time 0.302 (0.334) data 0.000 (0.007) loss 1.2105 (1.2809) lr 3.9604e-03 eta 0:06:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [86/96] time 0.305 (0.333) data 0.000 (0.007) loss 1.0493 (1.2831) lr 3.9604e-03 eta 0:06:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [88/96] time 0.300 (0.332) data 0.000 (0.007) loss 0.8620 (1.2837) lr 3.9604e-03 eta 0:06:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [90/96] time 0.311 (0.332) data 0.000 (0.007) loss 1.3854 (1.2813) lr 3.9604e-03 eta 0:06:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [92/96] time 0.303 (0.331) data 0.000 (0.006) loss 0.7961 (1.2774) lr 3.9604e-03 eta 0:06:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [94/96] time 0.304 (0.331) data 0.000 (0.006) loss 1.0139 (1.2709) lr 3.9604e-03 eta 0:06:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [18/30] batch [96/96] time 0.307 (0.330) data 0.000 (0.006) loss 1.0946 (1.2685) lr 3.4549e-03 eta 0:06:20
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.84s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.34s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 426
* accuracy: 74.0%
* error: 26.0%
* macro_f1: 72.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [2/96] time 0.308 (0.625) data 0.000 (0.262) loss 1.6405 (1.3729) lr 3.4549e-03 eta 0:11:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [4/96] time 0.347 (0.479) data 0.000 (0.131) loss 1.3387 (1.3927) lr 3.4549e-03 eta 0:09:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [6/96] time 0.345 (0.434) data 0.000 (0.087) loss 1.5927 (1.3732) lr 3.4549e-03 eta 0:08:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [8/96] time 0.335 (0.407) data 0.000 (0.066) loss 1.1367 (1.3062) lr 3.4549e-03 eta 0:07:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [10/96] time 0.327 (0.390) data 0.000 (0.053) loss 1.0550 (1.2890) lr 3.4549e-03 eta 0:07:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [12/96] time 0.322 (0.379) data 0.000 (0.044) loss 1.9923 (1.3371) lr 3.4549e-03 eta 0:07:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [14/96] time 0.329 (0.371) data 0.000 (0.038) loss 1.8528 (1.3825) lr 3.4549e-03 eta 0:07:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [16/96] time 0.330 (0.364) data 0.000 (0.033) loss 0.7213 (1.3162) lr 3.4549e-03 eta 0:06:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [18/96] time 0.320 (0.360) data 0.000 (0.029) loss 1.1084 (1.2885) lr 3.4549e-03 eta 0:06:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [20/96] time 0.321 (0.356) data 0.000 (0.026) loss 1.5435 (1.3235) lr 3.4549e-03 eta 0:06:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [22/96] time 0.317 (0.353) data 0.000 (0.024) loss 1.5152 (1.3259) lr 3.4549e-03 eta 0:06:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [24/96] time 0.325 (0.350) data 0.000 (0.022) loss 1.2981 (1.3383) lr 3.4549e-03 eta 0:06:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [26/96] time 0.322 (0.348) data 0.000 (0.020) loss 1.3177 (1.3494) lr 3.4549e-03 eta 0:06:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [28/96] time 0.316 (0.345) data 0.000 (0.019) loss 1.5730 (1.3454) lr 3.4549e-03 eta 0:06:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [30/96] time 0.318 (0.344) data 0.000 (0.018) loss 1.2193 (1.3390) lr 3.4549e-03 eta 0:06:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [32/96] time 0.328 (0.342) data 0.000 (0.017) loss 1.4311 (1.3352) lr 3.4549e-03 eta 0:06:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [34/96] time 0.334 (0.342) data 0.000 (0.016) loss 1.5548 (1.3354) lr 3.4549e-03 eta 0:06:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [36/96] time 0.421 (0.344) data 0.000 (0.015) loss 0.6869 (1.3018) lr 3.4549e-03 eta 0:06:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [38/96] time 0.317 (0.342) data 0.000 (0.014) loss 0.9760 (1.3066) lr 3.4549e-03 eta 0:06:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [40/96] time 0.334 (0.342) data 0.000 (0.013) loss 1.2068 (1.3162) lr 3.4549e-03 eta 0:06:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [42/96] time 0.344 (0.342) data 0.000 (0.013) loss 2.2357 (1.3440) lr 3.4549e-03 eta 0:06:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [44/96] time 0.338 (0.341) data 0.000 (0.012) loss 1.3137 (1.3301) lr 3.4549e-03 eta 0:06:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [46/96] time 0.323 (0.341) data 0.000 (0.012) loss 1.1738 (1.3205) lr 3.4549e-03 eta 0:06:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [48/96] time 0.325 (0.340) data 0.000 (0.011) loss 1.2667 (1.3160) lr 3.4549e-03 eta 0:06:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [50/96] time 0.320 (0.339) data 0.000 (0.011) loss 1.1803 (1.3054) lr 3.4549e-03 eta 0:06:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [52/96] time 0.332 (0.339) data 0.000 (0.010) loss 1.1442 (1.2993) lr 3.4549e-03 eta 0:06:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [54/96] time 0.333 (0.338) data 0.000 (0.010) loss 1.2005 (1.3014) lr 3.4549e-03 eta 0:06:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [56/96] time 0.320 (0.338) data 0.000 (0.010) loss 0.9339 (1.2938) lr 3.4549e-03 eta 0:06:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [58/96] time 0.321 (0.337) data 0.000 (0.009) loss 0.9210 (1.2790) lr 3.4549e-03 eta 0:06:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [60/96] time 0.326 (0.337) data 0.000 (0.009) loss 0.7331 (1.2670) lr 3.4549e-03 eta 0:06:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [62/96] time 0.332 (0.336) data 0.000 (0.009) loss 1.0099 (1.2543) lr 3.4549e-03 eta 0:06:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [64/96] time 0.319 (0.336) data 0.000 (0.008) loss 0.9739 (1.2519) lr 3.4549e-03 eta 0:06:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [66/96] time 0.318 (0.336) data 0.000 (0.008) loss 1.1709 (1.2564) lr 3.4549e-03 eta 0:06:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [68/96] time 0.328 (0.335) data 0.000 (0.008) loss 1.3867 (1.2575) lr 3.4549e-03 eta 0:06:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [70/96] time 0.331 (0.335) data 0.000 (0.008) loss 2.0631 (1.2675) lr 3.4549e-03 eta 0:06:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [72/96] time 0.313 (0.335) data 0.000 (0.008) loss 1.2326 (1.2718) lr 3.4549e-03 eta 0:06:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [74/96] time 0.309 (0.334) data 0.000 (0.007) loss 1.5370 (1.2682) lr 3.4549e-03 eta 0:05:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [76/96] time 0.301 (0.333) data 0.000 (0.007) loss 1.0424 (1.2651) lr 3.4549e-03 eta 0:05:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [78/96] time 0.306 (0.332) data 0.000 (0.007) loss 2.2997 (1.2719) lr 3.4549e-03 eta 0:05:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [80/96] time 0.308 (0.332) data 0.000 (0.007) loss 1.0117 (1.2695) lr 3.4549e-03 eta 0:05:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [82/96] time 0.304 (0.331) data 0.000 (0.007) loss 0.9179 (1.2678) lr 3.4549e-03 eta 0:05:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [84/96] time 0.306 (0.330) data 0.000 (0.007) loss 1.2449 (1.2668) lr 3.4549e-03 eta 0:05:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [86/96] time 0.303 (0.330) data 0.000 (0.006) loss 1.1705 (1.2704) lr 3.4549e-03 eta 0:05:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [88/96] time 0.307 (0.329) data 0.000 (0.006) loss 0.8488 (1.2615) lr 3.4549e-03 eta 0:05:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [90/96] time 0.306 (0.329) data 0.000 (0.006) loss 0.8408 (1.2567) lr 3.4549e-03 eta 0:05:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [92/96] time 0.300 (0.328) data 0.000 (0.006) loss 0.7589 (1.2456) lr 3.4549e-03 eta 0:05:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [94/96] time 0.303 (0.328) data 0.000 (0.006) loss 1.6326 (1.2501) lr 3.4549e-03 eta 0:05:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [19/30] batch [96/96] time 0.304 (0.327) data 0.000 (0.006) loss 1.3697 (1.2495) lr 2.9663e-03 eta 0:05:45
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.82s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.33s/it]100%|██████████| 3/3 [00:03<00:00,  1.18it/s]100%|██████████| 3/3 [00:03<00:00,  1.16s/it]=> result
* total: 576
* correct: 425
* accuracy: 73.8%
* error: 26.2%
* macro_f1: 72.7%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [2/96] time 0.334 (0.659) data 0.000 (0.281) loss 1.8062 (1.5249) lr 2.9663e-03 eta 0:11:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [4/96] time 0.344 (0.500) data 0.000 (0.141) loss 1.3430 (1.4378) lr 2.9663e-03 eta 0:08:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [6/96] time 0.346 (0.451) data 0.000 (0.094) loss 1.0870 (1.4022) lr 2.9663e-03 eta 0:07:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [8/96] time 0.339 (0.423) data 0.000 (0.070) loss 1.8221 (1.4599) lr 2.9663e-03 eta 0:07:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [10/96] time 0.339 (0.405) data 0.000 (0.056) loss 0.8786 (1.4315) lr 2.9663e-03 eta 0:07:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [12/96] time 0.334 (0.394) data 0.000 (0.047) loss 0.7673 (1.3755) lr 2.9663e-03 eta 0:06:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [14/96] time 0.338 (0.385) data 0.000 (0.040) loss 1.4048 (1.3475) lr 2.9663e-03 eta 0:06:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [16/96] time 0.322 (0.378) data 0.000 (0.035) loss 0.8670 (1.3434) lr 2.9663e-03 eta 0:06:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [18/96] time 0.334 (0.373) data 0.000 (0.031) loss 0.9214 (1.3246) lr 2.9663e-03 eta 0:06:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [20/96] time 0.334 (0.369) data 0.000 (0.028) loss 1.1824 (1.3411) lr 2.9663e-03 eta 0:06:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [22/96] time 0.324 (0.366) data 0.000 (0.026) loss 0.9512 (1.2940) lr 2.9663e-03 eta 0:06:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [24/96] time 0.330 (0.363) data 0.000 (0.024) loss 1.2084 (1.2702) lr 2.9663e-03 eta 0:06:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [26/96] time 0.321 (0.360) data 0.000 (0.022) loss 0.8309 (1.2650) lr 2.9663e-03 eta 0:06:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [28/96] time 0.322 (0.357) data 0.000 (0.020) loss 1.5836 (1.2840) lr 2.9663e-03 eta 0:06:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [30/96] time 0.328 (0.355) data 0.000 (0.019) loss 1.4336 (1.2732) lr 2.9663e-03 eta 0:06:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [32/96] time 0.327 (0.354) data 0.000 (0.018) loss 2.0196 (1.3054) lr 2.9663e-03 eta 0:06:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [34/96] time 0.321 (0.352) data 0.000 (0.017) loss 0.7304 (1.2731) lr 2.9663e-03 eta 0:05:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [36/96] time 0.434 (0.353) data 0.000 (0.016) loss 1.9557 (1.2895) lr 2.9663e-03 eta 0:06:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [38/96] time 0.340 (0.353) data 0.000 (0.015) loss 0.9510 (1.2850) lr 2.9663e-03 eta 0:05:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [40/96] time 0.341 (0.352) data 0.000 (0.014) loss 1.0461 (1.2707) lr 2.9663e-03 eta 0:05:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [42/96] time 0.321 (0.351) data 0.000 (0.014) loss 1.1914 (1.2761) lr 2.9663e-03 eta 0:05:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [44/96] time 0.326 (0.350) data 0.000 (0.013) loss 0.8436 (1.2666) lr 2.9663e-03 eta 0:05:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [46/96] time 0.349 (0.349) data 0.000 (0.013) loss 0.8827 (1.2508) lr 2.9663e-03 eta 0:05:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [48/96] time 0.343 (0.349) data 0.000 (0.012) loss 1.3468 (1.2490) lr 2.9663e-03 eta 0:05:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [50/96] time 0.333 (0.349) data 0.000 (0.012) loss 1.1786 (1.2529) lr 2.9663e-03 eta 0:05:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [52/96] time 0.347 (0.348) data 0.000 (0.011) loss 1.8610 (1.2657) lr 2.9663e-03 eta 0:05:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [54/96] time 0.346 (0.348) data 0.000 (0.011) loss 1.0604 (1.2667) lr 2.9663e-03 eta 0:05:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [56/96] time 0.338 (0.348) data 0.000 (0.010) loss 0.8379 (1.2491) lr 2.9663e-03 eta 0:05:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [58/96] time 0.332 (0.347) data 0.000 (0.010) loss 0.9105 (1.2607) lr 2.9663e-03 eta 0:05:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [60/96] time 0.343 (0.347) data 0.000 (0.010) loss 0.9154 (1.2576) lr 2.9663e-03 eta 0:05:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [62/96] time 0.361 (0.347) data 0.000 (0.009) loss 1.0166 (1.2479) lr 2.9663e-03 eta 0:05:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [64/96] time 0.343 (0.347) data 0.000 (0.009) loss 1.0185 (1.2385) lr 2.9663e-03 eta 0:05:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [66/96] time 0.342 (0.347) data 0.000 (0.009) loss 1.1070 (1.2435) lr 2.9663e-03 eta 0:05:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [68/96] time 0.338 (0.347) data 0.000 (0.009) loss 1.3071 (1.2367) lr 2.9663e-03 eta 0:05:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [70/96] time 0.346 (0.347) data 0.000 (0.008) loss 1.4052 (1.2436) lr 2.9663e-03 eta 0:05:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [72/96] time 0.322 (0.346) data 0.000 (0.008) loss 1.0866 (1.2416) lr 2.9663e-03 eta 0:05:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [74/96] time 0.309 (0.345) data 0.000 (0.008) loss 1.4723 (1.2449) lr 2.9663e-03 eta 0:05:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [76/96] time 0.308 (0.344) data 0.000 (0.008) loss 1.6936 (1.2492) lr 2.9663e-03 eta 0:05:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [78/96] time 0.312 (0.343) data 0.000 (0.008) loss 1.5181 (1.2473) lr 2.9663e-03 eta 0:05:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [80/96] time 0.313 (0.343) data 0.000 (0.007) loss 0.9294 (1.2398) lr 2.9663e-03 eta 0:05:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [82/96] time 0.304 (0.342) data 0.000 (0.007) loss 1.4911 (1.2448) lr 2.9663e-03 eta 0:05:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [84/96] time 0.313 (0.341) data 0.000 (0.007) loss 1.1711 (1.2393) lr 2.9663e-03 eta 0:05:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [86/96] time 0.312 (0.340) data 0.000 (0.007) loss 1.2391 (1.2391) lr 2.9663e-03 eta 0:05:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [88/96] time 0.309 (0.340) data 0.000 (0.007) loss 1.1903 (1.2371) lr 2.9663e-03 eta 0:05:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [90/96] time 0.317 (0.339) data 0.000 (0.007) loss 1.1870 (1.2396) lr 2.9663e-03 eta 0:05:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [92/96] time 0.310 (0.339) data 0.000 (0.006) loss 0.8166 (1.2322) lr 2.9663e-03 eta 0:05:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [94/96] time 0.312 (0.338) data 0.000 (0.006) loss 1.1056 (1.2256) lr 2.9663e-03 eta 0:05:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [20/30] batch [96/96] time 0.309 (0.337) data 0.000 (0.006) loss 0.9457 (1.2265) lr 2.5000e-03 eta 0:05:23
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.83s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.34s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.16s/it]=> result
* total: 576
* correct: 428
* accuracy: 74.3%
* error: 25.7%
* macro_f1: 73.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model.pth.tar-20

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [2/96] time 0.322 (0.644) data 0.001 (0.263) loss 0.8747 (1.0014) lr 2.5000e-03 eta 0:10:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [4/96] time 0.331 (0.488) data 0.000 (0.132) loss 0.9591 (1.0816) lr 2.5000e-03 eta 0:07:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [6/96] time 0.317 (0.432) data 0.000 (0.088) loss 1.1330 (1.0914) lr 2.5000e-03 eta 0:06:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [8/96] time 0.330 (0.405) data 0.000 (0.066) loss 1.5389 (1.2075) lr 2.5000e-03 eta 0:06:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [10/96] time 0.325 (0.389) data 0.000 (0.053) loss 1.0018 (1.1749) lr 2.5000e-03 eta 0:06:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [12/96] time 0.325 (0.379) data 0.000 (0.044) loss 0.9541 (1.1390) lr 2.5000e-03 eta 0:05:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [14/96] time 0.338 (0.373) data 0.000 (0.038) loss 1.1819 (1.1342) lr 2.5000e-03 eta 0:05:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [16/96] time 0.320 (0.367) data 0.000 (0.033) loss 0.8110 (1.1122) lr 2.5000e-03 eta 0:05:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [18/96] time 0.330 (0.364) data 0.000 (0.029) loss 1.2131 (1.1355) lr 2.5000e-03 eta 0:05:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [20/96] time 0.341 (0.361) data 0.000 (0.027) loss 1.2023 (1.1559) lr 2.5000e-03 eta 0:05:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [22/96] time 0.329 (0.358) data 0.000 (0.024) loss 1.1520 (1.1475) lr 2.5000e-03 eta 0:05:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [24/96] time 0.334 (0.356) data 0.000 (0.022) loss 1.7839 (1.1855) lr 2.5000e-03 eta 0:05:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [26/96] time 0.344 (0.354) data 0.000 (0.020) loss 1.3309 (1.2137) lr 2.5000e-03 eta 0:05:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [28/96] time 0.321 (0.351) data 0.000 (0.019) loss 1.0519 (1.2231) lr 2.5000e-03 eta 0:05:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [30/96] time 0.333 (0.351) data 0.000 (0.018) loss 0.8717 (1.2007) lr 2.5000e-03 eta 0:05:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [32/96] time 0.323 (0.349) data 0.000 (0.017) loss 1.4818 (1.2027) lr 2.5000e-03 eta 0:05:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [34/96] time 0.327 (0.347) data 0.000 (0.016) loss 1.2648 (1.2017) lr 2.5000e-03 eta 0:05:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [36/96] time 0.422 (0.349) data 0.000 (0.015) loss 1.0890 (1.1925) lr 2.5000e-03 eta 0:05:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [38/96] time 0.330 (0.348) data 0.000 (0.014) loss 0.9678 (1.1832) lr 2.5000e-03 eta 0:05:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [40/96] time 0.342 (0.347) data 0.000 (0.013) loss 0.9480 (1.1653) lr 2.5000e-03 eta 0:05:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [42/96] time 0.323 (0.346) data 0.000 (0.013) loss 1.6420 (1.1782) lr 2.5000e-03 eta 0:05:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [44/96] time 0.327 (0.345) data 0.000 (0.012) loss 0.9187 (1.1619) lr 2.5000e-03 eta 0:05:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [46/96] time 0.322 (0.344) data 0.000 (0.012) loss 1.1889 (1.1801) lr 2.5000e-03 eta 0:05:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [48/96] time 0.322 (0.343) data 0.000 (0.011) loss 0.6709 (1.1759) lr 2.5000e-03 eta 0:05:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [50/96] time 0.334 (0.343) data 0.000 (0.011) loss 1.2330 (1.1791) lr 2.5000e-03 eta 0:05:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [52/96] time 0.332 (0.343) data 0.000 (0.010) loss 0.7645 (1.1699) lr 2.5000e-03 eta 0:05:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [54/96] time 0.338 (0.343) data 0.001 (0.010) loss 1.0234 (1.1679) lr 2.5000e-03 eta 0:05:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [56/96] time 0.324 (0.342) data 0.000 (0.010) loss 0.9681 (1.1612) lr 2.5000e-03 eta 0:05:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [58/96] time 0.316 (0.341) data 0.000 (0.009) loss 0.8912 (1.1541) lr 2.5000e-03 eta 0:05:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [60/96] time 0.343 (0.341) data 0.000 (0.009) loss 0.7118 (1.1522) lr 2.5000e-03 eta 0:05:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [62/96] time 0.322 (0.340) data 0.000 (0.009) loss 1.7546 (1.1674) lr 2.5000e-03 eta 0:05:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [64/96] time 0.326 (0.340) data 0.000 (0.009) loss 1.1230 (1.1771) lr 2.5000e-03 eta 0:05:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [66/96] time 0.317 (0.339) data 0.000 (0.008) loss 1.1598 (1.1774) lr 2.5000e-03 eta 0:05:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [68/96] time 0.322 (0.339) data 0.000 (0.008) loss 0.7337 (1.1649) lr 2.5000e-03 eta 0:05:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [70/96] time 0.316 (0.338) data 0.000 (0.008) loss 1.3225 (1.1718) lr 2.5000e-03 eta 0:05:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [72/96] time 0.343 (0.338) data 0.000 (0.008) loss 0.7742 (1.1668) lr 2.5000e-03 eta 0:05:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [74/96] time 0.307 (0.337) data 0.000 (0.007) loss 1.1028 (1.1700) lr 2.5000e-03 eta 0:04:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [76/96] time 0.304 (0.336) data 0.000 (0.007) loss 0.6402 (1.1605) lr 2.5000e-03 eta 0:04:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [78/96] time 0.309 (0.336) data 0.000 (0.007) loss 1.5083 (1.1664) lr 2.5000e-03 eta 0:04:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [80/96] time 0.301 (0.335) data 0.000 (0.007) loss 1.3687 (1.1712) lr 2.5000e-03 eta 0:04:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [82/96] time 0.306 (0.334) data 0.000 (0.007) loss 1.4627 (1.1755) lr 2.5000e-03 eta 0:04:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [84/96] time 0.306 (0.333) data 0.000 (0.007) loss 1.2894 (1.1728) lr 2.5000e-03 eta 0:04:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [86/96] time 0.299 (0.333) data 0.000 (0.006) loss 1.5209 (1.1739) lr 2.5000e-03 eta 0:04:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [88/96] time 0.308 (0.332) data 0.000 (0.006) loss 1.2056 (1.1725) lr 2.5000e-03 eta 0:04:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [90/96] time 0.303 (0.331) data 0.000 (0.006) loss 1.7395 (1.1775) lr 2.5000e-03 eta 0:04:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [92/96] time 0.308 (0.331) data 0.000 (0.006) loss 1.8402 (1.1851) lr 2.5000e-03 eta 0:04:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [94/96] time 0.306 (0.330) data 0.000 (0.006) loss 1.6397 (1.1916) lr 2.5000e-03 eta 0:04:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [21/30] batch [96/96] time 0.311 (0.330) data 0.000 (0.006) loss 0.8381 (1.1949) lr 2.0611e-03 eta 0:04:45
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.88s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.19s/it]=> result
* total: 576
* correct: 428
* accuracy: 74.3%
* error: 25.7%
* macro_f1: 73.6%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [2/96] time 0.338 (0.669) data 0.001 (0.278) loss 1.0221 (1.1423) lr 2.0611e-03 eta 0:09:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [4/96] time 0.329 (0.498) data 0.000 (0.139) loss 1.0143 (1.1326) lr 2.0611e-03 eta 0:07:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [6/96] time 0.334 (0.442) data 0.000 (0.093) loss 1.1351 (1.1596) lr 2.0611e-03 eta 0:06:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [8/96] time 0.332 (0.414) data 0.000 (0.070) loss 1.5999 (1.1948) lr 2.0611e-03 eta 0:05:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [10/96] time 0.331 (0.398) data 0.000 (0.056) loss 0.9896 (1.1450) lr 2.0611e-03 eta 0:05:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [12/96] time 0.339 (0.389) data 0.000 (0.047) loss 0.7902 (1.1016) lr 2.0611e-03 eta 0:05:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [14/96] time 0.342 (0.382) data 0.000 (0.040) loss 1.3630 (1.1040) lr 2.0611e-03 eta 0:05:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [16/96] time 0.325 (0.375) data 0.000 (0.035) loss 1.0973 (1.1161) lr 2.0611e-03 eta 0:05:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [18/96] time 0.331 (0.371) data 0.000 (0.031) loss 1.2763 (1.1772) lr 2.0611e-03 eta 0:05:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [20/96] time 0.332 (0.367) data 0.000 (0.028) loss 1.0055 (1.1678) lr 2.0611e-03 eta 0:05:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [22/96] time 0.334 (0.364) data 0.000 (0.026) loss 0.8757 (1.1472) lr 2.0611e-03 eta 0:05:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [24/96] time 0.334 (0.361) data 0.000 (0.023) loss 0.7981 (1.1428) lr 2.0611e-03 eta 0:05:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [26/96] time 0.322 (0.358) data 0.000 (0.022) loss 0.9219 (1.1520) lr 2.0611e-03 eta 0:05:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [28/96] time 0.323 (0.357) data 0.000 (0.020) loss 1.0954 (1.1470) lr 2.0611e-03 eta 0:04:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [30/96] time 0.324 (0.354) data 0.000 (0.019) loss 1.2235 (1.1476) lr 2.0611e-03 eta 0:04:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [32/96] time 0.324 (0.353) data 0.000 (0.018) loss 1.5889 (1.1657) lr 2.0611e-03 eta 0:04:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [34/96] time 0.331 (0.352) data 0.000 (0.017) loss 1.1408 (1.1678) lr 2.0611e-03 eta 0:04:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [36/96] time 0.421 (0.353) data 0.000 (0.016) loss 1.3095 (1.1677) lr 2.0611e-03 eta 0:04:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [38/96] time 0.310 (0.351) data 0.000 (0.015) loss 1.4618 (1.1755) lr 2.0611e-03 eta 0:04:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [40/96] time 0.316 (0.349) data 0.000 (0.014) loss 0.8090 (1.1640) lr 2.0611e-03 eta 0:04:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [42/96] time 0.326 (0.348) data 0.000 (0.014) loss 1.3246 (1.1691) lr 2.0611e-03 eta 0:04:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [44/96] time 0.325 (0.347) data 0.000 (0.013) loss 1.0221 (1.1759) lr 2.0611e-03 eta 0:04:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [46/96] time 0.321 (0.346) data 0.000 (0.012) loss 1.0946 (1.1747) lr 2.0611e-03 eta 0:04:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [48/96] time 0.314 (0.345) data 0.000 (0.012) loss 2.3773 (1.2042) lr 2.0611e-03 eta 0:04:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [50/96] time 0.332 (0.344) data 0.000 (0.011) loss 1.2357 (1.2150) lr 2.0611e-03 eta 0:04:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [52/96] time 0.322 (0.343) data 0.000 (0.011) loss 1.6937 (1.2265) lr 2.0611e-03 eta 0:04:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [54/96] time 0.342 (0.343) data 0.001 (0.011) loss 1.1291 (1.2179) lr 2.0611e-03 eta 0:04:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [56/96] time 0.319 (0.342) data 0.000 (0.010) loss 0.8971 (1.2069) lr 2.0611e-03 eta 0:04:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [58/96] time 0.335 (0.342) data 0.000 (0.010) loss 1.0611 (1.2087) lr 2.0611e-03 eta 0:04:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [60/96] time 0.329 (0.342) data 0.000 (0.010) loss 0.9248 (1.2088) lr 2.0611e-03 eta 0:04:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [62/96] time 0.336 (0.342) data 0.000 (0.009) loss 1.5442 (1.2178) lr 2.0611e-03 eta 0:04:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [64/96] time 0.321 (0.341) data 0.000 (0.009) loss 0.8252 (1.2129) lr 2.0611e-03 eta 0:04:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [66/96] time 0.318 (0.341) data 0.000 (0.009) loss 0.9142 (1.2047) lr 2.0611e-03 eta 0:04:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [68/96] time 0.333 (0.340) data 0.000 (0.008) loss 1.3817 (1.2037) lr 2.0611e-03 eta 0:04:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [70/96] time 0.327 (0.340) data 0.000 (0.008) loss 0.9957 (1.2006) lr 2.0611e-03 eta 0:04:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [72/96] time 0.328 (0.339) data 0.000 (0.008) loss 1.7822 (1.2022) lr 2.0611e-03 eta 0:04:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [74/96] time 0.305 (0.339) data 0.000 (0.008) loss 1.0007 (1.1962) lr 2.0611e-03 eta 0:04:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [76/96] time 0.302 (0.338) data 0.000 (0.008) loss 1.7386 (1.2057) lr 2.0611e-03 eta 0:04:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [78/96] time 0.308 (0.337) data 0.000 (0.007) loss 1.1219 (1.2074) lr 2.0611e-03 eta 0:04:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [80/96] time 0.305 (0.336) data 0.000 (0.007) loss 1.7005 (1.2192) lr 2.0611e-03 eta 0:04:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [82/96] time 0.310 (0.335) data 0.000 (0.007) loss 1.5804 (1.2205) lr 2.0611e-03 eta 0:04:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [84/96] time 0.304 (0.335) data 0.000 (0.007) loss 1.0814 (1.2151) lr 2.0611e-03 eta 0:04:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [86/96] time 0.306 (0.334) data 0.000 (0.007) loss 1.0816 (1.2109) lr 2.0611e-03 eta 0:04:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [88/96] time 0.304 (0.333) data 0.000 (0.007) loss 1.4091 (1.2108) lr 2.0611e-03 eta 0:04:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [90/96] time 0.316 (0.333) data 0.000 (0.006) loss 1.0735 (1.2081) lr 2.0611e-03 eta 0:04:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [92/96] time 0.324 (0.333) data 0.000 (0.006) loss 1.0023 (1.2047) lr 2.0611e-03 eta 0:04:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [94/96] time 0.307 (0.332) data 0.000 (0.006) loss 1.1395 (1.2003) lr 2.0611e-03 eta 0:04:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [22/30] batch [96/96] time 0.303 (0.332) data 0.000 (0.006) loss 1.2205 (1.2032) lr 1.6543e-03 eta 0:04:14
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.82s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.33s/it]100%|██████████| 3/3 [00:03<00:00,  1.18it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 428
* accuracy: 74.3%
* error: 25.7%
* macro_f1: 73.6%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [2/96] time 0.328 (0.658) data 0.001 (0.276) loss 1.1519 (1.2872) lr 1.6543e-03 eta 0:08:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [4/96] time 0.319 (0.486) data 0.000 (0.138) loss 0.8305 (1.1020) lr 1.6543e-03 eta 0:06:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [6/96] time 0.329 (0.432) data 0.000 (0.092) loss 1.3326 (1.1534) lr 1.6543e-03 eta 0:05:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [8/96] time 0.342 (0.407) data 0.000 (0.069) loss 1.2338 (1.2066) lr 1.6543e-03 eta 0:05:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [10/96] time 0.322 (0.390) data 0.000 (0.056) loss 1.4030 (1.2043) lr 1.6543e-03 eta 0:04:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [12/96] time 0.331 (0.380) data 0.000 (0.046) loss 0.6837 (1.1804) lr 1.6543e-03 eta 0:04:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [14/96] time 0.326 (0.372) data 0.000 (0.040) loss 0.9946 (1.1561) lr 1.6543e-03 eta 0:04:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [16/96] time 0.327 (0.366) data 0.000 (0.035) loss 1.2450 (1.1636) lr 1.6543e-03 eta 0:04:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [18/96] time 0.324 (0.361) data 0.000 (0.031) loss 1.2023 (1.1776) lr 1.6543e-03 eta 0:04:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [20/96] time 0.313 (0.356) data 0.000 (0.028) loss 0.9884 (1.1637) lr 1.6543e-03 eta 0:04:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [22/96] time 0.339 (0.354) data 0.000 (0.025) loss 1.3991 (1.1915) lr 1.6543e-03 eta 0:04:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [24/96] time 0.330 (0.352) data 0.000 (0.023) loss 1.4392 (1.1943) lr 1.6543e-03 eta 0:04:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [26/96] time 0.323 (0.350) data 0.000 (0.022) loss 1.2122 (1.1954) lr 1.6543e-03 eta 0:04:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [28/96] time 0.323 (0.347) data 0.000 (0.020) loss 0.8464 (1.1683) lr 1.6543e-03 eta 0:04:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [30/96] time 0.317 (0.346) data 0.000 (0.019) loss 1.4689 (1.1891) lr 1.6543e-03 eta 0:04:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [32/96] time 0.317 (0.344) data 0.000 (0.018) loss 0.9640 (1.1837) lr 1.6543e-03 eta 0:04:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [34/96] time 0.334 (0.343) data 0.000 (0.017) loss 0.7533 (1.1760) lr 1.6543e-03 eta 0:04:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [36/96] time 0.431 (0.346) data 0.000 (0.016) loss 0.7630 (1.1597) lr 1.6543e-03 eta 0:04:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [38/96] time 0.332 (0.345) data 0.000 (0.015) loss 1.6804 (1.1668) lr 1.6543e-03 eta 0:04:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [40/96] time 0.320 (0.343) data 0.000 (0.014) loss 0.8012 (1.1481) lr 1.6543e-03 eta 0:04:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [42/96] time 0.316 (0.343) data 0.000 (0.013) loss 1.2467 (1.1561) lr 1.6543e-03 eta 0:04:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [44/96] time 0.322 (0.342) data 0.000 (0.013) loss 0.9333 (1.1658) lr 1.6543e-03 eta 0:04:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [46/96] time 0.318 (0.341) data 0.000 (0.012) loss 1.1794 (1.1714) lr 1.6543e-03 eta 0:04:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [48/96] time 0.312 (0.340) data 0.000 (0.012) loss 1.1283 (1.1705) lr 1.6543e-03 eta 0:04:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [50/96] time 0.324 (0.339) data 0.000 (0.011) loss 1.0815 (1.1658) lr 1.6543e-03 eta 0:04:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [52/96] time 0.318 (0.338) data 0.000 (0.011) loss 1.2070 (1.1737) lr 1.6543e-03 eta 0:04:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [54/96] time 0.320 (0.338) data 0.000 (0.011) loss 1.3224 (1.1677) lr 1.6543e-03 eta 0:04:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [56/96] time 0.319 (0.337) data 0.000 (0.010) loss 1.2366 (1.1715) lr 1.6543e-03 eta 0:03:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [58/96] time 0.319 (0.336) data 0.000 (0.010) loss 0.7823 (1.1657) lr 1.6543e-03 eta 0:03:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [60/96] time 0.322 (0.336) data 0.000 (0.010) loss 2.1100 (1.1798) lr 1.6543e-03 eta 0:03:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [62/96] time 0.318 (0.335) data 0.000 (0.009) loss 0.8234 (1.1724) lr 1.6543e-03 eta 0:03:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [64/96] time 0.330 (0.335) data 0.000 (0.009) loss 0.9535 (1.1707) lr 1.6543e-03 eta 0:03:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [66/96] time 0.317 (0.334) data 0.000 (0.009) loss 1.3064 (1.1691) lr 1.6543e-03 eta 0:03:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [68/96] time 0.326 (0.334) data 0.000 (0.008) loss 1.2781 (1.1645) lr 1.6543e-03 eta 0:03:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [70/96] time 0.317 (0.334) data 0.000 (0.008) loss 1.4529 (1.1687) lr 1.6543e-03 eta 0:03:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [72/96] time 0.318 (0.333) data 0.000 (0.008) loss 1.3127 (1.1772) lr 1.6543e-03 eta 0:03:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [74/96] time 0.305 (0.332) data 0.000 (0.008) loss 0.8777 (1.1698) lr 1.6543e-03 eta 0:03:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [76/96] time 0.307 (0.332) data 0.000 (0.008) loss 1.1411 (1.1646) lr 1.6543e-03 eta 0:03:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [78/96] time 0.302 (0.331) data 0.000 (0.007) loss 1.0047 (1.1625) lr 1.6543e-03 eta 0:03:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [80/96] time 0.307 (0.330) data 0.000 (0.007) loss 1.2245 (1.1649) lr 1.6543e-03 eta 0:03:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [82/96] time 0.308 (0.330) data 0.000 (0.007) loss 1.0090 (1.1606) lr 1.6543e-03 eta 0:03:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [84/96] time 0.304 (0.329) data 0.000 (0.007) loss 0.6947 (1.1597) lr 1.6543e-03 eta 0:03:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [86/96] time 0.306 (0.329) data 0.000 (0.007) loss 1.5040 (1.1627) lr 1.6543e-03 eta 0:03:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [88/96] time 0.298 (0.328) data 0.000 (0.007) loss 1.7035 (1.1718) lr 1.6543e-03 eta 0:03:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [90/96] time 0.307 (0.327) data 0.000 (0.006) loss 0.7829 (1.1662) lr 1.6543e-03 eta 0:03:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [92/96] time 0.303 (0.327) data 0.000 (0.006) loss 1.0184 (1.1637) lr 1.6543e-03 eta 0:03:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [94/96] time 0.303 (0.326) data 0.000 (0.006) loss 1.9504 (1.1730) lr 1.6543e-03 eta 0:03:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [23/30] batch [96/96] time 0.309 (0.326) data 0.000 (0.006) loss 0.9944 (1.1711) lr 1.2843e-03 eta 0:03:39
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.85s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 433
* accuracy: 75.2%
* error: 24.8%
* macro_f1: 74.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [2/96] time 0.318 (0.647) data 0.000 (0.282) loss 1.9381 (1.3464) lr 1.2843e-03 eta 0:07:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [4/96] time 0.325 (0.487) data 0.000 (0.141) loss 1.3506 (1.3520) lr 1.2843e-03 eta 0:05:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [6/96] time 0.331 (0.434) data 0.000 (0.094) loss 1.0618 (1.2289) lr 1.2843e-03 eta 0:04:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [8/96] time 0.316 (0.405) data 0.000 (0.071) loss 0.9945 (1.1722) lr 1.2843e-03 eta 0:04:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [10/96] time 0.324 (0.387) data 0.000 (0.057) loss 0.8026 (1.1276) lr 1.2843e-03 eta 0:04:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [12/96] time 0.324 (0.378) data 0.000 (0.047) loss 0.9155 (1.1581) lr 1.2843e-03 eta 0:04:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [14/96] time 0.311 (0.369) data 0.000 (0.041) loss 0.7677 (1.1156) lr 1.2843e-03 eta 0:04:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [16/96] time 0.331 (0.364) data 0.000 (0.035) loss 0.8258 (1.1344) lr 1.2843e-03 eta 0:03:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [18/96] time 0.328 (0.359) data 0.000 (0.032) loss 1.5057 (1.1652) lr 1.2843e-03 eta 0:03:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [20/96] time 0.334 (0.356) data 0.000 (0.028) loss 1.4216 (1.1746) lr 1.2843e-03 eta 0:03:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [22/96] time 0.321 (0.353) data 0.000 (0.026) loss 0.9294 (1.1593) lr 1.2843e-03 eta 0:03:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [24/96] time 0.319 (0.351) data 0.000 (0.024) loss 1.0905 (1.1507) lr 1.2843e-03 eta 0:03:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [26/96] time 0.320 (0.349) data 0.000 (0.022) loss 0.7004 (1.1274) lr 1.2843e-03 eta 0:03:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [28/96] time 0.343 (0.348) data 0.000 (0.020) loss 1.4666 (1.1346) lr 1.2843e-03 eta 0:03:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [30/96] time 0.319 (0.346) data 0.000 (0.019) loss 1.0072 (1.1441) lr 1.2843e-03 eta 0:03:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [32/96] time 0.320 (0.345) data 0.000 (0.018) loss 0.9800 (1.1606) lr 1.2843e-03 eta 0:03:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [34/96] time 0.326 (0.344) data 0.000 (0.017) loss 1.0225 (1.1562) lr 1.2843e-03 eta 0:03:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [36/96] time 0.335 (0.343) data 0.000 (0.016) loss 1.5377 (1.1568) lr 1.2843e-03 eta 0:03:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [38/96] time 0.322 (0.342) data 0.000 (0.015) loss 1.0476 (1.1547) lr 1.2843e-03 eta 0:03:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [40/96] time 0.325 (0.341) data 0.000 (0.014) loss 0.9698 (1.1450) lr 1.2843e-03 eta 0:03:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [42/96] time 0.334 (0.341) data 0.000 (0.014) loss 1.6803 (1.1642) lr 1.2843e-03 eta 0:03:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [44/96] time 0.320 (0.340) data 0.000 (0.013) loss 0.9186 (1.1613) lr 1.2843e-03 eta 0:03:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [46/96] time 0.329 (0.339) data 0.000 (0.013) loss 1.0428 (1.1527) lr 1.2843e-03 eta 0:03:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [48/96] time 0.322 (0.341) data 0.000 (0.012) loss 0.8798 (1.1553) lr 1.2843e-03 eta 0:03:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [50/96] time 0.325 (0.340) data 0.000 (0.012) loss 1.0580 (1.1608) lr 1.2843e-03 eta 0:03:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [52/96] time 0.327 (0.340) data 0.000 (0.011) loss 0.9310 (1.1718) lr 1.2843e-03 eta 0:03:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [54/96] time 0.314 (0.339) data 0.000 (0.011) loss 0.8755 (1.1748) lr 1.2843e-03 eta 0:03:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [56/96] time 0.315 (0.338) data 0.000 (0.010) loss 1.2157 (1.1719) lr 1.2843e-03 eta 0:03:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [58/96] time 0.328 (0.338) data 0.000 (0.010) loss 1.6351 (1.1758) lr 1.2843e-03 eta 0:03:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [60/96] time 0.325 (0.337) data 0.000 (0.010) loss 1.1490 (1.1741) lr 1.2843e-03 eta 0:03:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [62/96] time 0.332 (0.337) data 0.000 (0.009) loss 0.9154 (1.1634) lr 1.2843e-03 eta 0:03:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [64/96] time 0.333 (0.337) data 0.000 (0.009) loss 1.0464 (1.1558) lr 1.2843e-03 eta 0:03:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [66/96] time 0.328 (0.337) data 0.000 (0.009) loss 1.0351 (1.1518) lr 1.2843e-03 eta 0:03:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [68/96] time 0.336 (0.336) data 0.000 (0.009) loss 0.7847 (1.1421) lr 1.2843e-03 eta 0:03:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [70/96] time 0.344 (0.336) data 0.000 (0.008) loss 1.2310 (1.1384) lr 1.2843e-03 eta 0:03:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [72/96] time 0.336 (0.336) data 0.000 (0.008) loss 0.9342 (1.1347) lr 1.2843e-03 eta 0:03:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [74/96] time 0.324 (0.336) data 0.000 (0.008) loss 2.0409 (1.1470) lr 1.2843e-03 eta 0:03:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [76/96] time 0.317 (0.336) data 0.000 (0.008) loss 1.2191 (1.1459) lr 1.2843e-03 eta 0:03:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [78/96] time 0.317 (0.335) data 0.000 (0.008) loss 1.0752 (1.1465) lr 1.2843e-03 eta 0:03:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [80/96] time 0.325 (0.335) data 0.000 (0.007) loss 1.4643 (1.1481) lr 1.2843e-03 eta 0:03:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [82/96] time 0.317 (0.335) data 0.000 (0.007) loss 1.1939 (1.1470) lr 1.2843e-03 eta 0:03:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [84/96] time 0.324 (0.334) data 0.000 (0.007) loss 0.9885 (1.1438) lr 1.2843e-03 eta 0:03:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [86/96] time 0.324 (0.334) data 0.000 (0.007) loss 1.3136 (1.1489) lr 1.2843e-03 eta 0:03:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [88/96] time 0.318 (0.334) data 0.000 (0.007) loss 0.9334 (1.1546) lr 1.2843e-03 eta 0:03:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [90/96] time 0.323 (0.333) data 0.000 (0.007) loss 1.1352 (1.1574) lr 1.2843e-03 eta 0:03:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [92/96] time 0.320 (0.333) data 0.000 (0.006) loss 1.7199 (1.1642) lr 1.2843e-03 eta 0:03:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [94/96] time 0.318 (0.333) data 0.000 (0.006) loss 1.1844 (1.1632) lr 1.2843e-03 eta 0:03:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [24/30] batch [96/96] time 0.324 (0.333) data 0.000 (0.006) loss 1.2303 (1.1595) lr 9.5492e-04 eta 0:03:11
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.85s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 437
* accuracy: 75.9%
* error: 24.1%
* macro_f1: 75.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [2/96] time 0.348 (0.672) data 0.000 (0.279) loss 0.7275 (0.8265) lr 9.5492e-04 eta 0:06:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [4/96] time 0.349 (0.517) data 0.000 (0.140) loss 1.3540 (1.1888) lr 9.5492e-04 eta 0:04:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [6/96] time 0.319 (0.452) data 0.000 (0.093) loss 1.2649 (1.2584) lr 9.5492e-04 eta 0:04:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [8/96] time 0.424 (0.433) data 0.000 (0.070) loss 1.3148 (1.2707) lr 9.5492e-04 eta 0:04:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [10/96] time 0.330 (0.413) data 0.000 (0.056) loss 1.5052 (1.2567) lr 9.5492e-04 eta 0:03:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [12/96] time 0.337 (0.400) data 0.000 (0.047) loss 1.5513 (1.2769) lr 9.5492e-04 eta 0:03:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [14/96] time 0.333 (0.390) data 0.000 (0.040) loss 1.1346 (1.2880) lr 9.5492e-04 eta 0:03:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [16/96] time 0.333 (0.383) data 0.000 (0.035) loss 0.8791 (1.2523) lr 9.5492e-04 eta 0:03:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [18/96] time 0.323 (0.377) data 0.000 (0.031) loss 0.7289 (1.2084) lr 9.5492e-04 eta 0:03:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [20/96] time 0.329 (0.372) data 0.000 (0.028) loss 1.4552 (1.2090) lr 9.5492e-04 eta 0:03:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [22/96] time 0.320 (0.368) data 0.000 (0.026) loss 0.8850 (1.2596) lr 9.5492e-04 eta 0:03:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [24/96] time 0.315 (0.364) data 0.000 (0.024) loss 1.5245 (1.2699) lr 9.5492e-04 eta 0:03:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [26/96] time 0.331 (0.361) data 0.000 (0.022) loss 0.6723 (1.2409) lr 9.5492e-04 eta 0:03:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [28/96] time 0.329 (0.358) data 0.000 (0.020) loss 1.1155 (1.2292) lr 9.5492e-04 eta 0:03:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [30/96] time 0.313 (0.357) data 0.000 (0.019) loss 1.4236 (1.2276) lr 9.5492e-04 eta 0:03:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [32/96] time 0.351 (0.356) data 0.000 (0.018) loss 0.8806 (1.2254) lr 9.5492e-04 eta 0:03:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [34/96] time 0.342 (0.355) data 0.000 (0.017) loss 0.7594 (1.2009) lr 9.5492e-04 eta 0:03:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [36/96] time 0.348 (0.355) data 0.000 (0.016) loss 1.1314 (1.1998) lr 9.5492e-04 eta 0:03:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [38/96] time 0.331 (0.353) data 0.000 (0.015) loss 1.3893 (1.1974) lr 9.5492e-04 eta 0:03:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [40/96] time 0.326 (0.352) data 0.000 (0.014) loss 1.0835 (1.1953) lr 9.5492e-04 eta 0:03:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [42/96] time 0.319 (0.351) data 0.000 (0.014) loss 0.8983 (1.1928) lr 9.5492e-04 eta 0:03:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [44/96] time 0.334 (0.350) data 0.000 (0.013) loss 1.8136 (1.2067) lr 9.5492e-04 eta 0:03:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [46/96] time 0.333 (0.350) data 0.000 (0.012) loss 0.8449 (1.1932) lr 9.5492e-04 eta 0:03:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [48/96] time 0.328 (0.349) data 0.000 (0.012) loss 1.0708 (1.1992) lr 9.5492e-04 eta 0:03:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [50/96] time 0.362 (0.348) data 0.000 (0.011) loss 1.0343 (1.2005) lr 9.5492e-04 eta 0:03:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [52/96] time 0.328 (0.348) data 0.000 (0.011) loss 0.9556 (1.2040) lr 9.5492e-04 eta 0:03:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [54/96] time 0.325 (0.347) data 0.000 (0.011) loss 1.4257 (1.2277) lr 9.5492e-04 eta 0:03:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [56/96] time 0.329 (0.346) data 0.000 (0.010) loss 1.6358 (1.2316) lr 9.5492e-04 eta 0:03:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [58/96] time 0.319 (0.345) data 0.001 (0.010) loss 2.1320 (1.2507) lr 9.5492e-04 eta 0:02:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [60/96] time 0.326 (0.345) data 0.000 (0.010) loss 1.2856 (1.2512) lr 9.5492e-04 eta 0:02:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [62/96] time 0.327 (0.344) data 0.000 (0.009) loss 1.2442 (1.2466) lr 9.5492e-04 eta 0:02:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [64/96] time 0.346 (0.344) data 0.000 (0.009) loss 1.0472 (1.2455) lr 9.5492e-04 eta 0:02:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [66/96] time 0.417 (0.345) data 0.000 (0.009) loss 0.8327 (1.2376) lr 9.5492e-04 eta 0:02:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [68/96] time 0.323 (0.344) data 0.000 (0.009) loss 1.3387 (1.2350) lr 9.5492e-04 eta 0:02:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [70/96] time 0.333 (0.344) data 0.000 (0.008) loss 1.3617 (1.2352) lr 9.5492e-04 eta 0:02:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [72/96] time 0.323 (0.344) data 0.000 (0.008) loss 0.8395 (1.2287) lr 9.5492e-04 eta 0:02:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [74/96] time 0.307 (0.343) data 0.000 (0.008) loss 0.9248 (1.2215) lr 9.5492e-04 eta 0:02:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [76/96] time 0.310 (0.342) data 0.000 (0.008) loss 0.9129 (1.2157) lr 9.5492e-04 eta 0:02:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [78/96] time 0.311 (0.341) data 0.000 (0.007) loss 0.8679 (1.2077) lr 9.5492e-04 eta 0:02:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [80/96] time 0.312 (0.340) data 0.000 (0.007) loss 0.8407 (1.2005) lr 9.5492e-04 eta 0:02:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [82/96] time 0.309 (0.339) data 0.000 (0.007) loss 0.8086 (1.1892) lr 9.5492e-04 eta 0:02:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [84/96] time 0.303 (0.338) data 0.000 (0.007) loss 0.9728 (1.1829) lr 9.5492e-04 eta 0:02:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [86/96] time 0.311 (0.338) data 0.000 (0.007) loss 1.1647 (1.1850) lr 9.5492e-04 eta 0:02:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [88/96] time 0.304 (0.337) data 0.000 (0.007) loss 1.1838 (1.1860) lr 9.5492e-04 eta 0:02:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [90/96] time 0.311 (0.336) data 0.000 (0.007) loss 1.4251 (1.1840) lr 9.5492e-04 eta 0:02:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [92/96] time 0.313 (0.336) data 0.000 (0.006) loss 0.9865 (1.1822) lr 9.5492e-04 eta 0:02:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [94/96] time 0.309 (0.335) data 0.000 (0.006) loss 1.1056 (1.1854) lr 9.5492e-04 eta 0:02:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [25/30] batch [96/96] time 0.310 (0.335) data 0.000 (0.006) loss 1.5751 (1.1900) lr 6.6987e-04 eta 0:02:40
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.83s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.34s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.16s/it]=> result
* total: 576
* correct: 433
* accuracy: 75.2%
* error: 24.8%
* macro_f1: 74.5%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [2/96] time 0.314 (0.657) data 0.001 (0.280) loss 0.7233 (0.9767) lr 6.6987e-04 eta 0:05:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [4/96] time 0.341 (0.492) data 0.000 (0.140) loss 1.3414 (1.2290) lr 6.6987e-04 eta 0:03:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [6/96] time 0.331 (0.438) data 0.000 (0.093) loss 1.3165 (1.2475) lr 6.6987e-04 eta 0:03:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [8/96] time 0.326 (0.411) data 0.000 (0.070) loss 1.1337 (1.2220) lr 6.6987e-04 eta 0:03:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [10/96] time 0.319 (0.393) data 0.000 (0.056) loss 0.7848 (1.1626) lr 6.6987e-04 eta 0:03:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [12/96] time 0.327 (0.383) data 0.000 (0.047) loss 1.3442 (1.1920) lr 6.6987e-04 eta 0:02:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [14/96] time 0.319 (0.374) data 0.000 (0.040) loss 2.0131 (1.2950) lr 6.6987e-04 eta 0:02:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [16/96] time 0.324 (0.367) data 0.000 (0.035) loss 1.2406 (1.2888) lr 6.6987e-04 eta 0:02:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [18/96] time 0.331 (0.363) data 0.000 (0.031) loss 1.5107 (1.2901) lr 6.6987e-04 eta 0:02:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [20/96] time 0.334 (0.360) data 0.000 (0.028) loss 1.1117 (1.3349) lr 6.6987e-04 eta 0:02:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [22/96] time 0.334 (0.357) data 0.000 (0.026) loss 1.3290 (1.3176) lr 6.6987e-04 eta 0:02:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [24/96] time 0.321 (0.354) data 0.000 (0.024) loss 0.9052 (1.3033) lr 6.6987e-04 eta 0:02:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [26/96] time 0.356 (0.353) data 0.000 (0.022) loss 1.6681 (1.3178) lr 6.6987e-04 eta 0:02:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [28/96] time 0.326 (0.351) data 0.000 (0.020) loss 1.3521 (1.2988) lr 6.6987e-04 eta 0:02:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [30/96] time 0.323 (0.349) data 0.000 (0.019) loss 0.7734 (1.2650) lr 6.6987e-04 eta 0:02:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [32/96] time 0.328 (0.348) data 0.000 (0.018) loss 1.2350 (1.2678) lr 6.6987e-04 eta 0:02:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [34/96] time 0.317 (0.347) data 0.000 (0.017) loss 1.2396 (1.2599) lr 6.6987e-04 eta 0:02:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [36/96] time 0.324 (0.346) data 0.000 (0.016) loss 1.1451 (1.2472) lr 6.6987e-04 eta 0:02:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [38/96] time 0.331 (0.345) data 0.000 (0.015) loss 1.1349 (1.2394) lr 6.6987e-04 eta 0:02:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [40/96] time 0.328 (0.344) data 0.000 (0.014) loss 1.4228 (1.2668) lr 6.6987e-04 eta 0:02:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [42/96] time 0.349 (0.344) data 0.000 (0.014) loss 1.0640 (1.2597) lr 6.6987e-04 eta 0:02:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [44/96] time 0.332 (0.343) data 0.000 (0.013) loss 0.9685 (1.2473) lr 6.6987e-04 eta 0:02:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [46/96] time 0.320 (0.345) data 0.000 (0.012) loss 1.0174 (1.2566) lr 6.6987e-04 eta 0:02:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [48/96] time 0.334 (0.344) data 0.000 (0.012) loss 0.8472 (1.2375) lr 6.6987e-04 eta 0:02:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [50/96] time 0.330 (0.344) data 0.000 (0.012) loss 0.8659 (1.2231) lr 6.6987e-04 eta 0:02:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [52/96] time 0.329 (0.343) data 0.000 (0.011) loss 1.0888 (1.2317) lr 6.6987e-04 eta 0:02:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [54/96] time 0.327 (0.342) data 0.000 (0.011) loss 1.0046 (1.2179) lr 6.6987e-04 eta 0:02:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [56/96] time 0.342 (0.342) data 0.000 (0.010) loss 0.9692 (1.2078) lr 6.6987e-04 eta 0:02:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [58/96] time 0.336 (0.342) data 0.000 (0.010) loss 1.3005 (1.2093) lr 6.6987e-04 eta 0:02:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [60/96] time 0.320 (0.342) data 0.000 (0.010) loss 0.8932 (1.2008) lr 6.6987e-04 eta 0:02:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [62/96] time 0.339 (0.341) data 0.001 (0.009) loss 0.8411 (1.1920) lr 6.6987e-04 eta 0:02:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [64/96] time 0.328 (0.341) data 0.000 (0.009) loss 0.7859 (1.1882) lr 6.6987e-04 eta 0:02:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [66/96] time 0.338 (0.341) data 0.000 (0.009) loss 1.3834 (1.1931) lr 6.6987e-04 eta 0:02:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [68/96] time 0.323 (0.340) data 0.000 (0.009) loss 1.4215 (1.2031) lr 6.6987e-04 eta 0:02:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [70/96] time 0.319 (0.340) data 0.000 (0.008) loss 1.0538 (1.2045) lr 6.6987e-04 eta 0:02:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [72/96] time 0.329 (0.340) data 0.000 (0.008) loss 0.9724 (1.1974) lr 6.6987e-04 eta 0:02:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [74/96] time 0.308 (0.339) data 0.000 (0.008) loss 0.8409 (1.1905) lr 6.6987e-04 eta 0:02:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [76/96] time 0.313 (0.338) data 0.000 (0.008) loss 1.0980 (1.1918) lr 6.6987e-04 eta 0:02:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [78/96] time 0.308 (0.337) data 0.000 (0.007) loss 0.9015 (1.1832) lr 6.6987e-04 eta 0:02:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [80/96] time 0.309 (0.337) data 0.000 (0.007) loss 0.7891 (1.1759) lr 6.6987e-04 eta 0:02:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [82/96] time 0.302 (0.336) data 0.000 (0.007) loss 0.9711 (1.1728) lr 6.6987e-04 eta 0:02:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [84/96] time 0.314 (0.335) data 0.000 (0.007) loss 1.1068 (1.1692) lr 6.6987e-04 eta 0:02:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [86/96] time 0.312 (0.335) data 0.000 (0.007) loss 1.3781 (1.1741) lr 6.6987e-04 eta 0:02:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [88/96] time 0.307 (0.334) data 0.000 (0.007) loss 1.0280 (1.1747) lr 6.6987e-04 eta 0:02:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [90/96] time 0.311 (0.334) data 0.000 (0.007) loss 0.7734 (1.1685) lr 6.6987e-04 eta 0:02:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [92/96] time 0.313 (0.333) data 0.000 (0.006) loss 1.2463 (1.1656) lr 6.6987e-04 eta 0:02:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [94/96] time 0.308 (0.333) data 0.000 (0.006) loss 1.9121 (1.1746) lr 6.6987e-04 eta 0:02:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [26/30] batch [96/96] time 0.312 (0.332) data 0.000 (0.006) loss 1.2990 (1.1748) lr 4.3227e-04 eta 0:02:07
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.80s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.33s/it]100%|██████████| 3/3 [00:03<00:00,  1.18it/s]100%|██████████| 3/3 [00:03<00:00,  1.16s/it]=> result
* total: 576
* correct: 434
* accuracy: 75.3%
* error: 24.7%
* macro_f1: 74.6%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [2/96] time 0.335 (0.658) data 0.001 (0.279) loss 1.6482 (1.1821) lr 4.3227e-04 eta 0:04:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [4/96] time 0.324 (0.493) data 0.000 (0.140) loss 0.8858 (1.0188) lr 4.3227e-04 eta 0:03:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [6/96] time 0.347 (0.443) data 0.000 (0.093) loss 1.1505 (1.0530) lr 4.3227e-04 eta 0:02:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [8/96] time 0.340 (0.416) data 0.000 (0.070) loss 1.9707 (1.1509) lr 4.3227e-04 eta 0:02:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [10/96] time 0.325 (0.400) data 0.000 (0.056) loss 0.9691 (1.1102) lr 4.3227e-04 eta 0:02:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [12/96] time 0.336 (0.388) data 0.000 (0.047) loss 0.7954 (1.1266) lr 4.3227e-04 eta 0:02:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [14/96] time 0.318 (0.379) data 0.000 (0.040) loss 0.7994 (1.0963) lr 4.3227e-04 eta 0:02:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [16/96] time 0.323 (0.372) data 0.000 (0.035) loss 1.1001 (1.0800) lr 4.3227e-04 eta 0:02:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [18/96] time 0.319 (0.366) data 0.000 (0.031) loss 0.9950 (1.0796) lr 4.3227e-04 eta 0:02:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [20/96] time 0.328 (0.362) data 0.000 (0.028) loss 1.7912 (1.1090) lr 4.3227e-04 eta 0:02:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [22/96] time 0.336 (0.359) data 0.000 (0.026) loss 1.1274 (1.1071) lr 4.3227e-04 eta 0:02:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [24/96] time 0.338 (0.357) data 0.000 (0.024) loss 1.7414 (1.1169) lr 4.3227e-04 eta 0:02:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [26/96] time 0.337 (0.356) data 0.001 (0.022) loss 1.4301 (1.1351) lr 4.3227e-04 eta 0:02:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [28/96] time 0.336 (0.354) data 0.000 (0.020) loss 1.2364 (1.1687) lr 4.3227e-04 eta 0:02:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [30/96] time 0.322 (0.353) data 0.000 (0.019) loss 0.9197 (1.1599) lr 4.3227e-04 eta 0:02:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [32/96] time 0.329 (0.351) data 0.000 (0.018) loss 1.0347 (1.1519) lr 4.3227e-04 eta 0:02:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [34/96] time 0.326 (0.350) data 0.000 (0.017) loss 1.0685 (1.1496) lr 4.3227e-04 eta 0:02:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [36/96] time 0.316 (0.348) data 0.000 (0.016) loss 0.9310 (1.1470) lr 4.3227e-04 eta 0:02:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [38/96] time 0.329 (0.347) data 0.000 (0.015) loss 0.9071 (1.1361) lr 4.3227e-04 eta 0:01:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [40/96] time 0.334 (0.348) data 0.000 (0.014) loss 0.9370 (1.1295) lr 4.3227e-04 eta 0:01:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [42/96] time 0.338 (0.348) data 0.000 (0.014) loss 0.7622 (1.1219) lr 4.3227e-04 eta 0:01:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [44/96] time 0.332 (0.347) data 0.000 (0.013) loss 1.0435 (1.1160) lr 4.3227e-04 eta 0:01:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [46/96] time 0.323 (0.346) data 0.000 (0.012) loss 0.9133 (1.1084) lr 4.3227e-04 eta 0:01:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [48/96] time 0.331 (0.345) data 0.000 (0.012) loss 1.0127 (1.1005) lr 4.3227e-04 eta 0:01:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [50/96] time 0.345 (0.345) data 0.000 (0.012) loss 0.9827 (1.1018) lr 4.3227e-04 eta 0:01:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [52/96] time 0.332 (0.344) data 0.000 (0.011) loss 1.0349 (1.1095) lr 4.3227e-04 eta 0:01:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [54/96] time 0.342 (0.345) data 0.000 (0.011) loss 1.8674 (1.1220) lr 4.3227e-04 eta 0:01:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [56/96] time 0.330 (0.344) data 0.000 (0.010) loss 0.6925 (1.1135) lr 4.3227e-04 eta 0:01:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [58/96] time 0.325 (0.344) data 0.000 (0.010) loss 0.8699 (1.1027) lr 4.3227e-04 eta 0:01:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [60/96] time 0.345 (0.344) data 0.000 (0.010) loss 0.7768 (1.0964) lr 4.3227e-04 eta 0:01:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [62/96] time 0.337 (0.343) data 0.000 (0.009) loss 1.3453 (1.1019) lr 4.3227e-04 eta 0:01:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [64/96] time 0.340 (0.343) data 0.000 (0.009) loss 0.9908 (1.1034) lr 4.3227e-04 eta 0:01:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [66/96] time 0.337 (0.343) data 0.000 (0.009) loss 0.9637 (1.1066) lr 4.3227e-04 eta 0:01:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [68/96] time 0.327 (0.343) data 0.000 (0.009) loss 0.8402 (1.0988) lr 4.3227e-04 eta 0:01:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [70/96] time 0.337 (0.342) data 0.000 (0.008) loss 1.3722 (1.0982) lr 4.3227e-04 eta 0:01:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [72/96] time 0.348 (0.343) data 0.000 (0.008) loss 0.9672 (1.0959) lr 4.3227e-04 eta 0:01:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [74/96] time 0.370 (0.343) data 0.000 (0.008) loss 1.2693 (1.1006) lr 4.3227e-04 eta 0:01:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [76/96] time 0.325 (0.342) data 0.000 (0.008) loss 1.2586 (1.1109) lr 4.3227e-04 eta 0:01:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [78/96] time 0.320 (0.342) data 0.000 (0.007) loss 0.9882 (1.1092) lr 4.3227e-04 eta 0:01:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [80/96] time 0.315 (0.341) data 0.000 (0.007) loss 0.9456 (1.1117) lr 4.3227e-04 eta 0:01:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [82/96] time 0.318 (0.341) data 0.000 (0.007) loss 2.5150 (1.1295) lr 4.3227e-04 eta 0:01:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [84/96] time 0.319 (0.340) data 0.000 (0.007) loss 0.7907 (1.1229) lr 4.3227e-04 eta 0:01:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [86/96] time 0.317 (0.339) data 0.000 (0.007) loss 1.7476 (1.1269) lr 4.3227e-04 eta 0:01:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [88/96] time 0.314 (0.339) data 0.000 (0.007) loss 0.8583 (1.1245) lr 4.3227e-04 eta 0:01:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [90/96] time 0.312 (0.338) data 0.000 (0.007) loss 0.8449 (1.1180) lr 4.3227e-04 eta 0:01:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [92/96] time 0.318 (0.338) data 0.000 (0.006) loss 1.2339 (1.1183) lr 4.3227e-04 eta 0:01:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [94/96] time 0.320 (0.337) data 0.000 (0.006) loss 1.7895 (1.1290) lr 4.3227e-04 eta 0:01:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [27/30] batch [96/96] time 0.316 (0.337) data 0.000 (0.006) loss 1.0013 (1.1280) lr 2.4472e-04 eta 0:01:37
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.83s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.33s/it]100%|██████████| 3/3 [00:03<00:00,  1.18it/s]100%|██████████| 3/3 [00:03<00:00,  1.16s/it]=> result
* total: 576
* correct: 437
* accuracy: 75.9%
* error: 24.1%
* macro_f1: 75.1%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [2/96] time 0.335 (0.646) data 0.000 (0.277) loss 1.2719 (1.2566) lr 2.4472e-04 eta 0:03:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [4/96] time 0.339 (0.490) data 0.000 (0.139) loss 1.2308 (1.2166) lr 2.4472e-04 eta 0:02:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [6/96] time 0.327 (0.438) data 0.000 (0.093) loss 1.2771 (1.1706) lr 2.4472e-04 eta 0:02:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [8/96] time 0.343 (0.413) data 0.000 (0.070) loss 1.7059 (1.2853) lr 2.4472e-04 eta 0:01:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [10/96] time 0.337 (0.398) data 0.000 (0.056) loss 0.7857 (1.2173) lr 2.4472e-04 eta 0:01:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [12/96] time 0.346 (0.388) data 0.000 (0.046) loss 0.6968 (1.1544) lr 2.4472e-04 eta 0:01:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [14/96] time 0.334 (0.380) data 0.001 (0.040) loss 1.8506 (1.1885) lr 2.4472e-04 eta 0:01:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [16/96] time 0.338 (0.375) data 0.000 (0.035) loss 0.8269 (1.1458) lr 2.4472e-04 eta 0:01:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [18/96] time 0.337 (0.371) data 0.000 (0.031) loss 1.1306 (1.1385) lr 2.4472e-04 eta 0:01:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [20/96] time 0.341 (0.367) data 0.000 (0.028) loss 1.0722 (1.1227) lr 2.4472e-04 eta 0:01:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [22/96] time 0.329 (0.364) data 0.000 (0.026) loss 1.1686 (1.1347) lr 2.4472e-04 eta 0:01:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [24/96] time 0.346 (0.362) data 0.000 (0.023) loss 0.7838 (1.1262) lr 2.4472e-04 eta 0:01:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [26/96] time 0.344 (0.360) data 0.000 (0.022) loss 0.9985 (1.1195) lr 2.4472e-04 eta 0:01:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [28/96] time 0.325 (0.359) data 0.000 (0.020) loss 0.9645 (1.1035) lr 2.4472e-04 eta 0:01:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [30/96] time 0.331 (0.357) data 0.000 (0.019) loss 1.1259 (1.1045) lr 2.4472e-04 eta 0:01:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [32/96] time 0.334 (0.355) data 0.000 (0.018) loss 1.3219 (1.1151) lr 2.4472e-04 eta 0:01:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [34/96] time 0.330 (0.354) data 0.000 (0.017) loss 1.6523 (1.1417) lr 2.4472e-04 eta 0:01:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [36/96] time 0.430 (0.356) data 0.000 (0.016) loss 1.2865 (1.1482) lr 2.4472e-04 eta 0:01:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [38/96] time 0.334 (0.354) data 0.000 (0.015) loss 1.2956 (1.1561) lr 2.4472e-04 eta 0:01:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [40/96] time 0.331 (0.354) data 0.000 (0.014) loss 0.9964 (1.1471) lr 2.4472e-04 eta 0:01:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [42/96] time 0.344 (0.353) data 0.000 (0.014) loss 1.1048 (1.1447) lr 2.4472e-04 eta 0:01:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [44/96] time 0.339 (0.353) data 0.000 (0.013) loss 1.7299 (1.1567) lr 2.4472e-04 eta 0:01:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [46/96] time 0.331 (0.352) data 0.000 (0.012) loss 0.8126 (1.1473) lr 2.4472e-04 eta 0:01:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [48/96] time 0.352 (0.352) data 0.000 (0.012) loss 0.7268 (1.1364) lr 2.4472e-04 eta 0:01:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [50/96] time 0.329 (0.351) data 0.000 (0.011) loss 1.5196 (1.1438) lr 2.4472e-04 eta 0:01:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [52/96] time 0.330 (0.350) data 0.000 (0.011) loss 0.9739 (1.1426) lr 2.4472e-04 eta 0:01:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [54/96] time 0.331 (0.350) data 0.000 (0.011) loss 1.1236 (1.1416) lr 2.4472e-04 eta 0:01:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [56/96] time 0.325 (0.349) data 0.000 (0.010) loss 1.7605 (1.1592) lr 2.4472e-04 eta 0:01:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [58/96] time 0.327 (0.348) data 0.000 (0.010) loss 1.2596 (1.1552) lr 2.4472e-04 eta 0:01:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [60/96] time 0.328 (0.347) data 0.000 (0.010) loss 0.9580 (1.1542) lr 2.4472e-04 eta 0:01:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [62/96] time 0.307 (0.346) data 0.000 (0.009) loss 1.3070 (1.1532) lr 2.4472e-04 eta 0:01:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [64/96] time 0.339 (0.346) data 0.000 (0.009) loss 0.9108 (1.1473) lr 2.4472e-04 eta 0:01:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [66/96] time 0.320 (0.345) data 0.000 (0.009) loss 1.0438 (1.1453) lr 2.4472e-04 eta 0:01:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [68/96] time 0.312 (0.344) data 0.000 (0.008) loss 1.1704 (1.1485) lr 2.4472e-04 eta 0:01:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [70/96] time 0.340 (0.344) data 0.000 (0.008) loss 0.7120 (1.1440) lr 2.4472e-04 eta 0:01:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [72/96] time 0.326 (0.344) data 0.000 (0.008) loss 1.4471 (1.1517) lr 2.4472e-04 eta 0:01:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [74/96] time 0.319 (0.343) data 0.000 (0.008) loss 1.1653 (1.1505) lr 2.4472e-04 eta 0:01:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [76/96] time 0.318 (0.342) data 0.000 (0.008) loss 1.0018 (1.1451) lr 2.4472e-04 eta 0:01:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [78/96] time 0.311 (0.341) data 0.000 (0.007) loss 1.1240 (1.1446) lr 2.4472e-04 eta 0:01:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [80/96] time 0.315 (0.341) data 0.000 (0.007) loss 1.0516 (1.1393) lr 2.4472e-04 eta 0:01:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [82/96] time 0.312 (0.340) data 0.000 (0.007) loss 1.1214 (1.1378) lr 2.4472e-04 eta 0:01:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [84/96] time 0.315 (0.340) data 0.000 (0.007) loss 1.1820 (1.1436) lr 2.4472e-04 eta 0:01:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [86/96] time 0.324 (0.339) data 0.000 (0.007) loss 1.3268 (1.1439) lr 2.4472e-04 eta 0:01:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [88/96] time 0.318 (0.339) data 0.000 (0.007) loss 1.2696 (1.1477) lr 2.4472e-04 eta 0:01:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [90/96] time 0.320 (0.338) data 0.000 (0.006) loss 1.4559 (1.1498) lr 2.4472e-04 eta 0:01:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [92/96] time 0.317 (0.338) data 0.000 (0.006) loss 1.1715 (1.1544) lr 2.4472e-04 eta 0:01:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [94/96] time 0.318 (0.337) data 0.000 (0.006) loss 0.8303 (1.1463) lr 2.4472e-04 eta 0:01:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [28/30] batch [96/96] time 0.323 (0.337) data 0.000 (0.006) loss 1.3272 (1.1469) lr 1.0926e-04 eta 0:01:04
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.86s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 435
* accuracy: 75.5%
* error: 24.5%
* macro_f1: 74.7%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [2/96] time 0.316 (0.649) data 0.000 (0.279) loss 1.1044 (1.2674) lr 1.0926e-04 eta 0:02:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [4/96] time 0.338 (0.488) data 0.000 (0.140) loss 1.0686 (1.1484) lr 1.0926e-04 eta 0:01:31
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [6/96] time 0.318 (0.433) data 0.000 (0.093) loss 0.9478 (1.0885) lr 1.0926e-04 eta 0:01:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [8/96] time 0.323 (0.406) data 0.000 (0.070) loss 0.8928 (1.0876) lr 1.0926e-04 eta 0:01:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [10/96] time 0.334 (0.390) data 0.000 (0.056) loss 1.0684 (1.1400) lr 1.0926e-04 eta 0:01:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [12/96] time 0.328 (0.382) data 0.000 (0.047) loss 1.4744 (1.1803) lr 1.0926e-04 eta 0:01:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [14/96] time 0.320 (0.374) data 0.000 (0.040) loss 1.2471 (1.1707) lr 1.0926e-04 eta 0:01:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [16/96] time 0.323 (0.367) data 0.000 (0.035) loss 1.3842 (1.1874) lr 1.0926e-04 eta 0:01:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [18/96] time 0.329 (0.363) data 0.000 (0.031) loss 1.8848 (1.2154) lr 1.0926e-04 eta 0:01:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [20/96] time 0.335 (0.360) data 0.000 (0.028) loss 1.0947 (1.2137) lr 1.0926e-04 eta 0:01:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [22/96] time 0.329 (0.357) data 0.000 (0.026) loss 0.8747 (1.2185) lr 1.0926e-04 eta 0:01:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [24/96] time 0.323 (0.355) data 0.000 (0.024) loss 1.2386 (1.2249) lr 1.0926e-04 eta 0:00:59
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [26/96] time 0.340 (0.354) data 0.000 (0.022) loss 1.0952 (1.2304) lr 1.0926e-04 eta 0:00:58
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [28/96] time 0.320 (0.351) data 0.000 (0.020) loss 0.7796 (1.2185) lr 1.0926e-04 eta 0:00:57
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [30/96] time 0.320 (0.350) data 0.000 (0.019) loss 1.4181 (1.2213) lr 1.0926e-04 eta 0:00:56
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [32/96] time 0.325 (0.348) data 0.000 (0.018) loss 0.9601 (1.2099) lr 1.0926e-04 eta 0:00:55
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [34/96] time 0.325 (0.347) data 0.000 (0.017) loss 1.3183 (1.2074) lr 1.0926e-04 eta 0:00:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [36/96] time 0.446 (0.349) data 0.000 (0.016) loss 1.0213 (1.2051) lr 1.0926e-04 eta 0:00:54
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [38/96] time 0.370 (0.349) data 0.000 (0.015) loss 1.2220 (1.2252) lr 1.0926e-04 eta 0:00:53
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [40/96] time 0.334 (0.349) data 0.000 (0.014) loss 1.6911 (1.2366) lr 1.0926e-04 eta 0:00:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [42/96] time 0.359 (0.349) data 0.000 (0.014) loss 0.9931 (1.2206) lr 1.0926e-04 eta 0:00:52
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [44/96] time 0.346 (0.349) data 0.001 (0.013) loss 1.1437 (1.2137) lr 1.0926e-04 eta 0:00:51
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [46/96] time 0.333 (0.348) data 0.000 (0.012) loss 0.7729 (1.1962) lr 1.0926e-04 eta 0:00:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [48/96] time 0.342 (0.348) data 0.000 (0.012) loss 1.4611 (1.2012) lr 1.0926e-04 eta 0:00:50
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [50/96] time 0.342 (0.348) data 0.000 (0.011) loss 1.2110 (1.1917) lr 1.0926e-04 eta 0:00:49
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [52/96] time 0.339 (0.347) data 0.000 (0.011) loss 1.5567 (1.2007) lr 1.0926e-04 eta 0:00:48
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [54/96] time 0.337 (0.347) data 0.000 (0.011) loss 1.3471 (1.1940) lr 1.0926e-04 eta 0:00:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [56/96] time 0.342 (0.347) data 0.000 (0.010) loss 1.3258 (1.1898) lr 1.0926e-04 eta 0:00:47
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [58/96] time 0.350 (0.347) data 0.000 (0.010) loss 0.8656 (1.1836) lr 1.0926e-04 eta 0:00:46
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [60/96] time 0.336 (0.346) data 0.000 (0.010) loss 0.9497 (1.1740) lr 1.0926e-04 eta 0:00:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [62/96] time 0.339 (0.346) data 0.000 (0.009) loss 1.4810 (1.1745) lr 1.0926e-04 eta 0:00:45
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [64/96] time 0.336 (0.346) data 0.000 (0.009) loss 0.9089 (1.1666) lr 1.0926e-04 eta 0:00:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [66/96] time 0.332 (0.346) data 0.000 (0.009) loss 1.0301 (1.1649) lr 1.0926e-04 eta 0:00:43
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [68/96] time 0.319 (0.345) data 0.000 (0.009) loss 1.3831 (1.1636) lr 1.0926e-04 eta 0:00:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [70/96] time 0.322 (0.345) data 0.000 (0.008) loss 1.1437 (1.1764) lr 1.0926e-04 eta 0:00:42
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [72/96] time 0.331 (0.344) data 0.000 (0.008) loss 2.0888 (1.1862) lr 1.0926e-04 eta 0:00:41
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [74/96] time 0.307 (0.343) data 0.000 (0.008) loss 1.0971 (1.1825) lr 1.0926e-04 eta 0:00:40
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [76/96] time 0.307 (0.342) data 0.000 (0.008) loss 1.5558 (1.1873) lr 1.0926e-04 eta 0:00:39
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [78/96] time 0.313 (0.341) data 0.000 (0.007) loss 1.5938 (1.1941) lr 1.0926e-04 eta 0:00:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [80/96] time 0.308 (0.340) data 0.000 (0.007) loss 1.3028 (1.1940) lr 1.0926e-04 eta 0:00:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [82/96] time 0.312 (0.340) data 0.000 (0.007) loss 0.8933 (1.1871) lr 1.0926e-04 eta 0:00:37
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [84/96] time 0.302 (0.339) data 0.000 (0.007) loss 1.1451 (1.1919) lr 1.0926e-04 eta 0:00:36
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [86/96] time 0.304 (0.338) data 0.000 (0.007) loss 0.9523 (1.1932) lr 1.0926e-04 eta 0:00:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [88/96] time 0.306 (0.337) data 0.000 (0.007) loss 1.0856 (1.1903) lr 1.0926e-04 eta 0:00:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [90/96] time 0.307 (0.337) data 0.000 (0.007) loss 1.1464 (1.1864) lr 1.0926e-04 eta 0:00:34
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [92/96] time 0.307 (0.336) data 0.000 (0.006) loss 0.9419 (1.1812) lr 1.0926e-04 eta 0:00:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [94/96] time 0.306 (0.335) data 0.000 (0.006) loss 1.3397 (1.1830) lr 1.0926e-04 eta 0:00:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [29/30] batch [96/96] time 0.308 (0.335) data 0.000 (0.006) loss 0.8062 (1.1769) lr 2.7391e-05 eta 0:00:32
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.80s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.33s/it]100%|██████████| 3/3 [00:03<00:00,  1.18it/s]100%|██████████| 3/3 [00:03<00:00,  1.16s/it]=> result
* total: 576
* correct: 435
* accuracy: 75.5%
* error: 24.5%
* macro_f1: 74.7%

ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [2/96] time 0.325 (0.642) data 0.000 (0.272) loss 1.3563 (1.4611) lr 2.7391e-05 eta 0:01:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [4/96] time 0.323 (0.481) data 0.000 (0.136) loss 0.8880 (1.1534) lr 2.7391e-05 eta 0:00:44
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [6/96] time 0.327 (0.430) data 0.000 (0.091) loss 1.6179 (1.2728) lr 2.7391e-05 eta 0:00:38
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [8/96] time 0.341 (0.407) data 0.000 (0.068) loss 1.1940 (1.2577) lr 2.7391e-05 eta 0:00:35
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [10/96] time 0.340 (0.394) data 0.000 (0.055) loss 1.5250 (1.2529) lr 2.7391e-05 eta 0:00:33
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [12/96] time 0.326 (0.384) data 0.000 (0.046) loss 0.9371 (1.1851) lr 2.7391e-05 eta 0:00:32
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [14/96] time 0.326 (0.376) data 0.000 (0.039) loss 0.8483 (1.1299) lr 2.7391e-05 eta 0:00:30
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [16/96] time 0.328 (0.370) data 0.000 (0.034) loss 0.8346 (1.1287) lr 2.7391e-05 eta 0:00:29
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [18/96] time 0.315 (0.364) data 0.000 (0.031) loss 0.9557 (1.1007) lr 2.7391e-05 eta 0:00:28
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [20/96] time 0.372 (0.363) data 0.000 (0.028) loss 0.7073 (1.0974) lr 2.7391e-05 eta 0:00:27
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [22/96] time 0.338 (0.360) data 0.000 (0.025) loss 0.9142 (1.0974) lr 2.7391e-05 eta 0:00:26
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [24/96] time 0.322 (0.358) data 0.000 (0.023) loss 1.0873 (1.1179) lr 2.7391e-05 eta 0:00:25
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [26/96] time 0.327 (0.355) data 0.000 (0.021) loss 0.7740 (1.1210) lr 2.7391e-05 eta 0:00:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [28/96] time 0.329 (0.353) data 0.000 (0.020) loss 1.6074 (1.1321) lr 2.7391e-05 eta 0:00:24
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [30/96] time 0.324 (0.352) data 0.000 (0.018) loss 0.9831 (1.1270) lr 2.7391e-05 eta 0:00:23
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [32/96] time 0.338 (0.351) data 0.000 (0.017) loss 0.8147 (1.1274) lr 2.7391e-05 eta 0:00:22
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [34/96] time 0.316 (0.349) data 0.000 (0.016) loss 1.0185 (1.1378) lr 2.7391e-05 eta 0:00:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [36/96] time 0.414 (0.350) data 0.000 (0.015) loss 1.4986 (1.1406) lr 2.7391e-05 eta 0:00:21
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [38/96] time 0.330 (0.349) data 0.001 (0.015) loss 1.1387 (1.1450) lr 2.7391e-05 eta 0:00:20
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [40/96] time 0.327 (0.348) data 0.000 (0.014) loss 1.5269 (1.1566) lr 2.7391e-05 eta 0:00:19
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [42/96] time 0.330 (0.347) data 0.000 (0.013) loss 0.9436 (1.1686) lr 2.7391e-05 eta 0:00:18
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [44/96] time 0.328 (0.346) data 0.000 (0.013) loss 0.9481 (1.1618) lr 2.7391e-05 eta 0:00:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [46/96] time 0.336 (0.345) data 0.000 (0.012) loss 1.0398 (1.1507) lr 2.7391e-05 eta 0:00:17
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [48/96] time 0.329 (0.345) data 0.000 (0.012) loss 1.4359 (1.1521) lr 2.7391e-05 eta 0:00:16
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [50/96] time 0.348 (0.344) data 0.000 (0.011) loss 1.2121 (1.1533) lr 2.7391e-05 eta 0:00:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [52/96] time 0.325 (0.344) data 0.000 (0.011) loss 1.1121 (1.1592) lr 2.7391e-05 eta 0:00:15
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [54/96] time 0.326 (0.343) data 0.000 (0.010) loss 1.4748 (1.1614) lr 2.7391e-05 eta 0:00:14
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [56/96] time 0.329 (0.342) data 0.000 (0.010) loss 1.0342 (1.1599) lr 2.7391e-05 eta 0:00:13
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [58/96] time 0.331 (0.342) data 0.000 (0.010) loss 1.3267 (1.1679) lr 2.7391e-05 eta 0:00:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [60/96] time 0.327 (0.341) data 0.000 (0.009) loss 0.9405 (1.1673) lr 2.7391e-05 eta 0:00:12
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [62/96] time 0.323 (0.341) data 0.000 (0.009) loss 0.9827 (1.1649) lr 2.7391e-05 eta 0:00:11
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [64/96] time 0.337 (0.341) data 0.000 (0.009) loss 1.4624 (1.1716) lr 2.7391e-05 eta 0:00:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [66/96] time 0.339 (0.340) data 0.001 (0.009) loss 0.9651 (1.1704) lr 2.7391e-05 eta 0:00:10
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [68/96] time 0.320 (0.340) data 0.000 (0.008) loss 0.9962 (1.1671) lr 2.7391e-05 eta 0:00:09
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [70/96] time 0.336 (0.340) data 0.000 (0.008) loss 0.9907 (1.1656) lr 2.7391e-05 eta 0:00:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [72/96] time 0.337 (0.340) data 0.000 (0.008) loss 0.9481 (1.1645) lr 2.7391e-05 eta 0:00:08
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [74/96] time 0.319 (0.339) data 0.000 (0.008) loss 1.1437 (1.1738) lr 2.7391e-05 eta 0:00:07
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [76/96] time 0.324 (0.339) data 0.000 (0.007) loss 1.3972 (1.1729) lr 2.7391e-05 eta 0:00:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [78/96] time 0.322 (0.338) data 0.000 (0.007) loss 1.1946 (1.1675) lr 2.7391e-05 eta 0:00:06
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [80/96] time 0.317 (0.338) data 0.000 (0.007) loss 0.7403 (1.1565) lr 2.7391e-05 eta 0:00:05
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [82/96] time 0.336 (0.337) data 0.000 (0.007) loss 1.4382 (1.1568) lr 2.7391e-05 eta 0:00:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [84/96] time 0.321 (0.337) data 0.000 (0.007) loss 0.6869 (1.1517) lr 2.7391e-05 eta 0:00:04
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [86/96] time 0.324 (0.337) data 0.000 (0.007) loss 1.4093 (1.1547) lr 2.7391e-05 eta 0:00:03
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [88/96] time 0.326 (0.336) data 0.000 (0.007) loss 0.8087 (1.1544) lr 2.7391e-05 eta 0:00:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [90/96] time 0.321 (0.336) data 0.000 (0.006) loss 1.0182 (1.1508) lr 2.7391e-05 eta 0:00:02
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [92/96] time 0.325 (0.336) data 0.000 (0.006) loss 1.3700 (1.1490) lr 2.7391e-05 eta 0:00:01
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [94/96] time 0.325 (0.336) data 0.000 (0.006) loss 1.3287 (1.1488) lr 2.7391e-05 eta 0:00:00
ce_loss: torch.Size([324])
ce_loss: torch.Size([324])
epoch [30/30] batch [96/96] time 0.325 (0.336) data 0.000 (0.006) loss 1.3649 (1.1479) lr 0.0000e+00 eta 0:00:00
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.85s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]
=> result
* total: 576
* correct: 435
* accuracy: 75.5%
* error: 24.5%
* macro_f1: 74.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar" (epoch = 24)
Evaluate on the *test* set
  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [00:03<00:12,  3.05s/it] 40%|████      | 2/5 [00:03<00:04,  1.43s/it] 60%|██████    | 3/5 [00:03<00:01,  1.10it/s] 80%|████████  | 4/5 [00:03<00:00,  1.50it/s]100%|██████████| 5/5 [00:04<00:00,  2.10it/s]100%|██████████| 5/5 [00:04<00:00,  1.20it/s]
=> result
* total: 864
* correct: 653
* accuracy: 75.6%
* error: 24.4%
* macro_f1: 74.9%
Elapsed: 0:17:56
+ sh scripts/rpo_prime/base2new_test_sdl.sh dtd 1 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
368 552 828
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  23
# train_x  368
# val      552
# test     828
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar" (epoch = 24)
Evaluate on the *test* set
  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [00:05<00:20,  5.23s/it] 40%|████      | 2/5 [00:05<00:06,  2.33s/it] 60%|██████    | 3/5 [00:05<00:02,  1.40s/it] 80%|████████  | 4/5 [00:06<00:00,  1.04it/s]100%|██████████| 5/5 [00:06<00:00,  1.26s/it]
=> result
* total: 828
* correct: 538
* accuracy: 65.0%
* error: 35.0%
* macro_f1: 64.6%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh dtd 2 0 main_tmp1_0.1sdl 16
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
384 576 864
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  24
# train_x  384
# val      576
# test     864
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/30] batch [2/96] time 0.341 (1.695) data 0.000 (0.566) loss 2.3448 (2.1962) lr 1.0000e-02 eta 1:21:17
epoch [1/30] batch [4/96] time 0.347 (1.021) data 0.000 (0.283) loss 1.9544 (1.8430) lr 1.0000e-02 eta 0:48:56
epoch [1/30] batch [6/96] time 0.349 (0.795) data 0.000 (0.189) loss 0.6834 (1.5772) lr 1.0000e-02 eta 0:38:05
epoch [1/30] batch [8/96] time 0.335 (0.681) data 0.000 (0.142) loss 2.1115 (1.6473) lr 1.0000e-02 eta 0:32:35
epoch [1/30] batch [10/96] time 0.321 (0.609) data 0.000 (0.113) loss 0.9422 (1.5695) lr 1.0000e-02 eta 0:29:09
epoch [1/30] batch [12/96] time 0.325 (0.562) data 0.000 (0.095) loss 2.3361 (1.6096) lr 1.0000e-02 eta 0:26:52
epoch [1/30] batch [14/96] time 0.325 (0.529) data 0.000 (0.081) loss 1.4880 (1.6205) lr 1.0000e-02 eta 0:25:16
epoch [1/30] batch [16/96] time 0.338 (0.505) data 0.000 (0.071) loss 2.2917 (1.6231) lr 1.0000e-02 eta 0:24:05
epoch [1/30] batch [18/96] time 0.330 (0.486) data 0.000 (0.063) loss 1.5091 (1.6676) lr 1.0000e-02 eta 0:23:11
epoch [1/30] batch [20/96] time 0.329 (0.470) data 0.000 (0.057) loss 2.3716 (1.6800) lr 1.0000e-02 eta 0:22:25
epoch [1/30] batch [22/96] time 0.321 (0.457) data 0.000 (0.052) loss 1.0503 (1.6095) lr 1.0000e-02 eta 0:21:45
epoch [1/30] batch [24/96] time 0.333 (0.446) data 0.000 (0.047) loss 0.9746 (1.5913) lr 1.0000e-02 eta 0:21:15
epoch [1/30] batch [26/96] time 0.332 (0.438) data 0.000 (0.044) loss 2.0363 (1.6417) lr 1.0000e-02 eta 0:20:50
epoch [1/30] batch [28/96] time 0.325 (0.430) data 0.000 (0.041) loss 2.4680 (1.6863) lr 1.0000e-02 eta 0:20:27
epoch [1/30] batch [30/96] time 0.315 (0.423) data 0.000 (0.038) loss 2.8704 (1.7604) lr 1.0000e-02 eta 0:20:06
epoch [1/30] batch [32/96] time 0.324 (0.417) data 0.000 (0.036) loss 1.9990 (1.7710) lr 1.0000e-02 eta 0:19:47
epoch [1/30] batch [34/96] time 0.326 (0.411) data 0.001 (0.034) loss 2.3498 (1.8217) lr 1.0000e-02 eta 0:19:30
epoch [1/30] batch [36/96] time 0.322 (0.406) data 0.000 (0.032) loss 1.8367 (1.8334) lr 1.0000e-02 eta 0:19:15
epoch [1/30] batch [38/96] time 0.319 (0.402) data 0.000 (0.030) loss 0.7841 (1.8074) lr 1.0000e-02 eta 0:19:02
epoch [1/30] batch [40/96] time 0.322 (0.398) data 0.000 (0.029) loss 1.0717 (1.7891) lr 1.0000e-02 eta 0:18:50
epoch [1/30] batch [42/96] time 0.330 (0.395) data 0.001 (0.027) loss 1.4058 (1.7945) lr 1.0000e-02 eta 0:18:39
epoch [1/30] batch [44/96] time 0.329 (0.391) data 0.000 (0.026) loss 2.1196 (1.8014) lr 1.0000e-02 eta 0:18:30
epoch [1/30] batch [46/96] time 0.324 (0.389) data 0.000 (0.025) loss 2.5127 (1.8332) lr 1.0000e-02 eta 0:18:21
epoch [1/30] batch [48/96] time 0.330 (0.386) data 0.000 (0.024) loss 1.8307 (1.8514) lr 1.0000e-02 eta 0:18:13
epoch [1/30] batch [50/96] time 0.314 (0.383) data 0.000 (0.023) loss 1.2300 (1.8456) lr 1.0000e-02 eta 0:18:04
epoch [1/30] batch [52/96] time 0.323 (0.381) data 0.000 (0.022) loss 1.2923 (1.8291) lr 1.0000e-02 eta 0:17:57
epoch [1/30] batch [54/96] time 0.322 (0.379) data 0.000 (0.021) loss 2.2261 (1.8348) lr 1.0000e-02 eta 0:17:50
epoch [1/30] batch [56/96] time 0.328 (0.377) data 0.000 (0.021) loss 1.2985 (1.8192) lr 1.0000e-02 eta 0:17:44
epoch [1/30] batch [58/96] time 0.332 (0.375) data 0.000 (0.020) loss 1.5470 (1.8236) lr 1.0000e-02 eta 0:17:39
epoch [1/30] batch [60/96] time 0.320 (0.374) data 0.000 (0.019) loss 1.2971 (1.8017) lr 1.0000e-02 eta 0:17:33
epoch [1/30] batch [62/96] time 0.316 (0.372) data 0.000 (0.019) loss 1.2874 (1.7985) lr 1.0000e-02 eta 0:17:28
epoch [1/30] batch [64/96] time 0.324 (0.371) data 0.000 (0.018) loss 2.6551 (1.8057) lr 1.0000e-02 eta 0:17:23
epoch [1/30] batch [66/96] time 0.321 (0.369) data 0.000 (0.017) loss 2.0466 (1.8186) lr 1.0000e-02 eta 0:17:18
epoch [1/30] batch [68/96] time 0.343 (0.368) data 0.000 (0.017) loss 3.0645 (1.8255) lr 1.0000e-02 eta 0:17:14
epoch [1/30] batch [70/96] time 0.324 (0.367) data 0.000 (0.016) loss 1.0344 (1.8165) lr 1.0000e-02 eta 0:17:10
epoch [1/30] batch [72/96] time 0.399 (0.367) data 0.000 (0.016) loss 1.2614 (1.8058) lr 1.0000e-02 eta 0:17:09
epoch [1/30] batch [74/96] time 0.328 (0.365) data 0.000 (0.016) loss 1.2799 (1.8128) lr 1.0000e-02 eta 0:17:05
epoch [1/30] batch [76/96] time 0.325 (0.364) data 0.000 (0.015) loss 2.0771 (1.8113) lr 1.0000e-02 eta 0:17:01
epoch [1/30] batch [78/96] time 0.309 (0.363) data 0.000 (0.015) loss 1.2194 (1.7998) lr 1.0000e-02 eta 0:16:57
epoch [1/30] batch [80/96] time 0.313 (0.362) data 0.000 (0.014) loss 1.8717 (1.7952) lr 1.0000e-02 eta 0:16:53
epoch [1/30] batch [82/96] time 0.313 (0.361) data 0.000 (0.014) loss 1.1660 (1.7915) lr 1.0000e-02 eta 0:16:49
epoch [1/30] batch [84/96] time 0.314 (0.360) data 0.000 (0.014) loss 1.3226 (1.7843) lr 1.0000e-02 eta 0:16:45
epoch [1/30] batch [86/96] time 0.312 (0.359) data 0.000 (0.013) loss 1.2617 (1.7766) lr 1.0000e-02 eta 0:16:42
epoch [1/30] batch [88/96] time 0.314 (0.358) data 0.000 (0.013) loss 1.8334 (1.7827) lr 1.0000e-02 eta 0:16:38
epoch [1/30] batch [90/96] time 0.319 (0.357) data 0.000 (0.013) loss 1.7507 (1.7942) lr 1.0000e-02 eta 0:16:35
epoch [1/30] batch [92/96] time 0.311 (0.356) data 0.000 (0.013) loss 1.5876 (1.7999) lr 1.0000e-02 eta 0:16:31
epoch [1/30] batch [94/96] time 0.318 (0.355) data 0.000 (0.012) loss 2.0923 (1.8105) lr 1.0000e-02 eta 0:16:29
epoch [1/30] batch [96/96] time 0.319 (0.354) data 0.000 (0.012) loss 1.8768 (1.8158) lr 9.9726e-03 eta 0:16:26
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:03<00:06,  3.03s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.42s/it]100%|██████████| 3/3 [00:03<00:00,  1.11it/s]100%|██████████| 3/3 [00:03<00:00,  1.23s/it]=> result
* total: 576
* correct: 349
* accuracy: 60.6%
* error: 39.4%
* macro_f1: 56.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [2/30] batch [2/96] time 0.331 (0.645) data 0.000 (0.284) loss 1.4307 (1.7579) lr 9.9726e-03 eta 0:29:55
epoch [2/30] batch [4/96] time 0.343 (0.491) data 0.000 (0.142) loss 1.5972 (1.9393) lr 9.9726e-03 eta 0:22:44
epoch [2/30] batch [6/96] time 0.331 (0.436) data 0.000 (0.095) loss 1.9225 (1.9781) lr 9.9726e-03 eta 0:20:10
epoch [2/30] batch [8/96] time 0.334 (0.411) data 0.000 (0.071) loss 1.2485 (1.8219) lr 9.9726e-03 eta 0:19:02
epoch [2/30] batch [10/96] time 0.330 (0.397) data 0.000 (0.057) loss 1.8236 (1.7299) lr 9.9726e-03 eta 0:18:21
epoch [2/30] batch [12/96] time 0.316 (0.385) data 0.000 (0.048) loss 1.6384 (1.8141) lr 9.9726e-03 eta 0:17:48
epoch [2/30] batch [14/96] time 0.336 (0.377) data 0.000 (0.041) loss 1.6619 (1.7732) lr 9.9726e-03 eta 0:17:25
epoch [2/30] batch [16/96] time 0.328 (0.371) data 0.000 (0.036) loss 2.8512 (1.8360) lr 9.9726e-03 eta 0:17:07
epoch [2/30] batch [18/96] time 0.318 (0.367) data 0.000 (0.032) loss 2.3027 (1.8442) lr 9.9726e-03 eta 0:16:54
epoch [2/30] batch [20/96] time 0.326 (0.363) data 0.000 (0.029) loss 1.4562 (1.7986) lr 9.9726e-03 eta 0:16:42
epoch [2/30] batch [22/96] time 0.330 (0.360) data 0.000 (0.026) loss 1.9587 (1.8360) lr 9.9726e-03 eta 0:16:34
epoch [2/30] batch [24/96] time 0.335 (0.358) data 0.000 (0.024) loss 1.5428 (1.8038) lr 9.9726e-03 eta 0:16:28
epoch [2/30] batch [26/96] time 0.327 (0.356) data 0.000 (0.022) loss 1.5250 (1.7997) lr 9.9726e-03 eta 0:16:21
epoch [2/30] batch [28/96] time 0.325 (0.354) data 0.000 (0.021) loss 1.1289 (1.7726) lr 9.9726e-03 eta 0:16:15
epoch [2/30] batch [30/96] time 0.350 (0.353) data 0.000 (0.019) loss 2.5094 (1.8379) lr 9.9726e-03 eta 0:16:12
epoch [2/30] batch [32/96] time 0.338 (0.352) data 0.000 (0.018) loss 1.8268 (1.8303) lr 9.9726e-03 eta 0:16:09
epoch [2/30] batch [34/96] time 0.336 (0.352) data 0.000 (0.017) loss 2.4002 (1.8279) lr 9.9726e-03 eta 0:16:07
epoch [2/30] batch [36/96] time 0.346 (0.352) data 0.000 (0.016) loss 1.6542 (1.8293) lr 9.9726e-03 eta 0:16:06
epoch [2/30] batch [38/96] time 0.340 (0.351) data 0.000 (0.015) loss 1.1828 (1.8000) lr 9.9726e-03 eta 0:16:04
epoch [2/30] batch [40/96] time 0.348 (0.351) data 0.000 (0.015) loss 1.8781 (1.8058) lr 9.9726e-03 eta 0:16:01
epoch [2/30] batch [42/96] time 0.344 (0.350) data 0.000 (0.014) loss 0.9055 (1.7856) lr 9.9726e-03 eta 0:15:59
epoch [2/30] batch [44/96] time 0.343 (0.350) data 0.000 (0.013) loss 1.4870 (1.7900) lr 9.9726e-03 eta 0:15:57
epoch [2/30] batch [46/96] time 0.348 (0.349) data 0.000 (0.013) loss 0.9191 (1.7755) lr 9.9726e-03 eta 0:15:56
epoch [2/30] batch [48/96] time 0.344 (0.350) data 0.000 (0.012) loss 1.9943 (1.7654) lr 9.9726e-03 eta 0:15:56
epoch [2/30] batch [50/96] time 0.339 (0.349) data 0.000 (0.012) loss 1.8877 (1.7732) lr 9.9726e-03 eta 0:15:54
epoch [2/30] batch [52/96] time 0.329 (0.348) data 0.000 (0.011) loss 1.3684 (1.7662) lr 9.9726e-03 eta 0:15:51
epoch [2/30] batch [54/96] time 0.336 (0.348) data 0.000 (0.011) loss 1.2476 (1.7552) lr 9.9726e-03 eta 0:15:49
epoch [2/30] batch [56/96] time 0.328 (0.347) data 0.000 (0.010) loss 2.1334 (1.7771) lr 9.9726e-03 eta 0:15:46
epoch [2/30] batch [58/96] time 0.332 (0.347) data 0.000 (0.010) loss 1.2340 (1.7681) lr 9.9726e-03 eta 0:15:44
epoch [2/30] batch [60/96] time 0.339 (0.346) data 0.000 (0.010) loss 1.5520 (1.7571) lr 9.9726e-03 eta 0:15:43
epoch [2/30] batch [62/96] time 0.318 (0.345) data 0.000 (0.009) loss 1.6328 (1.7413) lr 9.9726e-03 eta 0:15:39
epoch [2/30] batch [64/96] time 0.333 (0.345) data 0.000 (0.009) loss 1.0863 (1.7274) lr 9.9726e-03 eta 0:15:37
epoch [2/30] batch [66/96] time 0.330 (0.344) data 0.000 (0.009) loss 1.9884 (1.7280) lr 9.9726e-03 eta 0:15:35
epoch [2/30] batch [68/96] time 0.319 (0.344) data 0.000 (0.009) loss 0.9642 (1.7172) lr 9.9726e-03 eta 0:15:33
epoch [2/30] batch [70/96] time 0.331 (0.343) data 0.000 (0.008) loss 1.6240 (1.7140) lr 9.9726e-03 eta 0:15:31
epoch [2/30] batch [72/96] time 0.340 (0.343) data 0.000 (0.008) loss 1.6227 (1.7018) lr 9.9726e-03 eta 0:15:30
epoch [2/30] batch [74/96] time 0.316 (0.342) data 0.000 (0.008) loss 1.7604 (1.7236) lr 9.9726e-03 eta 0:15:27
epoch [2/30] batch [76/96] time 0.320 (0.342) data 0.000 (0.008) loss 2.7731 (1.7351) lr 9.9726e-03 eta 0:15:25
epoch [2/30] batch [78/96] time 0.318 (0.341) data 0.000 (0.008) loss 1.1868 (1.7306) lr 9.9726e-03 eta 0:15:22
epoch [2/30] batch [80/96] time 0.318 (0.340) data 0.000 (0.007) loss 1.2092 (1.7253) lr 9.9726e-03 eta 0:15:20
epoch [2/30] batch [82/96] time 0.328 (0.340) data 0.000 (0.007) loss 1.2383 (1.7152) lr 9.9726e-03 eta 0:15:18
epoch [2/30] batch [84/96] time 0.318 (0.339) data 0.000 (0.007) loss 0.8708 (1.7033) lr 9.9726e-03 eta 0:15:16
epoch [2/30] batch [86/96] time 0.322 (0.339) data 0.000 (0.007) loss 1.5610 (1.6968) lr 9.9726e-03 eta 0:15:14
epoch [2/30] batch [88/96] time 0.323 (0.339) data 0.000 (0.007) loss 1.3384 (1.6903) lr 9.9726e-03 eta 0:15:12
epoch [2/30] batch [90/96] time 0.326 (0.338) data 0.000 (0.007) loss 1.1190 (1.6870) lr 9.9726e-03 eta 0:15:11
epoch [2/30] batch [92/96] time 0.327 (0.338) data 0.000 (0.006) loss 1.4333 (1.6795) lr 9.9726e-03 eta 0:15:10
epoch [2/30] batch [94/96] time 0.324 (0.338) data 0.000 (0.006) loss 1.4812 (1.6789) lr 9.9726e-03 eta 0:15:08
epoch [2/30] batch [96/96] time 0.327 (0.338) data 0.000 (0.006) loss 1.7784 (1.6990) lr 9.8907e-03 eta 0:15:07
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.85s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 355
* accuracy: 61.6%
* error: 38.4%
* macro_f1: 57.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [3/30] batch [2/96] time 0.376 (0.710) data 0.001 (0.301) loss 1.5844 (1.6313) lr 9.8907e-03 eta 0:31:46
epoch [3/30] batch [4/96] time 0.370 (0.559) data 0.000 (0.151) loss 2.5869 (1.7746) lr 9.8907e-03 eta 0:25:01
epoch [3/30] batch [6/96] time 0.347 (0.491) data 0.000 (0.101) loss 1.2856 (1.6159) lr 9.8907e-03 eta 0:21:57
epoch [3/30] batch [8/96] time 0.333 (0.453) data 0.000 (0.075) loss 1.5819 (1.5733) lr 9.8907e-03 eta 0:20:14
epoch [3/30] batch [10/96] time 0.342 (0.430) data 0.000 (0.060) loss 1.0749 (1.4930) lr 9.8907e-03 eta 0:19:11
epoch [3/30] batch [12/96] time 0.342 (0.415) data 0.000 (0.050) loss 1.3465 (1.4901) lr 9.8907e-03 eta 0:18:30
epoch [3/30] batch [14/96] time 0.348 (0.406) data 0.000 (0.043) loss 1.6601 (1.4878) lr 9.8907e-03 eta 0:18:05
epoch [3/30] batch [16/96] time 0.340 (0.397) data 0.000 (0.038) loss 1.2852 (1.5084) lr 9.8907e-03 eta 0:17:41
epoch [3/30] batch [18/96] time 0.341 (0.391) data 0.000 (0.034) loss 1.8712 (1.5415) lr 9.8907e-03 eta 0:17:24
epoch [3/30] batch [20/96] time 0.330 (0.386) data 0.000 (0.030) loss 2.0499 (1.6127) lr 9.8907e-03 eta 0:17:10
epoch [3/30] batch [22/96] time 0.327 (0.380) data 0.000 (0.028) loss 1.2830 (1.5927) lr 9.8907e-03 eta 0:16:53
epoch [3/30] batch [24/96] time 0.328 (0.376) data 0.000 (0.025) loss 2.6105 (1.6251) lr 9.8907e-03 eta 0:16:41
epoch [3/30] batch [26/96] time 0.321 (0.372) data 0.000 (0.023) loss 2.0360 (1.6613) lr 9.8907e-03 eta 0:16:29
epoch [3/30] batch [28/96] time 0.319 (0.368) data 0.000 (0.022) loss 1.5059 (1.6363) lr 9.8907e-03 eta 0:16:19
epoch [3/30] batch [30/96] time 0.325 (0.365) data 0.000 (0.020) loss 1.5585 (1.6482) lr 9.8907e-03 eta 0:16:11
epoch [3/30] batch [32/96] time 0.342 (0.363) data 0.000 (0.019) loss 1.5814 (1.6335) lr 9.8907e-03 eta 0:16:04
epoch [3/30] batch [34/96] time 0.326 (0.361) data 0.000 (0.018) loss 1.0825 (1.6212) lr 9.8907e-03 eta 0:15:57
epoch [3/30] batch [36/96] time 0.323 (0.359) data 0.000 (0.017) loss 1.7838 (1.6444) lr 9.8907e-03 eta 0:15:51
epoch [3/30] batch [38/96] time 0.324 (0.357) data 0.000 (0.016) loss 2.1725 (1.6563) lr 9.8907e-03 eta 0:15:46
epoch [3/30] batch [40/96] time 0.349 (0.356) data 0.000 (0.015) loss 1.2141 (1.6296) lr 9.8907e-03 eta 0:15:42
epoch [3/30] batch [42/96] time 0.355 (0.356) data 0.000 (0.015) loss 1.4732 (1.6433) lr 9.8907e-03 eta 0:15:41
epoch [3/30] batch [44/96] time 0.343 (0.355) data 0.000 (0.014) loss 2.3413 (1.6523) lr 9.8907e-03 eta 0:15:38
epoch [3/30] batch [46/96] time 0.324 (0.354) data 0.000 (0.013) loss 1.8190 (1.6499) lr 9.8907e-03 eta 0:15:35
epoch [3/30] batch [48/96] time 0.330 (0.353) data 0.000 (0.013) loss 1.7496 (1.6539) lr 9.8907e-03 eta 0:15:32
epoch [3/30] batch [50/96] time 0.336 (0.353) data 0.000 (0.012) loss 2.4016 (1.6756) lr 9.8907e-03 eta 0:15:30
epoch [3/30] batch [52/96] time 0.337 (0.352) data 0.000 (0.012) loss 1.0651 (1.6688) lr 9.8907e-03 eta 0:15:28
epoch [3/30] batch [54/96] time 0.348 (0.352) data 0.000 (0.011) loss 1.1513 (1.6512) lr 9.8907e-03 eta 0:15:27
epoch [3/30] batch [56/96] time 0.340 (0.352) data 0.000 (0.011) loss 1.7296 (1.6411) lr 9.8907e-03 eta 0:15:25
epoch [3/30] batch [58/96] time 0.342 (0.351) data 0.000 (0.011) loss 1.7156 (1.6403) lr 9.8907e-03 eta 0:15:23
epoch [3/30] batch [60/96] time 0.327 (0.350) data 0.001 (0.010) loss 1.2387 (1.6325) lr 9.8907e-03 eta 0:15:20
epoch [3/30] batch [62/96] time 0.338 (0.350) data 0.000 (0.010) loss 1.5494 (1.6229) lr 9.8907e-03 eta 0:15:18
epoch [3/30] batch [64/96] time 0.328 (0.349) data 0.000 (0.010) loss 2.4174 (1.6336) lr 9.8907e-03 eta 0:15:16
epoch [3/30] batch [66/96] time 0.320 (0.348) data 0.000 (0.009) loss 1.0477 (1.6268) lr 9.8907e-03 eta 0:15:13
epoch [3/30] batch [68/96] time 0.339 (0.348) data 0.000 (0.009) loss 0.7649 (1.6073) lr 9.8907e-03 eta 0:15:12
epoch [3/30] batch [70/96] time 0.327 (0.347) data 0.000 (0.009) loss 1.4736 (1.6084) lr 9.8907e-03 eta 0:15:09
epoch [3/30] batch [72/96] time 0.338 (0.347) data 0.000 (0.009) loss 0.8527 (1.5951) lr 9.8907e-03 eta 0:15:08
epoch [3/30] batch [74/96] time 0.314 (0.346) data 0.000 (0.008) loss 1.9007 (1.5975) lr 9.8907e-03 eta 0:15:04
epoch [3/30] batch [76/96] time 0.309 (0.345) data 0.000 (0.008) loss 1.0415 (1.5927) lr 9.8907e-03 eta 0:15:01
epoch [3/30] batch [78/96] time 0.308 (0.344) data 0.000 (0.008) loss 3.0086 (1.6224) lr 9.8907e-03 eta 0:14:58
epoch [3/30] batch [80/96] time 0.310 (0.343) data 0.000 (0.008) loss 1.4252 (1.6198) lr 9.8907e-03 eta 0:14:55
epoch [3/30] batch [82/96] time 0.304 (0.342) data 0.000 (0.008) loss 0.9471 (1.6113) lr 9.8907e-03 eta 0:14:52
epoch [3/30] batch [84/96] time 0.310 (0.342) data 0.000 (0.007) loss 1.2927 (1.6048) lr 9.8907e-03 eta 0:14:49
epoch [3/30] batch [86/96] time 0.307 (0.341) data 0.000 (0.007) loss 2.7044 (1.6132) lr 9.8907e-03 eta 0:14:46
epoch [3/30] batch [88/96] time 0.308 (0.340) data 0.000 (0.007) loss 1.0283 (1.5965) lr 9.8907e-03 eta 0:14:43
epoch [3/30] batch [90/96] time 0.306 (0.339) data 0.000 (0.007) loss 1.3018 (1.5970) lr 9.8907e-03 eta 0:14:41
epoch [3/30] batch [92/96] time 0.306 (0.339) data 0.000 (0.007) loss 2.5400 (1.6001) lr 9.8907e-03 eta 0:14:38
epoch [3/30] batch [94/96] time 0.309 (0.338) data 0.000 (0.007) loss 1.3994 (1.5941) lr 9.8907e-03 eta 0:14:36
epoch [3/30] batch [96/96] time 0.307 (0.337) data 0.000 (0.007) loss 2.0787 (1.5998) lr 9.7553e-03 eta 0:14:34
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.88s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 357
* accuracy: 62.0%
* error: 38.0%
* macro_f1: 58.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [4/30] batch [2/96] time 0.327 (0.659) data 0.000 (0.292) loss 1.2623 (1.4230) lr 9.7553e-03 eta 0:28:26
epoch [4/30] batch [4/96] time 0.331 (0.492) data 0.000 (0.146) loss 1.1940 (1.3051) lr 9.7553e-03 eta 0:21:13
epoch [4/30] batch [6/96] time 0.330 (0.439) data 0.000 (0.098) loss 1.2549 (1.3205) lr 9.7553e-03 eta 0:18:55
epoch [4/30] batch [8/96] time 0.328 (0.411) data 0.000 (0.073) loss 2.4722 (1.5766) lr 9.7553e-03 eta 0:17:43
epoch [4/30] batch [10/96] time 0.338 (0.397) data 0.000 (0.059) loss 1.5793 (1.5194) lr 9.7553e-03 eta 0:17:05
epoch [4/30] batch [12/96] time 0.328 (0.387) data 0.000 (0.049) loss 1.6567 (1.4798) lr 9.7553e-03 eta 0:16:37
epoch [4/30] batch [14/96] time 0.344 (0.380) data 0.000 (0.042) loss 1.4952 (1.5628) lr 9.7553e-03 eta 0:16:20
epoch [4/30] batch [16/96] time 0.331 (0.374) data 0.000 (0.037) loss 0.8381 (1.4961) lr 9.7553e-03 eta 0:16:03
epoch [4/30] batch [18/96] time 0.327 (0.370) data 0.000 (0.033) loss 1.6032 (1.4940) lr 9.7553e-03 eta 0:15:52
epoch [4/30] batch [20/96] time 0.336 (0.366) data 0.000 (0.030) loss 1.4768 (1.4934) lr 9.7553e-03 eta 0:15:40
epoch [4/30] batch [22/96] time 0.316 (0.362) data 0.000 (0.027) loss 2.1428 (1.5241) lr 9.7553e-03 eta 0:15:29
epoch [4/30] batch [24/96] time 0.329 (0.359) data 0.000 (0.025) loss 2.2381 (1.5711) lr 9.7553e-03 eta 0:15:22
epoch [4/30] batch [26/96] time 0.329 (0.357) data 0.000 (0.023) loss 2.7996 (1.6412) lr 9.7553e-03 eta 0:15:15
epoch [4/30] batch [28/96] time 0.334 (0.359) data 0.000 (0.021) loss 1.8999 (1.6304) lr 9.7553e-03 eta 0:15:19
epoch [4/30] batch [30/96] time 0.332 (0.357) data 0.000 (0.020) loss 1.4500 (1.6328) lr 9.7553e-03 eta 0:15:15
epoch [4/30] batch [32/96] time 0.320 (0.355) data 0.000 (0.019) loss 1.8770 (1.6200) lr 9.7553e-03 eta 0:15:09
epoch [4/30] batch [34/96] time 0.316 (0.353) data 0.000 (0.018) loss 1.3141 (1.6215) lr 9.7553e-03 eta 0:15:03
epoch [4/30] batch [36/96] time 0.328 (0.352) data 0.000 (0.017) loss 2.2047 (1.6456) lr 9.7553e-03 eta 0:14:58
epoch [4/30] batch [38/96] time 0.316 (0.350) data 0.000 (0.016) loss 0.9451 (1.6309) lr 9.7553e-03 eta 0:14:54
epoch [4/30] batch [40/96] time 0.320 (0.349) data 0.000 (0.015) loss 1.3050 (1.6072) lr 9.7553e-03 eta 0:14:50
epoch [4/30] batch [42/96] time 0.330 (0.348) data 0.000 (0.014) loss 1.4042 (1.6028) lr 9.7553e-03 eta 0:14:46
epoch [4/30] batch [44/96] time 0.308 (0.346) data 0.000 (0.014) loss 1.8613 (1.5925) lr 9.7553e-03 eta 0:14:42
epoch [4/30] batch [46/96] time 0.316 (0.345) data 0.000 (0.013) loss 1.6483 (1.6038) lr 9.7553e-03 eta 0:14:38
epoch [4/30] batch [48/96] time 0.317 (0.344) data 0.000 (0.013) loss 1.7142 (1.6077) lr 9.7553e-03 eta 0:14:35
epoch [4/30] batch [50/96] time 0.324 (0.343) data 0.000 (0.012) loss 1.4722 (1.5913) lr 9.7553e-03 eta 0:14:33
epoch [4/30] batch [52/96] time 0.329 (0.343) data 0.000 (0.012) loss 1.3978 (1.6119) lr 9.7553e-03 eta 0:14:30
epoch [4/30] batch [54/96] time 0.317 (0.342) data 0.000 (0.011) loss 3.2000 (1.6440) lr 9.7553e-03 eta 0:14:27
epoch [4/30] batch [56/96] time 0.326 (0.341) data 0.000 (0.011) loss 1.7438 (1.6402) lr 9.7553e-03 eta 0:14:24
epoch [4/30] batch [58/96] time 0.330 (0.341) data 0.000 (0.010) loss 0.9961 (1.6403) lr 9.7553e-03 eta 0:14:22
epoch [4/30] batch [60/96] time 0.320 (0.340) data 0.000 (0.010) loss 1.4015 (1.6514) lr 9.7553e-03 eta 0:14:20
epoch [4/30] batch [62/96] time 0.333 (0.340) data 0.001 (0.010) loss 1.8416 (1.6434) lr 9.7553e-03 eta 0:14:20
epoch [4/30] batch [64/96] time 0.324 (0.339) data 0.000 (0.009) loss 1.4768 (1.6398) lr 9.7553e-03 eta 0:14:18
epoch [4/30] batch [66/96] time 0.330 (0.339) data 0.000 (0.009) loss 1.3803 (1.6369) lr 9.7553e-03 eta 0:14:16
epoch [4/30] batch [68/96] time 0.329 (0.339) data 0.000 (0.009) loss 0.9413 (1.6251) lr 9.7553e-03 eta 0:14:16
epoch [4/30] batch [70/96] time 0.329 (0.339) data 0.000 (0.009) loss 2.0328 (1.6311) lr 9.7553e-03 eta 0:14:14
epoch [4/30] batch [72/96] time 0.326 (0.338) data 0.000 (0.008) loss 2.1276 (1.6534) lr 9.7553e-03 eta 0:14:13
epoch [4/30] batch [74/96] time 0.306 (0.338) data 0.000 (0.008) loss 1.9483 (1.6552) lr 9.7553e-03 eta 0:14:09
epoch [4/30] batch [76/96] time 0.304 (0.337) data 0.000 (0.008) loss 1.9587 (1.6566) lr 9.7553e-03 eta 0:14:06
epoch [4/30] batch [78/96] time 0.306 (0.336) data 0.000 (0.008) loss 2.2520 (1.6626) lr 9.7553e-03 eta 0:14:04
epoch [4/30] batch [80/96] time 0.297 (0.335) data 0.000 (0.008) loss 2.4689 (1.6660) lr 9.7553e-03 eta 0:14:01
epoch [4/30] batch [82/96] time 0.300 (0.334) data 0.000 (0.007) loss 1.9603 (1.6622) lr 9.7553e-03 eta 0:13:58
epoch [4/30] batch [84/96] time 0.300 (0.333) data 0.000 (0.007) loss 1.1163 (1.6561) lr 9.7553e-03 eta 0:13:56
epoch [4/30] batch [86/96] time 0.302 (0.333) data 0.000 (0.007) loss 1.2761 (1.6454) lr 9.7553e-03 eta 0:13:53
epoch [4/30] batch [88/96] time 0.304 (0.332) data 0.000 (0.007) loss 1.4690 (1.6470) lr 9.7553e-03 eta 0:13:51
epoch [4/30] batch [90/96] time 0.304 (0.331) data 0.000 (0.007) loss 1.8081 (1.6413) lr 9.7553e-03 eta 0:13:49
epoch [4/30] batch [92/96] time 0.308 (0.331) data 0.000 (0.007) loss 1.1701 (1.6345) lr 9.7553e-03 eta 0:13:47
epoch [4/30] batch [94/96] time 0.304 (0.330) data 0.000 (0.007) loss 1.6379 (1.6384) lr 9.7553e-03 eta 0:13:45
epoch [4/30] batch [96/96] time 0.303 (0.330) data 0.000 (0.006) loss 1.7785 (1.6406) lr 9.5677e-03 eta 0:13:42
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.87s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 363
* accuracy: 63.0%
* error: 37.0%
* macro_f1: 59.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [5/30] batch [2/96] time 0.325 (0.655) data 0.000 (0.279) loss 2.3863 (1.9347) lr 9.5677e-03 eta 0:27:14
epoch [5/30] batch [4/96] time 0.323 (0.486) data 0.000 (0.140) loss 1.7083 (1.7785) lr 9.5677e-03 eta 0:20:11
epoch [5/30] batch [6/96] time 0.330 (0.433) data 0.000 (0.093) loss 1.3194 (1.6784) lr 9.5677e-03 eta 0:17:58
epoch [5/30] batch [8/96] time 0.323 (0.406) data 0.000 (0.070) loss 1.4234 (1.5875) lr 9.5677e-03 eta 0:16:49
epoch [5/30] batch [10/96] time 0.321 (0.389) data 0.000 (0.056) loss 1.2842 (1.5212) lr 9.5677e-03 eta 0:16:07
epoch [5/30] batch [12/96] time 0.356 (0.382) data 0.000 (0.047) loss 1.5546 (1.5026) lr 9.5677e-03 eta 0:15:49
epoch [5/30] batch [14/96] time 0.361 (0.380) data 0.001 (0.040) loss 2.4613 (1.5313) lr 9.5677e-03 eta 0:15:43
epoch [5/30] batch [16/96] time 0.326 (0.374) data 0.000 (0.035) loss 1.6531 (1.6501) lr 9.5677e-03 eta 0:15:27
epoch [5/30] batch [18/96] time 0.345 (0.370) data 0.000 (0.031) loss 1.8229 (1.7382) lr 9.5677e-03 eta 0:15:17
epoch [5/30] batch [20/96] time 0.313 (0.365) data 0.000 (0.028) loss 1.0344 (1.6782) lr 9.5677e-03 eta 0:15:03
epoch [5/30] batch [22/96] time 0.329 (0.362) data 0.000 (0.026) loss 1.2442 (1.6519) lr 9.5677e-03 eta 0:14:54
epoch [5/30] batch [24/96] time 0.327 (0.358) data 0.000 (0.024) loss 1.7217 (1.6583) lr 9.5677e-03 eta 0:14:45
epoch [5/30] batch [26/96] time 0.339 (0.356) data 0.000 (0.022) loss 0.8349 (1.6322) lr 9.5677e-03 eta 0:14:39
epoch [5/30] batch [28/96] time 0.323 (0.354) data 0.000 (0.020) loss 2.2365 (1.6860) lr 9.5677e-03 eta 0:14:33
epoch [5/30] batch [30/96] time 0.316 (0.351) data 0.000 (0.019) loss 0.9904 (1.6698) lr 9.5677e-03 eta 0:14:26
epoch [5/30] batch [32/96] time 0.320 (0.350) data 0.000 (0.018) loss 1.9270 (1.6632) lr 9.5677e-03 eta 0:14:21
epoch [5/30] batch [34/96] time 0.333 (0.352) data 0.001 (0.017) loss 1.3702 (1.6375) lr 9.5677e-03 eta 0:14:25
epoch [5/30] batch [36/96] time 0.328 (0.350) data 0.000 (0.016) loss 1.4130 (1.6396) lr 9.5677e-03 eta 0:14:21
epoch [5/30] batch [38/96] time 0.321 (0.349) data 0.000 (0.015) loss 1.4708 (1.6394) lr 9.5677e-03 eta 0:14:16
epoch [5/30] batch [40/96] time 0.332 (0.348) data 0.000 (0.014) loss 2.0771 (1.6602) lr 9.5677e-03 eta 0:14:14
epoch [5/30] batch [42/96] time 0.338 (0.348) data 0.000 (0.014) loss 1.2815 (1.6421) lr 9.5677e-03 eta 0:14:13
epoch [5/30] batch [44/96] time 0.320 (0.347) data 0.000 (0.013) loss 1.9172 (1.6524) lr 9.5677e-03 eta 0:14:10
epoch [5/30] batch [46/96] time 0.327 (0.346) data 0.000 (0.012) loss 1.5296 (1.6465) lr 9.5677e-03 eta 0:14:06
epoch [5/30] batch [48/96] time 0.316 (0.345) data 0.000 (0.012) loss 0.9421 (1.6319) lr 9.5677e-03 eta 0:14:03
epoch [5/30] batch [50/96] time 0.314 (0.343) data 0.000 (0.011) loss 0.9460 (1.6199) lr 9.5677e-03 eta 0:13:59
epoch [5/30] batch [52/96] time 0.338 (0.343) data 0.000 (0.011) loss 2.0301 (1.6648) lr 9.5677e-03 eta 0:13:57
epoch [5/30] batch [54/96] time 0.317 (0.342) data 0.000 (0.011) loss 1.3286 (1.6494) lr 9.5677e-03 eta 0:13:54
epoch [5/30] batch [56/96] time 0.325 (0.341) data 0.000 (0.010) loss 1.0806 (1.6379) lr 9.5677e-03 eta 0:13:52
epoch [5/30] batch [58/96] time 0.318 (0.340) data 0.000 (0.010) loss 1.8535 (1.6482) lr 9.5677e-03 eta 0:13:49
epoch [5/30] batch [60/96] time 0.313 (0.340) data 0.000 (0.010) loss 2.5510 (1.6808) lr 9.5677e-03 eta 0:13:47
epoch [5/30] batch [62/96] time 0.321 (0.339) data 0.000 (0.009) loss 1.2969 (1.6793) lr 9.5677e-03 eta 0:13:44
epoch [5/30] batch [64/96] time 0.328 (0.339) data 0.000 (0.009) loss 2.4407 (1.6784) lr 9.5677e-03 eta 0:13:43
epoch [5/30] batch [66/96] time 0.324 (0.338) data 0.000 (0.009) loss 1.2711 (1.6662) lr 9.5677e-03 eta 0:13:41
epoch [5/30] batch [68/96] time 0.326 (0.338) data 0.000 (0.009) loss 1.1582 (1.6546) lr 9.5677e-03 eta 0:13:39
epoch [5/30] batch [70/96] time 0.327 (0.337) data 0.000 (0.008) loss 1.2334 (1.6574) lr 9.5677e-03 eta 0:13:38
epoch [5/30] batch [72/96] time 0.326 (0.337) data 0.000 (0.008) loss 0.8321 (1.6412) lr 9.5677e-03 eta 0:13:36
epoch [5/30] batch [74/96] time 0.307 (0.336) data 0.000 (0.008) loss 0.8747 (1.6246) lr 9.5677e-03 eta 0:13:33
epoch [5/30] batch [76/96] time 0.308 (0.335) data 0.000 (0.008) loss 2.3423 (1.6374) lr 9.5677e-03 eta 0:13:31
epoch [5/30] batch [78/96] time 0.309 (0.334) data 0.000 (0.007) loss 1.0880 (1.6228) lr 9.5677e-03 eta 0:13:28
epoch [5/30] batch [80/96] time 0.302 (0.334) data 0.000 (0.007) loss 1.7048 (1.6232) lr 9.5677e-03 eta 0:13:26
epoch [5/30] batch [82/96] time 0.308 (0.333) data 0.000 (0.007) loss 1.2880 (1.6094) lr 9.5677e-03 eta 0:13:23
epoch [5/30] batch [84/96] time 0.304 (0.332) data 0.000 (0.007) loss 1.3386 (1.6002) lr 9.5677e-03 eta 0:13:21
epoch [5/30] batch [86/96] time 0.306 (0.332) data 0.000 (0.007) loss 2.2953 (1.5999) lr 9.5677e-03 eta 0:13:19
epoch [5/30] batch [88/96] time 0.311 (0.331) data 0.000 (0.007) loss 1.7489 (1.5970) lr 9.5677e-03 eta 0:13:17
epoch [5/30] batch [90/96] time 0.303 (0.330) data 0.000 (0.007) loss 1.3214 (1.5948) lr 9.5677e-03 eta 0:13:14
epoch [5/30] batch [92/96] time 0.317 (0.330) data 0.000 (0.006) loss 1.8361 (1.5906) lr 9.5677e-03 eta 0:13:13
epoch [5/30] batch [94/96] time 0.301 (0.330) data 0.000 (0.006) loss 1.0093 (1.5799) lr 9.5677e-03 eta 0:13:11
epoch [5/30] batch [96/96] time 0.308 (0.329) data 0.000 (0.006) loss 1.6592 (1.5793) lr 9.3301e-03 eta 0:13:09
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.82s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.33s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.16s/it]=> result
* total: 576
* correct: 370
* accuracy: 64.2%
* error: 35.8%
* macro_f1: 61.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [6/30] batch [2/96] time 0.313 (0.629) data 0.000 (0.276) loss 1.7468 (1.6682) lr 9.3301e-03 eta 0:25:08
epoch [6/30] batch [4/96] time 0.326 (0.476) data 0.000 (0.138) loss 1.4322 (1.5668) lr 9.3301e-03 eta 0:18:59
epoch [6/30] batch [6/96] time 0.324 (0.426) data 0.000 (0.092) loss 0.9102 (1.5281) lr 9.3301e-03 eta 0:16:59
epoch [6/30] batch [8/96] time 0.343 (0.403) data 0.000 (0.069) loss 1.3352 (1.4399) lr 9.3301e-03 eta 0:16:04
epoch [6/30] batch [10/96] time 0.341 (0.389) data 0.000 (0.055) loss 2.1213 (1.4490) lr 9.3301e-03 eta 0:15:29
epoch [6/30] batch [12/96] time 0.318 (0.378) data 0.000 (0.046) loss 1.6057 (1.4534) lr 9.3301e-03 eta 0:15:03
epoch [6/30] batch [14/96] time 0.332 (0.373) data 0.000 (0.040) loss 0.8621 (1.4401) lr 9.3301e-03 eta 0:14:49
epoch [6/30] batch [16/96] time 0.328 (0.367) data 0.000 (0.035) loss 1.0274 (1.3753) lr 9.3301e-03 eta 0:14:35
epoch [6/30] batch [18/96] time 0.321 (0.362) data 0.000 (0.031) loss 1.4778 (1.3584) lr 9.3301e-03 eta 0:14:23
epoch [6/30] batch [20/96] time 0.321 (0.358) data 0.000 (0.028) loss 2.3497 (1.3984) lr 9.3301e-03 eta 0:14:13
epoch [6/30] batch [22/96] time 0.332 (0.356) data 0.000 (0.025) loss 1.1590 (1.3725) lr 9.3301e-03 eta 0:14:05
epoch [6/30] batch [24/96] time 0.316 (0.353) data 0.000 (0.023) loss 1.3935 (1.3843) lr 9.3301e-03 eta 0:13:58
epoch [6/30] batch [26/96] time 0.331 (0.351) data 0.000 (0.021) loss 1.9038 (1.3869) lr 9.3301e-03 eta 0:13:52
epoch [6/30] batch [28/96] time 0.336 (0.349) data 0.000 (0.020) loss 1.4946 (1.3851) lr 9.3301e-03 eta 0:13:48
epoch [6/30] batch [30/96] time 0.307 (0.347) data 0.000 (0.019) loss 1.5210 (1.4059) lr 9.3301e-03 eta 0:13:42
epoch [6/30] batch [32/96] time 0.323 (0.345) data 0.000 (0.018) loss 1.3734 (1.4159) lr 9.3301e-03 eta 0:13:37
epoch [6/30] batch [34/96] time 0.320 (0.344) data 0.000 (0.017) loss 1.5554 (1.4188) lr 9.3301e-03 eta 0:13:33
epoch [6/30] batch [36/96] time 0.429 (0.346) data 0.000 (0.016) loss 1.5417 (1.4114) lr 9.3301e-03 eta 0:13:38
epoch [6/30] batch [38/96] time 0.329 (0.345) data 0.000 (0.015) loss 0.9789 (1.4111) lr 9.3301e-03 eta 0:13:35
epoch [6/30] batch [40/96] time 0.329 (0.344) data 0.000 (0.014) loss 1.7991 (1.4363) lr 9.3301e-03 eta 0:13:32
epoch [6/30] batch [42/96] time 0.320 (0.343) data 0.000 (0.013) loss 1.5739 (1.4668) lr 9.3301e-03 eta 0:13:28
epoch [6/30] batch [44/96] time 0.336 (0.342) data 0.000 (0.013) loss 2.0676 (1.4980) lr 9.3301e-03 eta 0:13:26
epoch [6/30] batch [46/96] time 0.320 (0.341) data 0.000 (0.012) loss 1.6671 (1.5195) lr 9.3301e-03 eta 0:13:23
epoch [6/30] batch [48/96] time 0.329 (0.341) data 0.000 (0.012) loss 2.3989 (1.5481) lr 9.3301e-03 eta 0:13:21
epoch [6/30] batch [50/96] time 0.323 (0.340) data 0.000 (0.011) loss 1.4439 (1.5406) lr 9.3301e-03 eta 0:13:19
epoch [6/30] batch [52/96] time 0.321 (0.339) data 0.000 (0.011) loss 1.9007 (1.5335) lr 9.3301e-03 eta 0:13:16
epoch [6/30] batch [54/96] time 0.325 (0.339) data 0.000 (0.011) loss 1.5722 (1.5281) lr 9.3301e-03 eta 0:13:15
epoch [6/30] batch [56/96] time 0.327 (0.339) data 0.000 (0.010) loss 2.5952 (1.5444) lr 9.3301e-03 eta 0:13:13
epoch [6/30] batch [58/96] time 0.346 (0.339) data 0.000 (0.010) loss 1.3603 (1.5403) lr 9.3301e-03 eta 0:13:13
epoch [6/30] batch [60/96] time 0.347 (0.339) data 0.000 (0.010) loss 1.3095 (1.5419) lr 9.3301e-03 eta 0:13:13
epoch [6/30] batch [62/96] time 0.342 (0.339) data 0.000 (0.009) loss 2.0108 (1.5423) lr 9.3301e-03 eta 0:13:12
epoch [6/30] batch [64/96] time 0.336 (0.339) data 0.000 (0.009) loss 1.6431 (1.5339) lr 9.3301e-03 eta 0:13:11
epoch [6/30] batch [66/96] time 0.310 (0.338) data 0.000 (0.009) loss 1.2841 (1.5285) lr 9.3301e-03 eta 0:13:09
epoch [6/30] batch [68/96] time 0.334 (0.338) data 0.000 (0.008) loss 1.2520 (1.5123) lr 9.3301e-03 eta 0:13:08
epoch [6/30] batch [70/96] time 0.330 (0.338) data 0.001 (0.008) loss 1.4541 (1.5204) lr 9.3301e-03 eta 0:13:07
epoch [6/30] batch [72/96] time 0.343 (0.338) data 0.000 (0.008) loss 2.5406 (1.5367) lr 9.3301e-03 eta 0:13:06
epoch [6/30] batch [74/96] time 0.320 (0.337) data 0.000 (0.008) loss 1.6754 (1.5420) lr 9.3301e-03 eta 0:13:04
epoch [6/30] batch [76/96] time 0.313 (0.337) data 0.000 (0.008) loss 2.0863 (1.5401) lr 9.3301e-03 eta 0:13:02
epoch [6/30] batch [78/96] time 0.317 (0.336) data 0.000 (0.007) loss 1.6379 (1.5323) lr 9.3301e-03 eta 0:13:00
epoch [6/30] batch [80/96] time 0.319 (0.336) data 0.000 (0.007) loss 1.4897 (1.5308) lr 9.3301e-03 eta 0:12:59
epoch [6/30] batch [82/96] time 0.315 (0.335) data 0.000 (0.007) loss 2.5297 (1.5487) lr 9.3301e-03 eta 0:12:57
epoch [6/30] batch [84/96] time 0.320 (0.335) data 0.000 (0.007) loss 2.0470 (1.5543) lr 9.3301e-03 eta 0:12:55
epoch [6/30] batch [86/96] time 0.316 (0.335) data 0.000 (0.007) loss 1.5770 (1.5533) lr 9.3301e-03 eta 0:12:54
epoch [6/30] batch [88/96] time 0.317 (0.334) data 0.000 (0.007) loss 1.4268 (1.5475) lr 9.3301e-03 eta 0:12:52
epoch [6/30] batch [90/96] time 0.321 (0.334) data 0.000 (0.006) loss 1.6896 (1.5416) lr 9.3301e-03 eta 0:12:51
epoch [6/30] batch [92/96] time 0.331 (0.334) data 0.000 (0.006) loss 0.7776 (1.5398) lr 9.3301e-03 eta 0:12:50
epoch [6/30] batch [94/96] time 0.317 (0.334) data 0.000 (0.006) loss 1.5547 (1.5423) lr 9.3301e-03 eta 0:12:49
epoch [6/30] batch [96/96] time 0.309 (0.333) data 0.000 (0.006) loss 1.9649 (1.5435) lr 9.0451e-03 eta 0:12:47
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.92s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.38s/it]100%|██████████| 3/3 [00:03<00:00,  1.14it/s]100%|██████████| 3/3 [00:03<00:00,  1.20s/it]=> result
* total: 576
* correct: 377
* accuracy: 65.5%
* error: 34.5%
* macro_f1: 63.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [7/30] batch [2/96] time 0.335 (0.657) data 0.000 (0.264) loss 1.1263 (1.4654) lr 9.0451e-03 eta 0:25:11
epoch [7/30] batch [4/96] time 0.354 (0.500) data 0.000 (0.132) loss 1.5818 (1.4548) lr 9.0451e-03 eta 0:19:09
epoch [7/30] batch [6/96] time 0.338 (0.445) data 0.000 (0.088) loss 1.7013 (1.4406) lr 9.0451e-03 eta 0:17:02
epoch [7/30] batch [8/96] time 0.336 (0.417) data 0.000 (0.066) loss 1.7760 (1.4243) lr 9.0451e-03 eta 0:15:57
epoch [7/30] batch [10/96] time 0.341 (0.402) data 0.000 (0.053) loss 1.8362 (1.4511) lr 9.0451e-03 eta 0:15:22
epoch [7/30] batch [12/96] time 0.331 (0.389) data 0.000 (0.044) loss 1.1385 (1.4809) lr 9.0451e-03 eta 0:14:52
epoch [7/30] batch [14/96] time 0.319 (0.380) data 0.000 (0.038) loss 0.7144 (1.3802) lr 9.0451e-03 eta 0:14:29
epoch [7/30] batch [16/96] time 0.331 (0.374) data 0.000 (0.033) loss 1.4144 (1.3609) lr 9.0451e-03 eta 0:14:15
epoch [7/30] batch [18/96] time 0.326 (0.368) data 0.000 (0.030) loss 1.2666 (1.3461) lr 9.0451e-03 eta 0:14:02
epoch [7/30] batch [20/96] time 0.334 (0.365) data 0.000 (0.027) loss 1.1078 (1.3206) lr 9.0451e-03 eta 0:13:53
epoch [7/30] batch [22/96] time 0.328 (0.361) data 0.000 (0.024) loss 1.1783 (1.3085) lr 9.0451e-03 eta 0:13:44
epoch [7/30] batch [24/96] time 0.326 (0.359) data 0.000 (0.022) loss 3.0935 (1.3971) lr 9.0451e-03 eta 0:13:37
epoch [7/30] batch [26/96] time 0.329 (0.356) data 0.000 (0.021) loss 0.8025 (1.3797) lr 9.0451e-03 eta 0:13:31
epoch [7/30] batch [28/96] time 0.340 (0.354) data 0.000 (0.019) loss 1.3613 (1.3827) lr 9.0451e-03 eta 0:13:26
epoch [7/30] batch [30/96] time 0.319 (0.353) data 0.000 (0.018) loss 1.4017 (1.4005) lr 9.0451e-03 eta 0:13:21
epoch [7/30] batch [32/96] time 0.330 (0.351) data 0.000 (0.017) loss 1.9108 (1.4312) lr 9.0451e-03 eta 0:13:17
epoch [7/30] batch [34/96] time 0.328 (0.350) data 0.000 (0.016) loss 1.8810 (1.4313) lr 9.0451e-03 eta 0:13:13
epoch [7/30] batch [36/96] time 0.419 (0.351) data 0.000 (0.015) loss 1.1103 (1.4503) lr 9.0451e-03 eta 0:13:16
epoch [7/30] batch [38/96] time 0.342 (0.351) data 0.000 (0.014) loss 1.2841 (1.4540) lr 9.0451e-03 eta 0:13:14
epoch [7/30] batch [40/96] time 0.339 (0.350) data 0.000 (0.014) loss 1.3448 (1.4520) lr 9.0451e-03 eta 0:13:11
epoch [7/30] batch [42/96] time 0.332 (0.349) data 0.000 (0.013) loss 1.2194 (1.4358) lr 9.0451e-03 eta 0:13:09
epoch [7/30] batch [44/96] time 0.323 (0.348) data 0.000 (0.012) loss 1.0928 (1.4468) lr 9.0451e-03 eta 0:13:06
epoch [7/30] batch [46/96] time 0.317 (0.347) data 0.000 (0.012) loss 1.7863 (1.4526) lr 9.0451e-03 eta 0:13:03
epoch [7/30] batch [48/96] time 0.322 (0.346) data 0.000 (0.011) loss 1.1283 (1.4502) lr 9.0451e-03 eta 0:13:00
epoch [7/30] batch [50/96] time 0.331 (0.345) data 0.000 (0.011) loss 1.3811 (1.4471) lr 9.0451e-03 eta 0:12:58
epoch [7/30] batch [52/96] time 0.329 (0.345) data 0.000 (0.010) loss 0.8539 (1.4467) lr 9.0451e-03 eta 0:12:56
epoch [7/30] batch [54/96] time 0.319 (0.344) data 0.000 (0.010) loss 2.1056 (1.4590) lr 9.0451e-03 eta 0:12:53
epoch [7/30] batch [56/96] time 0.326 (0.343) data 0.000 (0.010) loss 2.3431 (1.4842) lr 9.0451e-03 eta 0:12:51
epoch [7/30] batch [58/96] time 0.322 (0.342) data 0.000 (0.009) loss 0.9387 (1.4803) lr 9.0451e-03 eta 0:12:48
epoch [7/30] batch [60/96] time 0.326 (0.342) data 0.000 (0.009) loss 1.7518 (1.4900) lr 9.0451e-03 eta 0:12:47
epoch [7/30] batch [62/96] time 0.328 (0.342) data 0.000 (0.009) loss 1.3167 (1.4900) lr 9.0451e-03 eta 0:12:45
epoch [7/30] batch [64/96] time 0.320 (0.341) data 0.000 (0.009) loss 1.1675 (1.4992) lr 9.0451e-03 eta 0:12:43
epoch [7/30] batch [66/96] time 0.325 (0.341) data 0.000 (0.008) loss 1.6571 (1.4931) lr 9.0451e-03 eta 0:12:42
epoch [7/30] batch [68/96] time 0.328 (0.340) data 0.000 (0.008) loss 0.9867 (1.4889) lr 9.0451e-03 eta 0:12:40
epoch [7/30] batch [70/96] time 0.337 (0.340) data 0.000 (0.008) loss 1.4373 (1.4913) lr 9.0451e-03 eta 0:12:39
epoch [7/30] batch [72/96] time 0.324 (0.340) data 0.000 (0.008) loss 2.0618 (1.4977) lr 9.0451e-03 eta 0:12:38
epoch [7/30] batch [74/96] time 0.307 (0.339) data 0.000 (0.007) loss 2.0875 (1.5026) lr 9.0451e-03 eta 0:12:35
epoch [7/30] batch [76/96] time 0.312 (0.338) data 0.000 (0.007) loss 2.2115 (1.5081) lr 9.0451e-03 eta 0:12:33
epoch [7/30] batch [78/96] time 0.300 (0.337) data 0.000 (0.007) loss 1.3624 (1.5064) lr 9.0451e-03 eta 0:12:30
epoch [7/30] batch [80/96] time 0.301 (0.337) data 0.000 (0.007) loss 1.8508 (1.5155) lr 9.0451e-03 eta 0:12:28
epoch [7/30] batch [82/96] time 0.303 (0.336) data 0.000 (0.007) loss 1.7984 (1.5267) lr 9.0451e-03 eta 0:12:25
epoch [7/30] batch [84/96] time 0.308 (0.335) data 0.000 (0.007) loss 1.5935 (1.5267) lr 9.0451e-03 eta 0:12:23
epoch [7/30] batch [86/96] time 0.307 (0.334) data 0.000 (0.006) loss 1.5612 (1.5218) lr 9.0451e-03 eta 0:12:21
epoch [7/30] batch [88/96] time 0.305 (0.334) data 0.000 (0.006) loss 1.4690 (1.5169) lr 9.0451e-03 eta 0:12:19
epoch [7/30] batch [90/96] time 0.308 (0.333) data 0.000 (0.006) loss 1.2198 (1.5121) lr 9.0451e-03 eta 0:12:17
epoch [7/30] batch [92/96] time 0.299 (0.332) data 0.000 (0.006) loss 2.2210 (1.5215) lr 9.0451e-03 eta 0:12:14
epoch [7/30] batch [94/96] time 0.311 (0.332) data 0.000 (0.006) loss 1.3987 (1.5125) lr 9.0451e-03 eta 0:12:13
epoch [7/30] batch [96/96] time 0.312 (0.331) data 0.000 (0.006) loss 1.8563 (1.5118) lr 8.7157e-03 eta 0:12:11
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.87s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 378
* accuracy: 65.6%
* error: 34.4%
* macro_f1: 63.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [8/30] batch [2/96] time 0.309 (0.628) data 0.000 (0.265) loss 1.6885 (1.3839) lr 8.7157e-03 eta 0:23:06
epoch [8/30] batch [4/96] time 0.327 (0.476) data 0.000 (0.133) loss 1.4120 (1.3142) lr 8.7157e-03 eta 0:17:28
epoch [8/30] batch [6/96] time 0.333 (0.429) data 0.000 (0.089) loss 2.7546 (1.5489) lr 8.7157e-03 eta 0:15:45
epoch [8/30] batch [8/96] time 0.324 (0.404) data 0.000 (0.067) loss 1.4501 (1.4601) lr 8.7157e-03 eta 0:14:49
epoch [8/30] batch [10/96] time 0.330 (0.390) data 0.000 (0.053) loss 1.2524 (1.4602) lr 8.7157e-03 eta 0:14:16
epoch [8/30] batch [12/96] time 0.321 (0.379) data 0.000 (0.044) loss 1.3107 (1.4975) lr 8.7157e-03 eta 0:13:51
epoch [8/30] batch [14/96] time 0.326 (0.371) data 0.000 (0.038) loss 1.3729 (1.4665) lr 8.7157e-03 eta 0:13:34
epoch [8/30] batch [16/96] time 0.331 (0.366) data 0.000 (0.033) loss 1.3752 (1.4816) lr 8.7157e-03 eta 0:13:21
epoch [8/30] batch [18/96] time 0.334 (0.363) data 0.000 (0.030) loss 1.4807 (1.5503) lr 8.7157e-03 eta 0:13:14
epoch [8/30] batch [20/96] time 0.326 (0.359) data 0.000 (0.027) loss 0.7692 (1.5227) lr 8.7157e-03 eta 0:13:06
epoch [8/30] batch [22/96] time 0.323 (0.356) data 0.000 (0.024) loss 1.0932 (1.5047) lr 8.7157e-03 eta 0:12:58
epoch [8/30] batch [24/96] time 0.329 (0.354) data 0.000 (0.022) loss 0.8431 (1.4864) lr 8.7157e-03 eta 0:12:52
epoch [8/30] batch [26/96] time 0.328 (0.352) data 0.000 (0.021) loss 0.7987 (1.4682) lr 8.7157e-03 eta 0:12:47
epoch [8/30] batch [28/96] time 0.336 (0.350) data 0.000 (0.019) loss 1.2623 (1.5062) lr 8.7157e-03 eta 0:12:43
epoch [8/30] batch [30/96] time 0.326 (0.348) data 0.000 (0.018) loss 1.5207 (1.4988) lr 8.7157e-03 eta 0:12:38
epoch [8/30] batch [32/96] time 0.331 (0.347) data 0.000 (0.017) loss 2.1209 (1.5362) lr 8.7157e-03 eta 0:12:35
epoch [8/30] batch [34/96] time 0.330 (0.346) data 0.000 (0.016) loss 0.9236 (1.5147) lr 8.7157e-03 eta 0:12:32
epoch [8/30] batch [36/96] time 0.423 (0.348) data 0.000 (0.015) loss 1.2819 (1.5362) lr 8.7157e-03 eta 0:12:35
epoch [8/30] batch [38/96] time 0.332 (0.347) data 0.000 (0.014) loss 1.9366 (1.5454) lr 8.7157e-03 eta 0:12:32
epoch [8/30] batch [40/96] time 0.329 (0.346) data 0.000 (0.014) loss 1.8087 (1.5547) lr 8.7157e-03 eta 0:12:30
epoch [8/30] batch [42/96] time 0.331 (0.345) data 0.000 (0.013) loss 2.3322 (1.5515) lr 8.7157e-03 eta 0:12:27
epoch [8/30] batch [44/96] time 0.334 (0.344) data 0.000 (0.012) loss 1.8339 (1.5802) lr 8.7157e-03 eta 0:12:25
epoch [8/30] batch [46/96] time 0.327 (0.344) data 0.000 (0.012) loss 1.1393 (1.5651) lr 8.7157e-03 eta 0:12:23
epoch [8/30] batch [48/96] time 0.330 (0.343) data 0.000 (0.011) loss 1.0528 (1.5526) lr 8.7157e-03 eta 0:12:21
epoch [8/30] batch [50/96] time 0.320 (0.343) data 0.000 (0.011) loss 1.2519 (1.5482) lr 8.7157e-03 eta 0:12:19
epoch [8/30] batch [52/96] time 0.339 (0.342) data 0.000 (0.011) loss 0.7637 (1.5303) lr 8.7157e-03 eta 0:12:18
epoch [8/30] batch [54/96] time 0.330 (0.342) data 0.000 (0.010) loss 1.1783 (1.5171) lr 8.7157e-03 eta 0:12:16
epoch [8/30] batch [56/96] time 0.345 (0.342) data 0.000 (0.010) loss 0.8820 (1.4994) lr 8.7157e-03 eta 0:12:15
epoch [8/30] batch [58/96] time 0.338 (0.341) data 0.000 (0.009) loss 1.9739 (1.5085) lr 8.7157e-03 eta 0:12:14
epoch [8/30] batch [60/96] time 0.336 (0.341) data 0.000 (0.009) loss 1.5715 (1.5001) lr 8.7157e-03 eta 0:12:12
epoch [8/30] batch [62/96] time 0.348 (0.341) data 0.000 (0.009) loss 1.4650 (1.4912) lr 8.7157e-03 eta 0:12:12
epoch [8/30] batch [64/96] time 0.339 (0.341) data 0.000 (0.009) loss 0.8421 (1.4822) lr 8.7157e-03 eta 0:12:10
epoch [8/30] batch [66/96] time 0.325 (0.340) data 0.000 (0.008) loss 1.4009 (1.4747) lr 8.7157e-03 eta 0:12:08
epoch [8/30] batch [68/96] time 0.321 (0.340) data 0.000 (0.008) loss 0.8105 (1.4590) lr 8.7157e-03 eta 0:12:07
epoch [8/30] batch [70/96] time 0.326 (0.339) data 0.000 (0.008) loss 1.6601 (1.4572) lr 8.7157e-03 eta 0:12:05
epoch [8/30] batch [72/96] time 0.325 (0.339) data 0.000 (0.008) loss 1.9465 (1.4700) lr 8.7157e-03 eta 0:12:04
epoch [8/30] batch [74/96] time 0.316 (0.338) data 0.000 (0.007) loss 2.5735 (1.4892) lr 8.7157e-03 eta 0:12:01
epoch [8/30] batch [76/96] time 0.308 (0.337) data 0.000 (0.007) loss 0.8146 (1.4787) lr 8.7157e-03 eta 0:11:59
epoch [8/30] batch [78/96] time 0.311 (0.337) data 0.000 (0.007) loss 1.6163 (1.4746) lr 8.7157e-03 eta 0:11:57
epoch [8/30] batch [80/96] time 0.312 (0.336) data 0.000 (0.007) loss 1.6475 (1.4769) lr 8.7157e-03 eta 0:11:55
epoch [8/30] batch [82/96] time 0.305 (0.335) data 0.000 (0.007) loss 1.7829 (1.4781) lr 8.7157e-03 eta 0:11:53
epoch [8/30] batch [84/96] time 0.307 (0.335) data 0.000 (0.007) loss 1.5576 (1.4812) lr 8.7157e-03 eta 0:11:51
epoch [8/30] batch [86/96] time 0.313 (0.334) data 0.000 (0.006) loss 1.6332 (1.4806) lr 8.7157e-03 eta 0:11:49
epoch [8/30] batch [88/96] time 0.304 (0.334) data 0.000 (0.006) loss 0.6947 (1.4716) lr 8.7157e-03 eta 0:11:47
epoch [8/30] batch [90/96] time 0.304 (0.333) data 0.000 (0.006) loss 1.6008 (1.4684) lr 8.7157e-03 eta 0:11:45
epoch [8/30] batch [92/96] time 0.301 (0.332) data 0.000 (0.006) loss 1.4268 (1.4701) lr 8.7157e-03 eta 0:11:43
epoch [8/30] batch [94/96] time 0.305 (0.332) data 0.000 (0.006) loss 1.0162 (1.4648) lr 8.7157e-03 eta 0:11:41
epoch [8/30] batch [96/96] time 0.300 (0.331) data 0.000 (0.006) loss 1.4898 (1.4608) lr 8.3457e-03 eta 0:11:39
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.87s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 386
* accuracy: 67.0%
* error: 33.0%
* macro_f1: 65.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [9/30] batch [2/96] time 0.315 (0.660) data 0.000 (0.273) loss 1.0411 (0.9244) lr 8.3457e-03 eta 0:23:12
epoch [9/30] batch [4/96] time 0.328 (0.489) data 0.000 (0.137) loss 1.9532 (1.1923) lr 8.3457e-03 eta 0:17:11
epoch [9/30] batch [6/96] time 0.325 (0.435) data 0.000 (0.091) loss 1.2652 (1.3055) lr 8.3457e-03 eta 0:15:15
epoch [9/30] batch [8/96] time 0.333 (0.409) data 0.000 (0.069) loss 1.4127 (1.3591) lr 8.3457e-03 eta 0:14:20
epoch [9/30] batch [10/96] time 0.330 (0.392) data 0.000 (0.055) loss 2.0490 (1.3954) lr 8.3457e-03 eta 0:13:44
epoch [9/30] batch [12/96] time 0.317 (0.381) data 0.000 (0.046) loss 2.2522 (1.4428) lr 8.3457e-03 eta 0:13:19
epoch [9/30] batch [14/96] time 0.324 (0.373) data 0.000 (0.039) loss 2.1907 (1.4641) lr 8.3457e-03 eta 0:13:01
epoch [9/30] batch [16/96] time 0.321 (0.366) data 0.000 (0.034) loss 1.2317 (1.4264) lr 8.3457e-03 eta 0:12:47
epoch [9/30] batch [18/96] time 0.318 (0.362) data 0.000 (0.031) loss 0.7882 (1.4113) lr 8.3457e-03 eta 0:12:37
epoch [9/30] batch [20/96] time 0.324 (0.359) data 0.000 (0.028) loss 1.1484 (1.4026) lr 8.3457e-03 eta 0:12:30
epoch [9/30] batch [22/96] time 0.317 (0.356) data 0.000 (0.025) loss 1.1007 (1.3718) lr 8.3457e-03 eta 0:12:24
epoch [9/30] batch [24/96] time 0.327 (0.355) data 0.000 (0.023) loss 1.1011 (1.3721) lr 8.3457e-03 eta 0:12:20
epoch [9/30] batch [26/96] time 0.324 (0.352) data 0.000 (0.021) loss 1.0121 (1.3708) lr 8.3457e-03 eta 0:12:14
epoch [9/30] batch [28/96] time 0.311 (0.350) data 0.000 (0.020) loss 1.2558 (1.3632) lr 8.3457e-03 eta 0:12:09
epoch [9/30] batch [30/96] time 0.336 (0.349) data 0.000 (0.018) loss 2.0217 (1.3827) lr 8.3457e-03 eta 0:12:05
epoch [9/30] batch [32/96] time 0.328 (0.347) data 0.000 (0.017) loss 0.9878 (1.3620) lr 8.3457e-03 eta 0:12:02
epoch [9/30] batch [34/96] time 0.321 (0.346) data 0.000 (0.016) loss 1.2770 (1.3564) lr 8.3457e-03 eta 0:11:58
epoch [9/30] batch [36/96] time 0.439 (0.347) data 0.000 (0.015) loss 0.7166 (1.3320) lr 8.3457e-03 eta 0:12:01
epoch [9/30] batch [38/96] time 0.331 (0.347) data 0.000 (0.015) loss 2.5134 (1.3823) lr 8.3457e-03 eta 0:11:58
epoch [9/30] batch [40/96] time 0.325 (0.345) data 0.000 (0.014) loss 1.1184 (1.3755) lr 8.3457e-03 eta 0:11:55
epoch [9/30] batch [42/96] time 0.324 (0.345) data 0.000 (0.013) loss 1.1905 (1.3586) lr 8.3457e-03 eta 0:11:53
epoch [9/30] batch [44/96] time 0.316 (0.343) data 0.000 (0.013) loss 0.8798 (1.3620) lr 8.3457e-03 eta 0:11:50
epoch [9/30] batch [46/96] time 0.327 (0.343) data 0.000 (0.012) loss 1.4051 (1.3603) lr 8.3457e-03 eta 0:11:47
epoch [9/30] batch [48/96] time 0.324 (0.342) data 0.000 (0.012) loss 2.3783 (1.3818) lr 8.3457e-03 eta 0:11:45
epoch [9/30] batch [50/96] time 0.317 (0.341) data 0.000 (0.011) loss 1.2566 (1.3903) lr 8.3457e-03 eta 0:11:43
epoch [9/30] batch [52/96] time 0.319 (0.340) data 0.000 (0.011) loss 1.2252 (1.3937) lr 8.3457e-03 eta 0:11:40
epoch [9/30] batch [54/96] time 0.324 (0.340) data 0.000 (0.010) loss 1.1697 (1.3845) lr 8.3457e-03 eta 0:11:38
epoch [9/30] batch [56/96] time 0.321 (0.339) data 0.000 (0.010) loss 1.3964 (1.3929) lr 8.3457e-03 eta 0:11:36
epoch [9/30] batch [58/96] time 0.326 (0.338) data 0.000 (0.010) loss 2.3320 (1.4149) lr 8.3457e-03 eta 0:11:35
epoch [9/30] batch [60/96] time 0.319 (0.338) data 0.000 (0.009) loss 1.7062 (1.4227) lr 8.3457e-03 eta 0:11:33
epoch [9/30] batch [62/96] time 0.330 (0.337) data 0.000 (0.009) loss 1.3532 (1.4179) lr 8.3457e-03 eta 0:11:31
epoch [9/30] batch [64/96] time 0.335 (0.337) data 0.000 (0.009) loss 2.4736 (1.4323) lr 8.3457e-03 eta 0:11:30
epoch [9/30] batch [66/96] time 0.333 (0.337) data 0.000 (0.009) loss 1.3365 (1.4276) lr 8.3457e-03 eta 0:11:29
epoch [9/30] batch [68/96] time 0.326 (0.337) data 0.000 (0.008) loss 1.0662 (1.4270) lr 8.3457e-03 eta 0:11:28
epoch [9/30] batch [70/96] time 0.317 (0.336) data 0.000 (0.008) loss 0.9888 (1.4115) lr 8.3457e-03 eta 0:11:26
epoch [9/30] batch [72/96] time 0.331 (0.336) data 0.000 (0.008) loss 1.7952 (1.4096) lr 8.3457e-03 eta 0:11:25
epoch [9/30] batch [74/96] time 0.308 (0.335) data 0.000 (0.008) loss 2.2026 (1.4180) lr 8.3457e-03 eta 0:11:22
epoch [9/30] batch [76/96] time 0.306 (0.334) data 0.000 (0.007) loss 1.0594 (1.4104) lr 8.3457e-03 eta 0:11:20
epoch [9/30] batch [78/96] time 0.311 (0.334) data 0.000 (0.007) loss 1.5925 (1.4182) lr 8.3457e-03 eta 0:11:18
epoch [9/30] batch [80/96] time 0.304 (0.333) data 0.000 (0.007) loss 1.2123 (1.4162) lr 8.3457e-03 eta 0:11:16
epoch [9/30] batch [82/96] time 0.305 (0.332) data 0.000 (0.007) loss 1.8633 (1.4196) lr 8.3457e-03 eta 0:11:14
epoch [9/30] batch [84/96] time 0.304 (0.332) data 0.000 (0.007) loss 1.2718 (1.4164) lr 8.3457e-03 eta 0:11:12
epoch [9/30] batch [86/96] time 0.304 (0.331) data 0.000 (0.007) loss 1.7423 (1.4312) lr 8.3457e-03 eta 0:11:10
epoch [9/30] batch [88/96] time 0.309 (0.330) data 0.000 (0.006) loss 1.2377 (1.4251) lr 8.3457e-03 eta 0:11:08
epoch [9/30] batch [90/96] time 0.309 (0.330) data 0.000 (0.006) loss 1.5466 (1.4305) lr 8.3457e-03 eta 0:11:07
epoch [9/30] batch [92/96] time 0.306 (0.329) data 0.000 (0.006) loss 0.9859 (1.4261) lr 8.3457e-03 eta 0:11:05
epoch [9/30] batch [94/96] time 0.303 (0.329) data 0.000 (0.006) loss 1.6077 (1.4250) lr 8.3457e-03 eta 0:11:03
epoch [9/30] batch [96/96] time 0.306 (0.328) data 0.000 (0.006) loss 1.5643 (1.4233) lr 7.9389e-03 eta 0:11:01
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.86s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 391
* accuracy: 67.9%
* error: 32.1%
* macro_f1: 66.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [10/30] batch [2/96] time 0.353 (0.662) data 0.000 (0.276) loss 1.6403 (2.3099) lr 7.9389e-03 eta 0:22:13
epoch [10/30] batch [4/96] time 0.331 (0.492) data 0.000 (0.138) loss 1.1284 (1.8487) lr 7.9389e-03 eta 0:16:29
epoch [10/30] batch [6/96] time 0.349 (0.444) data 0.000 (0.092) loss 1.2397 (1.6093) lr 7.9389e-03 eta 0:14:52
epoch [10/30] batch [8/96] time 0.328 (0.414) data 0.000 (0.069) loss 1.2555 (1.5168) lr 7.9389e-03 eta 0:13:52
epoch [10/30] batch [10/96] time 0.327 (0.398) data 0.000 (0.056) loss 1.1340 (1.4291) lr 7.9389e-03 eta 0:13:18
epoch [10/30] batch [12/96] time 0.318 (0.387) data 0.000 (0.046) loss 1.4518 (1.4350) lr 7.9389e-03 eta 0:12:55
epoch [10/30] batch [14/96] time 0.323 (0.378) data 0.000 (0.040) loss 1.9936 (1.4469) lr 7.9389e-03 eta 0:12:37
epoch [10/30] batch [16/96] time 0.330 (0.372) data 0.000 (0.035) loss 0.8465 (1.4281) lr 7.9389e-03 eta 0:12:23
epoch [10/30] batch [18/96] time 0.332 (0.367) data 0.000 (0.031) loss 2.4401 (1.4535) lr 7.9389e-03 eta 0:12:13
epoch [10/30] batch [20/96] time 0.336 (0.363) data 0.000 (0.028) loss 0.9106 (1.4214) lr 7.9389e-03 eta 0:12:04
epoch [10/30] batch [22/96] time 0.331 (0.360) data 0.000 (0.025) loss 1.6849 (1.4434) lr 7.9389e-03 eta 0:11:57
epoch [10/30] batch [24/96] time 0.331 (0.358) data 0.000 (0.023) loss 1.2029 (1.4333) lr 7.9389e-03 eta 0:11:53
epoch [10/30] batch [26/96] time 0.333 (0.356) data 0.000 (0.022) loss 1.2136 (1.4228) lr 7.9389e-03 eta 0:11:48
epoch [10/30] batch [28/96] time 0.317 (0.354) data 0.000 (0.020) loss 1.2527 (1.4266) lr 7.9389e-03 eta 0:11:44
epoch [10/30] batch [30/96] time 0.334 (0.352) data 0.000 (0.019) loss 1.6529 (1.4267) lr 7.9389e-03 eta 0:11:39
epoch [10/30] batch [32/96] time 0.318 (0.351) data 0.000 (0.018) loss 1.5270 (1.4144) lr 7.9389e-03 eta 0:11:35
epoch [10/30] batch [34/96] time 0.355 (0.350) data 0.000 (0.017) loss 1.4758 (1.4161) lr 7.9389e-03 eta 0:11:33
epoch [10/30] batch [36/96] time 0.428 (0.352) data 0.000 (0.016) loss 1.8481 (1.4526) lr 7.9389e-03 eta 0:11:37
epoch [10/30] batch [38/96] time 0.312 (0.351) data 0.000 (0.015) loss 1.5478 (1.4486) lr 7.9389e-03 eta 0:11:33
epoch [10/30] batch [40/96] time 0.325 (0.349) data 0.000 (0.014) loss 1.0430 (1.4298) lr 7.9389e-03 eta 0:11:30
epoch [10/30] batch [42/96] time 0.320 (0.348) data 0.000 (0.013) loss 0.7238 (1.4000) lr 7.9389e-03 eta 0:11:26
epoch [10/30] batch [44/96] time 0.322 (0.347) data 0.000 (0.013) loss 1.8673 (1.4019) lr 7.9389e-03 eta 0:11:23
epoch [10/30] batch [46/96] time 0.326 (0.346) data 0.000 (0.012) loss 2.2550 (1.4171) lr 7.9389e-03 eta 0:11:21
epoch [10/30] batch [48/96] time 0.333 (0.345) data 0.000 (0.012) loss 1.5811 (1.4139) lr 7.9389e-03 eta 0:11:19
epoch [10/30] batch [50/96] time 0.325 (0.344) data 0.000 (0.011) loss 1.9698 (1.4299) lr 7.9389e-03 eta 0:11:16
epoch [10/30] batch [52/96] time 0.341 (0.344) data 0.000 (0.011) loss 2.2504 (1.4440) lr 7.9389e-03 eta 0:11:15
epoch [10/30] batch [54/96] time 0.322 (0.343) data 0.000 (0.011) loss 1.4902 (1.4365) lr 7.9389e-03 eta 0:11:13
epoch [10/30] batch [56/96] time 0.315 (0.342) data 0.000 (0.010) loss 1.5670 (1.4384) lr 7.9389e-03 eta 0:11:10
epoch [10/30] batch [58/96] time 0.318 (0.341) data 0.000 (0.010) loss 2.5771 (1.4497) lr 7.9389e-03 eta 0:11:08
epoch [10/30] batch [60/96] time 0.326 (0.341) data 0.000 (0.010) loss 0.9332 (1.4336) lr 7.9389e-03 eta 0:11:07
epoch [10/30] batch [62/96] time 0.317 (0.340) data 0.000 (0.009) loss 1.0484 (1.4148) lr 7.9389e-03 eta 0:11:05
epoch [10/30] batch [64/96] time 0.320 (0.340) data 0.000 (0.009) loss 1.7221 (1.4116) lr 7.9389e-03 eta 0:11:03
epoch [10/30] batch [66/96] time 0.332 (0.339) data 0.000 (0.009) loss 1.2088 (1.4120) lr 7.9389e-03 eta 0:11:01
epoch [10/30] batch [68/96] time 0.333 (0.339) data 0.000 (0.008) loss 1.0094 (1.4068) lr 7.9389e-03 eta 0:11:00
epoch [10/30] batch [70/96] time 0.332 (0.339) data 0.000 (0.008) loss 1.5763 (1.4108) lr 7.9389e-03 eta 0:10:59
epoch [10/30] batch [72/96] time 0.345 (0.339) data 0.000 (0.008) loss 1.4312 (1.4135) lr 7.9389e-03 eta 0:10:59
epoch [10/30] batch [74/96] time 0.310 (0.338) data 0.000 (0.008) loss 1.6050 (1.4160) lr 7.9389e-03 eta 0:10:56
epoch [10/30] batch [76/96] time 0.303 (0.337) data 0.000 (0.008) loss 1.0494 (1.4124) lr 7.9389e-03 eta 0:10:54
epoch [10/30] batch [78/96] time 0.308 (0.337) data 0.000 (0.007) loss 1.5327 (1.4156) lr 7.9389e-03 eta 0:10:52
epoch [10/30] batch [80/96] time 0.299 (0.336) data 0.000 (0.007) loss 1.2655 (1.4097) lr 7.9389e-03 eta 0:10:49
epoch [10/30] batch [82/96] time 0.306 (0.335) data 0.000 (0.007) loss 1.9746 (1.4171) lr 7.9389e-03 eta 0:10:47
epoch [10/30] batch [84/96] time 0.306 (0.334) data 0.000 (0.007) loss 1.1740 (1.4117) lr 7.9389e-03 eta 0:10:45
epoch [10/30] batch [86/96] time 0.302 (0.334) data 0.000 (0.007) loss 0.8460 (1.4040) lr 7.9389e-03 eta 0:10:43
epoch [10/30] batch [88/96] time 0.309 (0.333) data 0.000 (0.007) loss 1.2192 (1.3977) lr 7.9389e-03 eta 0:10:41
epoch [10/30] batch [90/96] time 0.303 (0.332) data 0.000 (0.006) loss 2.6110 (1.4135) lr 7.9389e-03 eta 0:10:39
epoch [10/30] batch [92/96] time 0.308 (0.332) data 0.000 (0.006) loss 1.2985 (1.4097) lr 7.9389e-03 eta 0:10:38
epoch [10/30] batch [94/96] time 0.310 (0.331) data 0.000 (0.006) loss 1.2419 (1.4046) lr 7.9389e-03 eta 0:10:36
epoch [10/30] batch [96/96] time 0.314 (0.331) data 0.000 (0.006) loss 1.0982 (1.3994) lr 7.5000e-03 eta 0:10:35
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.89s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.37s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.19s/it]=> result
* total: 576
* correct: 397
* accuracy: 68.9%
* error: 31.1%
* macro_f1: 67.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-10

epoch [11/30] batch [2/96] time 0.321 (0.666) data 0.000 (0.302) loss 1.0711 (1.1206) lr 7.5000e-03 eta 0:21:18
epoch [11/30] batch [4/96] time 0.344 (0.499) data 0.000 (0.151) loss 1.6720 (1.2630) lr 7.5000e-03 eta 0:15:55
epoch [11/30] batch [6/96] time 0.335 (0.445) data 0.000 (0.101) loss 0.9940 (1.2658) lr 7.5000e-03 eta 0:14:10
epoch [11/30] batch [8/96] time 0.330 (0.417) data 0.001 (0.076) loss 0.9891 (1.3000) lr 7.5000e-03 eta 0:13:16
epoch [11/30] batch [10/96] time 0.325 (0.398) data 0.000 (0.061) loss 1.3430 (1.2946) lr 7.5000e-03 eta 0:12:40
epoch [11/30] batch [12/96] time 0.330 (0.387) data 0.000 (0.051) loss 1.0071 (1.2630) lr 7.5000e-03 eta 0:12:18
epoch [11/30] batch [14/96] time 0.352 (0.380) data 0.000 (0.043) loss 1.6664 (1.3046) lr 7.5000e-03 eta 0:12:04
epoch [11/30] batch [16/96] time 0.326 (0.374) data 0.000 (0.038) loss 1.3841 (1.3340) lr 7.5000e-03 eta 0:11:52
epoch [11/30] batch [18/96] time 0.325 (0.369) data 0.000 (0.034) loss 1.5042 (1.3285) lr 7.5000e-03 eta 0:11:42
epoch [11/30] batch [20/96] time 0.330 (0.365) data 0.000 (0.031) loss 1.7362 (1.3455) lr 7.5000e-03 eta 0:11:34
epoch [11/30] batch [22/96] time 0.327 (0.362) data 0.000 (0.028) loss 1.3528 (1.3813) lr 7.5000e-03 eta 0:11:26
epoch [11/30] batch [24/96] time 0.336 (0.359) data 0.000 (0.026) loss 1.1309 (1.3628) lr 7.5000e-03 eta 0:11:20
epoch [11/30] batch [26/96] time 0.342 (0.357) data 0.000 (0.024) loss 1.5781 (1.4073) lr 7.5000e-03 eta 0:11:16
epoch [11/30] batch [28/96] time 0.330 (0.355) data 0.000 (0.022) loss 1.6209 (1.4219) lr 7.5000e-03 eta 0:11:11
epoch [11/30] batch [30/96] time 0.346 (0.354) data 0.001 (0.020) loss 1.1658 (1.4124) lr 7.5000e-03 eta 0:11:08
epoch [11/30] batch [32/96] time 0.325 (0.352) data 0.000 (0.019) loss 1.0966 (1.3889) lr 7.5000e-03 eta 0:11:05
epoch [11/30] batch [34/96] time 0.314 (0.351) data 0.000 (0.018) loss 1.1226 (1.3842) lr 7.5000e-03 eta 0:11:01
epoch [11/30] batch [36/96] time 0.442 (0.353) data 0.000 (0.017) loss 1.0209 (1.3664) lr 7.5000e-03 eta 0:11:04
epoch [11/30] batch [38/96] time 0.332 (0.352) data 0.000 (0.016) loss 1.6029 (1.3703) lr 7.5000e-03 eta 0:11:01
epoch [11/30] batch [40/96] time 0.335 (0.351) data 0.000 (0.015) loss 1.2194 (1.3690) lr 7.5000e-03 eta 0:10:59
epoch [11/30] batch [42/96] time 0.328 (0.349) data 0.000 (0.015) loss 2.2828 (1.3755) lr 7.5000e-03 eta 0:10:56
epoch [11/30] batch [44/96] time 0.322 (0.348) data 0.000 (0.014) loss 0.9774 (1.3608) lr 7.5000e-03 eta 0:10:53
epoch [11/30] batch [46/96] time 0.340 (0.348) data 0.000 (0.013) loss 1.3569 (1.3813) lr 7.5000e-03 eta 0:10:52
epoch [11/30] batch [48/96] time 0.323 (0.347) data 0.000 (0.013) loss 1.3724 (1.3804) lr 7.5000e-03 eta 0:10:49
epoch [11/30] batch [50/96] time 0.333 (0.347) data 0.000 (0.012) loss 1.8929 (1.3986) lr 7.5000e-03 eta 0:10:48
epoch [11/30] batch [52/96] time 0.327 (0.346) data 0.001 (0.012) loss 1.0294 (1.3777) lr 7.5000e-03 eta 0:10:46
epoch [11/30] batch [54/96] time 0.327 (0.346) data 0.000 (0.012) loss 0.9587 (1.3727) lr 7.5000e-03 eta 0:10:44
epoch [11/30] batch [56/96] time 0.336 (0.345) data 0.000 (0.011) loss 1.7720 (1.3844) lr 7.5000e-03 eta 0:10:43
epoch [11/30] batch [58/96] time 0.340 (0.345) data 0.000 (0.011) loss 1.2779 (1.3745) lr 7.5000e-03 eta 0:10:41
epoch [11/30] batch [60/96] time 0.327 (0.344) data 0.000 (0.010) loss 1.2404 (1.3744) lr 7.5000e-03 eta 0:10:40
epoch [11/30] batch [62/96] time 0.320 (0.344) data 0.001 (0.010) loss 2.4002 (1.3835) lr 7.5000e-03 eta 0:10:38
epoch [11/30] batch [64/96] time 0.325 (0.343) data 0.000 (0.010) loss 0.9859 (1.3811) lr 7.5000e-03 eta 0:10:36
epoch [11/30] batch [66/96] time 0.330 (0.343) data 0.000 (0.010) loss 1.2209 (1.3721) lr 7.5000e-03 eta 0:10:35
epoch [11/30] batch [68/96] time 0.321 (0.342) data 0.000 (0.009) loss 2.1700 (1.3839) lr 7.5000e-03 eta 0:10:33
epoch [11/30] batch [70/96] time 0.342 (0.342) data 0.000 (0.009) loss 0.8147 (1.3852) lr 7.5000e-03 eta 0:10:32
epoch [11/30] batch [72/96] time 0.326 (0.341) data 0.000 (0.009) loss 1.4140 (1.3895) lr 7.5000e-03 eta 0:10:30
epoch [11/30] batch [74/96] time 0.309 (0.341) data 0.000 (0.009) loss 0.9463 (1.3747) lr 7.5000e-03 eta 0:10:28
epoch [11/30] batch [76/96] time 0.310 (0.340) data 0.000 (0.008) loss 0.9857 (1.3756) lr 7.5000e-03 eta 0:10:26
epoch [11/30] batch [78/96] time 0.318 (0.339) data 0.000 (0.008) loss 0.8864 (1.3710) lr 7.5000e-03 eta 0:10:24
epoch [11/30] batch [80/96] time 0.305 (0.338) data 0.000 (0.008) loss 1.8916 (1.3704) lr 7.5000e-03 eta 0:10:22
epoch [11/30] batch [82/96] time 0.310 (0.338) data 0.000 (0.008) loss 0.9896 (1.3635) lr 7.5000e-03 eta 0:10:20
epoch [11/30] batch [84/96] time 0.312 (0.337) data 0.000 (0.008) loss 1.5849 (1.3838) lr 7.5000e-03 eta 0:10:18
epoch [11/30] batch [86/96] time 0.306 (0.336) data 0.000 (0.007) loss 1.6284 (1.3822) lr 7.5000e-03 eta 0:10:16
epoch [11/30] batch [88/96] time 0.311 (0.335) data 0.000 (0.007) loss 1.1358 (1.3715) lr 7.5000e-03 eta 0:10:14
epoch [11/30] batch [90/96] time 0.305 (0.335) data 0.000 (0.007) loss 1.0568 (1.3659) lr 7.5000e-03 eta 0:10:12
epoch [11/30] batch [92/96] time 0.311 (0.334) data 0.000 (0.007) loss 1.5050 (1.3653) lr 7.5000e-03 eta 0:10:11
epoch [11/30] batch [94/96] time 0.310 (0.334) data 0.000 (0.007) loss 1.3991 (1.3619) lr 7.5000e-03 eta 0:10:09
epoch [11/30] batch [96/96] time 0.306 (0.333) data 0.000 (0.007) loss 1.0856 (1.3606) lr 7.0337e-03 eta 0:10:07
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.83s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.34s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 413
* accuracy: 71.7%
* error: 28.3%
* macro_f1: 71.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [12/30] batch [2/96] time 0.318 (0.655) data 0.000 (0.292) loss 1.3788 (1.2603) lr 7.0337e-03 eta 0:19:53
epoch [12/30] batch [4/96] time 0.324 (0.487) data 0.000 (0.146) loss 0.9883 (1.1602) lr 7.0337e-03 eta 0:14:45
epoch [12/30] batch [6/96] time 0.327 (0.434) data 0.000 (0.098) loss 1.5856 (1.3599) lr 7.0337e-03 eta 0:13:09
epoch [12/30] batch [8/96] time 0.329 (0.408) data 0.000 (0.073) loss 0.7720 (1.2907) lr 7.0337e-03 eta 0:12:20
epoch [12/30] batch [10/96] time 0.333 (0.393) data 0.000 (0.059) loss 1.1750 (1.2898) lr 7.0337e-03 eta 0:11:52
epoch [12/30] batch [12/96] time 0.330 (0.383) data 0.000 (0.049) loss 0.9060 (1.2648) lr 7.0337e-03 eta 0:11:33
epoch [12/30] batch [14/96] time 0.337 (0.377) data 0.000 (0.042) loss 1.2104 (1.2901) lr 7.0337e-03 eta 0:11:22
epoch [12/30] batch [16/96] time 0.329 (0.371) data 0.000 (0.037) loss 1.9610 (1.3897) lr 7.0337e-03 eta 0:11:10
epoch [12/30] batch [18/96] time 0.324 (0.365) data 0.000 (0.033) loss 1.3108 (1.3809) lr 7.0337e-03 eta 0:11:00
epoch [12/30] batch [20/96] time 0.321 (0.361) data 0.000 (0.030) loss 0.7627 (1.3808) lr 7.0337e-03 eta 0:10:51
epoch [12/30] batch [22/96] time 0.335 (0.358) data 0.000 (0.027) loss 0.7342 (1.3499) lr 7.0337e-03 eta 0:10:45
epoch [12/30] batch [24/96] time 0.341 (0.356) data 0.000 (0.025) loss 1.9771 (1.3583) lr 7.0337e-03 eta 0:10:41
epoch [12/30] batch [26/96] time 0.341 (0.355) data 0.000 (0.023) loss 1.0349 (1.3897) lr 7.0337e-03 eta 0:10:38
epoch [12/30] batch [28/96] time 0.355 (0.354) data 0.000 (0.021) loss 1.0352 (1.3599) lr 7.0337e-03 eta 0:10:36
epoch [12/30] batch [30/96] time 0.351 (0.354) data 0.000 (0.020) loss 1.4190 (1.3407) lr 7.0337e-03 eta 0:10:34
epoch [12/30] batch [32/96] time 0.345 (0.353) data 0.000 (0.019) loss 0.9792 (1.3199) lr 7.0337e-03 eta 0:10:32
epoch [12/30] batch [34/96] time 0.346 (0.353) data 0.000 (0.018) loss 0.7352 (1.3166) lr 7.0337e-03 eta 0:10:31
epoch [12/30] batch [36/96] time 0.461 (0.355) data 0.000 (0.017) loss 0.8489 (1.3171) lr 7.0337e-03 eta 0:10:35
epoch [12/30] batch [38/96] time 0.343 (0.355) data 0.000 (0.016) loss 1.3554 (1.3155) lr 7.0337e-03 eta 0:10:33
epoch [12/30] batch [40/96] time 0.344 (0.354) data 0.000 (0.015) loss 1.1045 (1.3031) lr 7.0337e-03 eta 0:10:31
epoch [12/30] batch [42/96] time 0.347 (0.354) data 0.000 (0.014) loss 1.5214 (1.3085) lr 7.0337e-03 eta 0:10:30
epoch [12/30] batch [44/96] time 0.323 (0.352) data 0.000 (0.014) loss 2.0093 (1.3180) lr 7.0337e-03 eta 0:10:27
epoch [12/30] batch [46/96] time 0.331 (0.352) data 0.000 (0.013) loss 1.0768 (1.3165) lr 7.0337e-03 eta 0:10:25
epoch [12/30] batch [48/96] time 0.321 (0.350) data 0.000 (0.013) loss 1.3155 (1.3211) lr 7.0337e-03 eta 0:10:22
epoch [12/30] batch [50/96] time 0.319 (0.349) data 0.000 (0.012) loss 1.2330 (1.3187) lr 7.0337e-03 eta 0:10:19
epoch [12/30] batch [52/96] time 0.337 (0.348) data 0.000 (0.012) loss 2.8576 (1.3441) lr 7.0337e-03 eta 0:10:17
epoch [12/30] batch [54/96] time 0.343 (0.348) data 0.000 (0.011) loss 1.2210 (1.3573) lr 7.0337e-03 eta 0:10:16
epoch [12/30] batch [56/96] time 0.330 (0.347) data 0.001 (0.011) loss 1.7274 (1.3587) lr 7.0337e-03 eta 0:10:14
epoch [12/30] batch [58/96] time 0.326 (0.347) data 0.000 (0.010) loss 2.2584 (1.3706) lr 7.0337e-03 eta 0:10:12
epoch [12/30] batch [60/96] time 0.334 (0.346) data 0.000 (0.010) loss 1.2666 (1.3614) lr 7.0337e-03 eta 0:10:10
epoch [12/30] batch [62/96] time 0.336 (0.346) data 0.000 (0.010) loss 1.2422 (1.3543) lr 7.0337e-03 eta 0:10:09
epoch [12/30] batch [64/96] time 0.327 (0.345) data 0.000 (0.009) loss 1.6194 (1.3522) lr 7.0337e-03 eta 0:10:07
epoch [12/30] batch [66/96] time 0.330 (0.345) data 0.000 (0.009) loss 1.3817 (1.3757) lr 7.0337e-03 eta 0:10:06
epoch [12/30] batch [68/96] time 0.329 (0.345) data 0.000 (0.009) loss 1.3662 (1.3666) lr 7.0337e-03 eta 0:10:05
epoch [12/30] batch [70/96] time 0.334 (0.344) data 0.000 (0.009) loss 1.3958 (1.3635) lr 7.0337e-03 eta 0:10:04
epoch [12/30] batch [72/96] time 0.324 (0.344) data 0.000 (0.008) loss 1.0699 (1.3574) lr 7.0337e-03 eta 0:10:02
epoch [12/30] batch [74/96] time 0.312 (0.343) data 0.000 (0.008) loss 0.8465 (1.3502) lr 7.0337e-03 eta 0:10:00
epoch [12/30] batch [76/96] time 0.311 (0.342) data 0.000 (0.008) loss 1.5618 (1.3483) lr 7.0337e-03 eta 0:09:58
epoch [12/30] batch [78/96] time 0.309 (0.341) data 0.000 (0.008) loss 0.9893 (1.3394) lr 7.0337e-03 eta 0:09:56
epoch [12/30] batch [80/96] time 0.311 (0.341) data 0.000 (0.008) loss 0.8164 (1.3340) lr 7.0337e-03 eta 0:09:54
epoch [12/30] batch [82/96] time 0.301 (0.340) data 0.000 (0.007) loss 0.8038 (1.3307) lr 7.0337e-03 eta 0:09:52
epoch [12/30] batch [84/96] time 0.306 (0.339) data 0.000 (0.007) loss 1.8018 (1.3327) lr 7.0337e-03 eta 0:09:49
epoch [12/30] batch [86/96] time 0.303 (0.338) data 0.000 (0.007) loss 0.8294 (1.3326) lr 7.0337e-03 eta 0:09:47
epoch [12/30] batch [88/96] time 0.307 (0.338) data 0.000 (0.007) loss 1.5097 (1.3403) lr 7.0337e-03 eta 0:09:46
epoch [12/30] batch [90/96] time 0.304 (0.337) data 0.000 (0.007) loss 1.2796 (1.3375) lr 7.0337e-03 eta 0:09:44
epoch [12/30] batch [92/96] time 0.306 (0.336) data 0.000 (0.007) loss 1.2814 (1.3412) lr 7.0337e-03 eta 0:09:42
epoch [12/30] batch [94/96] time 0.307 (0.336) data 0.000 (0.007) loss 1.9774 (1.3442) lr 7.0337e-03 eta 0:09:40
epoch [12/30] batch [96/96] time 0.308 (0.335) data 0.000 (0.006) loss 1.1823 (1.3355) lr 6.5451e-03 eta 0:09:38
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.85s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.34s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 419
* accuracy: 72.7%
* error: 27.3%
* macro_f1: 72.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [13/30] batch [2/96] time 0.329 (0.659) data 0.001 (0.287) loss 1.5963 (1.5316) lr 6.5451e-03 eta 0:18:56
epoch [13/30] batch [4/96] time 0.329 (0.496) data 0.000 (0.144) loss 0.9725 (1.2584) lr 6.5451e-03 eta 0:14:15
epoch [13/30] batch [6/96] time 0.345 (0.447) data 0.001 (0.096) loss 1.4095 (1.2519) lr 6.5451e-03 eta 0:12:50
epoch [13/30] batch [8/96] time 0.334 (0.421) data 0.000 (0.072) loss 1.2050 (1.2120) lr 6.5451e-03 eta 0:12:04
epoch [13/30] batch [10/96] time 0.323 (0.402) data 0.000 (0.058) loss 1.3612 (1.1984) lr 6.5451e-03 eta 0:11:30
epoch [13/30] batch [12/96] time 0.349 (0.392) data 0.000 (0.048) loss 1.2571 (1.2469) lr 6.5451e-03 eta 0:11:12
epoch [13/30] batch [14/96] time 0.337 (0.386) data 0.000 (0.041) loss 1.1891 (1.2430) lr 6.5451e-03 eta 0:11:01
epoch [13/30] batch [16/96] time 0.333 (0.381) data 0.000 (0.036) loss 1.2578 (1.2601) lr 6.5451e-03 eta 0:10:51
epoch [13/30] batch [18/96] time 0.325 (0.375) data 0.000 (0.032) loss 1.3415 (1.2863) lr 6.5451e-03 eta 0:10:42
epoch [13/30] batch [20/96] time 0.339 (0.372) data 0.000 (0.029) loss 0.9123 (1.2645) lr 6.5451e-03 eta 0:10:34
epoch [13/30] batch [22/96] time 0.316 (0.367) data 0.000 (0.026) loss 1.6061 (1.2952) lr 6.5451e-03 eta 0:10:26
epoch [13/30] batch [24/96] time 0.335 (0.364) data 0.000 (0.024) loss 1.2352 (1.2895) lr 6.5451e-03 eta 0:10:20
epoch [13/30] batch [26/96] time 0.325 (0.361) data 0.000 (0.022) loss 1.0781 (1.2648) lr 6.5451e-03 eta 0:10:14
epoch [13/30] batch [28/96] time 0.314 (0.358) data 0.000 (0.021) loss 0.8321 (1.2726) lr 6.5451e-03 eta 0:10:08
epoch [13/30] batch [30/96] time 0.322 (0.356) data 0.000 (0.019) loss 1.0241 (1.2463) lr 6.5451e-03 eta 0:10:03
epoch [13/30] batch [32/96] time 0.339 (0.354) data 0.000 (0.018) loss 0.9143 (1.2635) lr 6.5451e-03 eta 0:10:00
epoch [13/30] batch [34/96] time 0.327 (0.352) data 0.000 (0.017) loss 1.3057 (1.2699) lr 6.5451e-03 eta 0:09:56
epoch [13/30] batch [36/96] time 0.417 (0.353) data 0.000 (0.016) loss 1.0497 (1.2570) lr 6.5451e-03 eta 0:09:57
epoch [13/30] batch [38/96] time 0.333 (0.352) data 0.000 (0.015) loss 1.1642 (1.2525) lr 6.5451e-03 eta 0:09:54
epoch [13/30] batch [40/96] time 0.322 (0.350) data 0.000 (0.015) loss 1.9694 (1.2665) lr 6.5451e-03 eta 0:09:51
epoch [13/30] batch [42/96] time 0.321 (0.349) data 0.000 (0.014) loss 1.2093 (1.2646) lr 6.5451e-03 eta 0:09:48
epoch [13/30] batch [44/96] time 0.338 (0.348) data 0.000 (0.013) loss 1.6512 (1.2729) lr 6.5451e-03 eta 0:09:46
epoch [13/30] batch [46/96] time 0.333 (0.348) data 0.000 (0.013) loss 1.7993 (1.2879) lr 6.5451e-03 eta 0:09:44
epoch [13/30] batch [48/96] time 0.323 (0.347) data 0.001 (0.012) loss 1.6773 (1.3169) lr 6.5451e-03 eta 0:09:42
epoch [13/30] batch [50/96] time 0.335 (0.346) data 0.000 (0.012) loss 1.5311 (1.3153) lr 6.5451e-03 eta 0:09:40
epoch [13/30] batch [52/96] time 0.327 (0.345) data 0.000 (0.011) loss 1.0802 (1.3073) lr 6.5451e-03 eta 0:09:38
epoch [13/30] batch [54/96] time 0.327 (0.344) data 0.000 (0.011) loss 2.0243 (1.3144) lr 6.5451e-03 eta 0:09:36
epoch [13/30] batch [56/96] time 0.316 (0.343) data 0.000 (0.011) loss 1.1155 (1.3033) lr 6.5451e-03 eta 0:09:34
epoch [13/30] batch [58/96] time 0.328 (0.343) data 0.000 (0.010) loss 1.4119 (1.3029) lr 6.5451e-03 eta 0:09:32
epoch [13/30] batch [60/96] time 0.336 (0.342) data 0.000 (0.010) loss 1.0536 (1.3072) lr 6.5451e-03 eta 0:09:30
epoch [13/30] batch [62/96] time 0.311 (0.342) data 0.000 (0.010) loss 1.1787 (1.3065) lr 6.5451e-03 eta 0:09:29
epoch [13/30] batch [64/96] time 0.322 (0.341) data 0.000 (0.009) loss 1.5928 (1.3233) lr 6.5451e-03 eta 0:09:27
epoch [13/30] batch [66/96] time 0.320 (0.340) data 0.000 (0.009) loss 1.3981 (1.3185) lr 6.5451e-03 eta 0:09:25
epoch [13/30] batch [68/96] time 0.322 (0.340) data 0.000 (0.009) loss 1.9519 (1.3245) lr 6.5451e-03 eta 0:09:24
epoch [13/30] batch [70/96] time 0.320 (0.340) data 0.000 (0.009) loss 1.1955 (1.3156) lr 6.5451e-03 eta 0:09:23
epoch [13/30] batch [72/96] time 0.319 (0.339) data 0.000 (0.008) loss 2.3710 (1.3288) lr 6.5451e-03 eta 0:09:21
epoch [13/30] batch [74/96] time 0.308 (0.338) data 0.000 (0.008) loss 2.6966 (1.3431) lr 6.5451e-03 eta 0:09:19
epoch [13/30] batch [76/96] time 0.305 (0.337) data 0.000 (0.008) loss 1.7900 (1.3556) lr 6.5451e-03 eta 0:09:17
epoch [13/30] batch [78/96] time 0.305 (0.336) data 0.000 (0.008) loss 1.6707 (1.3572) lr 6.5451e-03 eta 0:09:15
epoch [13/30] batch [80/96] time 0.302 (0.336) data 0.000 (0.008) loss 1.3036 (1.3512) lr 6.5451e-03 eta 0:09:13
epoch [13/30] batch [82/96] time 0.301 (0.335) data 0.000 (0.007) loss 0.9046 (1.3446) lr 6.5451e-03 eta 0:09:11
epoch [13/30] batch [84/96] time 0.325 (0.335) data 0.000 (0.007) loss 1.2099 (1.3428) lr 6.5451e-03 eta 0:09:09
epoch [13/30] batch [86/96] time 0.304 (0.334) data 0.000 (0.007) loss 1.5952 (1.3469) lr 6.5451e-03 eta 0:09:08
epoch [13/30] batch [88/96] time 0.308 (0.333) data 0.000 (0.007) loss 2.0981 (1.3500) lr 6.5451e-03 eta 0:09:06
epoch [13/30] batch [90/96] time 0.306 (0.333) data 0.000 (0.007) loss 1.6307 (1.3516) lr 6.5451e-03 eta 0:09:04
epoch [13/30] batch [92/96] time 0.304 (0.332) data 0.000 (0.007) loss 0.9153 (1.3408) lr 6.5451e-03 eta 0:09:02
epoch [13/30] batch [94/96] time 0.309 (0.331) data 0.000 (0.006) loss 1.2296 (1.3386) lr 6.5451e-03 eta 0:09:01
epoch [13/30] batch [96/96] time 0.302 (0.331) data 0.000 (0.006) loss 1.9597 (1.3473) lr 6.0396e-03 eta 0:08:59
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.84s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 423
* accuracy: 73.4%
* error: 26.6%
* macro_f1: 72.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [14/30] batch [2/96] time 0.324 (0.652) data 0.000 (0.307) loss 0.8313 (1.0506) lr 6.0396e-03 eta 0:17:42
epoch [14/30] batch [4/96] time 0.320 (0.483) data 0.000 (0.154) loss 0.7986 (1.0494) lr 6.0396e-03 eta 0:13:06
epoch [14/30] batch [6/96] time 0.334 (0.430) data 0.000 (0.103) loss 0.8330 (1.1019) lr 6.0396e-03 eta 0:11:39
epoch [14/30] batch [8/96] time 0.327 (0.405) data 0.000 (0.077) loss 1.4066 (1.2077) lr 6.0396e-03 eta 0:10:58
epoch [14/30] batch [10/96] time 0.327 (0.388) data 0.000 (0.062) loss 0.8196 (1.1421) lr 6.0396e-03 eta 0:10:29
epoch [14/30] batch [12/96] time 0.330 (0.378) data 0.000 (0.051) loss 1.4249 (1.1546) lr 6.0396e-03 eta 0:10:12
epoch [14/30] batch [14/96] time 0.326 (0.369) data 0.000 (0.044) loss 1.4717 (1.2582) lr 6.0396e-03 eta 0:09:57
epoch [14/30] batch [16/96] time 0.319 (0.364) data 0.000 (0.039) loss 1.9820 (1.2788) lr 6.0396e-03 eta 0:09:48
epoch [14/30] batch [18/96] time 0.321 (0.360) data 0.000 (0.034) loss 1.1043 (1.3032) lr 6.0396e-03 eta 0:09:40
epoch [14/30] batch [20/96] time 0.321 (0.355) data 0.000 (0.031) loss 1.0331 (1.2659) lr 6.0396e-03 eta 0:09:32
epoch [14/30] batch [22/96] time 0.317 (0.352) data 0.000 (0.028) loss 1.1157 (1.2819) lr 6.0396e-03 eta 0:09:26
epoch [14/30] batch [24/96] time 0.316 (0.349) data 0.000 (0.026) loss 0.8895 (1.2620) lr 6.0396e-03 eta 0:09:21
epoch [14/30] batch [26/96] time 0.319 (0.347) data 0.000 (0.024) loss 1.2155 (1.2510) lr 6.0396e-03 eta 0:09:16
epoch [14/30] batch [28/96] time 0.344 (0.346) data 0.000 (0.022) loss 1.6252 (1.2785) lr 6.0396e-03 eta 0:09:15
epoch [14/30] batch [30/96] time 0.322 (0.345) data 0.000 (0.021) loss 1.0185 (1.2719) lr 6.0396e-03 eta 0:09:12
epoch [14/30] batch [32/96] time 0.336 (0.344) data 0.000 (0.019) loss 1.8616 (1.3031) lr 6.0396e-03 eta 0:09:10
epoch [14/30] batch [34/96] time 0.328 (0.343) data 0.000 (0.018) loss 1.1271 (1.2922) lr 6.0396e-03 eta 0:09:07
epoch [14/30] batch [36/96] time 0.419 (0.344) data 0.000 (0.017) loss 1.1213 (1.2816) lr 6.0396e-03 eta 0:09:09
epoch [14/30] batch [38/96] time 0.319 (0.343) data 0.000 (0.016) loss 1.3635 (1.2848) lr 6.0396e-03 eta 0:09:06
epoch [14/30] batch [40/96] time 0.315 (0.341) data 0.000 (0.016) loss 2.7354 (1.3086) lr 6.0396e-03 eta 0:09:03
epoch [14/30] batch [42/96] time 0.329 (0.341) data 0.000 (0.015) loss 1.0008 (1.3172) lr 6.0396e-03 eta 0:09:01
epoch [14/30] batch [44/96] time 0.331 (0.340) data 0.000 (0.014) loss 0.9392 (1.3084) lr 6.0396e-03 eta 0:09:00
epoch [14/30] batch [46/96] time 0.330 (0.340) data 0.000 (0.014) loss 0.7963 (1.3059) lr 6.0396e-03 eta 0:08:59
epoch [14/30] batch [48/96] time 0.330 (0.339) data 0.000 (0.013) loss 0.8712 (1.2862) lr 6.0396e-03 eta 0:08:57
epoch [14/30] batch [50/96] time 0.322 (0.339) data 0.000 (0.013) loss 1.1024 (1.2998) lr 6.0396e-03 eta 0:08:56
epoch [14/30] batch [52/96] time 0.349 (0.339) data 0.000 (0.012) loss 0.9761 (1.2920) lr 6.0396e-03 eta 0:08:55
epoch [14/30] batch [54/96] time 0.332 (0.338) data 0.000 (0.012) loss 1.5499 (1.2920) lr 6.0396e-03 eta 0:08:54
epoch [14/30] batch [56/96] time 0.348 (0.339) data 0.000 (0.011) loss 0.8139 (1.2762) lr 6.0396e-03 eta 0:08:53
epoch [14/30] batch [58/96] time 0.344 (0.339) data 0.000 (0.011) loss 1.0868 (1.2758) lr 6.0396e-03 eta 0:08:53
epoch [14/30] batch [60/96] time 0.340 (0.339) data 0.000 (0.011) loss 1.5913 (1.2834) lr 6.0396e-03 eta 0:08:52
epoch [14/30] batch [62/96] time 0.352 (0.339) data 0.000 (0.010) loss 0.9747 (1.2878) lr 6.0396e-03 eta 0:08:52
epoch [14/30] batch [64/96] time 0.341 (0.339) data 0.000 (0.010) loss 1.3595 (1.2947) lr 6.0396e-03 eta 0:08:51
epoch [14/30] batch [66/96] time 0.354 (0.340) data 0.000 (0.010) loss 1.6021 (1.2947) lr 6.0396e-03 eta 0:08:51
epoch [14/30] batch [68/96] time 0.343 (0.340) data 0.000 (0.009) loss 0.9909 (1.2930) lr 6.0396e-03 eta 0:08:51
epoch [14/30] batch [70/96] time 0.341 (0.340) data 0.000 (0.009) loss 1.6350 (1.2930) lr 6.0396e-03 eta 0:08:50
epoch [14/30] batch [72/96] time 0.346 (0.340) data 0.000 (0.009) loss 0.7811 (1.2883) lr 6.0396e-03 eta 0:08:50
epoch [14/30] batch [74/96] time 0.328 (0.340) data 0.000 (0.009) loss 1.4008 (1.2885) lr 6.0396e-03 eta 0:08:49
epoch [14/30] batch [76/96] time 0.322 (0.339) data 0.000 (0.008) loss 1.0324 (1.2830) lr 6.0396e-03 eta 0:08:47
epoch [14/30] batch [78/96] time 0.320 (0.339) data 0.000 (0.008) loss 0.9885 (1.2763) lr 6.0396e-03 eta 0:08:46
epoch [14/30] batch [80/96] time 0.324 (0.338) data 0.000 (0.008) loss 1.5573 (1.2792) lr 6.0396e-03 eta 0:08:45
epoch [14/30] batch [82/96] time 0.325 (0.338) data 0.000 (0.008) loss 1.1265 (1.2750) lr 6.0396e-03 eta 0:08:44
epoch [14/30] batch [84/96] time 0.316 (0.338) data 0.000 (0.008) loss 1.1679 (1.2824) lr 6.0396e-03 eta 0:08:42
epoch [14/30] batch [86/96] time 0.320 (0.337) data 0.000 (0.007) loss 0.9726 (1.2887) lr 6.0396e-03 eta 0:08:41
epoch [14/30] batch [88/96] time 0.318 (0.337) data 0.000 (0.007) loss 0.7781 (1.2945) lr 6.0396e-03 eta 0:08:39
epoch [14/30] batch [90/96] time 0.320 (0.336) data 0.000 (0.007) loss 0.8096 (1.2901) lr 6.0396e-03 eta 0:08:38
epoch [14/30] batch [92/96] time 0.316 (0.336) data 0.000 (0.007) loss 1.0202 (1.2888) lr 6.0396e-03 eta 0:08:37
epoch [14/30] batch [94/96] time 0.313 (0.336) data 0.000 (0.007) loss 0.8508 (1.2804) lr 6.0396e-03 eta 0:08:36
epoch [14/30] batch [96/96] time 0.322 (0.335) data 0.000 (0.007) loss 1.3020 (1.2773) lr 5.5226e-03 eta 0:08:34
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.81s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.33s/it]100%|██████████| 3/3 [00:03<00:00,  1.18it/s]100%|██████████| 3/3 [00:03<00:00,  1.16s/it]=> result
* total: 576
* correct: 414
* accuracy: 71.9%
* error: 28.1%
* macro_f1: 71.0%

epoch [15/30] batch [2/96] time 0.345 (0.660) data 0.001 (0.268) loss 1.9378 (1.4876) lr 5.5226e-03 eta 0:16:52
epoch [15/30] batch [4/96] time 0.333 (0.497) data 0.000 (0.134) loss 1.4617 (1.3048) lr 5.5226e-03 eta 0:12:41
epoch [15/30] batch [6/96] time 0.341 (0.445) data 0.000 (0.090) loss 2.3777 (1.4086) lr 5.5226e-03 eta 0:11:21
epoch [15/30] batch [8/96] time 0.333 (0.418) data 0.000 (0.067) loss 0.9807 (1.4216) lr 5.5226e-03 eta 0:10:38
epoch [15/30] batch [10/96] time 0.332 (0.402) data 0.000 (0.054) loss 1.4331 (1.4103) lr 5.5226e-03 eta 0:10:14
epoch [15/30] batch [12/96] time 0.329 (0.389) data 0.000 (0.045) loss 2.4605 (1.4643) lr 5.5226e-03 eta 0:09:52
epoch [15/30] batch [14/96] time 0.329 (0.381) data 0.000 (0.039) loss 2.0046 (1.4704) lr 5.5226e-03 eta 0:09:39
epoch [15/30] batch [16/96] time 0.328 (0.375) data 0.000 (0.034) loss 1.5011 (1.4464) lr 5.5226e-03 eta 0:09:30
epoch [15/30] batch [18/96] time 0.327 (0.369) data 0.000 (0.030) loss 1.5667 (1.4300) lr 5.5226e-03 eta 0:09:20
epoch [15/30] batch [20/96] time 0.322 (0.365) data 0.000 (0.027) loss 1.5747 (1.4343) lr 5.5226e-03 eta 0:09:13
epoch [15/30] batch [22/96] time 0.323 (0.362) data 0.000 (0.025) loss 1.8535 (1.4358) lr 5.5226e-03 eta 0:09:07
epoch [15/30] batch [24/96] time 0.322 (0.358) data 0.000 (0.023) loss 0.7986 (1.4137) lr 5.5226e-03 eta 0:09:01
epoch [15/30] batch [26/96] time 0.314 (0.355) data 0.000 (0.021) loss 1.1454 (1.3990) lr 5.5226e-03 eta 0:08:55
epoch [15/30] batch [28/96] time 0.319 (0.353) data 0.000 (0.019) loss 1.0961 (1.3938) lr 5.5226e-03 eta 0:08:51
epoch [15/30] batch [30/96] time 0.311 (0.350) data 0.000 (0.018) loss 1.0828 (1.3681) lr 5.5226e-03 eta 0:08:47
epoch [15/30] batch [32/96] time 0.347 (0.349) data 0.000 (0.017) loss 0.9929 (1.3784) lr 5.5226e-03 eta 0:08:45
epoch [15/30] batch [34/96] time 0.317 (0.348) data 0.000 (0.016) loss 1.6186 (1.3681) lr 5.5226e-03 eta 0:08:41
epoch [15/30] batch [36/96] time 0.412 (0.349) data 0.000 (0.015) loss 1.7303 (1.3953) lr 5.5226e-03 eta 0:08:43
epoch [15/30] batch [38/96] time 0.324 (0.348) data 0.000 (0.014) loss 0.9569 (1.3801) lr 5.5226e-03 eta 0:08:40
epoch [15/30] batch [40/96] time 0.330 (0.347) data 0.000 (0.014) loss 1.3596 (1.3795) lr 5.5226e-03 eta 0:08:38
epoch [15/30] batch [42/96] time 0.336 (0.346) data 0.000 (0.013) loss 0.8719 (1.3512) lr 5.5226e-03 eta 0:08:37
epoch [15/30] batch [44/96] time 0.324 (0.346) data 0.000 (0.013) loss 1.7162 (1.3529) lr 5.5226e-03 eta 0:08:35
epoch [15/30] batch [46/96] time 0.324 (0.345) data 0.000 (0.012) loss 1.5491 (1.3462) lr 5.5226e-03 eta 0:08:33
epoch [15/30] batch [48/96] time 0.325 (0.344) data 0.000 (0.011) loss 1.0721 (1.3300) lr 5.5226e-03 eta 0:08:31
epoch [15/30] batch [50/96] time 0.329 (0.343) data 0.000 (0.011) loss 0.8315 (1.3214) lr 5.5226e-03 eta 0:08:30
epoch [15/30] batch [52/96] time 0.328 (0.343) data 0.000 (0.011) loss 0.7029 (1.3040) lr 5.5226e-03 eta 0:08:29
epoch [15/30] batch [54/96] time 0.355 (0.343) data 0.000 (0.010) loss 2.0831 (1.3250) lr 5.5226e-03 eta 0:08:28
epoch [15/30] batch [56/96] time 0.337 (0.343) data 0.000 (0.010) loss 1.2146 (1.3170) lr 5.5226e-03 eta 0:08:27
epoch [15/30] batch [58/96] time 0.335 (0.343) data 0.000 (0.010) loss 1.6478 (1.3235) lr 5.5226e-03 eta 0:08:26
epoch [15/30] batch [60/96] time 0.346 (0.343) data 0.000 (0.009) loss 1.1379 (1.3121) lr 5.5226e-03 eta 0:08:26
epoch [15/30] batch [62/96] time 0.341 (0.343) data 0.000 (0.009) loss 1.4497 (1.3114) lr 5.5226e-03 eta 0:08:25
epoch [15/30] batch [64/96] time 0.332 (0.343) data 0.001 (0.009) loss 1.1063 (1.3022) lr 5.5226e-03 eta 0:08:24
epoch [15/30] batch [66/96] time 0.344 (0.343) data 0.000 (0.008) loss 1.0691 (1.2943) lr 5.5226e-03 eta 0:08:24
epoch [15/30] batch [68/96] time 0.335 (0.343) data 0.000 (0.008) loss 1.0001 (1.2849) lr 5.5226e-03 eta 0:08:23
epoch [15/30] batch [70/96] time 0.336 (0.342) data 0.000 (0.008) loss 1.3435 (1.2789) lr 5.5226e-03 eta 0:08:22
epoch [15/30] batch [72/96] time 0.327 (0.342) data 0.000 (0.008) loss 1.7232 (1.2846) lr 5.5226e-03 eta 0:08:20
epoch [15/30] batch [74/96] time 0.307 (0.341) data 0.000 (0.008) loss 1.5986 (1.2900) lr 5.5226e-03 eta 0:08:18
epoch [15/30] batch [76/96] time 0.314 (0.340) data 0.000 (0.007) loss 1.1054 (1.2814) lr 5.5226e-03 eta 0:08:16
epoch [15/30] batch [78/96] time 0.303 (0.339) data 0.000 (0.007) loss 0.6690 (1.2804) lr 5.5226e-03 eta 0:08:14
epoch [15/30] batch [80/96] time 0.310 (0.339) data 0.000 (0.007) loss 2.2101 (1.2921) lr 5.5226e-03 eta 0:08:13
epoch [15/30] batch [82/96] time 0.312 (0.338) data 0.000 (0.007) loss 1.4550 (1.2971) lr 5.5226e-03 eta 0:08:11
epoch [15/30] batch [84/96] time 0.309 (0.337) data 0.000 (0.007) loss 1.6403 (1.3129) lr 5.5226e-03 eta 0:08:09
epoch [15/30] batch [86/96] time 0.313 (0.337) data 0.000 (0.007) loss 0.8002 (1.3081) lr 5.5226e-03 eta 0:08:08
epoch [15/30] batch [88/96] time 0.310 (0.336) data 0.000 (0.006) loss 1.0830 (1.3032) lr 5.5226e-03 eta 0:08:06
epoch [15/30] batch [90/96] time 0.309 (0.336) data 0.000 (0.006) loss 0.7319 (1.2982) lr 5.5226e-03 eta 0:08:05
epoch [15/30] batch [92/96] time 0.310 (0.335) data 0.000 (0.006) loss 1.1877 (1.2926) lr 5.5226e-03 eta 0:08:03
epoch [15/30] batch [94/96] time 0.310 (0.334) data 0.000 (0.006) loss 0.7550 (1.2817) lr 5.5226e-03 eta 0:08:02
epoch [15/30] batch [96/96] time 0.313 (0.334) data 0.000 (0.006) loss 1.4450 (1.2825) lr 5.0000e-03 eta 0:08:00
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.80s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.32s/it]100%|██████████| 3/3 [00:03<00:00,  1.18it/s]100%|██████████| 3/3 [00:03<00:00,  1.16s/it]=> result
* total: 576
* correct: 428
* accuracy: 74.3%
* error: 25.7%
* macro_f1: 73.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [16/30] batch [2/96] time 0.330 (0.678) data 0.000 (0.316) loss 1.1893 (1.0905) lr 5.0000e-03 eta 0:16:14
epoch [16/30] batch [4/96] time 0.341 (0.509) data 0.000 (0.158) loss 1.9066 (1.3644) lr 5.0000e-03 eta 0:12:11
epoch [16/30] batch [6/96] time 0.349 (0.453) data 0.000 (0.105) loss 1.0488 (1.3163) lr 5.0000e-03 eta 0:10:48
epoch [16/30] batch [8/96] time 0.331 (0.422) data 0.000 (0.079) loss 1.3271 (1.3003) lr 5.0000e-03 eta 0:10:03
epoch [16/30] batch [10/96] time 0.322 (0.402) data 0.000 (0.063) loss 1.9097 (1.3474) lr 5.0000e-03 eta 0:09:34
epoch [16/30] batch [12/96] time 0.327 (0.389) data 0.000 (0.053) loss 1.6490 (1.3747) lr 5.0000e-03 eta 0:09:14
epoch [16/30] batch [14/96] time 0.319 (0.379) data 0.000 (0.045) loss 0.8236 (1.3177) lr 5.0000e-03 eta 0:09:00
epoch [16/30] batch [16/96] time 0.329 (0.373) data 0.000 (0.040) loss 0.9017 (1.3035) lr 5.0000e-03 eta 0:08:51
epoch [16/30] batch [18/96] time 0.328 (0.368) data 0.000 (0.035) loss 1.1230 (1.3080) lr 5.0000e-03 eta 0:08:43
epoch [16/30] batch [20/96] time 0.321 (0.365) data 0.000 (0.032) loss 0.7521 (1.2728) lr 5.0000e-03 eta 0:08:37
epoch [16/30] batch [22/96] time 0.330 (0.361) data 0.000 (0.029) loss 1.2127 (1.2765) lr 5.0000e-03 eta 0:08:32
epoch [16/30] batch [24/96] time 0.335 (0.359) data 0.000 (0.027) loss 1.3754 (1.2810) lr 5.0000e-03 eta 0:08:28
epoch [16/30] batch [26/96] time 0.334 (0.357) data 0.000 (0.025) loss 1.0216 (1.2673) lr 5.0000e-03 eta 0:08:24
epoch [16/30] batch [28/96] time 0.324 (0.355) data 0.000 (0.023) loss 0.9585 (1.2419) lr 5.0000e-03 eta 0:08:20
epoch [16/30] batch [30/96] time 0.326 (0.353) data 0.000 (0.021) loss 1.0881 (1.2279) lr 5.0000e-03 eta 0:08:17
epoch [16/30] batch [32/96] time 0.330 (0.352) data 0.000 (0.020) loss 0.8568 (1.2110) lr 5.0000e-03 eta 0:08:14
epoch [16/30] batch [34/96] time 0.325 (0.350) data 0.000 (0.019) loss 0.7389 (1.1989) lr 5.0000e-03 eta 0:08:11
epoch [16/30] batch [36/96] time 0.409 (0.351) data 0.000 (0.018) loss 1.5522 (1.2034) lr 5.0000e-03 eta 0:08:12
epoch [16/30] batch [38/96] time 0.322 (0.349) data 0.000 (0.017) loss 1.2090 (1.2056) lr 5.0000e-03 eta 0:08:09
epoch [16/30] batch [40/96] time 0.327 (0.348) data 0.000 (0.016) loss 0.8465 (1.1862) lr 5.0000e-03 eta 0:08:07
epoch [16/30] batch [42/96] time 0.317 (0.347) data 0.000 (0.015) loss 1.1947 (1.1951) lr 5.0000e-03 eta 0:08:04
epoch [16/30] batch [44/96] time 0.318 (0.346) data 0.000 (0.015) loss 1.4372 (1.2129) lr 5.0000e-03 eta 0:08:02
epoch [16/30] batch [46/96] time 0.326 (0.345) data 0.000 (0.014) loss 1.5512 (1.2199) lr 5.0000e-03 eta 0:08:00
epoch [16/30] batch [48/96] time 0.325 (0.344) data 0.000 (0.013) loss 0.7430 (1.2070) lr 5.0000e-03 eta 0:07:58
epoch [16/30] batch [50/96] time 0.328 (0.344) data 0.000 (0.013) loss 1.0696 (1.1995) lr 5.0000e-03 eta 0:07:57
epoch [16/30] batch [52/96] time 0.319 (0.343) data 0.000 (0.012) loss 0.9176 (1.2041) lr 5.0000e-03 eta 0:07:55
epoch [16/30] batch [54/96] time 0.329 (0.342) data 0.001 (0.012) loss 1.6917 (1.2124) lr 5.0000e-03 eta 0:07:54
epoch [16/30] batch [56/96] time 0.330 (0.342) data 0.000 (0.012) loss 1.0726 (1.2089) lr 5.0000e-03 eta 0:07:52
epoch [16/30] batch [58/96] time 0.322 (0.341) data 0.000 (0.011) loss 1.3872 (1.2190) lr 5.0000e-03 eta 0:07:51
epoch [16/30] batch [60/96] time 0.338 (0.341) data 0.000 (0.011) loss 1.6801 (1.2292) lr 5.0000e-03 eta 0:07:51
epoch [16/30] batch [62/96] time 0.352 (0.341) data 0.000 (0.010) loss 1.1800 (1.2258) lr 5.0000e-03 eta 0:07:50
epoch [16/30] batch [64/96] time 0.340 (0.342) data 0.000 (0.010) loss 1.6217 (1.2386) lr 5.0000e-03 eta 0:07:49
epoch [16/30] batch [66/96] time 0.347 (0.342) data 0.000 (0.010) loss 1.2787 (1.2388) lr 5.0000e-03 eta 0:07:49
epoch [16/30] batch [68/96] time 0.336 (0.342) data 0.000 (0.010) loss 0.7126 (1.2284) lr 5.0000e-03 eta 0:07:48
epoch [16/30] batch [70/96] time 0.340 (0.342) data 0.000 (0.009) loss 0.7188 (1.2217) lr 5.0000e-03 eta 0:07:48
epoch [16/30] batch [72/96] time 0.341 (0.342) data 0.000 (0.009) loss 1.5695 (1.2208) lr 5.0000e-03 eta 0:07:47
epoch [16/30] batch [74/96] time 0.326 (0.341) data 0.000 (0.009) loss 1.2577 (1.2266) lr 5.0000e-03 eta 0:07:46
epoch [16/30] batch [76/96] time 0.333 (0.341) data 0.000 (0.009) loss 1.5106 (1.2338) lr 5.0000e-03 eta 0:07:45
epoch [16/30] batch [78/96] time 0.326 (0.341) data 0.000 (0.008) loss 0.9571 (1.2348) lr 5.0000e-03 eta 0:07:43
epoch [16/30] batch [80/96] time 0.319 (0.340) data 0.000 (0.008) loss 1.5149 (1.2458) lr 5.0000e-03 eta 0:07:42
epoch [16/30] batch [82/96] time 0.324 (0.340) data 0.000 (0.008) loss 1.2352 (1.2421) lr 5.0000e-03 eta 0:07:41
epoch [16/30] batch [84/96] time 0.326 (0.339) data 0.000 (0.008) loss 1.3186 (1.2446) lr 5.0000e-03 eta 0:07:40
epoch [16/30] batch [86/96] time 0.321 (0.339) data 0.000 (0.008) loss 1.1387 (1.2373) lr 5.0000e-03 eta 0:07:39
epoch [16/30] batch [88/96] time 0.326 (0.339) data 0.000 (0.007) loss 1.3188 (1.2440) lr 5.0000e-03 eta 0:07:37
epoch [16/30] batch [90/96] time 0.321 (0.338) data 0.000 (0.007) loss 0.9645 (1.2414) lr 5.0000e-03 eta 0:07:36
epoch [16/30] batch [92/96] time 0.325 (0.338) data 0.000 (0.007) loss 1.0716 (1.2357) lr 5.0000e-03 eta 0:07:35
epoch [16/30] batch [94/96] time 0.326 (0.338) data 0.000 (0.007) loss 1.2348 (1.2355) lr 5.0000e-03 eta 0:07:34
epoch [16/30] batch [96/96] time 0.323 (0.337) data 0.000 (0.007) loss 1.3311 (1.2326) lr 4.4774e-03 eta 0:07:33
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.88s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 430
* accuracy: 74.7%
* error: 25.3%
* macro_f1: 74.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [17/30] batch [2/96] time 0.344 (0.685) data 0.000 (0.276) loss 1.4213 (1.3506) lr 4.4774e-03 eta 0:15:19
epoch [17/30] batch [4/96] time 0.350 (0.514) data 0.000 (0.138) loss 1.1078 (1.4435) lr 4.4774e-03 eta 0:11:29
epoch [17/30] batch [6/96] time 0.339 (0.457) data 0.000 (0.092) loss 1.7181 (1.4941) lr 4.4774e-03 eta 0:10:11
epoch [17/30] batch [8/96] time 0.347 (0.429) data 0.000 (0.069) loss 1.0604 (1.4322) lr 4.4774e-03 eta 0:09:33
epoch [17/30] batch [10/96] time 0.338 (0.411) data 0.000 (0.055) loss 1.0133 (1.3763) lr 4.4774e-03 eta 0:09:08
epoch [17/30] batch [12/96] time 0.338 (0.400) data 0.000 (0.046) loss 1.5338 (1.3694) lr 4.4774e-03 eta 0:08:52
epoch [17/30] batch [14/96] time 0.341 (0.392) data 0.000 (0.040) loss 1.2219 (1.3557) lr 4.4774e-03 eta 0:08:41
epoch [17/30] batch [16/96] time 0.345 (0.386) data 0.000 (0.035) loss 0.9044 (1.2905) lr 4.4774e-03 eta 0:08:32
epoch [17/30] batch [18/96] time 0.343 (0.381) data 0.000 (0.031) loss 0.9191 (1.2662) lr 4.4774e-03 eta 0:08:25
epoch [17/30] batch [20/96] time 0.376 (0.379) data 0.000 (0.028) loss 1.1337 (1.2541) lr 4.4774e-03 eta 0:08:22
epoch [17/30] batch [22/96] time 0.380 (0.378) data 0.000 (0.025) loss 1.4265 (1.2483) lr 4.4774e-03 eta 0:08:20
epoch [17/30] batch [24/96] time 0.335 (0.376) data 0.000 (0.023) loss 1.5615 (1.2736) lr 4.4774e-03 eta 0:08:16
epoch [17/30] batch [26/96] time 0.349 (0.374) data 0.000 (0.022) loss 1.1046 (1.2738) lr 4.4774e-03 eta 0:08:12
epoch [17/30] batch [28/96] time 0.321 (0.370) data 0.000 (0.020) loss 1.5604 (1.2699) lr 4.4774e-03 eta 0:08:07
epoch [17/30] batch [30/96] time 0.332 (0.368) data 0.000 (0.019) loss 1.6990 (1.2960) lr 4.4774e-03 eta 0:08:03
epoch [17/30] batch [32/96] time 0.345 (0.366) data 0.001 (0.018) loss 1.2208 (1.2969) lr 4.4774e-03 eta 0:08:00
epoch [17/30] batch [34/96] time 0.326 (0.364) data 0.000 (0.017) loss 1.4362 (1.3011) lr 4.4774e-03 eta 0:07:56
epoch [17/30] batch [36/96] time 0.424 (0.365) data 0.000 (0.016) loss 1.3779 (1.3104) lr 4.4774e-03 eta 0:07:56
epoch [17/30] batch [38/96] time 0.327 (0.363) data 0.000 (0.015) loss 1.1913 (1.3051) lr 4.4774e-03 eta 0:07:53
epoch [17/30] batch [40/96] time 0.319 (0.361) data 0.000 (0.014) loss 1.5088 (1.3050) lr 4.4774e-03 eta 0:07:50
epoch [17/30] batch [42/96] time 0.337 (0.359) data 0.000 (0.013) loss 1.0413 (1.2928) lr 4.4774e-03 eta 0:07:47
epoch [17/30] batch [44/96] time 0.323 (0.358) data 0.000 (0.013) loss 2.3423 (1.3132) lr 4.4774e-03 eta 0:07:45
epoch [17/30] batch [46/96] time 0.329 (0.357) data 0.001 (0.012) loss 1.0290 (1.2977) lr 4.4774e-03 eta 0:07:42
epoch [17/30] batch [48/96] time 0.320 (0.355) data 0.000 (0.012) loss 1.0596 (1.2938) lr 4.4774e-03 eta 0:07:40
epoch [17/30] batch [50/96] time 0.332 (0.354) data 0.000 (0.011) loss 1.6597 (1.3040) lr 4.4774e-03 eta 0:07:38
epoch [17/30] batch [52/96] time 0.329 (0.353) data 0.000 (0.011) loss 1.2070 (1.3104) lr 4.4774e-03 eta 0:07:36
epoch [17/30] batch [54/96] time 0.327 (0.352) data 0.000 (0.011) loss 0.7190 (1.2900) lr 4.4774e-03 eta 0:07:34
epoch [17/30] batch [56/96] time 0.325 (0.351) data 0.000 (0.010) loss 1.6795 (1.3057) lr 4.4774e-03 eta 0:07:32
epoch [17/30] batch [58/96] time 0.319 (0.350) data 0.001 (0.010) loss 0.8109 (1.2933) lr 4.4774e-03 eta 0:07:30
epoch [17/30] batch [60/96] time 0.347 (0.350) data 0.000 (0.010) loss 1.0005 (1.2865) lr 4.4774e-03 eta 0:07:29
epoch [17/30] batch [62/96] time 0.325 (0.349) data 0.000 (0.009) loss 1.0393 (1.2784) lr 4.4774e-03 eta 0:07:27
epoch [17/30] batch [64/96] time 0.324 (0.348) data 0.000 (0.009) loss 1.1063 (1.2680) lr 4.4774e-03 eta 0:07:25
epoch [17/30] batch [66/96] time 0.316 (0.347) data 0.000 (0.009) loss 0.8981 (1.2550) lr 4.4774e-03 eta 0:07:23
epoch [17/30] batch [68/96] time 0.334 (0.347) data 0.000 (0.008) loss 1.1680 (1.2493) lr 4.4774e-03 eta 0:07:22
epoch [17/30] batch [70/96] time 0.329 (0.346) data 0.001 (0.008) loss 0.9349 (1.2542) lr 4.4774e-03 eta 0:07:21
epoch [17/30] batch [72/96] time 0.330 (0.346) data 0.000 (0.008) loss 1.2940 (1.2605) lr 4.4774e-03 eta 0:07:19
epoch [17/30] batch [74/96] time 0.310 (0.345) data 0.000 (0.008) loss 1.7362 (1.2647) lr 4.4774e-03 eta 0:07:18
epoch [17/30] batch [76/96] time 0.316 (0.344) data 0.000 (0.008) loss 0.9821 (1.2618) lr 4.4774e-03 eta 0:07:16
epoch [17/30] batch [78/96] time 0.312 (0.343) data 0.000 (0.007) loss 1.3322 (1.2561) lr 4.4774e-03 eta 0:07:14
epoch [17/30] batch [80/96] time 0.317 (0.343) data 0.000 (0.007) loss 1.1923 (1.2552) lr 4.4774e-03 eta 0:07:13
epoch [17/30] batch [82/96] time 0.309 (0.342) data 0.000 (0.007) loss 1.2852 (1.2542) lr 4.4774e-03 eta 0:07:11
epoch [17/30] batch [84/96] time 0.315 (0.341) data 0.000 (0.007) loss 1.4880 (1.2595) lr 4.4774e-03 eta 0:07:09
epoch [17/30] batch [86/96] time 0.321 (0.341) data 0.000 (0.007) loss 1.7134 (1.2614) lr 4.4774e-03 eta 0:07:08
epoch [17/30] batch [88/96] time 0.316 (0.340) data 0.000 (0.007) loss 1.3158 (1.2588) lr 4.4774e-03 eta 0:07:07
epoch [17/30] batch [90/96] time 0.317 (0.340) data 0.000 (0.006) loss 1.5443 (1.2598) lr 4.4774e-03 eta 0:07:05
epoch [17/30] batch [92/96] time 0.320 (0.339) data 0.000 (0.006) loss 1.4570 (1.2573) lr 4.4774e-03 eta 0:07:04
epoch [17/30] batch [94/96] time 0.316 (0.339) data 0.000 (0.006) loss 1.2109 (1.2528) lr 4.4774e-03 eta 0:07:03
epoch [17/30] batch [96/96] time 0.319 (0.338) data 0.000 (0.006) loss 1.0888 (1.2519) lr 3.9604e-03 eta 0:07:02
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.82s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.34s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 431
* accuracy: 74.8%
* error: 25.2%
* macro_f1: 74.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [18/30] batch [2/96] time 0.332 (0.671) data 0.000 (0.272) loss 0.8895 (1.1725) lr 3.9604e-03 eta 0:13:56
epoch [18/30] batch [4/96] time 0.338 (0.503) data 0.000 (0.136) loss 1.1844 (1.1069) lr 3.9604e-03 eta 0:10:26
epoch [18/30] batch [6/96] time 0.320 (0.442) data 0.000 (0.091) loss 0.8221 (0.9955) lr 3.9604e-03 eta 0:09:08
epoch [18/30] batch [8/96] time 0.318 (0.412) data 0.000 (0.068) loss 1.9791 (1.1267) lr 3.9604e-03 eta 0:08:30
epoch [18/30] batch [10/96] time 0.323 (0.395) data 0.000 (0.055) loss 1.8703 (1.2231) lr 3.9604e-03 eta 0:08:08
epoch [18/30] batch [12/96] time 0.325 (0.383) data 0.000 (0.046) loss 1.2669 (1.1813) lr 3.9604e-03 eta 0:07:53
epoch [18/30] batch [14/96] time 0.322 (0.375) data 0.000 (0.039) loss 1.2367 (1.2188) lr 3.9604e-03 eta 0:07:42
epoch [18/30] batch [16/96] time 0.328 (0.368) data 0.000 (0.034) loss 2.5210 (1.2781) lr 3.9604e-03 eta 0:07:33
epoch [18/30] batch [18/96] time 0.326 (0.363) data 0.000 (0.030) loss 1.1258 (1.2469) lr 3.9604e-03 eta 0:07:27
epoch [18/30] batch [20/96] time 0.331 (0.360) data 0.000 (0.027) loss 1.2253 (1.2692) lr 3.9604e-03 eta 0:07:22
epoch [18/30] batch [22/96] time 0.330 (0.357) data 0.000 (0.025) loss 1.3122 (1.2717) lr 3.9604e-03 eta 0:07:18
epoch [18/30] batch [24/96] time 0.322 (0.355) data 0.000 (0.023) loss 1.1468 (1.2664) lr 3.9604e-03 eta 0:07:14
epoch [18/30] batch [26/96] time 0.323 (0.352) data 0.000 (0.021) loss 0.9748 (1.2451) lr 3.9604e-03 eta 0:07:10
epoch [18/30] batch [28/96] time 0.328 (0.351) data 0.000 (0.020) loss 0.7826 (1.2193) lr 3.9604e-03 eta 0:07:07
epoch [18/30] batch [30/96] time 0.327 (0.349) data 0.000 (0.018) loss 1.0312 (1.2183) lr 3.9604e-03 eta 0:07:04
epoch [18/30] batch [32/96] time 0.326 (0.347) data 0.000 (0.017) loss 1.2371 (1.2187) lr 3.9604e-03 eta 0:07:02
epoch [18/30] batch [34/96] time 0.329 (0.346) data 0.000 (0.016) loss 1.9068 (1.2459) lr 3.9604e-03 eta 0:07:00
epoch [18/30] batch [36/96] time 0.425 (0.348) data 0.000 (0.015) loss 1.2945 (1.2536) lr 3.9604e-03 eta 0:07:01
epoch [18/30] batch [38/96] time 0.323 (0.346) data 0.000 (0.015) loss 1.3409 (1.2586) lr 3.9604e-03 eta 0:06:58
epoch [18/30] batch [40/96] time 0.332 (0.346) data 0.000 (0.014) loss 1.1880 (1.2552) lr 3.9604e-03 eta 0:06:57
epoch [18/30] batch [42/96] time 0.335 (0.345) data 0.000 (0.013) loss 1.5610 (1.2515) lr 3.9604e-03 eta 0:06:56
epoch [18/30] batch [44/96] time 0.322 (0.344) data 0.000 (0.013) loss 1.0670 (1.2500) lr 3.9604e-03 eta 0:06:54
epoch [18/30] batch [46/96] time 0.328 (0.343) data 0.000 (0.012) loss 1.3320 (1.2439) lr 3.9604e-03 eta 0:06:52
epoch [18/30] batch [48/96] time 0.325 (0.343) data 0.000 (0.012) loss 1.2046 (1.2444) lr 3.9604e-03 eta 0:06:51
epoch [18/30] batch [50/96] time 0.333 (0.342) data 0.000 (0.011) loss 0.9159 (1.2330) lr 3.9604e-03 eta 0:06:49
epoch [18/30] batch [52/96] time 0.333 (0.342) data 0.000 (0.011) loss 1.2801 (1.2276) lr 3.9604e-03 eta 0:06:48
epoch [18/30] batch [54/96] time 0.339 (0.342) data 0.000 (0.010) loss 1.2450 (1.2235) lr 3.9604e-03 eta 0:06:48
epoch [18/30] batch [56/96] time 0.325 (0.341) data 0.000 (0.010) loss 1.1050 (1.2333) lr 3.9604e-03 eta 0:06:47
epoch [18/30] batch [58/96] time 0.331 (0.341) data 0.000 (0.010) loss 1.3419 (1.2371) lr 3.9604e-03 eta 0:06:46
epoch [18/30] batch [60/96] time 0.326 (0.341) data 0.000 (0.009) loss 1.1087 (1.2309) lr 3.9604e-03 eta 0:06:44
epoch [18/30] batch [62/96] time 0.322 (0.340) data 0.000 (0.009) loss 1.7051 (1.2380) lr 3.9604e-03 eta 0:06:43
epoch [18/30] batch [64/96] time 0.325 (0.340) data 0.000 (0.009) loss 2.1936 (1.2481) lr 3.9604e-03 eta 0:06:42
epoch [18/30] batch [66/96] time 0.327 (0.339) data 0.000 (0.009) loss 1.8416 (1.2602) lr 3.9604e-03 eta 0:06:41
epoch [18/30] batch [68/96] time 0.318 (0.339) data 0.000 (0.008) loss 1.2167 (1.2577) lr 3.9604e-03 eta 0:06:39
epoch [18/30] batch [70/96] time 0.325 (0.339) data 0.000 (0.008) loss 1.1712 (1.2582) lr 3.9604e-03 eta 0:06:38
epoch [18/30] batch [72/96] time 0.330 (0.338) data 0.000 (0.008) loss 1.3492 (1.2525) lr 3.9604e-03 eta 0:06:37
epoch [18/30] batch [74/96] time 0.308 (0.337) data 0.000 (0.008) loss 1.1754 (1.2490) lr 3.9604e-03 eta 0:06:36
epoch [18/30] batch [76/96] time 0.305 (0.337) data 0.000 (0.007) loss 1.0504 (1.2498) lr 3.9604e-03 eta 0:06:34
epoch [18/30] batch [78/96] time 0.305 (0.336) data 0.000 (0.007) loss 1.0654 (1.2431) lr 3.9604e-03 eta 0:06:32
epoch [18/30] batch [80/96] time 0.311 (0.335) data 0.000 (0.007) loss 1.3040 (1.2390) lr 3.9604e-03 eta 0:06:31
epoch [18/30] batch [82/96] time 0.307 (0.334) data 0.000 (0.007) loss 0.8397 (1.2312) lr 3.9604e-03 eta 0:06:29
epoch [18/30] batch [84/96] time 0.306 (0.334) data 0.000 (0.007) loss 1.3250 (1.2277) lr 3.9604e-03 eta 0:06:28
epoch [18/30] batch [86/96] time 0.313 (0.333) data 0.000 (0.007) loss 0.8436 (1.2286) lr 3.9604e-03 eta 0:06:27
epoch [18/30] batch [88/96] time 0.303 (0.333) data 0.000 (0.006) loss 0.9871 (1.2226) lr 3.9604e-03 eta 0:06:25
epoch [18/30] batch [90/96] time 0.309 (0.332) data 0.000 (0.006) loss 1.4645 (1.2245) lr 3.9604e-03 eta 0:06:24
epoch [18/30] batch [92/96] time 0.304 (0.331) data 0.000 (0.006) loss 1.4621 (1.2242) lr 3.9604e-03 eta 0:06:23
epoch [18/30] batch [94/96] time 0.306 (0.331) data 0.000 (0.006) loss 1.1445 (1.2267) lr 3.9604e-03 eta 0:06:21
epoch [18/30] batch [96/96] time 0.301 (0.330) data 0.000 (0.006) loss 2.2293 (1.2378) lr 3.4549e-03 eta 0:06:20
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.90s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.37s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.19s/it]=> result
* total: 576
* correct: 437
* accuracy: 75.9%
* error: 24.1%
* macro_f1: 75.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [19/30] batch [2/96] time 0.324 (0.643) data 0.000 (0.272) loss 0.8241 (1.3329) lr 3.4549e-03 eta 0:12:18
epoch [19/30] batch [4/96] time 0.321 (0.480) data 0.000 (0.136) loss 1.6453 (1.4616) lr 3.4549e-03 eta 0:09:11
epoch [19/30] batch [6/96] time 0.321 (0.428) data 0.000 (0.091) loss 1.0836 (1.3093) lr 3.4549e-03 eta 0:08:10
epoch [19/30] batch [8/96] time 0.326 (0.402) data 0.000 (0.068) loss 1.0759 (1.2063) lr 3.4549e-03 eta 0:07:39
epoch [19/30] batch [10/96] time 0.319 (0.386) data 0.000 (0.055) loss 0.8509 (1.1918) lr 3.4549e-03 eta 0:07:21
epoch [19/30] batch [12/96] time 0.326 (0.377) data 0.000 (0.046) loss 1.6001 (1.2366) lr 3.4549e-03 eta 0:07:09
epoch [19/30] batch [14/96] time 0.322 (0.370) data 0.000 (0.039) loss 0.8945 (1.1944) lr 3.4549e-03 eta 0:07:00
epoch [19/30] batch [16/96] time 0.323 (0.364) data 0.000 (0.034) loss 1.7765 (1.2400) lr 3.4549e-03 eta 0:06:53
epoch [19/30] batch [18/96] time 0.323 (0.359) data 0.000 (0.031) loss 1.6589 (1.2618) lr 3.4549e-03 eta 0:06:47
epoch [19/30] batch [20/96] time 0.333 (0.356) data 0.000 (0.028) loss 1.2411 (1.2487) lr 3.4549e-03 eta 0:06:42
epoch [19/30] batch [22/96] time 0.321 (0.352) data 0.000 (0.025) loss 1.0606 (1.2221) lr 3.4549e-03 eta 0:06:38
epoch [19/30] batch [24/96] time 0.327 (0.350) data 0.000 (0.023) loss 1.2896 (1.2024) lr 3.4549e-03 eta 0:06:34
epoch [19/30] batch [26/96] time 0.347 (0.349) data 0.000 (0.021) loss 0.6512 (1.1741) lr 3.4549e-03 eta 0:06:33
epoch [19/30] batch [28/96] time 0.323 (0.348) data 0.000 (0.020) loss 2.1242 (1.2061) lr 3.4549e-03 eta 0:06:30
epoch [19/30] batch [30/96] time 0.335 (0.346) data 0.000 (0.018) loss 1.2348 (1.2125) lr 3.4549e-03 eta 0:06:28
epoch [19/30] batch [32/96] time 0.323 (0.345) data 0.000 (0.017) loss 0.7662 (1.1960) lr 3.4549e-03 eta 0:06:26
epoch [19/30] batch [34/96] time 0.322 (0.343) data 0.000 (0.016) loss 0.9468 (1.1770) lr 3.4549e-03 eta 0:06:23
epoch [19/30] batch [36/96] time 0.420 (0.345) data 0.000 (0.015) loss 1.6333 (1.1938) lr 3.4549e-03 eta 0:06:25
epoch [19/30] batch [38/96] time 0.345 (0.345) data 0.000 (0.015) loss 1.4648 (1.2031) lr 3.4549e-03 eta 0:06:23
epoch [19/30] batch [40/96] time 0.310 (0.344) data 0.000 (0.014) loss 0.8421 (1.1943) lr 3.4549e-03 eta 0:06:22
epoch [19/30] batch [42/96] time 0.327 (0.343) data 0.000 (0.013) loss 0.9897 (1.1934) lr 3.4549e-03 eta 0:06:20
epoch [19/30] batch [44/96] time 0.326 (0.342) data 0.000 (0.013) loss 1.4256 (1.2020) lr 3.4549e-03 eta 0:06:18
epoch [19/30] batch [46/96] time 0.327 (0.341) data 0.000 (0.012) loss 0.8613 (1.2002) lr 3.4549e-03 eta 0:06:17
epoch [19/30] batch [48/96] time 0.331 (0.340) data 0.000 (0.012) loss 1.5500 (1.2004) lr 3.4549e-03 eta 0:06:15
epoch [19/30] batch [50/96] time 0.322 (0.340) data 0.000 (0.011) loss 2.2243 (1.2318) lr 3.4549e-03 eta 0:06:14
epoch [19/30] batch [52/96] time 0.325 (0.339) data 0.000 (0.011) loss 1.2966 (1.2254) lr 3.4549e-03 eta 0:06:12
epoch [19/30] batch [54/96] time 0.326 (0.338) data 0.000 (0.010) loss 1.2703 (1.2273) lr 3.4549e-03 eta 0:06:11
epoch [19/30] batch [56/96] time 0.317 (0.338) data 0.000 (0.010) loss 1.0669 (1.2255) lr 3.4549e-03 eta 0:06:10
epoch [19/30] batch [58/96] time 0.329 (0.337) data 0.000 (0.010) loss 1.0067 (1.2226) lr 3.4549e-03 eta 0:06:09
epoch [19/30] batch [60/96] time 0.341 (0.337) data 0.000 (0.009) loss 1.0251 (1.2118) lr 3.4549e-03 eta 0:06:08
epoch [19/30] batch [62/96] time 0.327 (0.337) data 0.000 (0.009) loss 1.1051 (1.2073) lr 3.4549e-03 eta 0:06:07
epoch [19/30] batch [64/96] time 0.334 (0.337) data 0.000 (0.009) loss 1.5163 (1.2043) lr 3.4549e-03 eta 0:06:06
epoch [19/30] batch [66/96] time 0.324 (0.336) data 0.000 (0.009) loss 1.0793 (1.2088) lr 3.4549e-03 eta 0:06:05
epoch [19/30] batch [68/96] time 0.322 (0.336) data 0.000 (0.008) loss 1.2123 (1.2107) lr 3.4549e-03 eta 0:06:04
epoch [19/30] batch [70/96] time 0.326 (0.336) data 0.000 (0.008) loss 0.9754 (1.2056) lr 3.4549e-03 eta 0:06:03
epoch [19/30] batch [72/96] time 0.343 (0.336) data 0.000 (0.008) loss 1.0394 (1.1987) lr 3.4549e-03 eta 0:06:02
epoch [19/30] batch [74/96] time 0.321 (0.335) data 0.000 (0.008) loss 1.1869 (1.1991) lr 3.4549e-03 eta 0:06:01
epoch [19/30] batch [76/96] time 0.316 (0.335) data 0.000 (0.007) loss 1.7203 (1.2048) lr 3.4549e-03 eta 0:06:00
epoch [19/30] batch [78/96] time 0.317 (0.334) data 0.000 (0.007) loss 1.0748 (1.2056) lr 3.4549e-03 eta 0:05:58
epoch [19/30] batch [80/96] time 0.320 (0.334) data 0.000 (0.007) loss 1.7780 (1.2077) lr 3.4549e-03 eta 0:05:57
epoch [19/30] batch [82/96] time 0.317 (0.333) data 0.000 (0.007) loss 1.7245 (1.2086) lr 3.4549e-03 eta 0:05:56
epoch [19/30] batch [84/96] time 0.324 (0.333) data 0.000 (0.007) loss 1.1760 (1.2119) lr 3.4549e-03 eta 0:05:55
epoch [19/30] batch [86/96] time 0.319 (0.333) data 0.000 (0.007) loss 0.9498 (1.2080) lr 3.4549e-03 eta 0:05:54
epoch [19/30] batch [88/96] time 0.314 (0.333) data 0.000 (0.006) loss 1.1762 (1.2056) lr 3.4549e-03 eta 0:05:53
epoch [19/30] batch [90/96] time 0.321 (0.332) data 0.000 (0.006) loss 1.0341 (1.2023) lr 3.4549e-03 eta 0:05:52
epoch [19/30] batch [92/96] time 0.318 (0.332) data 0.000 (0.006) loss 0.7313 (1.2035) lr 3.4549e-03 eta 0:05:51
epoch [19/30] batch [94/96] time 0.318 (0.332) data 0.000 (0.006) loss 1.0522 (1.2021) lr 3.4549e-03 eta 0:05:50
epoch [19/30] batch [96/96] time 0.324 (0.332) data 0.000 (0.006) loss 1.1948 (1.2083) lr 2.9663e-03 eta 0:05:50
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.83s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.34s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.16s/it]=> result
* total: 576
* correct: 433
* accuracy: 75.2%
* error: 24.8%
* macro_f1: 74.9%

epoch [20/30] batch [2/96] time 0.333 (0.674) data 0.000 (0.275) loss 1.5319 (1.2138) lr 2.9663e-03 eta 0:11:49
epoch [20/30] batch [4/96] time 0.330 (0.503) data 0.000 (0.138) loss 0.9391 (1.0920) lr 2.9663e-03 eta 0:08:48
epoch [20/30] batch [6/96] time 0.339 (0.448) data 0.000 (0.092) loss 1.1204 (1.0791) lr 2.9663e-03 eta 0:07:50
epoch [20/30] batch [8/96] time 0.310 (0.416) data 0.000 (0.069) loss 1.0683 (1.0752) lr 2.9663e-03 eta 0:07:16
epoch [20/30] batch [10/96] time 0.332 (0.399) data 0.000 (0.055) loss 0.9671 (1.0796) lr 2.9663e-03 eta 0:06:57
epoch [20/30] batch [12/96] time 0.326 (0.387) data 0.000 (0.046) loss 0.9416 (1.0705) lr 2.9663e-03 eta 0:06:43
epoch [20/30] batch [14/96] time 0.326 (0.378) data 0.000 (0.040) loss 1.3176 (1.1072) lr 2.9663e-03 eta 0:06:33
epoch [20/30] batch [16/96] time 0.325 (0.371) data 0.000 (0.035) loss 1.2047 (1.1574) lr 2.9663e-03 eta 0:06:25
epoch [20/30] batch [18/96] time 0.325 (0.366) data 0.000 (0.031) loss 1.8277 (1.1927) lr 2.9663e-03 eta 0:06:19
epoch [20/30] batch [20/96] time 0.329 (0.362) data 0.000 (0.028) loss 1.0829 (1.1685) lr 2.9663e-03 eta 0:06:15
epoch [20/30] batch [22/96] time 0.327 (0.359) data 0.000 (0.025) loss 0.7381 (1.1377) lr 2.9663e-03 eta 0:06:11
epoch [20/30] batch [24/96] time 0.327 (0.357) data 0.000 (0.023) loss 2.1071 (1.1883) lr 2.9663e-03 eta 0:06:08
epoch [20/30] batch [26/96] time 0.331 (0.355) data 0.000 (0.021) loss 1.0365 (1.1730) lr 2.9663e-03 eta 0:06:05
epoch [20/30] batch [28/96] time 0.324 (0.352) data 0.000 (0.020) loss 1.1039 (1.1579) lr 2.9663e-03 eta 0:06:02
epoch [20/30] batch [30/96] time 0.330 (0.351) data 0.000 (0.019) loss 1.0499 (1.1528) lr 2.9663e-03 eta 0:06:00
epoch [20/30] batch [32/96] time 0.321 (0.350) data 0.000 (0.017) loss 1.7373 (1.1858) lr 2.9663e-03 eta 0:05:57
epoch [20/30] batch [34/96] time 0.323 (0.348) data 0.000 (0.016) loss 1.0580 (1.1798) lr 2.9663e-03 eta 0:05:56
epoch [20/30] batch [36/96] time 0.424 (0.350) data 0.000 (0.016) loss 0.9830 (1.1800) lr 2.9663e-03 eta 0:05:56
epoch [20/30] batch [38/96] time 0.338 (0.349) data 0.000 (0.015) loss 0.9650 (1.1879) lr 2.9663e-03 eta 0:05:55
epoch [20/30] batch [40/96] time 0.330 (0.348) data 0.000 (0.014) loss 0.7799 (1.1798) lr 2.9663e-03 eta 0:05:53
epoch [20/30] batch [42/96] time 0.326 (0.347) data 0.000 (0.013) loss 1.2001 (1.1802) lr 2.9663e-03 eta 0:05:51
epoch [20/30] batch [44/96] time 0.320 (0.346) data 0.001 (0.013) loss 1.5137 (1.1902) lr 2.9663e-03 eta 0:05:50
epoch [20/30] batch [46/96] time 0.344 (0.345) data 0.000 (0.012) loss 0.9906 (1.1846) lr 2.9663e-03 eta 0:05:48
epoch [20/30] batch [48/96] time 0.335 (0.345) data 0.000 (0.012) loss 0.9836 (1.1718) lr 2.9663e-03 eta 0:05:47
epoch [20/30] batch [50/96] time 0.315 (0.344) data 0.000 (0.011) loss 0.9418 (1.1595) lr 2.9663e-03 eta 0:05:45
epoch [20/30] batch [52/96] time 0.328 (0.343) data 0.000 (0.011) loss 0.8760 (1.1453) lr 2.9663e-03 eta 0:05:44
epoch [20/30] batch [54/96] time 0.330 (0.343) data 0.000 (0.010) loss 1.3371 (1.1428) lr 2.9663e-03 eta 0:05:43
epoch [20/30] batch [56/96] time 0.328 (0.342) data 0.000 (0.010) loss 1.5688 (1.1625) lr 2.9663e-03 eta 0:05:42
epoch [20/30] batch [58/96] time 0.337 (0.342) data 0.001 (0.010) loss 1.5919 (1.1734) lr 2.9663e-03 eta 0:05:41
epoch [20/30] batch [60/96] time 0.346 (0.342) data 0.000 (0.009) loss 0.9651 (1.1793) lr 2.9663e-03 eta 0:05:40
epoch [20/30] batch [62/96] time 0.348 (0.343) data 0.000 (0.009) loss 1.0188 (1.1742) lr 2.9663e-03 eta 0:05:40
epoch [20/30] batch [64/96] time 0.342 (0.343) data 0.000 (0.009) loss 0.9292 (1.1680) lr 2.9663e-03 eta 0:05:39
epoch [20/30] batch [66/96] time 0.343 (0.342) data 0.000 (0.009) loss 1.5891 (1.1698) lr 2.9663e-03 eta 0:05:39
epoch [20/30] batch [68/96] time 0.356 (0.343) data 0.000 (0.008) loss 1.0746 (1.1665) lr 2.9663e-03 eta 0:05:38
epoch [20/30] batch [70/96] time 0.343 (0.343) data 0.001 (0.008) loss 1.4903 (1.1710) lr 2.9663e-03 eta 0:05:38
epoch [20/30] batch [72/96] time 0.348 (0.343) data 0.000 (0.008) loss 2.3583 (1.1900) lr 2.9663e-03 eta 0:05:37
epoch [20/30] batch [74/96] time 0.319 (0.342) data 0.000 (0.008) loss 1.6584 (1.2009) lr 2.9663e-03 eta 0:05:36
epoch [20/30] batch [76/96] time 0.321 (0.342) data 0.000 (0.008) loss 0.8737 (1.1937) lr 2.9663e-03 eta 0:05:35
epoch [20/30] batch [78/96] time 0.315 (0.341) data 0.000 (0.007) loss 1.0124 (1.1968) lr 2.9663e-03 eta 0:05:33
epoch [20/30] batch [80/96] time 0.308 (0.340) data 0.000 (0.007) loss 0.9785 (1.1902) lr 2.9663e-03 eta 0:05:32
epoch [20/30] batch [82/96] time 0.310 (0.340) data 0.000 (0.007) loss 1.0174 (1.1882) lr 2.9663e-03 eta 0:05:30
epoch [20/30] batch [84/96] time 0.306 (0.339) data 0.000 (0.007) loss 1.2348 (1.1875) lr 2.9663e-03 eta 0:05:29
epoch [20/30] batch [86/96] time 0.308 (0.338) data 0.000 (0.007) loss 1.4213 (1.1910) lr 2.9663e-03 eta 0:05:27
epoch [20/30] batch [88/96] time 0.304 (0.337) data 0.000 (0.007) loss 0.7221 (1.1839) lr 2.9663e-03 eta 0:05:26
epoch [20/30] batch [90/96] time 0.305 (0.337) data 0.000 (0.006) loss 0.9369 (1.1803) lr 2.9663e-03 eta 0:05:25
epoch [20/30] batch [92/96] time 0.313 (0.336) data 0.000 (0.006) loss 1.3931 (1.1833) lr 2.9663e-03 eta 0:05:24
epoch [20/30] batch [94/96] time 0.309 (0.336) data 0.000 (0.006) loss 1.2503 (1.1816) lr 2.9663e-03 eta 0:05:22
epoch [20/30] batch [96/96] time 0.307 (0.335) data 0.000 (0.006) loss 2.0091 (1.1890) lr 2.5000e-03 eta 0:05:21
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.83s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.34s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 431
* accuracy: 74.8%
* error: 25.2%
* macro_f1: 74.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-20

epoch [21/30] batch [2/96] time 0.320 (0.656) data 0.001 (0.288) loss 0.7806 (0.8654) lr 2.5000e-03 eta 0:10:28
epoch [21/30] batch [4/96] time 0.325 (0.490) data 0.000 (0.144) loss 0.8873 (0.9840) lr 2.5000e-03 eta 0:07:48
epoch [21/30] batch [6/96] time 0.331 (0.437) data 0.000 (0.096) loss 0.9861 (1.1520) lr 2.5000e-03 eta 0:06:56
epoch [21/30] batch [8/96] time 0.328 (0.409) data 0.000 (0.072) loss 1.2528 (1.1823) lr 2.5000e-03 eta 0:06:29
epoch [21/30] batch [10/96] time 0.331 (0.395) data 0.000 (0.058) loss 0.9055 (1.1416) lr 2.5000e-03 eta 0:06:14
epoch [21/30] batch [12/96] time 0.326 (0.382) data 0.000 (0.048) loss 0.9928 (1.1564) lr 2.5000e-03 eta 0:06:02
epoch [21/30] batch [14/96] time 0.324 (0.374) data 0.000 (0.041) loss 1.5341 (1.1634) lr 2.5000e-03 eta 0:05:53
epoch [21/30] batch [16/96] time 0.322 (0.368) data 0.000 (0.036) loss 1.2827 (1.1687) lr 2.5000e-03 eta 0:05:46
epoch [21/30] batch [18/96] time 0.327 (0.363) data 0.000 (0.032) loss 0.9358 (1.1332) lr 2.5000e-03 eta 0:05:41
epoch [21/30] batch [20/96] time 0.332 (0.359) data 0.000 (0.029) loss 1.3381 (1.1600) lr 2.5000e-03 eta 0:05:37
epoch [21/30] batch [22/96] time 0.327 (0.356) data 0.000 (0.026) loss 1.1776 (1.1455) lr 2.5000e-03 eta 0:05:34
epoch [21/30] batch [24/96] time 0.344 (0.355) data 0.000 (0.024) loss 1.2550 (1.1601) lr 2.5000e-03 eta 0:05:32
epoch [21/30] batch [26/96] time 0.330 (0.353) data 0.001 (0.022) loss 0.9197 (1.1754) lr 2.5000e-03 eta 0:05:29
epoch [21/30] batch [28/96] time 0.334 (0.351) data 0.000 (0.021) loss 1.0382 (1.1612) lr 2.5000e-03 eta 0:05:26
epoch [21/30] batch [30/96] time 0.322 (0.349) data 0.000 (0.020) loss 1.9362 (1.1871) lr 2.5000e-03 eta 0:05:24
epoch [21/30] batch [32/96] time 0.337 (0.348) data 0.000 (0.018) loss 1.7387 (1.1934) lr 2.5000e-03 eta 0:05:22
epoch [21/30] batch [34/96] time 0.332 (0.347) data 0.000 (0.017) loss 1.8170 (1.2150) lr 2.5000e-03 eta 0:05:21
epoch [21/30] batch [36/96] time 0.418 (0.349) data 0.000 (0.016) loss 0.8013 (1.2076) lr 2.5000e-03 eta 0:05:22
epoch [21/30] batch [38/96] time 0.327 (0.347) data 0.000 (0.015) loss 0.8936 (1.1903) lr 2.5000e-03 eta 0:05:20
epoch [21/30] batch [40/96] time 0.325 (0.346) data 0.000 (0.015) loss 1.1144 (1.1787) lr 2.5000e-03 eta 0:05:18
epoch [21/30] batch [42/96] time 0.336 (0.346) data 0.000 (0.014) loss 1.3158 (1.1728) lr 2.5000e-03 eta 0:05:17
epoch [21/30] batch [44/96] time 0.337 (0.345) data 0.000 (0.013) loss 2.0413 (1.1882) lr 2.5000e-03 eta 0:05:16
epoch [21/30] batch [46/96] time 0.317 (0.345) data 0.000 (0.013) loss 1.0857 (1.1771) lr 2.5000e-03 eta 0:05:15
epoch [21/30] batch [48/96] time 0.318 (0.344) data 0.000 (0.012) loss 0.6988 (1.1790) lr 2.5000e-03 eta 0:05:13
epoch [21/30] batch [50/96] time 0.327 (0.343) data 0.000 (0.012) loss 1.1344 (1.1756) lr 2.5000e-03 eta 0:05:12
epoch [21/30] batch [52/96] time 0.319 (0.342) data 0.000 (0.011) loss 1.7374 (1.1917) lr 2.5000e-03 eta 0:05:10
epoch [21/30] batch [54/96] time 0.344 (0.342) data 0.000 (0.011) loss 0.7953 (1.1866) lr 2.5000e-03 eta 0:05:10
epoch [21/30] batch [56/96] time 0.324 (0.342) data 0.000 (0.011) loss 1.9484 (1.2018) lr 2.5000e-03 eta 0:05:08
epoch [21/30] batch [58/96] time 0.330 (0.341) data 0.000 (0.010) loss 1.1870 (1.1958) lr 2.5000e-03 eta 0:05:07
epoch [21/30] batch [60/96] time 0.329 (0.341) data 0.000 (0.010) loss 0.8570 (1.2002) lr 2.5000e-03 eta 0:05:06
epoch [21/30] batch [62/96] time 0.344 (0.341) data 0.000 (0.010) loss 1.1864 (1.2006) lr 2.5000e-03 eta 0:05:05
epoch [21/30] batch [64/96] time 0.338 (0.340) data 0.000 (0.009) loss 1.2204 (1.2040) lr 2.5000e-03 eta 0:05:05
epoch [21/30] batch [66/96] time 0.328 (0.340) data 0.000 (0.009) loss 1.3420 (1.2008) lr 2.5000e-03 eta 0:05:04
epoch [21/30] batch [68/96] time 0.325 (0.340) data 0.000 (0.009) loss 0.9671 (1.1923) lr 2.5000e-03 eta 0:05:03
epoch [21/30] batch [70/96] time 0.336 (0.340) data 0.000 (0.009) loss 0.8686 (1.1806) lr 2.5000e-03 eta 0:05:02
epoch [21/30] batch [72/96] time 0.330 (0.339) data 0.000 (0.008) loss 1.0259 (1.1759) lr 2.5000e-03 eta 0:05:01
epoch [21/30] batch [74/96] time 0.307 (0.338) data 0.000 (0.008) loss 2.3238 (1.1874) lr 2.5000e-03 eta 0:04:59
epoch [21/30] batch [76/96] time 0.313 (0.338) data 0.000 (0.008) loss 1.0270 (1.1864) lr 2.5000e-03 eta 0:04:58
epoch [21/30] batch [78/96] time 0.310 (0.337) data 0.000 (0.008) loss 0.8637 (1.1853) lr 2.5000e-03 eta 0:04:57
epoch [21/30] batch [80/96] time 0.315 (0.336) data 0.000 (0.008) loss 1.4739 (1.1834) lr 2.5000e-03 eta 0:04:55
epoch [21/30] batch [82/96] time 0.309 (0.336) data 0.000 (0.007) loss 0.7965 (1.1811) lr 2.5000e-03 eta 0:04:54
epoch [21/30] batch [84/96] time 0.305 (0.335) data 0.000 (0.007) loss 0.9212 (1.1761) lr 2.5000e-03 eta 0:04:53
epoch [21/30] batch [86/96] time 0.311 (0.334) data 0.000 (0.007) loss 1.2421 (1.1752) lr 2.5000e-03 eta 0:04:52
epoch [21/30] batch [88/96] time 0.308 (0.334) data 0.000 (0.007) loss 0.8026 (1.1743) lr 2.5000e-03 eta 0:04:51
epoch [21/30] batch [90/96] time 0.310 (0.333) data 0.000 (0.007) loss 1.7108 (1.1776) lr 2.5000e-03 eta 0:04:50
epoch [21/30] batch [92/96] time 0.312 (0.333) data 0.000 (0.007) loss 1.8625 (1.1796) lr 2.5000e-03 eta 0:04:48
epoch [21/30] batch [94/96] time 0.308 (0.332) data 0.000 (0.006) loss 1.2197 (1.1853) lr 2.5000e-03 eta 0:04:47
epoch [21/30] batch [96/96] time 0.311 (0.332) data 0.000 (0.006) loss 1.5282 (1.1894) lr 2.0611e-03 eta 0:04:46
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.87s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.19s/it]=> result
* total: 576
* correct: 433
* accuracy: 75.2%
* error: 24.8%
* macro_f1: 74.8%

epoch [22/30] batch [2/96] time 0.308 (0.652) data 0.000 (0.285) loss 0.8560 (1.1180) lr 2.0611e-03 eta 0:09:22
epoch [22/30] batch [4/96] time 0.327 (0.487) data 0.000 (0.143) loss 0.7308 (1.2510) lr 2.0611e-03 eta 0:06:58
epoch [22/30] batch [6/96] time 0.337 (0.435) data 0.000 (0.095) loss 1.2940 (1.1991) lr 2.0611e-03 eta 0:06:13
epoch [22/30] batch [8/96] time 0.321 (0.408) data 0.000 (0.072) loss 2.2725 (1.3189) lr 2.0611e-03 eta 0:05:49
epoch [22/30] batch [10/96] time 0.330 (0.391) data 0.000 (0.057) loss 1.0757 (1.2465) lr 2.0611e-03 eta 0:05:34
epoch [22/30] batch [12/96] time 0.333 (0.380) data 0.000 (0.048) loss 0.9526 (1.2243) lr 2.0611e-03 eta 0:05:24
epoch [22/30] batch [14/96] time 0.317 (0.371) data 0.000 (0.041) loss 0.9094 (1.2474) lr 2.0611e-03 eta 0:05:15
epoch [22/30] batch [16/96] time 0.328 (0.365) data 0.000 (0.036) loss 1.2746 (1.2477) lr 2.0611e-03 eta 0:05:09
epoch [22/30] batch [18/96] time 0.333 (0.361) data 0.000 (0.032) loss 0.9406 (1.2215) lr 2.0611e-03 eta 0:05:05
epoch [22/30] batch [20/96] time 0.333 (0.358) data 0.000 (0.029) loss 0.7278 (1.2002) lr 2.0611e-03 eta 0:05:02
epoch [22/30] batch [22/96] time 0.335 (0.356) data 0.000 (0.026) loss 2.0566 (1.2259) lr 2.0611e-03 eta 0:04:59
epoch [22/30] batch [24/96] time 0.321 (0.353) data 0.000 (0.024) loss 0.8879 (1.2239) lr 2.0611e-03 eta 0:04:56
epoch [22/30] batch [26/96] time 0.334 (0.351) data 0.000 (0.022) loss 1.2849 (1.2143) lr 2.0611e-03 eta 0:04:54
epoch [22/30] batch [28/96] time 0.328 (0.349) data 0.000 (0.021) loss 0.9013 (1.1958) lr 2.0611e-03 eta 0:04:52
epoch [22/30] batch [30/96] time 0.313 (0.347) data 0.000 (0.019) loss 2.0057 (1.2192) lr 2.0611e-03 eta 0:04:49
epoch [22/30] batch [32/96] time 0.317 (0.345) data 0.000 (0.018) loss 1.8242 (1.2305) lr 2.0611e-03 eta 0:04:47
epoch [22/30] batch [34/96] time 0.308 (0.343) data 0.000 (0.017) loss 1.4373 (1.2360) lr 2.0611e-03 eta 0:04:44
epoch [22/30] batch [36/96] time 0.439 (0.346) data 0.000 (0.016) loss 1.8205 (1.2449) lr 2.0611e-03 eta 0:04:46
epoch [22/30] batch [38/96] time 0.329 (0.345) data 0.000 (0.015) loss 0.7038 (1.2256) lr 2.0611e-03 eta 0:04:44
epoch [22/30] batch [40/96] time 0.346 (0.344) data 0.000 (0.015) loss 1.0141 (1.2083) lr 2.0611e-03 eta 0:04:43
epoch [22/30] batch [42/96] time 0.344 (0.344) data 0.000 (0.014) loss 0.9287 (1.2003) lr 2.0611e-03 eta 0:04:42
epoch [22/30] batch [44/96] time 0.318 (0.343) data 0.000 (0.013) loss 1.0489 (1.1936) lr 2.0611e-03 eta 0:04:41
epoch [22/30] batch [46/96] time 0.318 (0.342) data 0.000 (0.013) loss 0.9666 (1.1804) lr 2.0611e-03 eta 0:04:39
epoch [22/30] batch [48/96] time 0.339 (0.342) data 0.000 (0.012) loss 1.1557 (1.1729) lr 2.0611e-03 eta 0:04:38
epoch [22/30] batch [50/96] time 0.325 (0.341) data 0.000 (0.012) loss 1.0831 (1.1628) lr 2.0611e-03 eta 0:04:37
epoch [22/30] batch [52/96] time 0.321 (0.340) data 0.000 (0.011) loss 1.5949 (1.1639) lr 2.0611e-03 eta 0:04:36
epoch [22/30] batch [54/96] time 0.329 (0.340) data 0.000 (0.011) loss 1.3424 (1.1647) lr 2.0611e-03 eta 0:04:35
epoch [22/30] batch [56/96] time 0.322 (0.339) data 0.000 (0.010) loss 1.2785 (1.1653) lr 2.0611e-03 eta 0:04:34
epoch [22/30] batch [58/96] time 0.331 (0.339) data 0.000 (0.010) loss 1.1603 (1.1666) lr 2.0611e-03 eta 0:04:33
epoch [22/30] batch [60/96] time 0.343 (0.339) data 0.000 (0.010) loss 0.9237 (1.1713) lr 2.0611e-03 eta 0:04:32
epoch [22/30] batch [62/96] time 0.331 (0.339) data 0.000 (0.010) loss 1.6769 (1.1865) lr 2.0611e-03 eta 0:04:31
epoch [22/30] batch [64/96] time 0.323 (0.339) data 0.000 (0.009) loss 0.9202 (1.1802) lr 2.0611e-03 eta 0:04:30
epoch [22/30] batch [66/96] time 0.325 (0.338) data 0.000 (0.009) loss 1.0038 (1.1872) lr 2.0611e-03 eta 0:04:29
epoch [22/30] batch [68/96] time 0.320 (0.338) data 0.000 (0.009) loss 0.9380 (1.1792) lr 2.0611e-03 eta 0:04:28
epoch [22/30] batch [70/96] time 0.338 (0.338) data 0.000 (0.008) loss 1.2486 (1.1788) lr 2.0611e-03 eta 0:04:28
epoch [22/30] batch [72/96] time 0.323 (0.337) data 0.000 (0.008) loss 1.0636 (1.1727) lr 2.0611e-03 eta 0:04:27
epoch [22/30] batch [74/96] time 0.306 (0.336) data 0.000 (0.008) loss 1.0712 (1.1749) lr 2.0611e-03 eta 0:04:25
epoch [22/30] batch [76/96] time 0.316 (0.336) data 0.000 (0.008) loss 0.8610 (1.1737) lr 2.0611e-03 eta 0:04:24
epoch [22/30] batch [78/96] time 0.322 (0.335) data 0.000 (0.008) loss 1.7822 (1.1792) lr 2.0611e-03 eta 0:04:23
epoch [22/30] batch [80/96] time 0.310 (0.335) data 0.000 (0.007) loss 1.3745 (1.1767) lr 2.0611e-03 eta 0:04:22
epoch [22/30] batch [82/96] time 0.302 (0.334) data 0.000 (0.007) loss 1.6248 (1.1775) lr 2.0611e-03 eta 0:04:20
epoch [22/30] batch [84/96] time 0.302 (0.333) data 0.000 (0.007) loss 1.8611 (1.1907) lr 2.0611e-03 eta 0:04:19
epoch [22/30] batch [86/96] time 0.309 (0.332) data 0.000 (0.007) loss 1.0173 (1.1908) lr 2.0611e-03 eta 0:04:18
epoch [22/30] batch [88/96] time 0.299 (0.332) data 0.000 (0.007) loss 0.9251 (1.1904) lr 2.0611e-03 eta 0:04:17
epoch [22/30] batch [90/96] time 0.300 (0.331) data 0.000 (0.007) loss 1.0642 (1.1860) lr 2.0611e-03 eta 0:04:16
epoch [22/30] batch [92/96] time 0.304 (0.331) data 0.000 (0.006) loss 1.8386 (1.1959) lr 2.0611e-03 eta 0:04:15
epoch [22/30] batch [94/96] time 0.306 (0.330) data 0.000 (0.006) loss 1.1010 (1.1907) lr 2.0611e-03 eta 0:04:14
epoch [22/30] batch [96/96] time 0.304 (0.329) data 0.000 (0.006) loss 0.8856 (1.1886) lr 1.6543e-03 eta 0:04:13
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.82s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.33s/it]100%|██████████| 3/3 [00:03<00:00,  1.18it/s]100%|██████████| 3/3 [00:03<00:00,  1.16s/it]=> result
* total: 576
* correct: 436
* accuracy: 75.7%
* error: 24.3%
* macro_f1: 75.6%

epoch [23/30] batch [2/96] time 0.305 (0.637) data 0.000 (0.282) loss 0.9609 (0.9218) lr 1.6543e-03 eta 0:08:08
epoch [23/30] batch [4/96] time 0.323 (0.476) data 0.000 (0.141) loss 1.1863 (0.9972) lr 1.6543e-03 eta 0:06:03
epoch [23/30] batch [6/96] time 0.323 (0.426) data 0.000 (0.094) loss 1.1731 (1.2856) lr 1.6543e-03 eta 0:05:24
epoch [23/30] batch [8/96] time 0.323 (0.400) data 0.000 (0.071) loss 0.8505 (1.2228) lr 1.6543e-03 eta 0:05:04
epoch [23/30] batch [10/96] time 0.324 (0.386) data 0.000 (0.057) loss 1.3390 (1.2330) lr 1.6543e-03 eta 0:04:52
epoch [23/30] batch [12/96] time 0.321 (0.375) data 0.000 (0.047) loss 1.0270 (1.2230) lr 1.6543e-03 eta 0:04:43
epoch [23/30] batch [14/96] time 0.332 (0.369) data 0.000 (0.041) loss 1.0798 (1.2090) lr 1.6543e-03 eta 0:04:37
epoch [23/30] batch [16/96] time 0.327 (0.363) data 0.000 (0.035) loss 2.1888 (1.2380) lr 1.6543e-03 eta 0:04:33
epoch [23/30] batch [18/96] time 0.332 (0.360) data 0.000 (0.032) loss 1.7266 (1.2480) lr 1.6543e-03 eta 0:04:29
epoch [23/30] batch [20/96] time 0.332 (0.357) data 0.000 (0.028) loss 0.9510 (1.2448) lr 1.6543e-03 eta 0:04:26
epoch [23/30] batch [22/96] time 0.329 (0.354) data 0.000 (0.026) loss 1.6101 (1.2499) lr 1.6543e-03 eta 0:04:23
epoch [23/30] batch [24/96] time 0.323 (0.351) data 0.000 (0.024) loss 1.2044 (1.2465) lr 1.6543e-03 eta 0:04:21
epoch [23/30] batch [26/96] time 0.325 (0.349) data 0.000 (0.022) loss 1.5102 (1.2759) lr 1.6543e-03 eta 0:04:18
epoch [23/30] batch [28/96] time 0.321 (0.347) data 0.000 (0.020) loss 1.3296 (1.2589) lr 1.6543e-03 eta 0:04:16
epoch [23/30] batch [30/96] time 0.322 (0.345) data 0.000 (0.019) loss 1.5328 (1.2833) lr 1.6543e-03 eta 0:04:14
epoch [23/30] batch [32/96] time 0.327 (0.344) data 0.000 (0.018) loss 0.8917 (1.2612) lr 1.6543e-03 eta 0:04:13
epoch [23/30] batch [34/96] time 0.327 (0.343) data 0.000 (0.017) loss 0.9580 (1.2414) lr 1.6543e-03 eta 0:04:11
epoch [23/30] batch [36/96] time 0.425 (0.344) data 0.000 (0.016) loss 0.9978 (1.2302) lr 1.6543e-03 eta 0:04:12
epoch [23/30] batch [38/96] time 0.331 (0.343) data 0.000 (0.015) loss 1.8342 (1.2419) lr 1.6543e-03 eta 0:04:10
epoch [23/30] batch [40/96] time 0.320 (0.343) data 0.000 (0.014) loss 0.9239 (1.2476) lr 1.6543e-03 eta 0:04:09
epoch [23/30] batch [42/96] time 0.332 (0.342) data 0.000 (0.014) loss 0.7299 (1.2232) lr 1.6543e-03 eta 0:04:08
epoch [23/30] batch [44/96] time 0.328 (0.341) data 0.000 (0.013) loss 1.1013 (1.2219) lr 1.6543e-03 eta 0:04:06
epoch [23/30] batch [46/96] time 0.330 (0.341) data 0.000 (0.013) loss 1.8674 (1.2376) lr 1.6543e-03 eta 0:04:05
epoch [23/30] batch [48/96] time 0.322 (0.340) data 0.000 (0.012) loss 1.4428 (1.2434) lr 1.6543e-03 eta 0:04:04
epoch [23/30] batch [50/96] time 0.329 (0.339) data 0.000 (0.012) loss 0.7260 (1.2284) lr 1.6543e-03 eta 0:04:03
epoch [23/30] batch [52/96] time 0.328 (0.339) data 0.001 (0.011) loss 1.2629 (1.2214) lr 1.6543e-03 eta 0:04:02
epoch [23/30] batch [54/96] time 0.319 (0.338) data 0.000 (0.011) loss 0.9028 (1.2121) lr 1.6543e-03 eta 0:04:01
epoch [23/30] batch [56/96] time 0.317 (0.338) data 0.000 (0.010) loss 0.9778 (1.2112) lr 1.6543e-03 eta 0:04:00
epoch [23/30] batch [58/96] time 0.326 (0.337) data 0.000 (0.010) loss 1.1450 (1.2118) lr 1.6543e-03 eta 0:03:59
epoch [23/30] batch [60/96] time 0.315 (0.337) data 0.000 (0.010) loss 1.0844 (1.2040) lr 1.6543e-03 eta 0:03:58
epoch [23/30] batch [62/96] time 0.326 (0.336) data 0.000 (0.009) loss 0.9054 (1.1938) lr 1.6543e-03 eta 0:03:57
epoch [23/30] batch [64/96] time 0.334 (0.336) data 0.000 (0.009) loss 1.5932 (1.2017) lr 1.6543e-03 eta 0:03:56
epoch [23/30] batch [66/96] time 0.331 (0.336) data 0.000 (0.009) loss 0.9020 (1.1930) lr 1.6543e-03 eta 0:03:55
epoch [23/30] batch [68/96] time 0.325 (0.335) data 0.000 (0.009) loss 1.8234 (1.1983) lr 1.6543e-03 eta 0:03:54
epoch [23/30] batch [70/96] time 0.343 (0.335) data 0.000 (0.008) loss 0.9287 (1.1910) lr 1.6543e-03 eta 0:03:54
epoch [23/30] batch [72/96] time 0.321 (0.335) data 0.000 (0.008) loss 1.0074 (1.1858) lr 1.6543e-03 eta 0:03:53
epoch [23/30] batch [74/96] time 0.308 (0.334) data 0.000 (0.008) loss 0.9886 (1.1794) lr 1.6543e-03 eta 0:03:52
epoch [23/30] batch [76/96] time 0.302 (0.334) data 0.000 (0.008) loss 0.8479 (1.1784) lr 1.6543e-03 eta 0:03:50
epoch [23/30] batch [78/96] time 0.315 (0.333) data 0.000 (0.008) loss 0.8488 (1.1703) lr 1.6543e-03 eta 0:03:49
epoch [23/30] batch [80/96] time 0.309 (0.332) data 0.000 (0.007) loss 1.0968 (1.1720) lr 1.6543e-03 eta 0:03:48
epoch [23/30] batch [82/96] time 0.305 (0.332) data 0.000 (0.007) loss 0.8845 (1.1675) lr 1.6543e-03 eta 0:03:47
epoch [23/30] batch [84/96] time 0.306 (0.331) data 0.000 (0.007) loss 1.2786 (1.1672) lr 1.6543e-03 eta 0:03:46
epoch [23/30] batch [86/96] time 0.301 (0.331) data 0.000 (0.007) loss 1.8992 (1.1707) lr 1.6543e-03 eta 0:03:45
epoch [23/30] batch [88/96] time 0.311 (0.330) data 0.000 (0.007) loss 0.9632 (1.1671) lr 1.6543e-03 eta 0:03:44
epoch [23/30] batch [90/96] time 0.302 (0.329) data 0.000 (0.007) loss 2.1218 (1.1736) lr 1.6543e-03 eta 0:03:43
epoch [23/30] batch [92/96] time 0.305 (0.329) data 0.000 (0.006) loss 1.5122 (1.1770) lr 1.6543e-03 eta 0:03:42
epoch [23/30] batch [94/96] time 0.309 (0.328) data 0.000 (0.006) loss 0.9180 (1.1750) lr 1.6543e-03 eta 0:03:41
epoch [23/30] batch [96/96] time 0.300 (0.328) data 0.000 (0.006) loss 1.1399 (1.1765) lr 1.2843e-03 eta 0:03:40
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.85s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 440
* accuracy: 76.4%
* error: 23.6%
* macro_f1: 76.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [24/30] batch [2/96] time 0.337 (0.656) data 0.000 (0.274) loss 0.6958 (0.7188) lr 1.2843e-03 eta 0:07:19
epoch [24/30] batch [4/96] time 0.330 (0.494) data 0.000 (0.137) loss 1.8862 (1.2624) lr 1.2843e-03 eta 0:05:29
epoch [24/30] batch [6/96] time 0.326 (0.439) data 0.001 (0.091) loss 1.2874 (1.2110) lr 1.2843e-03 eta 0:04:52
epoch [24/30] batch [8/96] time 0.351 (0.415) data 0.000 (0.069) loss 1.6724 (1.3060) lr 1.2843e-03 eta 0:04:35
epoch [24/30] batch [10/96] time 0.350 (0.401) data 0.000 (0.055) loss 1.4896 (1.2717) lr 1.2843e-03 eta 0:04:25
epoch [24/30] batch [12/96] time 0.352 (0.393) data 0.000 (0.046) loss 0.8633 (1.2106) lr 1.2843e-03 eta 0:04:19
epoch [24/30] batch [14/96] time 0.335 (0.386) data 0.000 (0.039) loss 1.6691 (1.2103) lr 1.2843e-03 eta 0:04:14
epoch [24/30] batch [16/96] time 0.346 (0.380) data 0.000 (0.035) loss 1.2373 (1.2028) lr 1.2843e-03 eta 0:04:09
epoch [24/30] batch [18/96] time 0.318 (0.374) data 0.000 (0.031) loss 1.1324 (1.1978) lr 1.2843e-03 eta 0:04:04
epoch [24/30] batch [20/96] time 0.328 (0.370) data 0.000 (0.028) loss 0.9576 (1.1970) lr 1.2843e-03 eta 0:04:00
epoch [24/30] batch [22/96] time 0.327 (0.366) data 0.000 (0.025) loss 1.1394 (1.1828) lr 1.2843e-03 eta 0:03:57
epoch [24/30] batch [24/96] time 0.334 (0.363) data 0.000 (0.023) loss 0.8317 (1.1513) lr 1.2843e-03 eta 0:03:55
epoch [24/30] batch [26/96] time 0.331 (0.360) data 0.000 (0.021) loss 1.2607 (1.1470) lr 1.2843e-03 eta 0:03:52
epoch [24/30] batch [28/96] time 0.333 (0.358) data 0.000 (0.020) loss 1.3283 (1.1652) lr 1.2843e-03 eta 0:03:50
epoch [24/30] batch [30/96] time 0.321 (0.355) data 0.000 (0.019) loss 0.7748 (1.1673) lr 1.2843e-03 eta 0:03:48
epoch [24/30] batch [32/96] time 0.342 (0.354) data 0.000 (0.017) loss 0.8520 (1.1452) lr 1.2843e-03 eta 0:03:46
epoch [24/30] batch [34/96] time 0.343 (0.353) data 0.000 (0.016) loss 1.5555 (1.1822) lr 1.2843e-03 eta 0:03:45
epoch [24/30] batch [36/96] time 0.445 (0.355) data 0.000 (0.016) loss 1.3103 (1.1764) lr 1.2843e-03 eta 0:03:45
epoch [24/30] batch [38/96] time 0.330 (0.354) data 0.000 (0.015) loss 0.7350 (1.1665) lr 1.2843e-03 eta 0:03:44
epoch [24/30] batch [40/96] time 0.327 (0.353) data 0.000 (0.014) loss 0.8150 (1.1541) lr 1.2843e-03 eta 0:03:42
epoch [24/30] batch [42/96] time 0.325 (0.351) data 0.000 (0.013) loss 0.9622 (1.1662) lr 1.2843e-03 eta 0:03:41
epoch [24/30] batch [44/96] time 0.321 (0.350) data 0.000 (0.013) loss 0.8773 (1.1590) lr 1.2843e-03 eta 0:03:40
epoch [24/30] batch [46/96] time 0.327 (0.350) data 0.000 (0.012) loss 1.2256 (1.1549) lr 1.2843e-03 eta 0:03:38
epoch [24/30] batch [48/96] time 0.333 (0.349) data 0.000 (0.012) loss 1.4394 (1.1515) lr 1.2843e-03 eta 0:03:37
epoch [24/30] batch [50/96] time 0.329 (0.348) data 0.000 (0.011) loss 1.0353 (1.1612) lr 1.2843e-03 eta 0:03:36
epoch [24/30] batch [52/96] time 0.319 (0.347) data 0.000 (0.011) loss 1.5402 (1.1673) lr 1.2843e-03 eta 0:03:35
epoch [24/30] batch [54/96] time 0.327 (0.346) data 0.000 (0.010) loss 1.0980 (1.1853) lr 1.2843e-03 eta 0:03:33
epoch [24/30] batch [56/96] time 0.329 (0.346) data 0.000 (0.010) loss 0.9513 (1.1884) lr 1.2843e-03 eta 0:03:33
epoch [24/30] batch [58/96] time 0.335 (0.345) data 0.000 (0.010) loss 0.8686 (1.1848) lr 1.2843e-03 eta 0:03:32
epoch [24/30] batch [60/96] time 0.334 (0.345) data 0.000 (0.009) loss 1.7244 (1.2089) lr 1.2843e-03 eta 0:03:31
epoch [24/30] batch [62/96] time 0.326 (0.344) data 0.000 (0.009) loss 2.7172 (1.2326) lr 1.2843e-03 eta 0:03:29
epoch [24/30] batch [64/96] time 0.335 (0.344) data 0.000 (0.009) loss 0.9069 (1.2229) lr 1.2843e-03 eta 0:03:29
epoch [24/30] batch [66/96] time 0.330 (0.343) data 0.000 (0.009) loss 0.9121 (1.2152) lr 1.2843e-03 eta 0:03:28
epoch [24/30] batch [68/96] time 0.328 (0.343) data 0.000 (0.008) loss 0.9098 (1.2140) lr 1.2843e-03 eta 0:03:27
epoch [24/30] batch [70/96] time 0.326 (0.342) data 0.001 (0.008) loss 1.0419 (1.2049) lr 1.2843e-03 eta 0:03:26
epoch [24/30] batch [72/96] time 0.326 (0.342) data 0.000 (0.008) loss 1.3393 (1.2062) lr 1.2843e-03 eta 0:03:25
epoch [24/30] batch [74/96] time 0.302 (0.341) data 0.000 (0.008) loss 0.8102 (1.2047) lr 1.2843e-03 eta 0:03:23
epoch [24/30] batch [76/96] time 0.309 (0.340) data 0.000 (0.008) loss 1.4283 (1.2041) lr 1.2843e-03 eta 0:03:22
epoch [24/30] batch [78/96] time 0.307 (0.339) data 0.000 (0.007) loss 0.7836 (1.1945) lr 1.2843e-03 eta 0:03:21
epoch [24/30] batch [80/96] time 0.313 (0.339) data 0.000 (0.007) loss 0.8762 (1.1858) lr 1.2843e-03 eta 0:03:20
epoch [24/30] batch [82/96] time 0.306 (0.338) data 0.000 (0.007) loss 0.8925 (1.1857) lr 1.2843e-03 eta 0:03:19
epoch [24/30] batch [84/96] time 0.313 (0.337) data 0.000 (0.007) loss 1.0094 (1.1800) lr 1.2843e-03 eta 0:03:18
epoch [24/30] batch [86/96] time 0.303 (0.336) data 0.000 (0.007) loss 0.8286 (1.1854) lr 1.2843e-03 eta 0:03:17
epoch [24/30] batch [88/96] time 0.310 (0.336) data 0.000 (0.007) loss 2.0301 (1.1950) lr 1.2843e-03 eta 0:03:16
epoch [24/30] batch [90/96] time 0.308 (0.335) data 0.000 (0.006) loss 1.0333 (1.1919) lr 1.2843e-03 eta 0:03:15
epoch [24/30] batch [92/96] time 0.308 (0.335) data 0.000 (0.006) loss 0.8461 (1.1867) lr 1.2843e-03 eta 0:03:14
epoch [24/30] batch [94/96] time 0.304 (0.334) data 0.000 (0.006) loss 1.4191 (1.1883) lr 1.2843e-03 eta 0:03:13
epoch [24/30] batch [96/96] time 0.306 (0.333) data 0.000 (0.006) loss 0.9401 (1.1861) lr 9.5492e-04 eta 0:03:12
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.89s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.37s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.20s/it]=> result
* total: 576
* correct: 439
* accuracy: 76.2%
* error: 23.8%
* macro_f1: 76.2%

epoch [25/30] batch [2/96] time 0.342 (0.649) data 0.000 (0.274) loss 1.2007 (2.0920) lr 9.5492e-04 eta 0:06:12
epoch [25/30] batch [4/96] time 0.337 (0.493) data 0.000 (0.137) loss 0.7182 (1.4273) lr 9.5492e-04 eta 0:04:41
epoch [25/30] batch [6/96] time 0.323 (0.437) data 0.000 (0.091) loss 1.3174 (1.3039) lr 9.5492e-04 eta 0:04:08
epoch [25/30] batch [8/96] time 0.331 (0.411) data 0.000 (0.069) loss 0.7878 (1.2527) lr 9.5492e-04 eta 0:03:53
epoch [25/30] batch [10/96] time 0.334 (0.394) data 0.000 (0.055) loss 1.1669 (1.2552) lr 9.5492e-04 eta 0:03:43
epoch [25/30] batch [12/96] time 0.316 (0.383) data 0.000 (0.046) loss 0.9845 (1.1937) lr 9.5492e-04 eta 0:03:36
epoch [25/30] batch [14/96] time 0.347 (0.378) data 0.000 (0.039) loss 1.1046 (1.1854) lr 9.5492e-04 eta 0:03:32
epoch [25/30] batch [16/96] time 0.319 (0.371) data 0.000 (0.034) loss 1.7521 (1.1984) lr 9.5492e-04 eta 0:03:27
epoch [25/30] batch [18/96] time 0.320 (0.366) data 0.000 (0.031) loss 0.9044 (1.1724) lr 9.5492e-04 eta 0:03:24
epoch [25/30] batch [20/96] time 0.343 (0.363) data 0.000 (0.028) loss 0.8135 (1.1417) lr 9.5492e-04 eta 0:03:21
epoch [25/30] batch [22/96] time 0.331 (0.360) data 0.000 (0.025) loss 0.7138 (1.1140) lr 9.5492e-04 eta 0:03:19
epoch [25/30] batch [24/96] time 0.341 (0.358) data 0.000 (0.023) loss 0.9207 (1.1101) lr 9.5492e-04 eta 0:03:17
epoch [25/30] batch [26/96] time 0.330 (0.356) data 0.000 (0.021) loss 1.0324 (1.1202) lr 9.5492e-04 eta 0:03:15
epoch [25/30] batch [28/96] time 0.326 (0.354) data 0.000 (0.020) loss 0.9814 (1.1299) lr 9.5492e-04 eta 0:03:13
epoch [25/30] batch [30/96] time 0.348 (0.353) data 0.000 (0.019) loss 1.1266 (1.1269) lr 9.5492e-04 eta 0:03:12
epoch [25/30] batch [32/96] time 0.318 (0.351) data 0.000 (0.017) loss 1.1473 (1.1279) lr 9.5492e-04 eta 0:03:11
epoch [25/30] batch [34/96] time 0.338 (0.350) data 0.000 (0.016) loss 0.7720 (1.1176) lr 9.5492e-04 eta 0:03:09
epoch [25/30] batch [36/96] time 0.446 (0.352) data 0.001 (0.016) loss 0.8278 (1.1235) lr 9.5492e-04 eta 0:03:10
epoch [25/30] batch [38/96] time 0.326 (0.351) data 0.000 (0.015) loss 1.1863 (1.1270) lr 9.5492e-04 eta 0:03:08
epoch [25/30] batch [40/96] time 0.330 (0.350) data 0.000 (0.014) loss 1.2670 (1.1249) lr 9.5492e-04 eta 0:03:07
epoch [25/30] batch [42/96] time 0.333 (0.349) data 0.000 (0.013) loss 1.4903 (1.1247) lr 9.5492e-04 eta 0:03:06
epoch [25/30] batch [44/96] time 0.336 (0.348) data 0.000 (0.013) loss 1.5110 (1.1296) lr 9.5492e-04 eta 0:03:05
epoch [25/30] batch [46/96] time 0.328 (0.348) data 0.000 (0.012) loss 1.3786 (1.1324) lr 9.5492e-04 eta 0:03:04
epoch [25/30] batch [48/96] time 0.335 (0.347) data 0.000 (0.012) loss 1.5490 (1.1435) lr 9.5492e-04 eta 0:03:03
epoch [25/30] batch [50/96] time 0.343 (0.347) data 0.000 (0.011) loss 0.8373 (1.1384) lr 9.5492e-04 eta 0:03:02
epoch [25/30] batch [52/96] time 0.341 (0.347) data 0.000 (0.011) loss 0.6772 (1.1222) lr 9.5492e-04 eta 0:03:01
epoch [25/30] batch [54/96] time 0.346 (0.347) data 0.000 (0.010) loss 1.1913 (1.1246) lr 9.5492e-04 eta 0:03:01
epoch [25/30] batch [56/96] time 0.342 (0.347) data 0.000 (0.010) loss 1.8622 (1.1395) lr 9.5492e-04 eta 0:03:00
epoch [25/30] batch [58/96] time 0.323 (0.346) data 0.000 (0.010) loss 1.4060 (1.1660) lr 9.5492e-04 eta 0:02:59
epoch [25/30] batch [60/96] time 0.330 (0.346) data 0.000 (0.009) loss 0.9805 (1.1678) lr 9.5492e-04 eta 0:02:58
epoch [25/30] batch [62/96] time 0.345 (0.346) data 0.000 (0.009) loss 1.0429 (1.1836) lr 9.5492e-04 eta 0:02:57
epoch [25/30] batch [64/96] time 0.330 (0.345) data 0.000 (0.009) loss 1.1272 (1.1810) lr 9.5492e-04 eta 0:02:56
epoch [25/30] batch [66/96] time 0.332 (0.345) data 0.000 (0.009) loss 1.7016 (1.1831) lr 9.5492e-04 eta 0:02:55
epoch [25/30] batch [68/96] time 0.320 (0.344) data 0.000 (0.008) loss 1.1279 (1.1787) lr 9.5492e-04 eta 0:02:54
epoch [25/30] batch [70/96] time 0.334 (0.344) data 0.000 (0.008) loss 0.8896 (1.1716) lr 9.5492e-04 eta 0:02:53
epoch [25/30] batch [72/96] time 0.333 (0.343) data 0.000 (0.008) loss 1.2916 (1.1708) lr 9.5492e-04 eta 0:02:53
epoch [25/30] batch [74/96] time 0.307 (0.342) data 0.000 (0.008) loss 0.7480 (1.1622) lr 9.5492e-04 eta 0:02:51
epoch [25/30] batch [76/96] time 0.307 (0.341) data 0.000 (0.008) loss 0.9555 (1.1699) lr 9.5492e-04 eta 0:02:50
epoch [25/30] batch [78/96] time 0.305 (0.340) data 0.000 (0.007) loss 0.6950 (1.1626) lr 9.5492e-04 eta 0:02:49
epoch [25/30] batch [80/96] time 0.312 (0.340) data 0.000 (0.007) loss 0.8115 (1.1568) lr 9.5492e-04 eta 0:02:48
epoch [25/30] batch [82/96] time 0.307 (0.339) data 0.000 (0.007) loss 0.9839 (1.1604) lr 9.5492e-04 eta 0:02:47
epoch [25/30] batch [84/96] time 0.314 (0.338) data 0.000 (0.007) loss 1.0548 (1.1554) lr 9.5492e-04 eta 0:02:46
epoch [25/30] batch [86/96] time 0.311 (0.338) data 0.000 (0.007) loss 0.8389 (1.1525) lr 9.5492e-04 eta 0:02:45
epoch [25/30] batch [88/96] time 0.313 (0.337) data 0.000 (0.007) loss 0.8906 (1.1511) lr 9.5492e-04 eta 0:02:44
epoch [25/30] batch [90/96] time 0.317 (0.336) data 0.000 (0.006) loss 1.0806 (1.1616) lr 9.5492e-04 eta 0:02:43
epoch [25/30] batch [92/96] time 0.306 (0.336) data 0.000 (0.006) loss 1.2338 (1.1693) lr 9.5492e-04 eta 0:02:42
epoch [25/30] batch [94/96] time 0.314 (0.335) data 0.000 (0.006) loss 0.8272 (1.1693) lr 9.5492e-04 eta 0:02:41
epoch [25/30] batch [96/96] time 0.312 (0.335) data 0.000 (0.006) loss 1.3335 (1.1702) lr 6.6987e-04 eta 0:02:40
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.89s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.19s/it]=> result
* total: 576
* correct: 437
* accuracy: 75.9%
* error: 24.1%
* macro_f1: 75.9%

epoch [26/30] batch [2/96] time 0.332 (0.671) data 0.001 (0.287) loss 1.5280 (1.3776) lr 6.6987e-04 eta 0:05:20
epoch [26/30] batch [4/96] time 0.318 (0.493) data 0.000 (0.144) loss 0.8958 (1.1098) lr 6.6987e-04 eta 0:03:54
epoch [26/30] batch [6/96] time 0.323 (0.437) data 0.000 (0.096) loss 0.8163 (1.0112) lr 6.6987e-04 eta 0:03:27
epoch [26/30] batch [8/96] time 0.333 (0.410) data 0.000 (0.072) loss 1.3842 (1.2349) lr 6.6987e-04 eta 0:03:13
epoch [26/30] batch [10/96] time 0.320 (0.394) data 0.000 (0.058) loss 1.1134 (1.2561) lr 6.6987e-04 eta 0:03:05
epoch [26/30] batch [12/96] time 0.319 (0.384) data 0.001 (0.048) loss 0.8183 (1.1830) lr 6.6987e-04 eta 0:02:59
epoch [26/30] batch [14/96] time 0.329 (0.376) data 0.000 (0.041) loss 0.9612 (1.1505) lr 6.6987e-04 eta 0:02:55
epoch [26/30] batch [16/96] time 0.338 (0.370) data 0.000 (0.036) loss 1.1325 (1.1665) lr 6.6987e-04 eta 0:02:51
epoch [26/30] batch [18/96] time 0.322 (0.366) data 0.000 (0.032) loss 0.7498 (1.1313) lr 6.6987e-04 eta 0:02:49
epoch [26/30] batch [20/96] time 0.337 (0.363) data 0.000 (0.029) loss 2.2938 (1.1807) lr 6.6987e-04 eta 0:02:46
epoch [26/30] batch [22/96] time 0.322 (0.360) data 0.000 (0.026) loss 0.9997 (1.1630) lr 6.6987e-04 eta 0:02:44
epoch [26/30] batch [24/96] time 0.337 (0.358) data 0.000 (0.024) loss 1.3377 (1.1653) lr 6.6987e-04 eta 0:02:43
epoch [26/30] batch [26/96] time 0.331 (0.356) data 0.000 (0.022) loss 0.9603 (1.1430) lr 6.6987e-04 eta 0:02:41
epoch [26/30] batch [28/96] time 0.321 (0.353) data 0.000 (0.021) loss 2.5168 (1.1997) lr 6.6987e-04 eta 0:02:39
epoch [26/30] batch [30/96] time 0.325 (0.351) data 0.000 (0.019) loss 1.0664 (1.1980) lr 6.6987e-04 eta 0:02:38
epoch [26/30] batch [32/96] time 0.320 (0.349) data 0.000 (0.018) loss 1.1931 (1.1874) lr 6.6987e-04 eta 0:02:36
epoch [26/30] batch [34/96] time 0.324 (0.348) data 0.000 (0.017) loss 0.9712 (1.1766) lr 6.6987e-04 eta 0:02:35
epoch [26/30] batch [36/96] time 0.441 (0.351) data 0.000 (0.016) loss 1.8981 (1.2029) lr 6.6987e-04 eta 0:02:35
epoch [26/30] batch [38/96] time 0.324 (0.350) data 0.000 (0.015) loss 0.6903 (1.1778) lr 6.6987e-04 eta 0:02:34
epoch [26/30] batch [40/96] time 0.332 (0.349) data 0.000 (0.015) loss 1.2766 (1.1850) lr 6.6987e-04 eta 0:02:33
epoch [26/30] batch [42/96] time 0.336 (0.348) data 0.000 (0.014) loss 1.0165 (1.1692) lr 6.6987e-04 eta 0:02:32
epoch [26/30] batch [44/96] time 0.359 (0.348) data 0.000 (0.013) loss 1.2022 (1.1647) lr 6.6987e-04 eta 0:02:31
epoch [26/30] batch [46/96] time 0.345 (0.348) data 0.000 (0.013) loss 1.3094 (1.1689) lr 6.6987e-04 eta 0:02:30
epoch [26/30] batch [48/96] time 0.340 (0.347) data 0.000 (0.012) loss 1.2178 (1.1759) lr 6.6987e-04 eta 0:02:30
epoch [26/30] batch [50/96] time 0.333 (0.347) data 0.000 (0.012) loss 1.3573 (1.1851) lr 6.6987e-04 eta 0:02:29
epoch [26/30] batch [52/96] time 0.336 (0.346) data 0.000 (0.011) loss 1.3204 (1.1999) lr 6.6987e-04 eta 0:02:28
epoch [26/30] batch [54/96] time 0.364 (0.346) data 0.000 (0.011) loss 0.8145 (1.1962) lr 6.6987e-04 eta 0:02:27
epoch [26/30] batch [56/96] time 0.320 (0.346) data 0.000 (0.011) loss 0.9143 (1.1908) lr 6.6987e-04 eta 0:02:26
epoch [26/30] batch [58/96] time 0.315 (0.345) data 0.000 (0.010) loss 0.9808 (1.1836) lr 6.6987e-04 eta 0:02:25
epoch [26/30] batch [60/96] time 0.320 (0.344) data 0.000 (0.010) loss 1.0660 (1.1830) lr 6.6987e-04 eta 0:02:24
epoch [26/30] batch [62/96] time 0.326 (0.343) data 0.000 (0.010) loss 1.1267 (1.1790) lr 6.6987e-04 eta 0:02:23
epoch [26/30] batch [64/96] time 0.330 (0.343) data 0.000 (0.009) loss 0.6976 (1.1658) lr 6.6987e-04 eta 0:02:22
epoch [26/30] batch [66/96] time 0.338 (0.343) data 0.000 (0.009) loss 1.1698 (1.1606) lr 6.6987e-04 eta 0:02:21
epoch [26/30] batch [68/96] time 0.326 (0.342) data 0.000 (0.009) loss 0.8271 (1.1524) lr 6.6987e-04 eta 0:02:21
epoch [26/30] batch [70/96] time 0.324 (0.342) data 0.000 (0.008) loss 1.2422 (1.1629) lr 6.6987e-04 eta 0:02:20
epoch [26/30] batch [72/96] time 0.347 (0.342) data 0.001 (0.008) loss 1.2414 (1.1613) lr 6.6987e-04 eta 0:02:19
epoch [26/30] batch [74/96] time 0.312 (0.341) data 0.000 (0.008) loss 0.7029 (1.1521) lr 6.6987e-04 eta 0:02:18
epoch [26/30] batch [76/96] time 0.318 (0.341) data 0.000 (0.008) loss 0.8214 (1.1573) lr 6.6987e-04 eta 0:02:17
epoch [26/30] batch [78/96] time 0.313 (0.340) data 0.000 (0.008) loss 1.0925 (1.1595) lr 6.6987e-04 eta 0:02:16
epoch [26/30] batch [80/96] time 0.318 (0.339) data 0.000 (0.007) loss 0.8867 (1.1529) lr 6.6987e-04 eta 0:02:15
epoch [26/30] batch [82/96] time 0.319 (0.339) data 0.000 (0.007) loss 2.0121 (1.1614) lr 6.6987e-04 eta 0:02:14
epoch [26/30] batch [84/96] time 0.310 (0.338) data 0.000 (0.007) loss 1.1798 (1.1681) lr 6.6987e-04 eta 0:02:13
epoch [26/30] batch [86/96] time 0.318 (0.338) data 0.000 (0.007) loss 0.8009 (1.1596) lr 6.6987e-04 eta 0:02:13
epoch [26/30] batch [88/96] time 0.311 (0.337) data 0.000 (0.007) loss 0.9378 (1.1628) lr 6.6987e-04 eta 0:02:12
epoch [26/30] batch [90/96] time 0.316 (0.337) data 0.000 (0.007) loss 1.1784 (1.1617) lr 6.6987e-04 eta 0:02:11
epoch [26/30] batch [92/96] time 0.318 (0.336) data 0.000 (0.007) loss 0.7541 (1.1542) lr 6.6987e-04 eta 0:02:10
epoch [26/30] batch [94/96] time 0.314 (0.336) data 0.000 (0.006) loss 0.7894 (1.1500) lr 6.6987e-04 eta 0:02:09
epoch [26/30] batch [96/96] time 0.318 (0.335) data 0.000 (0.006) loss 1.3323 (1.1518) lr 4.3227e-04 eta 0:02:08
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.81s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.33s/it]100%|██████████| 3/3 [00:03<00:00,  1.18it/s]100%|██████████| 3/3 [00:03<00:00,  1.16s/it]=> result
* total: 576
* correct: 441
* accuracy: 76.6%
* error: 23.4%
* macro_f1: 76.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar

epoch [27/30] batch [2/96] time 0.308 (0.636) data 0.001 (0.273) loss 1.4144 (1.2408) lr 4.3227e-04 eta 0:04:03
epoch [27/30] batch [4/96] time 0.322 (0.480) data 0.000 (0.136) loss 0.8652 (1.1739) lr 4.3227e-04 eta 0:03:02
epoch [27/30] batch [6/96] time 0.327 (0.431) data 0.000 (0.091) loss 0.9332 (1.1628) lr 4.3227e-04 eta 0:02:42
epoch [27/30] batch [8/96] time 0.331 (0.406) data 0.000 (0.068) loss 0.8587 (1.1060) lr 4.3227e-04 eta 0:02:32
epoch [27/30] batch [10/96] time 0.322 (0.389) data 0.000 (0.055) loss 0.8173 (1.1626) lr 4.3227e-04 eta 0:02:25
epoch [27/30] batch [12/96] time 0.335 (0.379) data 0.000 (0.046) loss 1.4561 (1.1556) lr 4.3227e-04 eta 0:02:21
epoch [27/30] batch [14/96] time 0.319 (0.371) data 0.000 (0.039) loss 1.0200 (1.1445) lr 4.3227e-04 eta 0:02:17
epoch [27/30] batch [16/96] time 0.323 (0.366) data 0.000 (0.034) loss 1.8001 (1.1803) lr 4.3227e-04 eta 0:02:14
epoch [27/30] batch [18/96] time 0.324 (0.361) data 0.000 (0.031) loss 0.7229 (1.1751) lr 4.3227e-04 eta 0:02:12
epoch [27/30] batch [20/96] time 0.329 (0.358) data 0.000 (0.028) loss 1.1563 (1.1659) lr 4.3227e-04 eta 0:02:10
epoch [27/30] batch [22/96] time 0.316 (0.354) data 0.000 (0.025) loss 1.1630 (1.1501) lr 4.3227e-04 eta 0:02:08
epoch [27/30] batch [24/96] time 0.317 (0.351) data 0.000 (0.023) loss 1.2766 (1.1469) lr 4.3227e-04 eta 0:02:06
epoch [27/30] batch [26/96] time 0.323 (0.349) data 0.000 (0.021) loss 1.0573 (1.1869) lr 4.3227e-04 eta 0:02:04
epoch [27/30] batch [28/96] time 0.321 (0.347) data 0.000 (0.020) loss 1.2105 (1.1761) lr 4.3227e-04 eta 0:02:03
epoch [27/30] batch [30/96] time 0.332 (0.346) data 0.000 (0.018) loss 1.1901 (1.1788) lr 4.3227e-04 eta 0:02:02
epoch [27/30] batch [32/96] time 0.327 (0.345) data 0.000 (0.017) loss 1.1412 (1.1678) lr 4.3227e-04 eta 0:02:01
epoch [27/30] batch [34/96] time 0.334 (0.344) data 0.000 (0.016) loss 1.5782 (1.1860) lr 4.3227e-04 eta 0:02:00
epoch [27/30] batch [36/96] time 0.417 (0.346) data 0.000 (0.015) loss 0.7744 (1.1877) lr 4.3227e-04 eta 0:02:00
epoch [27/30] batch [38/96] time 0.334 (0.345) data 0.000 (0.015) loss 0.7726 (1.1684) lr 4.3227e-04 eta 0:01:59
epoch [27/30] batch [40/96] time 0.322 (0.344) data 0.000 (0.014) loss 1.5288 (1.1746) lr 4.3227e-04 eta 0:01:58
epoch [27/30] batch [42/96] time 0.331 (0.344) data 0.000 (0.013) loss 1.1065 (1.1693) lr 4.3227e-04 eta 0:01:57
epoch [27/30] batch [44/96] time 0.329 (0.343) data 0.000 (0.013) loss 1.2170 (1.1712) lr 4.3227e-04 eta 0:01:56
epoch [27/30] batch [46/96] time 0.322 (0.342) data 0.000 (0.012) loss 1.6095 (1.1815) lr 4.3227e-04 eta 0:01:55
epoch [27/30] batch [48/96] time 0.328 (0.341) data 0.000 (0.012) loss 1.0610 (1.1817) lr 4.3227e-04 eta 0:01:54
epoch [27/30] batch [50/96] time 0.320 (0.341) data 0.000 (0.011) loss 1.2699 (1.1802) lr 4.3227e-04 eta 0:01:53
epoch [27/30] batch [52/96] time 0.318 (0.340) data 0.000 (0.011) loss 0.9702 (1.1734) lr 4.3227e-04 eta 0:01:52
epoch [27/30] batch [54/96] time 0.318 (0.339) data 0.000 (0.010) loss 1.3227 (1.1722) lr 4.3227e-04 eta 0:01:51
epoch [27/30] batch [56/96] time 0.318 (0.338) data 0.000 (0.010) loss 1.2185 (1.1835) lr 4.3227e-04 eta 0:01:51
epoch [27/30] batch [58/96] time 0.339 (0.338) data 0.000 (0.010) loss 1.0621 (1.1734) lr 4.3227e-04 eta 0:01:50
epoch [27/30] batch [60/96] time 0.349 (0.338) data 0.000 (0.009) loss 1.0024 (1.1725) lr 4.3227e-04 eta 0:01:49
epoch [27/30] batch [62/96] time 0.347 (0.339) data 0.000 (0.009) loss 1.0063 (1.1692) lr 4.3227e-04 eta 0:01:49
epoch [27/30] batch [64/96] time 0.343 (0.338) data 0.000 (0.009) loss 0.9967 (1.1718) lr 4.3227e-04 eta 0:01:48
epoch [27/30] batch [66/96] time 0.317 (0.338) data 0.000 (0.009) loss 0.9819 (1.1696) lr 4.3227e-04 eta 0:01:47
epoch [27/30] batch [68/96] time 0.339 (0.338) data 0.000 (0.008) loss 1.0274 (1.1622) lr 4.3227e-04 eta 0:01:46
epoch [27/30] batch [70/96] time 0.338 (0.338) data 0.000 (0.008) loss 1.3362 (1.1621) lr 4.3227e-04 eta 0:01:46
epoch [27/30] batch [72/96] time 0.328 (0.337) data 0.000 (0.008) loss 0.9020 (1.1592) lr 4.3227e-04 eta 0:01:45
epoch [27/30] batch [74/96] time 0.303 (0.337) data 0.000 (0.008) loss 0.9618 (1.1624) lr 4.3227e-04 eta 0:01:44
epoch [27/30] batch [76/96] time 0.306 (0.336) data 0.000 (0.007) loss 1.1841 (1.1611) lr 4.3227e-04 eta 0:01:43
epoch [27/30] batch [78/96] time 0.306 (0.335) data 0.000 (0.007) loss 1.3733 (1.1597) lr 4.3227e-04 eta 0:01:42
epoch [27/30] batch [80/96] time 0.307 (0.334) data 0.000 (0.007) loss 2.2213 (1.1794) lr 4.3227e-04 eta 0:01:41
epoch [27/30] batch [82/96] time 0.315 (0.334) data 0.000 (0.007) loss 0.8721 (1.1810) lr 4.3227e-04 eta 0:01:40
epoch [27/30] batch [84/96] time 0.315 (0.333) data 0.000 (0.007) loss 0.7463 (1.1816) lr 4.3227e-04 eta 0:01:39
epoch [27/30] batch [86/96] time 0.308 (0.333) data 0.000 (0.007) loss 1.0839 (1.1799) lr 4.3227e-04 eta 0:01:39
epoch [27/30] batch [88/96] time 0.302 (0.332) data 0.000 (0.007) loss 0.8863 (1.1723) lr 4.3227e-04 eta 0:01:38
epoch [27/30] batch [90/96] time 0.308 (0.331) data 0.000 (0.006) loss 0.6922 (1.1631) lr 4.3227e-04 eta 0:01:37
epoch [27/30] batch [92/96] time 0.301 (0.331) data 0.000 (0.006) loss 0.8516 (1.1594) lr 4.3227e-04 eta 0:01:36
epoch [27/30] batch [94/96] time 0.304 (0.330) data 0.000 (0.006) loss 0.8881 (1.1532) lr 4.3227e-04 eta 0:01:35
epoch [27/30] batch [96/96] time 0.310 (0.330) data 0.000 (0.006) loss 0.8404 (1.1499) lr 2.4472e-04 eta 0:01:34
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.86s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 439
* accuracy: 76.2%
* error: 23.8%
* macro_f1: 76.2%

epoch [28/30] batch [2/96] time 0.308 (0.628) data 0.000 (0.263) loss 1.0004 (0.9185) lr 2.4472e-04 eta 0:02:59
epoch [28/30] batch [4/96] time 0.335 (0.476) data 0.000 (0.132) loss 0.8033 (0.9098) lr 2.4472e-04 eta 0:02:15
epoch [28/30] batch [6/96] time 0.319 (0.425) data 0.000 (0.088) loss 0.7380 (0.8913) lr 2.4472e-04 eta 0:01:59
epoch [28/30] batch [8/96] time 0.335 (0.404) data 0.000 (0.066) loss 1.0404 (1.0192) lr 2.4472e-04 eta 0:01:53
epoch [28/30] batch [10/96] time 0.322 (0.388) data 0.000 (0.053) loss 1.1379 (1.0174) lr 2.4472e-04 eta 0:01:47
epoch [28/30] batch [12/96] time 0.325 (0.379) data 0.000 (0.044) loss 0.9185 (1.0195) lr 2.4472e-04 eta 0:01:44
epoch [28/30] batch [14/96] time 0.322 (0.371) data 0.000 (0.038) loss 1.0192 (1.0459) lr 2.4472e-04 eta 0:01:41
epoch [28/30] batch [16/96] time 0.328 (0.366) data 0.001 (0.033) loss 1.1159 (1.0494) lr 2.4472e-04 eta 0:01:39
epoch [28/30] batch [18/96] time 0.315 (0.361) data 0.000 (0.030) loss 0.6784 (1.0653) lr 2.4472e-04 eta 0:01:37
epoch [28/30] batch [20/96] time 0.334 (0.358) data 0.000 (0.027) loss 1.2710 (1.0732) lr 2.4472e-04 eta 0:01:35
epoch [28/30] batch [22/96] time 0.330 (0.355) data 0.000 (0.024) loss 1.1815 (1.0735) lr 2.4472e-04 eta 0:01:34
epoch [28/30] batch [24/96] time 0.345 (0.353) data 0.000 (0.022) loss 0.7971 (1.0671) lr 2.4472e-04 eta 0:01:33
epoch [28/30] batch [26/96] time 0.323 (0.351) data 0.001 (0.021) loss 0.8220 (1.0656) lr 2.4472e-04 eta 0:01:32
epoch [28/30] batch [28/96] time 0.320 (0.350) data 0.000 (0.019) loss 1.0540 (1.0615) lr 2.4472e-04 eta 0:01:30
epoch [28/30] batch [30/96] time 0.328 (0.348) data 0.000 (0.018) loss 1.3602 (1.0777) lr 2.4472e-04 eta 0:01:29
epoch [28/30] batch [32/96] time 0.334 (0.347) data 0.000 (0.017) loss 1.0255 (1.0782) lr 2.4472e-04 eta 0:01:28
epoch [28/30] batch [34/96] time 0.329 (0.346) data 0.000 (0.016) loss 0.8248 (1.0726) lr 2.4472e-04 eta 0:01:27
epoch [28/30] batch [36/96] time 0.446 (0.348) data 0.000 (0.015) loss 1.0413 (1.0714) lr 2.4472e-04 eta 0:01:27
epoch [28/30] batch [38/96] time 0.335 (0.347) data 0.000 (0.014) loss 1.3891 (1.0699) lr 2.4472e-04 eta 0:01:26
epoch [28/30] batch [40/96] time 0.330 (0.346) data 0.000 (0.013) loss 1.6276 (1.0812) lr 2.4472e-04 eta 0:01:25
epoch [28/30] batch [42/96] time 0.322 (0.345) data 0.000 (0.013) loss 1.3734 (1.0831) lr 2.4472e-04 eta 0:01:24
epoch [28/30] batch [44/96] time 0.325 (0.343) data 0.000 (0.012) loss 1.2175 (1.0998) lr 2.4472e-04 eta 0:01:23
epoch [28/30] batch [46/96] time 0.324 (0.342) data 0.001 (0.012) loss 0.7138 (1.0912) lr 2.4472e-04 eta 0:01:22
epoch [28/30] batch [48/96] time 0.332 (0.342) data 0.000 (0.011) loss 1.1553 (1.0869) lr 2.4472e-04 eta 0:01:21
epoch [28/30] batch [50/96] time 0.345 (0.341) data 0.000 (0.011) loss 2.2401 (1.1113) lr 2.4472e-04 eta 0:01:21
epoch [28/30] batch [52/96] time 0.328 (0.341) data 0.000 (0.010) loss 1.6102 (1.1171) lr 2.4472e-04 eta 0:01:20
epoch [28/30] batch [54/96] time 0.324 (0.341) data 0.000 (0.010) loss 1.3443 (1.1191) lr 2.4472e-04 eta 0:01:19
epoch [28/30] batch [56/96] time 0.327 (0.340) data 0.000 (0.010) loss 1.0435 (1.1244) lr 2.4472e-04 eta 0:01:18
epoch [28/30] batch [58/96] time 0.323 (0.340) data 0.000 (0.009) loss 1.2963 (1.1207) lr 2.4472e-04 eta 0:01:18
epoch [28/30] batch [60/96] time 0.329 (0.339) data 0.000 (0.009) loss 1.3156 (1.1322) lr 2.4472e-04 eta 0:01:17
epoch [28/30] batch [62/96] time 0.335 (0.339) data 0.000 (0.009) loss 1.4273 (1.1346) lr 2.4472e-04 eta 0:01:16
epoch [28/30] batch [64/96] time 0.326 (0.339) data 0.000 (0.009) loss 0.9261 (1.1266) lr 2.4472e-04 eta 0:01:15
epoch [28/30] batch [66/96] time 0.325 (0.338) data 0.000 (0.008) loss 0.8091 (1.1202) lr 2.4472e-04 eta 0:01:15
epoch [28/30] batch [68/96] time 0.327 (0.338) data 0.000 (0.008) loss 0.7633 (1.1164) lr 2.4472e-04 eta 0:01:14
epoch [28/30] batch [70/96] time 0.331 (0.338) data 0.000 (0.008) loss 1.2232 (1.1224) lr 2.4472e-04 eta 0:01:13
epoch [28/30] batch [72/96] time 0.317 (0.338) data 0.000 (0.008) loss 0.8841 (1.1170) lr 2.4472e-04 eta 0:01:12
epoch [28/30] batch [74/96] time 0.309 (0.337) data 0.000 (0.007) loss 0.6848 (1.1073) lr 2.4472e-04 eta 0:01:12
epoch [28/30] batch [76/96] time 0.310 (0.336) data 0.000 (0.007) loss 1.1124 (1.1062) lr 2.4472e-04 eta 0:01:11
epoch [28/30] batch [78/96] time 0.306 (0.335) data 0.000 (0.007) loss 1.4808 (1.1178) lr 2.4472e-04 eta 0:01:10
epoch [28/30] batch [80/96] time 0.312 (0.335) data 0.000 (0.007) loss 1.2279 (1.1228) lr 2.4472e-04 eta 0:01:09
epoch [28/30] batch [82/96] time 0.305 (0.334) data 0.000 (0.007) loss 1.1583 (1.1276) lr 2.4472e-04 eta 0:01:08
epoch [28/30] batch [84/96] time 0.312 (0.334) data 0.000 (0.007) loss 1.8813 (1.1347) lr 2.4472e-04 eta 0:01:08
epoch [28/30] batch [86/96] time 0.306 (0.333) data 0.000 (0.006) loss 0.9405 (1.1376) lr 2.4472e-04 eta 0:01:07
epoch [28/30] batch [88/96] time 0.309 (0.332) data 0.000 (0.006) loss 1.0647 (1.1401) lr 2.4472e-04 eta 0:01:06
epoch [28/30] batch [90/96] time 0.312 (0.332) data 0.000 (0.006) loss 1.2468 (1.1388) lr 2.4472e-04 eta 0:01:05
epoch [28/30] batch [92/96] time 0.306 (0.331) data 0.000 (0.006) loss 1.3637 (1.1392) lr 2.4472e-04 eta 0:01:04
epoch [28/30] batch [94/96] time 0.308 (0.331) data 0.000 (0.006) loss 1.0374 (1.1346) lr 2.4472e-04 eta 0:01:04
epoch [28/30] batch [96/96] time 0.304 (0.330) data 0.000 (0.006) loss 1.1379 (1.1367) lr 1.0926e-04 eta 0:01:03
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.87s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 441
* accuracy: 76.6%
* error: 23.4%
* macro_f1: 76.5%

epoch [29/30] batch [2/96] time 0.329 (0.644) data 0.000 (0.278) loss 0.9596 (1.0145) lr 1.0926e-04 eta 0:02:02
epoch [29/30] batch [4/96] time 0.347 (0.491) data 0.000 (0.139) loss 1.2909 (1.1289) lr 1.0926e-04 eta 0:01:32
epoch [29/30] batch [6/96] time 0.331 (0.439) data 0.000 (0.093) loss 1.0194 (1.0817) lr 1.0926e-04 eta 0:01:21
epoch [29/30] batch [8/96] time 0.332 (0.412) data 0.000 (0.070) loss 0.8198 (1.0305) lr 1.0926e-04 eta 0:01:15
epoch [29/30] batch [10/96] time 0.340 (0.397) data 0.000 (0.056) loss 1.0762 (1.0145) lr 1.0926e-04 eta 0:01:12
epoch [29/30] batch [12/96] time 0.332 (0.387) data 0.000 (0.046) loss 1.5205 (1.1699) lr 1.0926e-04 eta 0:01:09
epoch [29/30] batch [14/96] time 0.324 (0.378) data 0.000 (0.040) loss 1.0260 (1.1847) lr 1.0926e-04 eta 0:01:07
epoch [29/30] batch [16/96] time 0.326 (0.371) data 0.000 (0.035) loss 0.8587 (1.1371) lr 1.0926e-04 eta 0:01:05
epoch [29/30] batch [18/96] time 0.329 (0.367) data 0.000 (0.031) loss 1.1082 (1.1101) lr 1.0926e-04 eta 0:01:03
epoch [29/30] batch [20/96] time 0.334 (0.364) data 0.000 (0.028) loss 0.7466 (1.0773) lr 1.0926e-04 eta 0:01:02
epoch [29/30] batch [22/96] time 0.349 (0.363) data 0.000 (0.026) loss 1.3483 (1.0814) lr 1.0926e-04 eta 0:01:01
epoch [29/30] batch [24/96] time 0.338 (0.360) data 0.000 (0.023) loss 1.7799 (1.0996) lr 1.0926e-04 eta 0:01:00
epoch [29/30] batch [26/96] time 0.325 (0.358) data 0.000 (0.022) loss 1.0583 (1.0864) lr 1.0926e-04 eta 0:00:59
epoch [29/30] batch [28/96] time 0.324 (0.356) data 0.000 (0.020) loss 1.0324 (1.0712) lr 1.0926e-04 eta 0:00:58
epoch [29/30] batch [30/96] time 0.352 (0.355) data 0.001 (0.019) loss 1.7608 (1.1042) lr 1.0926e-04 eta 0:00:57
epoch [29/30] batch [32/96] time 0.350 (0.354) data 0.000 (0.018) loss 1.3222 (1.1170) lr 1.0926e-04 eta 0:00:56
epoch [29/30] batch [34/96] time 0.354 (0.354) data 0.000 (0.017) loss 1.5258 (1.1278) lr 1.0926e-04 eta 0:00:55
epoch [29/30] batch [36/96] time 0.426 (0.356) data 0.000 (0.016) loss 1.0300 (1.1329) lr 1.0926e-04 eta 0:00:55
epoch [29/30] batch [38/96] time 0.323 (0.354) data 0.000 (0.015) loss 1.3564 (1.1383) lr 1.0926e-04 eta 0:00:54
epoch [29/30] batch [40/96] time 0.338 (0.353) data 0.000 (0.014) loss 1.0484 (1.1397) lr 1.0926e-04 eta 0:00:53
epoch [29/30] batch [42/96] time 0.337 (0.352) data 0.000 (0.014) loss 0.9501 (1.1451) lr 1.0926e-04 eta 0:00:52
epoch [29/30] batch [44/96] time 0.330 (0.352) data 0.000 (0.013) loss 1.2780 (1.1534) lr 1.0926e-04 eta 0:00:52
epoch [29/30] batch [46/96] time 0.322 (0.350) data 0.000 (0.012) loss 1.0571 (1.1514) lr 1.0926e-04 eta 0:00:51
epoch [29/30] batch [48/96] time 0.332 (0.350) data 0.001 (0.012) loss 1.2451 (1.1488) lr 1.0926e-04 eta 0:00:50
epoch [29/30] batch [50/96] time 0.331 (0.349) data 0.000 (0.011) loss 1.2960 (1.1611) lr 1.0926e-04 eta 0:00:49
epoch [29/30] batch [52/96] time 0.340 (0.348) data 0.000 (0.011) loss 1.5256 (1.1680) lr 1.0926e-04 eta 0:00:48
epoch [29/30] batch [54/96] time 0.324 (0.348) data 0.000 (0.011) loss 0.6854 (1.1559) lr 1.0926e-04 eta 0:00:47
epoch [29/30] batch [56/96] time 0.328 (0.347) data 0.000 (0.010) loss 1.1673 (1.1529) lr 1.0926e-04 eta 0:00:47
epoch [29/30] batch [58/96] time 0.334 (0.347) data 0.000 (0.010) loss 1.5580 (1.1679) lr 1.0926e-04 eta 0:00:46
epoch [29/30] batch [60/96] time 0.328 (0.346) data 0.000 (0.010) loss 1.7814 (1.1755) lr 1.0926e-04 eta 0:00:45
epoch [29/30] batch [62/96] time 0.326 (0.345) data 0.000 (0.009) loss 0.9585 (1.1683) lr 1.0926e-04 eta 0:00:44
epoch [29/30] batch [64/96] time 0.331 (0.345) data 0.000 (0.009) loss 0.7848 (1.1614) lr 1.0926e-04 eta 0:00:44
epoch [29/30] batch [66/96] time 0.331 (0.344) data 0.000 (0.009) loss 0.8903 (1.1646) lr 1.0926e-04 eta 0:00:43
epoch [29/30] batch [68/96] time 0.330 (0.344) data 0.000 (0.008) loss 1.7995 (1.1683) lr 1.0926e-04 eta 0:00:42
epoch [29/30] batch [70/96] time 0.332 (0.344) data 0.000 (0.008) loss 1.1756 (1.1666) lr 1.0926e-04 eta 0:00:41
epoch [29/30] batch [72/96] time 0.333 (0.343) data 0.000 (0.008) loss 0.9736 (1.1592) lr 1.0926e-04 eta 0:00:41
epoch [29/30] batch [74/96] time 0.317 (0.343) data 0.000 (0.008) loss 1.1714 (1.1741) lr 1.0926e-04 eta 0:00:40
epoch [29/30] batch [76/96] time 0.313 (0.342) data 0.000 (0.008) loss 0.8457 (1.1746) lr 1.0926e-04 eta 0:00:39
epoch [29/30] batch [78/96] time 0.310 (0.341) data 0.000 (0.007) loss 0.7813 (1.1824) lr 1.0926e-04 eta 0:00:38
epoch [29/30] batch [80/96] time 0.308 (0.340) data 0.000 (0.007) loss 1.1087 (1.1850) lr 1.0926e-04 eta 0:00:38
epoch [29/30] batch [82/96] time 0.305 (0.339) data 0.000 (0.007) loss 0.7569 (1.1755) lr 1.0926e-04 eta 0:00:37
epoch [29/30] batch [84/96] time 0.312 (0.339) data 0.000 (0.007) loss 1.3812 (1.1770) lr 1.0926e-04 eta 0:00:36
epoch [29/30] batch [86/96] time 0.311 (0.338) data 0.000 (0.007) loss 0.8278 (1.1717) lr 1.0926e-04 eta 0:00:35
epoch [29/30] batch [88/96] time 0.308 (0.337) data 0.000 (0.007) loss 0.8081 (1.1656) lr 1.0926e-04 eta 0:00:35
epoch [29/30] batch [90/96] time 0.312 (0.337) data 0.000 (0.006) loss 1.1287 (1.1689) lr 1.0926e-04 eta 0:00:34
epoch [29/30] batch [92/96] time 0.306 (0.336) data 0.000 (0.006) loss 0.8743 (1.1696) lr 1.0926e-04 eta 0:00:33
epoch [29/30] batch [94/96] time 0.311 (0.336) data 0.000 (0.006) loss 0.7334 (1.1686) lr 1.0926e-04 eta 0:00:32
epoch [29/30] batch [96/96] time 0.311 (0.335) data 0.000 (0.006) loss 0.7557 (1.1630) lr 2.7391e-05 eta 0:00:32
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.87s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 440
* accuracy: 76.4%
* error: 23.6%
* macro_f1: 76.4%

epoch [30/30] batch [2/96] time 0.325 (0.645) data 0.000 (0.283) loss 1.1630 (0.9706) lr 2.7391e-05 eta 0:01:00
epoch [30/30] batch [4/96] time 0.322 (0.485) data 0.000 (0.142) loss 1.1337 (1.1334) lr 2.7391e-05 eta 0:00:44
epoch [30/30] batch [6/96] time 0.318 (0.430) data 0.000 (0.095) loss 0.9551 (1.0970) lr 2.7391e-05 eta 0:00:38
epoch [30/30] batch [8/96] time 0.333 (0.407) data 0.000 (0.071) loss 2.6197 (1.2652) lr 2.7391e-05 eta 0:00:35
epoch [30/30] batch [10/96] time 0.339 (0.393) data 0.000 (0.057) loss 1.0554 (1.1932) lr 2.7391e-05 eta 0:00:33
epoch [30/30] batch [12/96] time 0.340 (0.384) data 0.000 (0.048) loss 1.4598 (1.1773) lr 2.7391e-05 eta 0:00:32
epoch [30/30] batch [14/96] time 0.351 (0.380) data 0.000 (0.041) loss 0.9948 (1.1801) lr 2.7391e-05 eta 0:00:31
epoch [30/30] batch [16/96] time 0.338 (0.373) data 0.000 (0.036) loss 1.0598 (1.1528) lr 2.7391e-05 eta 0:00:29
epoch [30/30] batch [18/96] time 0.332 (0.369) data 0.000 (0.032) loss 1.0781 (1.1340) lr 2.7391e-05 eta 0:00:28
epoch [30/30] batch [20/96] time 0.347 (0.367) data 0.000 (0.029) loss 1.6033 (1.1499) lr 2.7391e-05 eta 0:00:27
epoch [30/30] batch [22/96] time 0.341 (0.364) data 0.000 (0.026) loss 1.0993 (1.1342) lr 2.7391e-05 eta 0:00:26
epoch [30/30] batch [24/96] time 0.320 (0.361) data 0.000 (0.024) loss 0.7327 (1.1186) lr 2.7391e-05 eta 0:00:25
epoch [30/30] batch [26/96] time 0.322 (0.358) data 0.000 (0.022) loss 0.9237 (1.1497) lr 2.7391e-05 eta 0:00:25
epoch [30/30] batch [28/96] time 0.336 (0.356) data 0.000 (0.021) loss 1.8904 (1.1733) lr 2.7391e-05 eta 0:00:24
epoch [30/30] batch [30/96] time 0.333 (0.354) data 0.000 (0.019) loss 0.7829 (1.1650) lr 2.7391e-05 eta 0:00:23
epoch [30/30] batch [32/96] time 0.343 (0.353) data 0.000 (0.018) loss 1.0414 (1.1596) lr 2.7391e-05 eta 0:00:22
epoch [30/30] batch [34/96] time 0.337 (0.352) data 0.000 (0.017) loss 0.8662 (1.1452) lr 2.7391e-05 eta 0:00:21
epoch [30/30] batch [36/96] time 0.429 (0.354) data 0.000 (0.016) loss 1.5844 (1.1567) lr 2.7391e-05 eta 0:00:21
epoch [30/30] batch [38/96] time 0.343 (0.353) data 0.000 (0.015) loss 1.0875 (1.1559) lr 2.7391e-05 eta 0:00:20
epoch [30/30] batch [40/96] time 0.340 (0.352) data 0.000 (0.014) loss 1.3683 (1.1931) lr 2.7391e-05 eta 0:00:19
epoch [30/30] batch [42/96] time 0.334 (0.352) data 0.000 (0.014) loss 0.7999 (1.1781) lr 2.7391e-05 eta 0:00:18
epoch [30/30] batch [44/96] time 0.325 (0.350) data 0.000 (0.013) loss 0.9203 (1.1794) lr 2.7391e-05 eta 0:00:18
epoch [30/30] batch [46/96] time 0.337 (0.349) data 0.000 (0.013) loss 1.5312 (1.1982) lr 2.7391e-05 eta 0:00:17
epoch [30/30] batch [48/96] time 0.326 (0.349) data 0.000 (0.012) loss 0.7815 (1.1889) lr 2.7391e-05 eta 0:00:16
epoch [30/30] batch [50/96] time 0.327 (0.348) data 0.000 (0.012) loss 0.9321 (1.1803) lr 2.7391e-05 eta 0:00:15
epoch [30/30] batch [52/96] time 0.317 (0.347) data 0.000 (0.011) loss 0.8049 (1.1728) lr 2.7391e-05 eta 0:00:15
epoch [30/30] batch [54/96] time 0.321 (0.346) data 0.000 (0.011) loss 0.7515 (1.1626) lr 2.7391e-05 eta 0:00:14
epoch [30/30] batch [56/96] time 0.322 (0.345) data 0.000 (0.010) loss 1.4385 (1.1698) lr 2.7391e-05 eta 0:00:13
epoch [30/30] batch [58/96] time 0.343 (0.345) data 0.000 (0.010) loss 0.8213 (1.1621) lr 2.7391e-05 eta 0:00:13
epoch [30/30] batch [60/96] time 0.324 (0.344) data 0.000 (0.010) loss 0.9075 (1.1588) lr 2.7391e-05 eta 0:00:12
epoch [30/30] batch [62/96] time 0.340 (0.344) data 0.000 (0.009) loss 1.5443 (1.1655) lr 2.7391e-05 eta 0:00:11
epoch [30/30] batch [64/96] time 0.348 (0.344) data 0.001 (0.009) loss 0.9085 (1.1586) lr 2.7391e-05 eta 0:00:10
epoch [30/30] batch [66/96] time 0.323 (0.343) data 0.000 (0.009) loss 0.7912 (1.1466) lr 2.7391e-05 eta 0:00:10
epoch [30/30] batch [68/96] time 0.321 (0.342) data 0.000 (0.009) loss 1.6367 (1.1554) lr 2.7391e-05 eta 0:00:09
epoch [30/30] batch [70/96] time 0.327 (0.342) data 0.000 (0.008) loss 1.8086 (1.1658) lr 2.7391e-05 eta 0:00:08
epoch [30/30] batch [72/96] time 0.326 (0.341) data 0.000 (0.008) loss 0.9949 (1.1723) lr 2.7391e-05 eta 0:00:08
epoch [30/30] batch [74/96] time 0.309 (0.341) data 0.000 (0.008) loss 1.3569 (1.1693) lr 2.7391e-05 eta 0:00:07
epoch [30/30] batch [76/96] time 0.309 (0.340) data 0.000 (0.008) loss 0.8570 (1.1657) lr 2.7391e-05 eta 0:00:06
epoch [30/30] batch [78/96] time 0.306 (0.339) data 0.000 (0.008) loss 1.1170 (1.1638) lr 2.7391e-05 eta 0:00:06
epoch [30/30] batch [80/96] time 0.313 (0.338) data 0.000 (0.007) loss 0.9774 (1.1559) lr 2.7391e-05 eta 0:00:05
epoch [30/30] batch [82/96] time 0.310 (0.338) data 0.000 (0.007) loss 0.7558 (1.1537) lr 2.7391e-05 eta 0:00:04
epoch [30/30] batch [84/96] time 0.309 (0.337) data 0.000 (0.007) loss 1.1166 (1.1618) lr 2.7391e-05 eta 0:00:04
epoch [30/30] batch [86/96] time 0.309 (0.336) data 0.000 (0.007) loss 0.8590 (1.1632) lr 2.7391e-05 eta 0:00:03
epoch [30/30] batch [88/96] time 0.307 (0.336) data 0.000 (0.007) loss 0.9114 (1.1554) lr 2.7391e-05 eta 0:00:02
epoch [30/30] batch [90/96] time 0.313 (0.335) data 0.000 (0.007) loss 0.9976 (1.1523) lr 2.7391e-05 eta 0:00:02
epoch [30/30] batch [92/96] time 0.308 (0.335) data 0.000 (0.006) loss 1.0579 (1.1528) lr 2.7391e-05 eta 0:00:01
epoch [30/30] batch [94/96] time 0.308 (0.334) data 0.000 (0.006) loss 1.4745 (1.1526) lr 2.7391e-05 eta 0:00:00
epoch [30/30] batch [96/96] time 0.303 (0.333) data 0.000 (0.006) loss 1.2182 (1.1562) lr 0.0000e+00 eta 0:00:00
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.85s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]
=> result
* total: 576
* correct: 440
* accuracy: 76.4%
* error: 23.6%
* macro_f1: 76.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 26)
Evaluate on the *test* set
  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [00:03<00:13,  3.27s/it] 40%|████      | 2/5 [00:03<00:04,  1.52s/it] 60%|██████    | 3/5 [00:03<00:01,  1.05it/s] 80%|████████  | 4/5 [00:04<00:00,  1.44it/s]100%|██████████| 5/5 [00:04<00:00,  2.03it/s]100%|██████████| 5/5 [00:04<00:00,  1.14it/s]
=> result
* total: 864
* correct: 678
* accuracy: 78.5%
* error: 21.5%
* macro_f1: 78.2%
Elapsed: 0:17:55
+ sh scripts/rpo_prime/base2new_test_sdl.sh dtd 2 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
368 552 828
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  23
# train_x  368
# val      552
# test     828
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 26)
Evaluate on the *test* set
  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [00:05<00:21,  5.29s/it] 40%|████      | 2/5 [00:05<00:07,  2.35s/it] 60%|██████    | 3/5 [00:05<00:02,  1.41s/it] 80%|████████  | 4/5 [00:06<00:00,  1.03it/s]100%|██████████| 5/5 [00:06<00:00,  1.26s/it]
=> result
* total: 828
* correct: 518
* accuracy: 62.6%
* error: 37.4%
* macro_f1: 61.7%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh dtd 3 0 main_tmp1_0.1sdl 16
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
384 576 864
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  24
# train_x  384
# val      576
# test     864
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/30] batch [2/96] time 0.351 (1.668) data 0.000 (0.606) loss 2.5824 (2.2450) lr 1.0000e-02 eta 1:20:01
epoch [1/30] batch [4/96] time 0.348 (1.011) data 0.000 (0.303) loss 2.4994 (2.0795) lr 1.0000e-02 eta 0:48:27
epoch [1/30] batch [6/96] time 0.339 (0.791) data 0.000 (0.202) loss 2.2152 (2.2794) lr 1.0000e-02 eta 0:37:51
epoch [1/30] batch [8/96] time 0.331 (0.675) data 0.000 (0.152) loss 2.7875 (2.2522) lr 1.0000e-02 eta 0:32:18
epoch [1/30] batch [10/96] time 0.330 (0.607) data 0.000 (0.121) loss 2.0585 (2.1247) lr 1.0000e-02 eta 0:29:01
epoch [1/30] batch [12/96] time 0.337 (0.561) data 0.000 (0.101) loss 2.8593 (2.1057) lr 1.0000e-02 eta 0:26:49
epoch [1/30] batch [14/96] time 0.334 (0.529) data 0.000 (0.087) loss 1.2072 (2.0277) lr 1.0000e-02 eta 0:25:14
epoch [1/30] batch [16/96] time 0.341 (0.505) data 0.000 (0.076) loss 0.8535 (1.9174) lr 1.0000e-02 eta 0:24:05
epoch [1/30] batch [18/96] time 0.354 (0.487) data 0.000 (0.068) loss 1.3055 (1.9050) lr 1.0000e-02 eta 0:23:12
epoch [1/30] batch [20/96] time 0.317 (0.470) data 0.000 (0.061) loss 1.2189 (1.8663) lr 1.0000e-02 eta 0:22:25
epoch [1/30] batch [22/96] time 0.334 (0.457) data 0.000 (0.055) loss 2.6976 (1.8625) lr 1.0000e-02 eta 0:21:47
epoch [1/30] batch [24/96] time 0.316 (0.445) data 0.000 (0.051) loss 1.5111 (1.8621) lr 1.0000e-02 eta 0:21:11
epoch [1/30] batch [26/96] time 0.322 (0.436) data 0.000 (0.047) loss 3.2966 (1.9307) lr 1.0000e-02 eta 0:20:45
epoch [1/30] batch [28/96] time 0.314 (0.428) data 0.000 (0.044) loss 1.0462 (1.8876) lr 1.0000e-02 eta 0:20:19
epoch [1/30] batch [30/96] time 0.323 (0.421) data 0.000 (0.041) loss 3.0541 (1.9260) lr 1.0000e-02 eta 0:19:59
epoch [1/30] batch [32/96] time 0.323 (0.415) data 0.000 (0.038) loss 2.2561 (1.9441) lr 1.0000e-02 eta 0:19:42
epoch [1/30] batch [34/96] time 0.332 (0.410) data 0.000 (0.036) loss 1.8415 (1.9408) lr 1.0000e-02 eta 0:19:26
epoch [1/30] batch [36/96] time 0.309 (0.405) data 0.000 (0.034) loss 2.4715 (1.9520) lr 1.0000e-02 eta 0:19:10
epoch [1/30] batch [38/96] time 0.317 (0.400) data 0.000 (0.032) loss 2.0688 (1.9437) lr 1.0000e-02 eta 0:18:57
epoch [1/30] batch [40/96] time 0.326 (0.396) data 0.000 (0.031) loss 1.8703 (1.9448) lr 1.0000e-02 eta 0:18:45
epoch [1/30] batch [42/96] time 0.324 (0.393) data 0.000 (0.029) loss 1.5901 (1.9221) lr 1.0000e-02 eta 0:18:34
epoch [1/30] batch [44/96] time 0.322 (0.390) data 0.000 (0.028) loss 2.3981 (1.9326) lr 1.0000e-02 eta 0:18:24
epoch [1/30] batch [46/96] time 0.329 (0.387) data 0.000 (0.027) loss 1.3921 (1.9075) lr 1.0000e-02 eta 0:18:17
epoch [1/30] batch [48/96] time 0.334 (0.385) data 0.000 (0.026) loss 1.0375 (1.8895) lr 1.0000e-02 eta 0:18:10
epoch [1/30] batch [50/96] time 0.332 (0.383) data 0.000 (0.025) loss 2.0010 (1.8899) lr 1.0000e-02 eta 0:18:03
epoch [1/30] batch [52/96] time 0.321 (0.380) data 0.000 (0.024) loss 1.9053 (1.8757) lr 1.0000e-02 eta 0:17:55
epoch [1/30] batch [54/96] time 0.323 (0.378) data 0.000 (0.023) loss 1.6098 (1.8882) lr 1.0000e-02 eta 0:17:49
epoch [1/30] batch [56/96] time 0.323 (0.376) data 0.000 (0.022) loss 1.7975 (1.8847) lr 1.0000e-02 eta 0:17:42
epoch [1/30] batch [58/96] time 0.314 (0.374) data 0.000 (0.021) loss 2.0645 (1.8825) lr 1.0000e-02 eta 0:17:36
epoch [1/30] batch [60/96] time 0.325 (0.373) data 0.000 (0.021) loss 1.9283 (1.8803) lr 1.0000e-02 eta 0:17:30
epoch [1/30] batch [62/96] time 0.333 (0.371) data 0.000 (0.020) loss 1.7316 (1.8764) lr 1.0000e-02 eta 0:17:26
epoch [1/30] batch [64/96] time 0.346 (0.370) data 0.000 (0.019) loss 2.2654 (1.8646) lr 1.0000e-02 eta 0:17:23
epoch [1/30] batch [66/96] time 0.326 (0.369) data 0.000 (0.019) loss 2.3259 (1.8856) lr 1.0000e-02 eta 0:17:19
epoch [1/30] batch [68/96] time 0.323 (0.368) data 0.000 (0.018) loss 2.0712 (1.8753) lr 1.0000e-02 eta 0:17:14
epoch [1/30] batch [70/96] time 0.326 (0.367) data 0.000 (0.018) loss 3.1825 (1.8796) lr 1.0000e-02 eta 0:17:10
epoch [1/30] batch [72/96] time 0.415 (0.367) data 0.000 (0.017) loss 2.1014 (1.8856) lr 1.0000e-02 eta 0:17:09
epoch [1/30] batch [74/96] time 0.317 (0.365) data 0.000 (0.017) loss 1.7492 (1.8977) lr 1.0000e-02 eta 0:17:05
epoch [1/30] batch [76/96] time 0.319 (0.364) data 0.000 (0.016) loss 1.2527 (1.8771) lr 1.0000e-02 eta 0:17:01
epoch [1/30] batch [78/96] time 0.316 (0.363) data 0.000 (0.016) loss 2.8758 (1.8999) lr 1.0000e-02 eta 0:16:57
epoch [1/30] batch [80/96] time 0.317 (0.362) data 0.000 (0.015) loss 1.6723 (1.8996) lr 1.0000e-02 eta 0:16:53
epoch [1/30] batch [82/96] time 0.315 (0.361) data 0.000 (0.015) loss 1.7686 (1.8986) lr 1.0000e-02 eta 0:16:49
epoch [1/30] batch [84/96] time 0.322 (0.360) data 0.000 (0.015) loss 1.8296 (1.8946) lr 1.0000e-02 eta 0:16:46
epoch [1/30] batch [86/96] time 0.324 (0.359) data 0.000 (0.014) loss 1.2547 (1.8868) lr 1.0000e-02 eta 0:16:43
epoch [1/30] batch [88/96] time 0.319 (0.358) data 0.000 (0.014) loss 1.3369 (1.8782) lr 1.0000e-02 eta 0:16:39
epoch [1/30] batch [90/96] time 0.326 (0.357) data 0.000 (0.014) loss 1.9967 (1.8735) lr 1.0000e-02 eta 0:16:37
epoch [1/30] batch [92/96] time 0.321 (0.357) data 0.000 (0.013) loss 1.8518 (1.8842) lr 1.0000e-02 eta 0:16:34
epoch [1/30] batch [94/96] time 0.323 (0.356) data 0.000 (0.013) loss 2.4217 (1.8920) lr 1.0000e-02 eta 0:16:31
epoch [1/30] batch [96/96] time 0.325 (0.355) data 0.000 (0.013) loss 1.6889 (1.8782) lr 9.9726e-03 eta 0:16:28
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:03<00:06,  3.02s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.41s/it]100%|██████████| 3/3 [00:03<00:00,  1.11it/s]100%|██████████| 3/3 [00:03<00:00,  1.23s/it]=> result
* total: 576
* correct: 347
* accuracy: 60.2%
* error: 39.8%
* macro_f1: 56.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [2/30] batch [2/96] time 0.319 (0.654) data 0.000 (0.302) loss 1.0656 (1.8344) lr 9.9726e-03 eta 0:30:19
epoch [2/30] batch [4/96] time 0.329 (0.489) data 0.000 (0.151) loss 2.0752 (1.7535) lr 9.9726e-03 eta 0:22:38
epoch [2/30] batch [6/96] time 0.328 (0.435) data 0.000 (0.101) loss 2.8018 (1.9692) lr 9.9726e-03 eta 0:20:08
epoch [2/30] batch [8/96] time 0.322 (0.407) data 0.000 (0.076) loss 2.5494 (1.9285) lr 9.9726e-03 eta 0:18:50
epoch [2/30] batch [10/96] time 0.321 (0.392) data 0.000 (0.061) loss 2.5680 (2.0497) lr 9.9726e-03 eta 0:18:06
epoch [2/30] batch [12/96] time 0.324 (0.381) data 0.000 (0.051) loss 2.5410 (2.0840) lr 9.9726e-03 eta 0:17:34
epoch [2/30] batch [14/96] time 0.322 (0.372) data 0.000 (0.043) loss 2.0615 (2.1115) lr 9.9726e-03 eta 0:17:10
epoch [2/30] batch [16/96] time 0.324 (0.366) data 0.000 (0.038) loss 1.3843 (2.0367) lr 9.9726e-03 eta 0:16:54
epoch [2/30] batch [18/96] time 0.333 (0.362) data 0.000 (0.034) loss 1.8679 (2.0034) lr 9.9726e-03 eta 0:16:41
epoch [2/30] batch [20/96] time 0.327 (0.359) data 0.000 (0.030) loss 1.7666 (1.9420) lr 9.9726e-03 eta 0:16:31
epoch [2/30] batch [22/96] time 0.326 (0.356) data 0.000 (0.028) loss 1.9818 (1.9286) lr 9.9726e-03 eta 0:16:22
epoch [2/30] batch [24/96] time 0.329 (0.354) data 0.000 (0.025) loss 3.0835 (1.9268) lr 9.9726e-03 eta 0:16:16
epoch [2/30] batch [26/96] time 0.323 (0.352) data 0.000 (0.024) loss 1.6324 (1.8746) lr 9.9726e-03 eta 0:16:09
epoch [2/30] batch [28/96] time 0.327 (0.350) data 0.000 (0.022) loss 1.8463 (1.8526) lr 9.9726e-03 eta 0:16:04
epoch [2/30] batch [30/96] time 0.326 (0.348) data 0.000 (0.020) loss 1.4323 (1.8392) lr 9.9726e-03 eta 0:15:58
epoch [2/30] batch [32/96] time 0.324 (0.347) data 0.000 (0.019) loss 2.0220 (1.8329) lr 9.9726e-03 eta 0:15:53
epoch [2/30] batch [34/96] time 0.331 (0.345) data 0.000 (0.018) loss 1.6463 (1.8062) lr 9.9726e-03 eta 0:15:49
epoch [2/30] batch [36/96] time 0.334 (0.345) data 0.000 (0.017) loss 1.5490 (1.7975) lr 9.9726e-03 eta 0:15:47
epoch [2/30] batch [38/96] time 0.334 (0.344) data 0.000 (0.016) loss 1.4376 (1.7919) lr 9.9726e-03 eta 0:15:44
epoch [2/30] batch [40/96] time 0.328 (0.343) data 0.000 (0.015) loss 1.7363 (1.7746) lr 9.9726e-03 eta 0:15:41
epoch [2/30] batch [42/96] time 0.319 (0.342) data 0.000 (0.015) loss 1.4118 (1.8019) lr 9.9726e-03 eta 0:15:37
epoch [2/30] batch [44/96] time 0.341 (0.342) data 0.000 (0.014) loss 1.8287 (1.8088) lr 9.9726e-03 eta 0:15:35
epoch [2/30] batch [46/96] time 0.314 (0.341) data 0.000 (0.013) loss 2.1531 (1.7992) lr 9.9726e-03 eta 0:15:32
epoch [2/30] batch [48/96] time 0.338 (0.340) data 0.000 (0.013) loss 1.7637 (1.8030) lr 9.9726e-03 eta 0:15:30
epoch [2/30] batch [50/96] time 0.328 (0.340) data 0.000 (0.012) loss 2.9768 (1.8142) lr 9.9726e-03 eta 0:15:28
epoch [2/30] batch [52/96] time 0.325 (0.339) data 0.000 (0.012) loss 1.8467 (1.8053) lr 9.9726e-03 eta 0:15:27
epoch [2/30] batch [54/96] time 0.329 (0.339) data 0.000 (0.011) loss 1.3006 (1.7758) lr 9.9726e-03 eta 0:15:26
epoch [2/30] batch [56/96] time 0.332 (0.339) data 0.000 (0.011) loss 2.6734 (1.8035) lr 9.9726e-03 eta 0:15:25
epoch [2/30] batch [58/96] time 0.315 (0.339) data 0.000 (0.011) loss 1.0735 (1.7948) lr 9.9726e-03 eta 0:15:23
epoch [2/30] batch [60/96] time 0.321 (0.338) data 0.000 (0.010) loss 1.6929 (1.7803) lr 9.9726e-03 eta 0:15:21
epoch [2/30] batch [62/96] time 0.329 (0.338) data 0.000 (0.010) loss 1.6867 (1.7895) lr 9.9726e-03 eta 0:15:19
epoch [2/30] batch [64/96] time 0.332 (0.338) data 0.000 (0.010) loss 1.4102 (1.7744) lr 9.9726e-03 eta 0:15:19
epoch [2/30] batch [66/96] time 0.343 (0.338) data 0.000 (0.009) loss 1.4797 (1.7746) lr 9.9726e-03 eta 0:15:18
epoch [2/30] batch [68/96] time 0.322 (0.337) data 0.000 (0.009) loss 2.1237 (1.7665) lr 9.9726e-03 eta 0:15:16
epoch [2/30] batch [70/96] time 0.345 (0.337) data 0.000 (0.009) loss 1.4177 (1.7800) lr 9.9726e-03 eta 0:15:15
epoch [2/30] batch [72/96] time 0.334 (0.337) data 0.000 (0.009) loss 2.0406 (1.7728) lr 9.9726e-03 eta 0:15:14
epoch [2/30] batch [74/96] time 0.298 (0.336) data 0.000 (0.008) loss 1.5482 (1.7743) lr 9.9726e-03 eta 0:15:11
epoch [2/30] batch [76/96] time 0.303 (0.335) data 0.000 (0.008) loss 1.0523 (1.7711) lr 9.9726e-03 eta 0:15:08
epoch [2/30] batch [78/96] time 0.299 (0.334) data 0.000 (0.008) loss 2.7290 (1.7875) lr 9.9726e-03 eta 0:15:05
epoch [2/30] batch [80/96] time 0.301 (0.334) data 0.000 (0.008) loss 1.4363 (1.7766) lr 9.9726e-03 eta 0:15:02
epoch [2/30] batch [82/96] time 0.300 (0.333) data 0.000 (0.008) loss 2.1117 (1.7742) lr 9.9726e-03 eta 0:14:59
epoch [2/30] batch [84/96] time 0.299 (0.332) data 0.000 (0.007) loss 1.3994 (1.7652) lr 9.9726e-03 eta 0:14:56
epoch [2/30] batch [86/96] time 0.306 (0.331) data 0.000 (0.007) loss 1.5141 (1.7664) lr 9.9726e-03 eta 0:14:54
epoch [2/30] batch [88/96] time 0.300 (0.331) data 0.000 (0.007) loss 1.1529 (1.7567) lr 9.9726e-03 eta 0:14:51
epoch [2/30] batch [90/96] time 0.301 (0.330) data 0.000 (0.007) loss 1.4405 (1.7459) lr 9.9726e-03 eta 0:14:49
epoch [2/30] batch [92/96] time 0.303 (0.329) data 0.000 (0.007) loss 1.3214 (1.7349) lr 9.9726e-03 eta 0:14:46
epoch [2/30] batch [94/96] time 0.301 (0.329) data 0.000 (0.007) loss 1.5915 (1.7377) lr 9.9726e-03 eta 0:14:44
epoch [2/30] batch [96/96] time 0.305 (0.328) data 0.000 (0.007) loss 2.0447 (1.7423) lr 9.8907e-03 eta 0:14:42
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.89s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.19s/it]=> result
* total: 576
* correct: 358
* accuracy: 62.2%
* error: 37.8%
* macro_f1: 58.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [3/30] batch [2/96] time 0.327 (0.651) data 0.000 (0.270) loss 2.0779 (1.6025) lr 9.8907e-03 eta 0:29:09
epoch [3/30] batch [4/96] time 0.321 (0.513) data 0.000 (0.135) loss 1.2267 (1.2866) lr 9.8907e-03 eta 0:22:56
epoch [3/30] batch [6/96] time 0.325 (0.450) data 0.000 (0.090) loss 1.6802 (1.5509) lr 9.8907e-03 eta 0:20:05
epoch [3/30] batch [8/96] time 0.329 (0.420) data 0.001 (0.068) loss 2.2205 (1.7776) lr 9.8907e-03 eta 0:18:45
epoch [3/30] batch [10/96] time 0.343 (0.404) data 0.000 (0.054) loss 2.0183 (1.9116) lr 9.8907e-03 eta 0:18:01
epoch [3/30] batch [12/96] time 0.328 (0.391) data 0.000 (0.045) loss 1.4417 (1.8534) lr 9.8907e-03 eta 0:17:25
epoch [3/30] batch [14/96] time 0.321 (0.381) data 0.000 (0.039) loss 2.0100 (1.7942) lr 9.8907e-03 eta 0:16:58
epoch [3/30] batch [16/96] time 0.331 (0.374) data 0.000 (0.034) loss 1.8061 (1.8447) lr 9.8907e-03 eta 0:16:40
epoch [3/30] batch [18/96] time 0.313 (0.368) data 0.000 (0.030) loss 1.5300 (1.7880) lr 9.8907e-03 eta 0:16:23
epoch [3/30] batch [20/96] time 0.330 (0.364) data 0.000 (0.027) loss 1.9477 (1.7957) lr 9.8907e-03 eta 0:16:11
epoch [3/30] batch [22/96] time 0.331 (0.361) data 0.000 (0.025) loss 2.8265 (1.8355) lr 9.8907e-03 eta 0:16:01
epoch [3/30] batch [24/96] time 0.323 (0.358) data 0.000 (0.023) loss 2.0584 (1.8164) lr 9.8907e-03 eta 0:15:54
epoch [3/30] batch [26/96] time 0.331 (0.356) data 0.000 (0.021) loss 2.0372 (1.8203) lr 9.8907e-03 eta 0:15:46
epoch [3/30] batch [28/96] time 0.331 (0.354) data 0.000 (0.020) loss 1.0182 (1.7635) lr 9.8907e-03 eta 0:15:40
epoch [3/30] batch [30/96] time 0.321 (0.352) data 0.000 (0.018) loss 1.9675 (1.7528) lr 9.8907e-03 eta 0:15:35
epoch [3/30] batch [32/96] time 0.326 (0.350) data 0.000 (0.017) loss 1.3772 (1.7565) lr 9.8907e-03 eta 0:15:30
epoch [3/30] batch [34/96] time 0.335 (0.349) data 0.000 (0.016) loss 1.9403 (1.7392) lr 9.8907e-03 eta 0:15:27
epoch [3/30] batch [36/96] time 0.336 (0.349) data 0.000 (0.015) loss 2.5849 (1.7832) lr 9.8907e-03 eta 0:15:25
epoch [3/30] batch [38/96] time 0.348 (0.348) data 0.000 (0.015) loss 2.1550 (1.7841) lr 9.8907e-03 eta 0:15:22
epoch [3/30] batch [40/96] time 0.324 (0.347) data 0.000 (0.014) loss 1.9709 (1.7833) lr 9.8907e-03 eta 0:15:19
epoch [3/30] batch [42/96] time 0.337 (0.347) data 0.000 (0.013) loss 1.7976 (1.7907) lr 9.8907e-03 eta 0:15:17
epoch [3/30] batch [44/96] time 0.320 (0.346) data 0.000 (0.013) loss 0.9030 (1.7634) lr 9.8907e-03 eta 0:15:13
epoch [3/30] batch [46/96] time 0.340 (0.345) data 0.000 (0.012) loss 1.0923 (1.7569) lr 9.8907e-03 eta 0:15:12
epoch [3/30] batch [48/96] time 0.327 (0.344) data 0.000 (0.012) loss 1.2551 (1.7250) lr 9.8907e-03 eta 0:15:09
epoch [3/30] batch [50/96] time 0.331 (0.344) data 0.000 (0.011) loss 1.8449 (1.7306) lr 9.8907e-03 eta 0:15:06
epoch [3/30] batch [52/96] time 0.324 (0.343) data 0.000 (0.011) loss 0.7831 (1.7006) lr 9.8907e-03 eta 0:15:04
epoch [3/30] batch [54/96] time 0.337 (0.343) data 0.000 (0.010) loss 1.0351 (1.6878) lr 9.8907e-03 eta 0:15:03
epoch [3/30] batch [56/96] time 0.328 (0.343) data 0.000 (0.010) loss 0.9629 (1.6710) lr 9.8907e-03 eta 0:15:02
epoch [3/30] batch [58/96] time 0.347 (0.343) data 0.000 (0.010) loss 2.0834 (1.6862) lr 9.8907e-03 eta 0:15:01
epoch [3/30] batch [60/96] time 0.346 (0.343) data 0.000 (0.009) loss 2.7848 (1.6971) lr 9.8907e-03 eta 0:15:01
epoch [3/30] batch [62/96] time 0.331 (0.343) data 0.000 (0.009) loss 2.3010 (1.6947) lr 9.8907e-03 eta 0:15:00
epoch [3/30] batch [64/96] time 0.340 (0.343) data 0.000 (0.009) loss 1.5147 (1.6940) lr 9.8907e-03 eta 0:14:59
epoch [3/30] batch [66/96] time 0.351 (0.343) data 0.000 (0.009) loss 1.6455 (1.6847) lr 9.8907e-03 eta 0:14:59
epoch [3/30] batch [68/96] time 0.330 (0.343) data 0.000 (0.008) loss 1.8819 (1.7018) lr 9.8907e-03 eta 0:14:57
epoch [3/30] batch [70/96] time 0.345 (0.343) data 0.000 (0.008) loss 2.3100 (1.7190) lr 9.8907e-03 eta 0:14:57
epoch [3/30] batch [72/96] time 0.353 (0.343) data 0.000 (0.008) loss 2.1494 (1.7208) lr 9.8907e-03 eta 0:14:57
epoch [3/30] batch [74/96] time 0.318 (0.342) data 0.000 (0.008) loss 1.2295 (1.7115) lr 9.8907e-03 eta 0:14:54
epoch [3/30] batch [76/96] time 0.325 (0.342) data 0.000 (0.007) loss 2.1688 (1.7108) lr 9.8907e-03 eta 0:14:52
epoch [3/30] batch [78/96] time 0.326 (0.341) data 0.000 (0.007) loss 0.8459 (1.6956) lr 9.8907e-03 eta 0:14:50
epoch [3/30] batch [80/96] time 0.320 (0.341) data 0.000 (0.007) loss 1.5684 (1.6987) lr 9.8907e-03 eta 0:14:48
epoch [3/30] batch [82/96] time 0.325 (0.340) data 0.000 (0.007) loss 1.1664 (1.6866) lr 9.8907e-03 eta 0:14:46
epoch [3/30] batch [84/96] time 0.319 (0.340) data 0.000 (0.007) loss 1.8399 (1.6896) lr 9.8907e-03 eta 0:14:45
epoch [3/30] batch [86/96] time 0.321 (0.339) data 0.000 (0.007) loss 1.0500 (1.6743) lr 9.8907e-03 eta 0:14:43
epoch [3/30] batch [88/96] time 0.322 (0.339) data 0.000 (0.006) loss 1.2862 (1.6738) lr 9.8907e-03 eta 0:14:41
epoch [3/30] batch [90/96] time 0.319 (0.339) data 0.000 (0.006) loss 1.2320 (1.6740) lr 9.8907e-03 eta 0:14:39
epoch [3/30] batch [92/96] time 0.324 (0.338) data 0.000 (0.006) loss 2.0400 (1.6720) lr 9.8907e-03 eta 0:14:38
epoch [3/30] batch [94/96] time 0.323 (0.338) data 0.000 (0.006) loss 1.4925 (1.6636) lr 9.8907e-03 eta 0:14:36
epoch [3/30] batch [96/96] time 0.323 (0.338) data 0.000 (0.006) loss 1.3550 (1.6611) lr 9.7553e-03 eta 0:14:35
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.86s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 364
* accuracy: 63.2%
* error: 36.8%
* macro_f1: 60.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [4/30] batch [2/96] time 0.323 (0.661) data 0.000 (0.298) loss 0.9489 (1.0956) lr 9.7553e-03 eta 0:28:31
epoch [4/30] batch [4/96] time 0.326 (0.494) data 0.000 (0.149) loss 1.1280 (1.0649) lr 9.7553e-03 eta 0:21:17
epoch [4/30] batch [6/96] time 0.325 (0.440) data 0.000 (0.100) loss 1.7877 (1.2782) lr 9.7553e-03 eta 0:18:58
epoch [4/30] batch [8/96] time 0.339 (0.414) data 0.000 (0.075) loss 1.0972 (1.3463) lr 9.7553e-03 eta 0:17:50
epoch [4/30] batch [10/96] time 0.317 (0.396) data 0.000 (0.060) loss 2.2860 (1.4954) lr 9.7553e-03 eta 0:17:01
epoch [4/30] batch [12/96] time 0.339 (0.385) data 0.000 (0.050) loss 1.3845 (1.4565) lr 9.7553e-03 eta 0:16:34
epoch [4/30] batch [14/96] time 0.330 (0.377) data 0.000 (0.043) loss 2.5200 (1.5282) lr 9.7553e-03 eta 0:16:11
epoch [4/30] batch [16/96] time 0.316 (0.370) data 0.000 (0.038) loss 1.5466 (1.5623) lr 9.7553e-03 eta 0:15:52
epoch [4/30] batch [18/96] time 0.342 (0.366) data 0.000 (0.033) loss 2.7250 (1.6028) lr 9.7553e-03 eta 0:15:41
epoch [4/30] batch [20/96] time 0.328 (0.362) data 0.000 (0.030) loss 1.4237 (1.5606) lr 9.7553e-03 eta 0:15:30
epoch [4/30] batch [22/96] time 0.321 (0.358) data 0.001 (0.027) loss 2.1251 (1.5569) lr 9.7553e-03 eta 0:15:19
epoch [4/30] batch [24/96] time 0.326 (0.355) data 0.000 (0.025) loss 1.5000 (1.5454) lr 9.7553e-03 eta 0:15:12
epoch [4/30] batch [26/96] time 0.334 (0.353) data 0.000 (0.023) loss 1.3477 (1.5344) lr 9.7553e-03 eta 0:15:06
epoch [4/30] batch [28/96] time 0.323 (0.355) data 0.001 (0.022) loss 1.0582 (1.5070) lr 9.7553e-03 eta 0:15:10
epoch [4/30] batch [30/96] time 0.322 (0.353) data 0.000 (0.020) loss 1.2657 (1.4993) lr 9.7553e-03 eta 0:15:03
epoch [4/30] batch [32/96] time 0.325 (0.351) data 0.000 (0.019) loss 1.2677 (1.5000) lr 9.7553e-03 eta 0:14:58
epoch [4/30] batch [34/96] time 0.333 (0.350) data 0.000 (0.018) loss 1.6551 (1.5068) lr 9.7553e-03 eta 0:14:54
epoch [4/30] batch [36/96] time 0.323 (0.348) data 0.000 (0.017) loss 1.0282 (1.5008) lr 9.7553e-03 eta 0:14:49
epoch [4/30] batch [38/96] time 0.318 (0.347) data 0.000 (0.016) loss 1.9720 (1.5152) lr 9.7553e-03 eta 0:14:45
epoch [4/30] batch [40/96] time 0.335 (0.346) data 0.000 (0.015) loss 1.2903 (1.5204) lr 9.7553e-03 eta 0:14:42
epoch [4/30] batch [42/96] time 0.329 (0.345) data 0.000 (0.015) loss 2.6500 (1.5613) lr 9.7553e-03 eta 0:14:39
epoch [4/30] batch [44/96] time 0.324 (0.344) data 0.000 (0.014) loss 0.9573 (1.5390) lr 9.7553e-03 eta 0:14:36
epoch [4/30] batch [46/96] time 0.321 (0.343) data 0.000 (0.013) loss 2.6326 (1.5539) lr 9.7553e-03 eta 0:14:32
epoch [4/30] batch [48/96] time 0.320 (0.342) data 0.000 (0.013) loss 1.7492 (1.5630) lr 9.7553e-03 eta 0:14:29
epoch [4/30] batch [50/96] time 0.322 (0.341) data 0.000 (0.012) loss 1.0662 (1.5500) lr 9.7553e-03 eta 0:14:26
epoch [4/30] batch [52/96] time 0.344 (0.341) data 0.000 (0.012) loss 1.3261 (1.5444) lr 9.7553e-03 eta 0:14:24
epoch [4/30] batch [54/96] time 0.323 (0.340) data 0.000 (0.011) loss 1.8296 (1.5557) lr 9.7553e-03 eta 0:14:23
epoch [4/30] batch [56/96] time 0.349 (0.340) data 0.000 (0.011) loss 1.7848 (1.5494) lr 9.7553e-03 eta 0:14:22
epoch [4/30] batch [58/96] time 0.338 (0.340) data 0.000 (0.011) loss 1.5105 (1.5472) lr 9.7553e-03 eta 0:14:21
epoch [4/30] batch [60/96] time 0.322 (0.339) data 0.000 (0.010) loss 1.7457 (1.5685) lr 9.7553e-03 eta 0:14:19
epoch [4/30] batch [62/96] time 0.332 (0.339) data 0.000 (0.010) loss 1.4322 (1.5692) lr 9.7553e-03 eta 0:14:17
epoch [4/30] batch [64/96] time 0.325 (0.339) data 0.000 (0.010) loss 1.9978 (1.5874) lr 9.7553e-03 eta 0:14:15
epoch [4/30] batch [66/96] time 0.317 (0.338) data 0.000 (0.009) loss 2.1614 (1.5938) lr 9.7553e-03 eta 0:14:13
epoch [4/30] batch [68/96] time 0.332 (0.338) data 0.000 (0.009) loss 2.4964 (1.6076) lr 9.7553e-03 eta 0:14:12
epoch [4/30] batch [70/96] time 0.316 (0.337) data 0.000 (0.009) loss 1.1090 (1.6032) lr 9.7553e-03 eta 0:14:10
epoch [4/30] batch [72/96] time 0.321 (0.337) data 0.000 (0.009) loss 1.1123 (1.5934) lr 9.7553e-03 eta 0:14:08
epoch [4/30] batch [74/96] time 0.301 (0.336) data 0.000 (0.008) loss 1.3894 (1.5897) lr 9.7553e-03 eta 0:14:05
epoch [4/30] batch [76/96] time 0.307 (0.335) data 0.000 (0.008) loss 1.5866 (1.5921) lr 9.7553e-03 eta 0:14:03
epoch [4/30] batch [78/96] time 0.304 (0.334) data 0.000 (0.008) loss 0.7577 (1.5988) lr 9.7553e-03 eta 0:14:00
epoch [4/30] batch [80/96] time 0.309 (0.334) data 0.000 (0.008) loss 1.7359 (1.5963) lr 9.7553e-03 eta 0:13:57
epoch [4/30] batch [82/96] time 0.309 (0.333) data 0.000 (0.008) loss 2.2712 (1.6134) lr 9.7553e-03 eta 0:13:55
epoch [4/30] batch [84/96] time 0.301 (0.332) data 0.000 (0.007) loss 1.7656 (1.6099) lr 9.7553e-03 eta 0:13:53
epoch [4/30] batch [86/96] time 0.305 (0.332) data 0.000 (0.007) loss 1.8459 (1.6181) lr 9.7553e-03 eta 0:13:50
epoch [4/30] batch [88/96] time 0.307 (0.331) data 0.000 (0.007) loss 0.9046 (1.6050) lr 9.7553e-03 eta 0:13:48
epoch [4/30] batch [90/96] time 0.305 (0.330) data 0.000 (0.007) loss 1.6363 (1.6057) lr 9.7553e-03 eta 0:13:46
epoch [4/30] batch [92/96] time 0.308 (0.330) data 0.000 (0.007) loss 1.9834 (1.6074) lr 9.7553e-03 eta 0:13:44
epoch [4/30] batch [94/96] time 0.299 (0.329) data 0.000 (0.007) loss 1.7047 (1.6106) lr 9.7553e-03 eta 0:13:42
epoch [4/30] batch [96/96] time 0.307 (0.329) data 0.000 (0.007) loss 1.5311 (1.6133) lr 9.5677e-03 eta 0:13:40
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.85s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 373
* accuracy: 64.8%
* error: 35.2%
* macro_f1: 62.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [5/30] batch [2/96] time 0.303 (0.656) data 0.000 (0.308) loss 1.1683 (1.2698) lr 9.5677e-03 eta 0:27:15
epoch [5/30] batch [4/96] time 0.348 (0.498) data 0.000 (0.154) loss 1.0202 (1.1930) lr 9.5677e-03 eta 0:20:42
epoch [5/30] batch [6/96] time 0.351 (0.448) data 0.000 (0.103) loss 1.5096 (1.4610) lr 9.5677e-03 eta 0:18:34
epoch [5/30] batch [8/96] time 0.328 (0.422) data 0.000 (0.077) loss 1.5845 (1.4724) lr 9.5677e-03 eta 0:17:28
epoch [5/30] batch [10/96] time 0.327 (0.403) data 0.000 (0.062) loss 1.4092 (1.4302) lr 9.5677e-03 eta 0:16:40
epoch [5/30] batch [12/96] time 0.334 (0.391) data 0.000 (0.052) loss 1.3139 (1.4195) lr 9.5677e-03 eta 0:16:10
epoch [5/30] batch [14/96] time 0.313 (0.381) data 0.000 (0.044) loss 1.0930 (1.3929) lr 9.5677e-03 eta 0:15:45
epoch [5/30] batch [16/96] time 0.323 (0.375) data 0.000 (0.039) loss 1.9401 (1.4683) lr 9.5677e-03 eta 0:15:29
epoch [5/30] batch [18/96] time 0.328 (0.369) data 0.000 (0.034) loss 1.5747 (1.4711) lr 9.5677e-03 eta 0:15:13
epoch [5/30] batch [20/96] time 0.323 (0.364) data 0.000 (0.031) loss 1.3059 (1.5214) lr 9.5677e-03 eta 0:15:02
epoch [5/30] batch [22/96] time 0.344 (0.362) data 0.000 (0.028) loss 0.9981 (1.4858) lr 9.5677e-03 eta 0:14:54
epoch [5/30] batch [24/96] time 0.323 (0.359) data 0.000 (0.026) loss 1.4181 (1.5063) lr 9.5677e-03 eta 0:14:46
epoch [5/30] batch [26/96] time 0.318 (0.356) data 0.000 (0.024) loss 2.2116 (1.5086) lr 9.5677e-03 eta 0:14:39
epoch [5/30] batch [28/96] time 0.322 (0.354) data 0.000 (0.022) loss 1.3121 (1.5015) lr 9.5677e-03 eta 0:14:32
epoch [5/30] batch [30/96] time 0.329 (0.352) data 0.000 (0.021) loss 1.8034 (1.4925) lr 9.5677e-03 eta 0:14:27
epoch [5/30] batch [32/96] time 0.331 (0.350) data 0.000 (0.020) loss 0.7529 (1.4656) lr 9.5677e-03 eta 0:14:23
epoch [5/30] batch [34/96] time 0.328 (0.351) data 0.000 (0.018) loss 2.0760 (1.4754) lr 9.5677e-03 eta 0:14:24
epoch [5/30] batch [36/96] time 0.315 (0.350) data 0.000 (0.017) loss 0.9832 (1.4812) lr 9.5677e-03 eta 0:14:20
epoch [5/30] batch [38/96] time 0.323 (0.349) data 0.000 (0.017) loss 1.9675 (1.5056) lr 9.5677e-03 eta 0:14:16
epoch [5/30] batch [40/96] time 0.319 (0.347) data 0.000 (0.016) loss 1.2790 (1.5032) lr 9.5677e-03 eta 0:14:13
epoch [5/30] batch [42/96] time 0.322 (0.346) data 0.000 (0.015) loss 1.6468 (1.5037) lr 9.5677e-03 eta 0:14:09
epoch [5/30] batch [44/96] time 0.328 (0.345) data 0.000 (0.014) loss 1.7382 (1.5101) lr 9.5677e-03 eta 0:14:05
epoch [5/30] batch [46/96] time 0.321 (0.344) data 0.000 (0.014) loss 1.3718 (1.5050) lr 9.5677e-03 eta 0:14:02
epoch [5/30] batch [48/96] time 0.323 (0.343) data 0.000 (0.013) loss 2.3591 (1.5438) lr 9.5677e-03 eta 0:13:59
epoch [5/30] batch [50/96] time 0.331 (0.342) data 0.000 (0.013) loss 2.0525 (1.5611) lr 9.5677e-03 eta 0:13:57
epoch [5/30] batch [52/96] time 0.329 (0.342) data 0.000 (0.012) loss 1.6593 (1.5772) lr 9.5677e-03 eta 0:13:54
epoch [5/30] batch [54/96] time 0.326 (0.341) data 0.000 (0.012) loss 1.1773 (1.5600) lr 9.5677e-03 eta 0:13:52
epoch [5/30] batch [56/96] time 0.325 (0.340) data 0.000 (0.011) loss 1.8511 (1.5598) lr 9.5677e-03 eta 0:13:50
epoch [5/30] batch [58/96] time 0.329 (0.340) data 0.000 (0.011) loss 1.3494 (1.5500) lr 9.5677e-03 eta 0:13:49
epoch [5/30] batch [60/96] time 0.319 (0.340) data 0.000 (0.011) loss 1.5413 (1.5499) lr 9.5677e-03 eta 0:13:47
epoch [5/30] batch [62/96] time 0.322 (0.339) data 0.000 (0.010) loss 1.8694 (1.5474) lr 9.5677e-03 eta 0:13:45
epoch [5/30] batch [64/96] time 0.320 (0.338) data 0.000 (0.010) loss 2.8275 (1.5533) lr 9.5677e-03 eta 0:13:43
epoch [5/30] batch [66/96] time 0.324 (0.338) data 0.000 (0.010) loss 1.9245 (1.5582) lr 9.5677e-03 eta 0:13:41
epoch [5/30] batch [68/96] time 0.328 (0.338) data 0.000 (0.009) loss 1.5368 (1.5565) lr 9.5677e-03 eta 0:13:39
epoch [5/30] batch [70/96] time 0.324 (0.337) data 0.000 (0.009) loss 1.4556 (1.5522) lr 9.5677e-03 eta 0:13:38
epoch [5/30] batch [72/96] time 0.334 (0.337) data 0.000 (0.009) loss 1.5242 (1.5538) lr 9.5677e-03 eta 0:13:37
epoch [5/30] batch [74/96] time 0.306 (0.336) data 0.000 (0.009) loss 1.5978 (1.5592) lr 9.5677e-03 eta 0:13:34
epoch [5/30] batch [76/96] time 0.304 (0.335) data 0.000 (0.008) loss 1.1468 (1.5527) lr 9.5677e-03 eta 0:13:31
epoch [5/30] batch [78/96] time 0.303 (0.335) data 0.000 (0.008) loss 0.9353 (1.5435) lr 9.5677e-03 eta 0:13:29
epoch [5/30] batch [80/96] time 0.312 (0.334) data 0.000 (0.008) loss 1.2571 (1.5396) lr 9.5677e-03 eta 0:13:26
epoch [5/30] batch [82/96] time 0.307 (0.333) data 0.000 (0.008) loss 0.9005 (1.5304) lr 9.5677e-03 eta 0:13:24
epoch [5/30] batch [84/96] time 0.310 (0.333) data 0.000 (0.008) loss 1.1958 (1.5211) lr 9.5677e-03 eta 0:13:22
epoch [5/30] batch [86/96] time 0.313 (0.332) data 0.000 (0.007) loss 2.6763 (1.5359) lr 9.5677e-03 eta 0:13:20
epoch [5/30] batch [88/96] time 0.303 (0.331) data 0.000 (0.007) loss 2.2133 (1.5484) lr 9.5677e-03 eta 0:13:18
epoch [5/30] batch [90/96] time 0.313 (0.331) data 0.000 (0.007) loss 1.0286 (1.5447) lr 9.5677e-03 eta 0:13:16
epoch [5/30] batch [92/96] time 0.308 (0.330) data 0.000 (0.007) loss 1.8442 (1.5569) lr 9.5677e-03 eta 0:13:14
epoch [5/30] batch [94/96] time 0.308 (0.330) data 0.000 (0.007) loss 1.4677 (1.5558) lr 9.5677e-03 eta 0:13:12
epoch [5/30] batch [96/96] time 0.303 (0.329) data 0.000 (0.007) loss 0.8960 (1.5484) lr 9.3301e-03 eta 0:13:10
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.85s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 384
* accuracy: 66.7%
* error: 33.3%
* macro_f1: 64.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [6/30] batch [2/96] time 0.357 (0.659) data 0.001 (0.281) loss 1.1858 (1.4006) lr 9.3301e-03 eta 0:26:20
epoch [6/30] batch [4/96] time 0.333 (0.495) data 0.000 (0.141) loss 1.6296 (1.4264) lr 9.3301e-03 eta 0:19:44
epoch [6/30] batch [6/96] time 0.340 (0.443) data 0.000 (0.094) loss 1.4485 (1.4660) lr 9.3301e-03 eta 0:17:39
epoch [6/30] batch [8/96] time 0.333 (0.416) data 0.000 (0.071) loss 1.0204 (1.3601) lr 9.3301e-03 eta 0:16:33
epoch [6/30] batch [10/96] time 0.340 (0.400) data 0.000 (0.057) loss 1.8833 (1.3775) lr 9.3301e-03 eta 0:15:56
epoch [6/30] batch [12/96] time 0.341 (0.390) data 0.000 (0.047) loss 1.1619 (1.3966) lr 9.3301e-03 eta 0:15:32
epoch [6/30] batch [14/96] time 0.334 (0.383) data 0.001 (0.041) loss 1.5630 (1.4005) lr 9.3301e-03 eta 0:15:13
epoch [6/30] batch [16/96] time 0.340 (0.377) data 0.000 (0.036) loss 1.2436 (1.3810) lr 9.3301e-03 eta 0:14:59
epoch [6/30] batch [18/96] time 0.333 (0.373) data 0.000 (0.032) loss 1.2926 (1.4781) lr 9.3301e-03 eta 0:14:47
epoch [6/30] batch [20/96] time 0.338 (0.369) data 0.000 (0.029) loss 2.4307 (1.5198) lr 9.3301e-03 eta 0:14:38
epoch [6/30] batch [22/96] time 0.346 (0.367) data 0.000 (0.026) loss 1.6407 (1.5398) lr 9.3301e-03 eta 0:14:32
epoch [6/30] batch [24/96] time 0.337 (0.364) data 0.000 (0.024) loss 1.6829 (1.5450) lr 9.3301e-03 eta 0:14:25
epoch [6/30] batch [26/96] time 0.334 (0.362) data 0.001 (0.022) loss 1.1199 (1.5068) lr 9.3301e-03 eta 0:14:19
epoch [6/30] batch [28/96] time 0.343 (0.360) data 0.000 (0.020) loss 2.4630 (1.5911) lr 9.3301e-03 eta 0:14:14
epoch [6/30] batch [30/96] time 0.337 (0.358) data 0.000 (0.019) loss 1.8358 (1.5967) lr 9.3301e-03 eta 0:14:09
epoch [6/30] batch [32/96] time 0.341 (0.357) data 0.000 (0.018) loss 2.0570 (1.6000) lr 9.3301e-03 eta 0:14:06
epoch [6/30] batch [34/96] time 0.333 (0.356) data 0.000 (0.017) loss 1.1810 (1.5983) lr 9.3301e-03 eta 0:14:01
epoch [6/30] batch [36/96] time 0.417 (0.357) data 0.000 (0.016) loss 0.9367 (1.5558) lr 9.3301e-03 eta 0:14:03
epoch [6/30] batch [38/96] time 0.333 (0.356) data 0.000 (0.015) loss 1.4852 (1.5494) lr 9.3301e-03 eta 0:14:00
epoch [6/30] batch [40/96] time 0.333 (0.354) data 0.000 (0.014) loss 2.0823 (1.5581) lr 9.3301e-03 eta 0:13:56
epoch [6/30] batch [42/96] time 0.320 (0.353) data 0.000 (0.014) loss 1.5871 (1.5615) lr 9.3301e-03 eta 0:13:51
epoch [6/30] batch [44/96] time 0.315 (0.351) data 0.000 (0.013) loss 1.4056 (1.5525) lr 9.3301e-03 eta 0:13:47
epoch [6/30] batch [46/96] time 0.333 (0.350) data 0.000 (0.013) loss 0.9575 (1.5285) lr 9.3301e-03 eta 0:13:44
epoch [6/30] batch [48/96] time 0.332 (0.349) data 0.000 (0.012) loss 0.9748 (1.5227) lr 9.3301e-03 eta 0:13:41
epoch [6/30] batch [50/96] time 0.330 (0.348) data 0.000 (0.012) loss 1.7238 (1.5280) lr 9.3301e-03 eta 0:13:38
epoch [6/30] batch [52/96] time 0.332 (0.348) data 0.000 (0.011) loss 1.8936 (1.5299) lr 9.3301e-03 eta 0:13:36
epoch [6/30] batch [54/96] time 0.338 (0.347) data 0.000 (0.011) loss 0.9599 (1.5137) lr 9.3301e-03 eta 0:13:34
epoch [6/30] batch [56/96] time 0.346 (0.347) data 0.000 (0.010) loss 2.3988 (1.5336) lr 9.3301e-03 eta 0:13:32
epoch [6/30] batch [58/96] time 0.328 (0.346) data 0.000 (0.010) loss 1.1804 (1.5268) lr 9.3301e-03 eta 0:13:30
epoch [6/30] batch [60/96] time 0.331 (0.345) data 0.000 (0.010) loss 1.4572 (1.5203) lr 9.3301e-03 eta 0:13:28
epoch [6/30] batch [62/96] time 0.324 (0.345) data 0.000 (0.009) loss 1.0527 (1.5021) lr 9.3301e-03 eta 0:13:25
epoch [6/30] batch [64/96] time 0.318 (0.344) data 0.001 (0.009) loss 1.5145 (1.5127) lr 9.3301e-03 eta 0:13:23
epoch [6/30] batch [66/96] time 0.324 (0.343) data 0.000 (0.009) loss 2.0186 (1.5338) lr 9.3301e-03 eta 0:13:21
epoch [6/30] batch [68/96] time 0.326 (0.343) data 0.000 (0.009) loss 1.3486 (1.5200) lr 9.3301e-03 eta 0:13:19
epoch [6/30] batch [70/96] time 0.335 (0.343) data 0.000 (0.008) loss 1.3303 (1.5246) lr 9.3301e-03 eta 0:13:18
epoch [6/30] batch [72/96] time 0.327 (0.342) data 0.000 (0.008) loss 1.1135 (1.5119) lr 9.3301e-03 eta 0:13:16
epoch [6/30] batch [74/96] time 0.312 (0.341) data 0.000 (0.008) loss 1.2568 (1.5087) lr 9.3301e-03 eta 0:13:13
epoch [6/30] batch [76/96] time 0.326 (0.341) data 0.000 (0.008) loss 0.9409 (1.4977) lr 9.3301e-03 eta 0:13:12
epoch [6/30] batch [78/96] time 0.320 (0.340) data 0.000 (0.008) loss 2.8179 (1.5201) lr 9.3301e-03 eta 0:13:10
epoch [6/30] batch [80/96] time 0.325 (0.340) data 0.000 (0.007) loss 1.7078 (1.5140) lr 9.3301e-03 eta 0:13:08
epoch [6/30] batch [82/96] time 0.336 (0.340) data 0.000 (0.007) loss 1.8059 (1.5329) lr 9.3301e-03 eta 0:13:07
epoch [6/30] batch [84/96] time 0.340 (0.340) data 0.000 (0.007) loss 1.5779 (1.5336) lr 9.3301e-03 eta 0:13:06
epoch [6/30] batch [86/96] time 0.319 (0.339) data 0.000 (0.007) loss 2.2599 (1.5418) lr 9.3301e-03 eta 0:13:05
epoch [6/30] batch [88/96] time 0.322 (0.339) data 0.000 (0.007) loss 1.8237 (1.5457) lr 9.3301e-03 eta 0:13:03
epoch [6/30] batch [90/96] time 0.323 (0.339) data 0.000 (0.007) loss 1.4375 (1.5533) lr 9.3301e-03 eta 0:13:02
epoch [6/30] batch [92/96] time 0.328 (0.338) data 0.000 (0.006) loss 1.2400 (1.5627) lr 9.3301e-03 eta 0:13:00
epoch [6/30] batch [94/96] time 0.324 (0.338) data 0.000 (0.006) loss 1.9619 (1.5680) lr 9.3301e-03 eta 0:12:59
epoch [6/30] batch [96/96] time 0.311 (0.338) data 0.000 (0.006) loss 1.8525 (1.5620) lr 9.0451e-03 eta 0:12:57
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.84s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 390
* accuracy: 67.7%
* error: 32.3%
* macro_f1: 65.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [7/30] batch [2/96] time 0.350 (0.681) data 0.000 (0.310) loss 1.3899 (1.4009) lr 9.0451e-03 eta 0:26:08
epoch [7/30] batch [4/96] time 0.332 (0.510) data 0.000 (0.155) loss 1.7841 (1.5563) lr 9.0451e-03 eta 0:19:33
epoch [7/30] batch [6/96] time 0.352 (0.455) data 0.000 (0.103) loss 1.1153 (1.4342) lr 9.0451e-03 eta 0:17:25
epoch [7/30] batch [8/96] time 0.341 (0.426) data 0.000 (0.078) loss 1.3106 (1.4117) lr 9.0451e-03 eta 0:16:18
epoch [7/30] batch [10/96] time 0.334 (0.408) data 0.000 (0.062) loss 1.2727 (1.3445) lr 9.0451e-03 eta 0:15:35
epoch [7/30] batch [12/96] time 0.377 (0.399) data 0.000 (0.052) loss 0.9730 (1.2892) lr 9.0451e-03 eta 0:15:13
epoch [7/30] batch [14/96] time 0.346 (0.390) data 0.000 (0.044) loss 2.5844 (1.4401) lr 9.0451e-03 eta 0:14:53
epoch [7/30] batch [16/96] time 0.346 (0.385) data 0.000 (0.039) loss 1.1032 (1.4429) lr 9.0451e-03 eta 0:14:41
epoch [7/30] batch [18/96] time 0.345 (0.382) data 0.000 (0.035) loss 2.0577 (1.4527) lr 9.0451e-03 eta 0:14:33
epoch [7/30] batch [20/96] time 0.348 (0.379) data 0.000 (0.031) loss 1.2379 (1.4460) lr 9.0451e-03 eta 0:14:24
epoch [7/30] batch [22/96] time 0.339 (0.375) data 0.000 (0.028) loss 1.2656 (1.4110) lr 9.0451e-03 eta 0:14:15
epoch [7/30] batch [24/96] time 0.346 (0.372) data 0.000 (0.026) loss 2.3942 (1.4420) lr 9.0451e-03 eta 0:14:08
epoch [7/30] batch [26/96] time 0.326 (0.369) data 0.000 (0.024) loss 0.7802 (1.4141) lr 9.0451e-03 eta 0:14:01
epoch [7/30] batch [28/96] time 0.323 (0.366) data 0.000 (0.022) loss 1.0967 (1.4058) lr 9.0451e-03 eta 0:13:53
epoch [7/30] batch [30/96] time 0.324 (0.363) data 0.000 (0.021) loss 1.1040 (1.3901) lr 9.0451e-03 eta 0:13:46
epoch [7/30] batch [32/96] time 0.329 (0.361) data 0.000 (0.020) loss 1.0439 (1.3672) lr 9.0451e-03 eta 0:13:40
epoch [7/30] batch [34/96] time 0.328 (0.359) data 0.000 (0.019) loss 1.5794 (1.3705) lr 9.0451e-03 eta 0:13:35
epoch [7/30] batch [36/96] time 0.438 (0.361) data 0.000 (0.017) loss 2.8507 (1.4410) lr 9.0451e-03 eta 0:13:38
epoch [7/30] batch [38/96] time 0.322 (0.359) data 0.000 (0.017) loss 1.1104 (1.4478) lr 9.0451e-03 eta 0:13:33
epoch [7/30] batch [40/96] time 0.321 (0.357) data 0.000 (0.016) loss 0.9770 (1.4474) lr 9.0451e-03 eta 0:13:28
epoch [7/30] batch [42/96] time 0.324 (0.356) data 0.000 (0.015) loss 1.0280 (1.4330) lr 9.0451e-03 eta 0:13:24
epoch [7/30] batch [44/96] time 0.326 (0.355) data 0.000 (0.014) loss 1.4167 (1.4357) lr 9.0451e-03 eta 0:13:21
epoch [7/30] batch [46/96] time 0.325 (0.353) data 0.001 (0.014) loss 1.6496 (1.4549) lr 9.0451e-03 eta 0:13:18
epoch [7/30] batch [48/96] time 0.341 (0.353) data 0.000 (0.013) loss 0.9589 (1.4484) lr 9.0451e-03 eta 0:13:15
epoch [7/30] batch [50/96] time 0.332 (0.352) data 0.001 (0.013) loss 0.7768 (1.4318) lr 9.0451e-03 eta 0:13:13
epoch [7/30] batch [52/96] time 0.331 (0.351) data 0.000 (0.012) loss 0.9261 (1.4388) lr 9.0451e-03 eta 0:13:11
epoch [7/30] batch [54/96] time 0.329 (0.351) data 0.000 (0.012) loss 1.1963 (1.4325) lr 9.0451e-03 eta 0:13:08
epoch [7/30] batch [56/96] time 0.317 (0.349) data 0.000 (0.011) loss 2.7012 (1.4685) lr 9.0451e-03 eta 0:13:05
epoch [7/30] batch [58/96] time 0.346 (0.349) data 0.000 (0.011) loss 1.1311 (1.4586) lr 9.0451e-03 eta 0:13:04
epoch [7/30] batch [60/96] time 0.352 (0.349) data 0.000 (0.011) loss 1.9116 (1.4723) lr 9.0451e-03 eta 0:13:03
epoch [7/30] batch [62/96] time 0.320 (0.348) data 0.000 (0.010) loss 1.4172 (1.4650) lr 9.0451e-03 eta 0:13:00
epoch [7/30] batch [64/96] time 0.323 (0.348) data 0.000 (0.010) loss 1.4882 (1.4923) lr 9.0451e-03 eta 0:12:58
epoch [7/30] batch [66/96] time 0.325 (0.347) data 0.000 (0.010) loss 1.3586 (1.4876) lr 9.0451e-03 eta 0:12:56
epoch [7/30] batch [68/96] time 0.325 (0.346) data 0.001 (0.009) loss 1.2238 (1.4945) lr 9.0451e-03 eta 0:12:54
epoch [7/30] batch [70/96] time 0.332 (0.346) data 0.000 (0.009) loss 1.6597 (1.4883) lr 9.0451e-03 eta 0:12:52
epoch [7/30] batch [72/96] time 0.331 (0.345) data 0.000 (0.009) loss 1.8007 (1.4868) lr 9.0451e-03 eta 0:12:50
epoch [7/30] batch [74/96] time 0.311 (0.344) data 0.000 (0.009) loss 1.0277 (1.4774) lr 9.0451e-03 eta 0:12:48
epoch [7/30] batch [76/96] time 0.312 (0.344) data 0.000 (0.008) loss 1.2658 (1.4798) lr 9.0451e-03 eta 0:12:45
epoch [7/30] batch [78/96] time 0.324 (0.343) data 0.000 (0.008) loss 1.8978 (1.4901) lr 9.0451e-03 eta 0:12:43
epoch [7/30] batch [80/96] time 0.323 (0.343) data 0.000 (0.008) loss 2.5182 (1.5078) lr 9.0451e-03 eta 0:12:41
epoch [7/30] batch [82/96] time 0.322 (0.342) data 0.000 (0.008) loss 1.4946 (1.5068) lr 9.0451e-03 eta 0:12:40
epoch [7/30] batch [84/96] time 0.323 (0.342) data 0.000 (0.008) loss 1.4593 (1.5016) lr 9.0451e-03 eta 0:12:38
epoch [7/30] batch [86/96] time 0.309 (0.341) data 0.000 (0.008) loss 1.0992 (1.4891) lr 9.0451e-03 eta 0:12:35
epoch [7/30] batch [88/96] time 0.307 (0.340) data 0.000 (0.007) loss 1.3339 (1.4808) lr 9.0451e-03 eta 0:12:33
epoch [7/30] batch [90/96] time 0.309 (0.339) data 0.000 (0.007) loss 1.0883 (1.4759) lr 9.0451e-03 eta 0:12:31
epoch [7/30] batch [92/96] time 0.308 (0.339) data 0.000 (0.007) loss 2.2376 (1.4798) lr 9.0451e-03 eta 0:12:29
epoch [7/30] batch [94/96] time 0.313 (0.338) data 0.000 (0.007) loss 1.6005 (1.4803) lr 9.0451e-03 eta 0:12:27
epoch [7/30] batch [96/96] time 0.305 (0.338) data 0.000 (0.007) loss 0.9904 (1.4775) lr 8.7157e-03 eta 0:12:25
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.89s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 405
* accuracy: 70.3%
* error: 29.7%
* macro_f1: 68.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [8/30] batch [2/96] time 0.321 (0.666) data 0.001 (0.301) loss 1.3969 (1.5031) lr 8.7157e-03 eta 0:24:28
epoch [8/30] batch [4/96] time 0.340 (0.501) data 0.000 (0.151) loss 1.2361 (1.3501) lr 8.7157e-03 eta 0:18:23
epoch [8/30] batch [6/96] time 0.344 (0.448) data 0.000 (0.101) loss 1.3939 (1.3273) lr 8.7157e-03 eta 0:16:26
epoch [8/30] batch [8/96] time 0.354 (0.422) data 0.000 (0.076) loss 1.0245 (1.2664) lr 8.7157e-03 eta 0:15:28
epoch [8/30] batch [10/96] time 0.335 (0.403) data 0.000 (0.060) loss 1.5972 (1.3022) lr 8.7157e-03 eta 0:14:46
epoch [8/30] batch [12/96] time 0.324 (0.391) data 0.000 (0.050) loss 1.5674 (1.3652) lr 8.7157e-03 eta 0:14:19
epoch [8/30] batch [14/96] time 0.326 (0.382) data 0.000 (0.043) loss 1.9811 (1.4581) lr 8.7157e-03 eta 0:13:58
epoch [8/30] batch [16/96] time 0.317 (0.374) data 0.000 (0.038) loss 1.0587 (1.4107) lr 8.7157e-03 eta 0:13:40
epoch [8/30] batch [18/96] time 0.328 (0.370) data 0.000 (0.034) loss 1.1484 (1.4247) lr 8.7157e-03 eta 0:13:31
epoch [8/30] batch [20/96] time 0.326 (0.366) data 0.000 (0.030) loss 1.1856 (1.4754) lr 8.7157e-03 eta 0:13:20
epoch [8/30] batch [22/96] time 0.326 (0.362) data 0.000 (0.028) loss 1.9885 (1.4719) lr 8.7157e-03 eta 0:13:11
epoch [8/30] batch [24/96] time 0.321 (0.359) data 0.000 (0.025) loss 1.8730 (1.5054) lr 8.7157e-03 eta 0:13:04
epoch [8/30] batch [26/96] time 0.328 (0.356) data 0.000 (0.023) loss 1.0818 (1.5012) lr 8.7157e-03 eta 0:12:57
epoch [8/30] batch [28/96] time 0.319 (0.354) data 0.000 (0.022) loss 1.9254 (1.4968) lr 8.7157e-03 eta 0:12:52
epoch [8/30] batch [30/96] time 0.327 (0.352) data 0.000 (0.020) loss 1.0509 (1.4860) lr 8.7157e-03 eta 0:12:47
epoch [8/30] batch [32/96] time 0.324 (0.350) data 0.000 (0.019) loss 0.7266 (1.4479) lr 8.7157e-03 eta 0:12:42
epoch [8/30] batch [34/96] time 0.327 (0.349) data 0.000 (0.018) loss 1.8007 (1.4890) lr 8.7157e-03 eta 0:12:38
epoch [8/30] batch [36/96] time 0.418 (0.350) data 0.000 (0.017) loss 1.0926 (1.4787) lr 8.7157e-03 eta 0:12:40
epoch [8/30] batch [38/96] time 0.324 (0.349) data 0.000 (0.016) loss 1.7441 (1.4822) lr 8.7157e-03 eta 0:12:36
epoch [8/30] batch [40/96] time 0.323 (0.348) data 0.000 (0.015) loss 1.9837 (1.4992) lr 8.7157e-03 eta 0:12:33
epoch [8/30] batch [42/96] time 0.326 (0.346) data 0.000 (0.015) loss 1.2242 (1.4916) lr 8.7157e-03 eta 0:12:30
epoch [8/30] batch [44/96] time 0.336 (0.346) data 0.000 (0.014) loss 1.5964 (1.4884) lr 8.7157e-03 eta 0:12:29
epoch [8/30] batch [46/96] time 0.327 (0.346) data 0.000 (0.013) loss 1.0254 (1.4939) lr 8.7157e-03 eta 0:12:27
epoch [8/30] batch [48/96] time 0.317 (0.345) data 0.000 (0.013) loss 1.1367 (1.4840) lr 8.7157e-03 eta 0:12:24
epoch [8/30] batch [50/96] time 0.347 (0.344) data 0.000 (0.012) loss 1.6533 (1.5035) lr 8.7157e-03 eta 0:12:23
epoch [8/30] batch [52/96] time 0.321 (0.344) data 0.000 (0.012) loss 1.4580 (1.5033) lr 8.7157e-03 eta 0:12:21
epoch [8/30] batch [54/96] time 0.321 (0.343) data 0.000 (0.011) loss 1.0614 (1.4897) lr 8.7157e-03 eta 0:12:19
epoch [8/30] batch [56/96] time 0.328 (0.343) data 0.000 (0.011) loss 1.0356 (1.4880) lr 8.7157e-03 eta 0:12:17
epoch [8/30] batch [58/96] time 0.322 (0.342) data 0.000 (0.011) loss 1.4396 (1.4847) lr 8.7157e-03 eta 0:12:15
epoch [8/30] batch [60/96] time 0.324 (0.341) data 0.001 (0.010) loss 1.0076 (1.4696) lr 8.7157e-03 eta 0:12:13
epoch [8/30] batch [62/96] time 0.330 (0.341) data 0.000 (0.010) loss 1.4862 (1.4728) lr 8.7157e-03 eta 0:12:12
epoch [8/30] batch [64/96] time 0.320 (0.341) data 0.000 (0.010) loss 1.5402 (1.4812) lr 8.7157e-03 eta 0:12:10
epoch [8/30] batch [66/96] time 0.326 (0.340) data 0.000 (0.009) loss 1.6668 (1.4722) lr 8.7157e-03 eta 0:12:08
epoch [8/30] batch [68/96] time 0.337 (0.340) data 0.000 (0.009) loss 1.4021 (1.4605) lr 8.7157e-03 eta 0:12:07
epoch [8/30] batch [70/96] time 0.318 (0.339) data 0.000 (0.009) loss 1.2469 (1.4585) lr 8.7157e-03 eta 0:12:05
epoch [8/30] batch [72/96] time 0.329 (0.339) data 0.000 (0.009) loss 1.1105 (1.4585) lr 8.7157e-03 eta 0:12:04
epoch [8/30] batch [74/96] time 0.310 (0.338) data 0.000 (0.008) loss 0.7651 (1.4482) lr 8.7157e-03 eta 0:12:01
epoch [8/30] batch [76/96] time 0.312 (0.338) data 0.000 (0.008) loss 1.2862 (1.4369) lr 8.7157e-03 eta 0:11:59
epoch [8/30] batch [78/96] time 0.314 (0.337) data 0.000 (0.008) loss 1.1537 (1.4391) lr 8.7157e-03 eta 0:11:57
epoch [8/30] batch [80/96] time 0.303 (0.336) data 0.000 (0.008) loss 1.1590 (1.4372) lr 8.7157e-03 eta 0:11:55
epoch [8/30] batch [82/96] time 0.307 (0.335) data 0.000 (0.008) loss 2.5793 (1.4469) lr 8.7157e-03 eta 0:11:53
epoch [8/30] batch [84/96] time 0.307 (0.335) data 0.000 (0.007) loss 1.4754 (1.4477) lr 8.7157e-03 eta 0:11:50
epoch [8/30] batch [86/96] time 0.307 (0.334) data 0.000 (0.007) loss 1.4788 (1.4509) lr 8.7157e-03 eta 0:11:49
epoch [8/30] batch [88/96] time 0.309 (0.334) data 0.000 (0.007) loss 1.5574 (1.4538) lr 8.7157e-03 eta 0:11:47
epoch [8/30] batch [90/96] time 0.307 (0.333) data 0.000 (0.007) loss 1.0158 (1.4528) lr 8.7157e-03 eta 0:11:45
epoch [8/30] batch [92/96] time 0.306 (0.332) data 0.000 (0.007) loss 1.4089 (1.4550) lr 8.7157e-03 eta 0:11:43
epoch [8/30] batch [94/96] time 0.310 (0.332) data 0.000 (0.007) loss 1.6515 (1.4645) lr 8.7157e-03 eta 0:11:41
epoch [8/30] batch [96/96] time 0.305 (0.331) data 0.000 (0.007) loss 1.6867 (1.4655) lr 8.3457e-03 eta 0:11:39
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.87s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.19s/it]=> result
* total: 576
* correct: 409
* accuracy: 71.0%
* error: 29.0%
* macro_f1: 69.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [9/30] batch [2/96] time 0.316 (0.656) data 0.001 (0.279) loss 1.9118 (1.6850) lr 8.3457e-03 eta 0:23:04
epoch [9/30] batch [4/96] time 0.322 (0.486) data 0.000 (0.140) loss 2.4695 (1.8479) lr 8.3457e-03 eta 0:17:03
epoch [9/30] batch [6/96] time 0.331 (0.435) data 0.000 (0.093) loss 1.3406 (1.7341) lr 8.3457e-03 eta 0:15:15
epoch [9/30] batch [8/96] time 0.329 (0.409) data 0.000 (0.070) loss 1.6127 (1.6489) lr 8.3457e-03 eta 0:14:21
epoch [9/30] batch [10/96] time 0.313 (0.391) data 0.000 (0.056) loss 1.1922 (1.5101) lr 8.3457e-03 eta 0:13:42
epoch [9/30] batch [12/96] time 0.323 (0.380) data 0.000 (0.047) loss 1.3447 (1.5867) lr 8.3457e-03 eta 0:13:18
epoch [9/30] batch [14/96] time 0.332 (0.373) data 0.000 (0.040) loss 1.3977 (1.5322) lr 8.3457e-03 eta 0:13:01
epoch [9/30] batch [16/96] time 0.312 (0.366) data 0.000 (0.035) loss 1.7958 (1.5183) lr 8.3457e-03 eta 0:12:47
epoch [9/30] batch [18/96] time 0.332 (0.362) data 0.000 (0.031) loss 1.3285 (1.5037) lr 8.3457e-03 eta 0:12:37
epoch [9/30] batch [20/96] time 0.324 (0.358) data 0.000 (0.028) loss 1.3458 (1.4906) lr 8.3457e-03 eta 0:12:29
epoch [9/30] batch [22/96] time 0.317 (0.355) data 0.000 (0.026) loss 1.2103 (1.4686) lr 8.3457e-03 eta 0:12:21
epoch [9/30] batch [24/96] time 0.329 (0.353) data 0.000 (0.024) loss 1.0478 (1.4735) lr 8.3457e-03 eta 0:12:17
epoch [9/30] batch [26/96] time 0.318 (0.351) data 0.000 (0.022) loss 1.1841 (1.4800) lr 8.3457e-03 eta 0:12:11
epoch [9/30] batch [28/96] time 0.333 (0.349) data 0.000 (0.020) loss 1.3729 (1.4680) lr 8.3457e-03 eta 0:12:08
epoch [9/30] batch [30/96] time 0.331 (0.348) data 0.000 (0.019) loss 1.7721 (1.5050) lr 8.3457e-03 eta 0:12:05
epoch [9/30] batch [32/96] time 0.323 (0.347) data 0.000 (0.018) loss 1.8392 (1.5046) lr 8.3457e-03 eta 0:12:00
epoch [9/30] batch [34/96] time 0.318 (0.345) data 0.000 (0.017) loss 1.8393 (1.4966) lr 8.3457e-03 eta 0:11:57
epoch [9/30] batch [36/96] time 0.432 (0.347) data 0.000 (0.016) loss 1.4611 (1.4948) lr 8.3457e-03 eta 0:12:00
epoch [9/30] batch [38/96] time 0.314 (0.345) data 0.000 (0.015) loss 1.1789 (1.4891) lr 8.3457e-03 eta 0:11:56
epoch [9/30] batch [40/96] time 0.316 (0.344) data 0.000 (0.014) loss 1.5214 (1.4714) lr 8.3457e-03 eta 0:11:53
epoch [9/30] batch [42/96] time 0.315 (0.343) data 0.000 (0.014) loss 1.5373 (1.4933) lr 8.3457e-03 eta 0:11:49
epoch [9/30] batch [44/96] time 0.316 (0.342) data 0.000 (0.013) loss 1.1316 (1.4818) lr 8.3457e-03 eta 0:11:46
epoch [9/30] batch [46/96] time 0.324 (0.341) data 0.000 (0.012) loss 0.9373 (1.4825) lr 8.3457e-03 eta 0:11:44
epoch [9/30] batch [48/96] time 0.325 (0.341) data 0.000 (0.012) loss 2.7682 (1.5063) lr 8.3457e-03 eta 0:11:42
epoch [9/30] batch [50/96] time 0.320 (0.340) data 0.000 (0.011) loss 1.3019 (1.4954) lr 8.3457e-03 eta 0:11:41
epoch [9/30] batch [52/96] time 0.326 (0.339) data 0.000 (0.011) loss 2.4178 (1.5146) lr 8.3457e-03 eta 0:11:39
epoch [9/30] batch [54/96] time 0.322 (0.339) data 0.000 (0.011) loss 1.2852 (1.5065) lr 8.3457e-03 eta 0:11:37
epoch [9/30] batch [56/96] time 0.317 (0.338) data 0.000 (0.010) loss 1.4134 (1.5103) lr 8.3457e-03 eta 0:11:35
epoch [9/30] batch [58/96] time 0.320 (0.338) data 0.000 (0.010) loss 1.4579 (1.5018) lr 8.3457e-03 eta 0:11:33
epoch [9/30] batch [60/96] time 0.318 (0.337) data 0.000 (0.010) loss 0.7956 (1.4926) lr 8.3457e-03 eta 0:11:31
epoch [9/30] batch [62/96] time 0.315 (0.337) data 0.000 (0.009) loss 1.2175 (1.4916) lr 8.3457e-03 eta 0:11:30
epoch [9/30] batch [64/96] time 0.336 (0.336) data 0.000 (0.009) loss 2.3371 (1.5018) lr 8.3457e-03 eta 0:11:28
epoch [9/30] batch [66/96] time 0.319 (0.336) data 0.000 (0.009) loss 1.6308 (1.5022) lr 8.3457e-03 eta 0:11:27
epoch [9/30] batch [68/96] time 0.332 (0.336) data 0.000 (0.008) loss 0.9270 (1.5023) lr 8.3457e-03 eta 0:11:26
epoch [9/30] batch [70/96] time 0.334 (0.336) data 0.000 (0.008) loss 1.6960 (1.5051) lr 8.3457e-03 eta 0:11:25
epoch [9/30] batch [72/96] time 0.323 (0.336) data 0.000 (0.008) loss 1.5703 (1.5021) lr 8.3457e-03 eta 0:11:24
epoch [9/30] batch [74/96] time 0.315 (0.335) data 0.000 (0.008) loss 1.5719 (1.5012) lr 8.3457e-03 eta 0:11:22
epoch [9/30] batch [76/96] time 0.314 (0.335) data 0.000 (0.008) loss 1.0936 (1.4940) lr 8.3457e-03 eta 0:11:21
epoch [9/30] batch [78/96] time 0.310 (0.334) data 0.000 (0.007) loss 1.4164 (1.4943) lr 8.3457e-03 eta 0:11:19
epoch [9/30] batch [80/96] time 0.321 (0.334) data 0.000 (0.007) loss 1.1795 (1.4954) lr 8.3457e-03 eta 0:11:17
epoch [9/30] batch [82/96] time 0.313 (0.333) data 0.000 (0.007) loss 1.5938 (1.4923) lr 8.3457e-03 eta 0:11:16
epoch [9/30] batch [84/96] time 0.319 (0.333) data 0.000 (0.007) loss 1.5701 (1.4948) lr 8.3457e-03 eta 0:11:14
epoch [9/30] batch [86/96] time 0.315 (0.332) data 0.000 (0.007) loss 1.7181 (1.4946) lr 8.3457e-03 eta 0:11:13
epoch [9/30] batch [88/96] time 0.313 (0.332) data 0.000 (0.007) loss 1.4587 (1.4851) lr 8.3457e-03 eta 0:11:11
epoch [9/30] batch [90/96] time 0.317 (0.332) data 0.000 (0.006) loss 1.4597 (1.4890) lr 8.3457e-03 eta 0:11:10
epoch [9/30] batch [92/96] time 0.319 (0.331) data 0.000 (0.006) loss 1.1875 (1.4973) lr 8.3457e-03 eta 0:11:09
epoch [9/30] batch [94/96] time 0.312 (0.331) data 0.000 (0.006) loss 1.2241 (1.4990) lr 8.3457e-03 eta 0:11:07
epoch [9/30] batch [96/96] time 0.321 (0.331) data 0.000 (0.006) loss 1.0147 (1.4894) lr 7.9389e-03 eta 0:11:06
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.81s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.33s/it]100%|██████████| 3/3 [00:03<00:00,  1.18it/s]100%|██████████| 3/3 [00:03<00:00,  1.16s/it]=> result
* total: 576
* correct: 410
* accuracy: 71.2%
* error: 28.8%
* macro_f1: 69.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [10/30] batch [2/96] time 0.319 (0.669) data 0.000 (0.284) loss 1.6457 (1.4020) lr 7.9389e-03 eta 0:22:27
epoch [10/30] batch [4/96] time 0.321 (0.497) data 0.000 (0.142) loss 1.1869 (1.1722) lr 7.9389e-03 eta 0:16:39
epoch [10/30] batch [6/96] time 0.324 (0.439) data 0.000 (0.095) loss 0.6336 (1.0915) lr 7.9389e-03 eta 0:14:41
epoch [10/30] batch [8/96] time 0.318 (0.408) data 0.000 (0.071) loss 1.4488 (1.1983) lr 7.9389e-03 eta 0:13:40
epoch [10/30] batch [10/96] time 0.330 (0.392) data 0.000 (0.057) loss 1.7357 (1.2566) lr 7.9389e-03 eta 0:13:06
epoch [10/30] batch [12/96] time 0.320 (0.380) data 0.000 (0.048) loss 1.4395 (1.3025) lr 7.9389e-03 eta 0:12:41
epoch [10/30] batch [14/96] time 0.320 (0.372) data 0.000 (0.041) loss 2.0749 (1.3899) lr 7.9389e-03 eta 0:12:24
epoch [10/30] batch [16/96] time 0.332 (0.367) data 0.000 (0.036) loss 1.7734 (1.4066) lr 7.9389e-03 eta 0:12:13
epoch [10/30] batch [18/96] time 0.320 (0.362) data 0.000 (0.032) loss 1.3505 (1.4225) lr 7.9389e-03 eta 0:12:02
epoch [10/30] batch [20/96] time 0.320 (0.358) data 0.000 (0.029) loss 0.9076 (1.4040) lr 7.9389e-03 eta 0:11:54
epoch [10/30] batch [22/96] time 0.345 (0.356) data 0.000 (0.026) loss 1.1163 (1.3619) lr 7.9389e-03 eta 0:11:50
epoch [10/30] batch [24/96] time 0.337 (0.355) data 0.000 (0.024) loss 1.4600 (1.3574) lr 7.9389e-03 eta 0:11:47
epoch [10/30] batch [26/96] time 0.324 (0.353) data 0.000 (0.022) loss 0.7772 (1.3371) lr 7.9389e-03 eta 0:11:41
epoch [10/30] batch [28/96] time 0.325 (0.351) data 0.000 (0.021) loss 1.4567 (1.3634) lr 7.9389e-03 eta 0:11:37
epoch [10/30] batch [30/96] time 0.329 (0.349) data 0.000 (0.019) loss 1.2670 (1.3461) lr 7.9389e-03 eta 0:11:33
epoch [10/30] batch [32/96] time 0.332 (0.348) data 0.000 (0.018) loss 1.4437 (1.3798) lr 7.9389e-03 eta 0:11:30
epoch [10/30] batch [34/96] time 0.339 (0.347) data 0.000 (0.017) loss 2.1161 (1.4097) lr 7.9389e-03 eta 0:11:28
epoch [10/30] batch [36/96] time 0.452 (0.351) data 0.000 (0.016) loss 2.4839 (1.4293) lr 7.9389e-03 eta 0:11:34
epoch [10/30] batch [38/96] time 0.341 (0.350) data 0.000 (0.015) loss 1.0844 (1.4232) lr 7.9389e-03 eta 0:11:32
epoch [10/30] batch [40/96] time 0.336 (0.349) data 0.000 (0.015) loss 0.7139 (1.4191) lr 7.9389e-03 eta 0:11:29
epoch [10/30] batch [42/96] time 0.329 (0.348) data 0.000 (0.014) loss 1.2004 (1.4019) lr 7.9389e-03 eta 0:11:27
epoch [10/30] batch [44/96] time 0.338 (0.348) data 0.000 (0.013) loss 1.4674 (1.3923) lr 7.9389e-03 eta 0:11:25
epoch [10/30] batch [46/96] time 0.325 (0.346) data 0.000 (0.013) loss 1.6162 (1.3981) lr 7.9389e-03 eta 0:11:22
epoch [10/30] batch [48/96] time 0.320 (0.345) data 0.000 (0.012) loss 1.1234 (1.3872) lr 7.9389e-03 eta 0:11:19
epoch [10/30] batch [50/96] time 0.331 (0.345) data 0.001 (0.012) loss 1.6999 (1.3823) lr 7.9389e-03 eta 0:11:17
epoch [10/30] batch [52/96] time 0.326 (0.344) data 0.000 (0.011) loss 1.1679 (1.3837) lr 7.9389e-03 eta 0:11:16
epoch [10/30] batch [54/96] time 0.321 (0.344) data 0.000 (0.011) loss 1.6928 (1.3787) lr 7.9389e-03 eta 0:11:14
epoch [10/30] batch [56/96] time 0.336 (0.343) data 0.000 (0.010) loss 1.0795 (1.3715) lr 7.9389e-03 eta 0:11:12
epoch [10/30] batch [58/96] time 0.332 (0.343) data 0.000 (0.010) loss 1.6212 (1.3747) lr 7.9389e-03 eta 0:11:11
epoch [10/30] batch [60/96] time 0.338 (0.342) data 0.000 (0.010) loss 1.5300 (1.3710) lr 7.9389e-03 eta 0:11:09
epoch [10/30] batch [62/96] time 0.334 (0.342) data 0.000 (0.009) loss 0.9855 (1.3605) lr 7.9389e-03 eta 0:11:09
epoch [10/30] batch [64/96] time 0.339 (0.342) data 0.000 (0.009) loss 1.2570 (1.3597) lr 7.9389e-03 eta 0:11:08
epoch [10/30] batch [66/96] time 0.329 (0.342) data 0.000 (0.009) loss 1.2019 (1.3746) lr 7.9389e-03 eta 0:11:06
epoch [10/30] batch [68/96] time 0.331 (0.341) data 0.000 (0.009) loss 1.0370 (1.3733) lr 7.9389e-03 eta 0:11:04
epoch [10/30] batch [70/96] time 0.331 (0.341) data 0.000 (0.008) loss 1.5917 (1.3762) lr 7.9389e-03 eta 0:11:03
epoch [10/30] batch [72/96] time 0.333 (0.341) data 0.000 (0.008) loss 1.5870 (1.3730) lr 7.9389e-03 eta 0:11:02
epoch [10/30] batch [74/96] time 0.307 (0.340) data 0.000 (0.008) loss 1.3331 (1.3840) lr 7.9389e-03 eta 0:10:59
epoch [10/30] batch [76/96] time 0.309 (0.339) data 0.000 (0.008) loss 0.8048 (1.3792) lr 7.9389e-03 eta 0:10:57
epoch [10/30] batch [78/96] time 0.308 (0.338) data 0.000 (0.008) loss 1.3408 (1.3809) lr 7.9389e-03 eta 0:10:55
epoch [10/30] batch [80/96] time 0.308 (0.337) data 0.000 (0.007) loss 1.9193 (1.3936) lr 7.9389e-03 eta 0:10:53
epoch [10/30] batch [82/96] time 0.307 (0.337) data 0.000 (0.007) loss 1.5068 (1.3929) lr 7.9389e-03 eta 0:10:51
epoch [10/30] batch [84/96] time 0.310 (0.336) data 0.000 (0.007) loss 1.4465 (1.3891) lr 7.9389e-03 eta 0:10:49
epoch [10/30] batch [86/96] time 0.307 (0.335) data 0.000 (0.007) loss 1.7028 (1.3935) lr 7.9389e-03 eta 0:10:47
epoch [10/30] batch [88/96] time 0.307 (0.335) data 0.000 (0.007) loss 1.5793 (1.3919) lr 7.9389e-03 eta 0:10:45
epoch [10/30] batch [90/96] time 0.315 (0.334) data 0.000 (0.007) loss 0.9790 (1.3950) lr 7.9389e-03 eta 0:10:43
epoch [10/30] batch [92/96] time 0.310 (0.334) data 0.000 (0.006) loss 2.4662 (1.4075) lr 7.9389e-03 eta 0:10:42
epoch [10/30] batch [94/96] time 0.306 (0.333) data 0.000 (0.006) loss 1.8167 (1.4209) lr 7.9389e-03 eta 0:10:40
epoch [10/30] batch [96/96] time 0.303 (0.333) data 0.000 (0.006) loss 2.0033 (1.4274) lr 7.5000e-03 eta 0:10:38
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.91s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.37s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.20s/it]=> result
* total: 576
* correct: 419
* accuracy: 72.7%
* error: 27.3%
* macro_f1: 71.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-10

epoch [11/30] batch [2/96] time 0.333 (0.671) data 0.000 (0.293) loss 1.4451 (1.6174) lr 7.5000e-03 eta 0:21:27
epoch [11/30] batch [4/96] time 0.340 (0.504) data 0.001 (0.147) loss 1.6875 (1.5261) lr 7.5000e-03 eta 0:16:05
epoch [11/30] batch [6/96] time 0.334 (0.448) data 0.000 (0.098) loss 1.0075 (1.5253) lr 7.5000e-03 eta 0:14:17
epoch [11/30] batch [8/96] time 0.321 (0.419) data 0.000 (0.074) loss 0.8670 (1.4718) lr 7.5000e-03 eta 0:13:21
epoch [11/30] batch [10/96] time 0.325 (0.401) data 0.000 (0.059) loss 1.7913 (1.4414) lr 7.5000e-03 eta 0:12:45
epoch [11/30] batch [12/96] time 0.328 (0.389) data 0.000 (0.049) loss 1.7732 (1.4143) lr 7.5000e-03 eta 0:12:21
epoch [11/30] batch [14/96] time 0.329 (0.380) data 0.000 (0.042) loss 1.5995 (1.4492) lr 7.5000e-03 eta 0:12:04
epoch [11/30] batch [16/96] time 0.328 (0.373) data 0.000 (0.037) loss 1.2927 (1.4274) lr 7.5000e-03 eta 0:11:50
epoch [11/30] batch [18/96] time 0.330 (0.369) data 0.000 (0.033) loss 1.6313 (1.4240) lr 7.5000e-03 eta 0:11:41
epoch [11/30] batch [20/96] time 0.330 (0.365) data 0.000 (0.030) loss 0.8181 (1.3900) lr 7.5000e-03 eta 0:11:33
epoch [11/30] batch [22/96] time 0.333 (0.362) data 0.000 (0.027) loss 1.7870 (1.4135) lr 7.5000e-03 eta 0:11:26
epoch [11/30] batch [24/96] time 0.354 (0.360) data 0.000 (0.025) loss 1.9631 (1.4208) lr 7.5000e-03 eta 0:11:22
epoch [11/30] batch [26/96] time 0.345 (0.358) data 0.000 (0.023) loss 1.0650 (1.4111) lr 7.5000e-03 eta 0:11:18
epoch [11/30] batch [28/96] time 0.323 (0.356) data 0.000 (0.021) loss 1.8420 (1.4308) lr 7.5000e-03 eta 0:11:13
epoch [11/30] batch [30/96] time 0.324 (0.354) data 0.001 (0.020) loss 1.8595 (1.4298) lr 7.5000e-03 eta 0:11:09
epoch [11/30] batch [32/96] time 0.326 (0.353) data 0.000 (0.019) loss 1.1275 (1.4488) lr 7.5000e-03 eta 0:11:05
epoch [11/30] batch [34/96] time 0.327 (0.351) data 0.000 (0.018) loss 1.1181 (1.4194) lr 7.5000e-03 eta 0:11:02
epoch [11/30] batch [36/96] time 0.424 (0.353) data 0.000 (0.017) loss 1.3656 (1.4241) lr 7.5000e-03 eta 0:11:05
epoch [11/30] batch [38/96] time 0.330 (0.352) data 0.001 (0.016) loss 0.9514 (1.4228) lr 7.5000e-03 eta 0:11:02
epoch [11/30] batch [40/96] time 0.329 (0.351) data 0.001 (0.015) loss 0.7767 (1.4112) lr 7.5000e-03 eta 0:10:59
epoch [11/30] batch [42/96] time 0.326 (0.350) data 0.000 (0.014) loss 1.1736 (1.3983) lr 7.5000e-03 eta 0:10:56
epoch [11/30] batch [44/96] time 0.328 (0.349) data 0.000 (0.014) loss 1.2154 (1.3952) lr 7.5000e-03 eta 0:10:54
epoch [11/30] batch [46/96] time 0.328 (0.348) data 0.000 (0.013) loss 1.1597 (1.3824) lr 7.5000e-03 eta 0:10:51
epoch [11/30] batch [48/96] time 0.321 (0.347) data 0.000 (0.013) loss 1.6749 (1.3934) lr 7.5000e-03 eta 0:10:49
epoch [11/30] batch [50/96] time 0.329 (0.346) data 0.000 (0.012) loss 1.0875 (1.3902) lr 7.5000e-03 eta 0:10:47
epoch [11/30] batch [52/96] time 0.339 (0.346) data 0.001 (0.012) loss 1.1131 (1.3839) lr 7.5000e-03 eta 0:10:46
epoch [11/30] batch [54/96] time 0.328 (0.345) data 0.000 (0.011) loss 1.0068 (1.3854) lr 7.5000e-03 eta 0:10:44
epoch [11/30] batch [56/96] time 0.325 (0.345) data 0.000 (0.011) loss 1.8612 (1.3856) lr 7.5000e-03 eta 0:10:42
epoch [11/30] batch [58/96] time 0.331 (0.344) data 0.000 (0.010) loss 1.1272 (1.3829) lr 7.5000e-03 eta 0:10:41
epoch [11/30] batch [60/96] time 0.331 (0.344) data 0.000 (0.010) loss 1.1642 (1.3836) lr 7.5000e-03 eta 0:10:39
epoch [11/30] batch [62/96] time 0.326 (0.344) data 0.000 (0.010) loss 1.0753 (1.3717) lr 7.5000e-03 eta 0:10:38
epoch [11/30] batch [64/96] time 0.323 (0.343) data 0.000 (0.010) loss 1.3289 (1.3796) lr 7.5000e-03 eta 0:10:36
epoch [11/30] batch [66/96] time 0.331 (0.343) data 0.000 (0.009) loss 1.4963 (1.3771) lr 7.5000e-03 eta 0:10:35
epoch [11/30] batch [68/96] time 0.327 (0.342) data 0.000 (0.009) loss 1.5578 (1.3751) lr 7.5000e-03 eta 0:10:33
epoch [11/30] batch [70/96] time 0.351 (0.342) data 0.000 (0.009) loss 2.4882 (1.4032) lr 7.5000e-03 eta 0:10:32
epoch [11/30] batch [72/96] time 0.346 (0.342) data 0.000 (0.008) loss 1.7049 (1.4105) lr 7.5000e-03 eta 0:10:32
epoch [11/30] batch [74/96] time 0.331 (0.342) data 0.000 (0.008) loss 1.4055 (1.4212) lr 7.5000e-03 eta 0:10:31
epoch [11/30] batch [76/96] time 0.327 (0.342) data 0.000 (0.008) loss 0.9508 (1.4137) lr 7.5000e-03 eta 0:10:29
epoch [11/30] batch [78/96] time 0.326 (0.341) data 0.000 (0.008) loss 0.9206 (1.4031) lr 7.5000e-03 eta 0:10:28
epoch [11/30] batch [80/96] time 0.331 (0.341) data 0.000 (0.008) loss 1.2576 (1.4026) lr 7.5000e-03 eta 0:10:27
epoch [11/30] batch [82/96] time 0.329 (0.341) data 0.000 (0.007) loss 0.8058 (1.3985) lr 7.5000e-03 eta 0:10:26
epoch [11/30] batch [84/96] time 0.328 (0.340) data 0.000 (0.007) loss 1.4281 (1.3984) lr 7.5000e-03 eta 0:10:24
epoch [11/30] batch [86/96] time 0.325 (0.340) data 0.000 (0.007) loss 1.1485 (1.3941) lr 7.5000e-03 eta 0:10:23
epoch [11/30] batch [88/96] time 0.325 (0.340) data 0.000 (0.007) loss 1.2124 (1.3943) lr 7.5000e-03 eta 0:10:22
epoch [11/30] batch [90/96] time 0.327 (0.339) data 0.000 (0.007) loss 1.5951 (1.3941) lr 7.5000e-03 eta 0:10:21
epoch [11/30] batch [92/96] time 0.328 (0.339) data 0.000 (0.007) loss 1.2505 (1.3942) lr 7.5000e-03 eta 0:10:19
epoch [11/30] batch [94/96] time 0.326 (0.339) data 0.000 (0.007) loss 1.4045 (1.3909) lr 7.5000e-03 eta 0:10:18
epoch [11/30] batch [96/96] time 0.330 (0.339) data 0.000 (0.006) loss 1.4641 (1.4006) lr 7.0337e-03 eta 0:10:17
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.91s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.37s/it]100%|██████████| 3/3 [00:03<00:00,  1.14it/s]100%|██████████| 3/3 [00:03<00:00,  1.20s/it]=> result
* total: 576
* correct: 419
* accuracy: 72.7%
* error: 27.3%
* macro_f1: 71.7%

epoch [12/30] batch [2/96] time 0.351 (0.669) data 0.001 (0.275) loss 1.6394 (1.3000) lr 7.0337e-03 eta 0:20:19
epoch [12/30] batch [4/96] time 0.351 (0.508) data 0.001 (0.138) loss 1.1719 (1.1001) lr 7.0337e-03 eta 0:15:23
epoch [12/30] batch [6/96] time 0.348 (0.456) data 0.000 (0.092) loss 0.7947 (1.0289) lr 7.0337e-03 eta 0:13:49
epoch [12/30] batch [8/96] time 0.356 (0.430) data 0.001 (0.069) loss 1.4708 (1.0851) lr 7.0337e-03 eta 0:13:01
epoch [12/30] batch [10/96] time 0.345 (0.413) data 0.000 (0.055) loss 0.8486 (1.1642) lr 7.0337e-03 eta 0:12:29
epoch [12/30] batch [12/96] time 0.345 (0.402) data 0.000 (0.046) loss 1.4717 (1.2056) lr 7.0337e-03 eta 0:12:08
epoch [12/30] batch [14/96] time 0.349 (0.394) data 0.000 (0.040) loss 1.3730 (1.2651) lr 7.0337e-03 eta 0:11:52
epoch [12/30] batch [16/96] time 0.338 (0.388) data 0.000 (0.035) loss 2.5588 (1.3410) lr 7.0337e-03 eta 0:11:41
epoch [12/30] batch [18/96] time 0.351 (0.384) data 0.000 (0.031) loss 1.8584 (1.3736) lr 7.0337e-03 eta 0:11:32
epoch [12/30] batch [20/96] time 0.356 (0.380) data 0.000 (0.028) loss 1.7824 (1.3875) lr 7.0337e-03 eta 0:11:26
epoch [12/30] batch [22/96] time 0.339 (0.377) data 0.000 (0.025) loss 2.1852 (1.4065) lr 7.0337e-03 eta 0:11:19
epoch [12/30] batch [24/96] time 0.336 (0.374) data 0.001 (0.023) loss 2.0627 (1.4351) lr 7.0337e-03 eta 0:11:13
epoch [12/30] batch [26/96] time 0.340 (0.372) data 0.000 (0.021) loss 0.9341 (1.4250) lr 7.0337e-03 eta 0:11:08
epoch [12/30] batch [28/96] time 0.341 (0.369) data 0.000 (0.020) loss 1.0374 (1.3917) lr 7.0337e-03 eta 0:11:03
epoch [12/30] batch [30/96] time 0.344 (0.368) data 0.000 (0.019) loss 1.3403 (1.4075) lr 7.0337e-03 eta 0:11:00
epoch [12/30] batch [32/96] time 0.350 (0.367) data 0.000 (0.018) loss 1.4682 (1.4066) lr 7.0337e-03 eta 0:10:57
epoch [12/30] batch [34/96] time 0.351 (0.366) data 0.000 (0.017) loss 0.8622 (1.3749) lr 7.0337e-03 eta 0:10:55
epoch [12/30] batch [36/96] time 0.449 (0.368) data 0.000 (0.016) loss 1.0396 (1.3769) lr 7.0337e-03 eta 0:10:57
epoch [12/30] batch [38/96] time 0.348 (0.367) data 0.000 (0.015) loss 1.1938 (1.3812) lr 7.0337e-03 eta 0:10:55
epoch [12/30] batch [40/96] time 0.346 (0.366) data 0.001 (0.014) loss 1.3037 (1.3776) lr 7.0337e-03 eta 0:10:52
epoch [12/30] batch [42/96] time 0.346 (0.365) data 0.000 (0.013) loss 1.3576 (1.3682) lr 7.0337e-03 eta 0:10:49
epoch [12/30] batch [44/96] time 0.344 (0.364) data 0.000 (0.013) loss 0.9294 (1.3529) lr 7.0337e-03 eta 0:10:47
epoch [12/30] batch [46/96] time 0.335 (0.363) data 0.000 (0.012) loss 1.0335 (1.3487) lr 7.0337e-03 eta 0:10:44
epoch [12/30] batch [48/96] time 0.324 (0.361) data 0.000 (0.012) loss 1.4581 (1.3391) lr 7.0337e-03 eta 0:10:41
epoch [12/30] batch [50/96] time 0.325 (0.360) data 0.000 (0.011) loss 1.3971 (1.3480) lr 7.0337e-03 eta 0:10:38
epoch [12/30] batch [52/96] time 0.331 (0.359) data 0.001 (0.011) loss 0.9614 (1.3576) lr 7.0337e-03 eta 0:10:35
epoch [12/30] batch [54/96] time 0.350 (0.358) data 0.000 (0.011) loss 1.0471 (1.3516) lr 7.0337e-03 eta 0:10:33
epoch [12/30] batch [56/96] time 0.332 (0.357) data 0.000 (0.010) loss 1.9531 (1.3703) lr 7.0337e-03 eta 0:10:31
epoch [12/30] batch [58/96] time 0.355 (0.357) data 0.000 (0.010) loss 1.7939 (1.3730) lr 7.0337e-03 eta 0:10:29
epoch [12/30] batch [60/96] time 0.331 (0.356) data 0.000 (0.010) loss 2.1908 (1.3790) lr 7.0337e-03 eta 0:10:27
epoch [12/30] batch [62/96] time 0.330 (0.355) data 0.000 (0.009) loss 2.0575 (1.3829) lr 7.0337e-03 eta 0:10:24
epoch [12/30] batch [64/96] time 0.338 (0.354) data 0.000 (0.009) loss 1.3819 (1.3793) lr 7.0337e-03 eta 0:10:23
epoch [12/30] batch [66/96] time 0.336 (0.353) data 0.000 (0.009) loss 1.5989 (1.3809) lr 7.0337e-03 eta 0:10:21
epoch [12/30] batch [68/96] time 0.333 (0.353) data 0.000 (0.008) loss 1.0387 (1.3722) lr 7.0337e-03 eta 0:10:19
epoch [12/30] batch [70/96] time 0.327 (0.352) data 0.000 (0.008) loss 0.8662 (1.3633) lr 7.0337e-03 eta 0:10:17
epoch [12/30] batch [72/96] time 0.323 (0.351) data 0.000 (0.008) loss 0.9011 (1.3642) lr 7.0337e-03 eta 0:10:15
epoch [12/30] batch [74/96] time 0.312 (0.350) data 0.000 (0.008) loss 1.4639 (1.3648) lr 7.0337e-03 eta 0:10:12
epoch [12/30] batch [76/96] time 0.313 (0.349) data 0.000 (0.008) loss 0.7552 (1.3701) lr 7.0337e-03 eta 0:10:10
epoch [12/30] batch [78/96] time 0.312 (0.348) data 0.000 (0.007) loss 2.1898 (1.3867) lr 7.0337e-03 eta 0:10:08
epoch [12/30] batch [80/96] time 0.316 (0.347) data 0.000 (0.007) loss 1.4609 (1.3927) lr 7.0337e-03 eta 0:10:05
epoch [12/30] batch [82/96] time 0.315 (0.347) data 0.000 (0.007) loss 1.1105 (1.3845) lr 7.0337e-03 eta 0:10:03
epoch [12/30] batch [84/96] time 0.316 (0.346) data 0.000 (0.007) loss 2.0768 (1.3945) lr 7.0337e-03 eta 0:10:01
epoch [12/30] batch [86/96] time 0.315 (0.345) data 0.000 (0.007) loss 2.1769 (1.3990) lr 7.0337e-03 eta 0:09:59
epoch [12/30] batch [88/96] time 0.313 (0.344) data 0.000 (0.007) loss 1.4085 (1.3948) lr 7.0337e-03 eta 0:09:57
epoch [12/30] batch [90/96] time 0.311 (0.344) data 0.000 (0.006) loss 1.1078 (1.4010) lr 7.0337e-03 eta 0:09:55
epoch [12/30] batch [92/96] time 0.313 (0.343) data 0.000 (0.006) loss 1.0660 (1.3943) lr 7.0337e-03 eta 0:09:53
epoch [12/30] batch [94/96] time 0.313 (0.342) data 0.000 (0.006) loss 1.8783 (1.4107) lr 7.0337e-03 eta 0:09:52
epoch [12/30] batch [96/96] time 0.319 (0.342) data 0.000 (0.006) loss 0.6684 (1.4042) lr 6.5451e-03 eta 0:09:50
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.84s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 412
* accuracy: 71.5%
* error: 28.5%
* macro_f1: 70.3%

epoch [13/30] batch [2/96] time 0.314 (0.653) data 0.000 (0.278) loss 1.0641 (1.2974) lr 6.5451e-03 eta 0:18:46
epoch [13/30] batch [4/96] time 0.324 (0.487) data 0.001 (0.139) loss 1.3714 (1.2918) lr 6.5451e-03 eta 0:14:00
epoch [13/30] batch [6/96] time 0.330 (0.435) data 0.000 (0.093) loss 1.3105 (1.4701) lr 6.5451e-03 eta 0:12:28
epoch [13/30] batch [8/96] time 0.328 (0.408) data 0.001 (0.070) loss 1.7014 (1.4625) lr 6.5451e-03 eta 0:11:41
epoch [13/30] batch [10/96] time 0.337 (0.394) data 0.001 (0.056) loss 1.2965 (1.3847) lr 6.5451e-03 eta 0:11:16
epoch [13/30] batch [12/96] time 0.337 (0.383) data 0.000 (0.047) loss 1.0290 (1.3622) lr 6.5451e-03 eta 0:10:58
epoch [13/30] batch [14/96] time 0.326 (0.375) data 0.000 (0.040) loss 2.1773 (1.4014) lr 6.5451e-03 eta 0:10:42
epoch [13/30] batch [16/96] time 0.323 (0.369) data 0.001 (0.035) loss 1.2085 (1.3830) lr 6.5451e-03 eta 0:10:31
epoch [13/30] batch [18/96] time 0.328 (0.364) data 0.000 (0.031) loss 1.4485 (1.3496) lr 6.5451e-03 eta 0:10:22
epoch [13/30] batch [20/96] time 0.322 (0.361) data 0.000 (0.028) loss 1.9384 (1.3831) lr 6.5451e-03 eta 0:10:15
epoch [13/30] batch [22/96] time 0.327 (0.358) data 0.000 (0.026) loss 1.5356 (1.3773) lr 6.5451e-03 eta 0:10:10
epoch [13/30] batch [24/96] time 0.325 (0.355) data 0.000 (0.024) loss 0.7408 (1.3400) lr 6.5451e-03 eta 0:10:05
epoch [13/30] batch [26/96] time 0.333 (0.353) data 0.000 (0.022) loss 3.0797 (1.3852) lr 6.5451e-03 eta 0:10:01
epoch [13/30] batch [28/96] time 0.325 (0.351) data 0.000 (0.020) loss 1.1979 (1.3684) lr 6.5451e-03 eta 0:09:57
epoch [13/30] batch [30/96] time 0.318 (0.349) data 0.000 (0.019) loss 1.3434 (1.3760) lr 6.5451e-03 eta 0:09:52
epoch [13/30] batch [32/96] time 0.318 (0.347) data 0.000 (0.018) loss 1.4018 (1.3817) lr 6.5451e-03 eta 0:09:48
epoch [13/30] batch [34/96] time 0.321 (0.345) data 0.000 (0.017) loss 0.9025 (1.3804) lr 6.5451e-03 eta 0:09:45
epoch [13/30] batch [36/96] time 0.331 (0.344) data 0.001 (0.016) loss 1.0339 (1.3665) lr 6.5451e-03 eta 0:09:42
epoch [13/30] batch [38/96] time 0.324 (0.343) data 0.000 (0.015) loss 1.3498 (1.3705) lr 6.5451e-03 eta 0:09:40
epoch [13/30] batch [40/96] time 0.337 (0.343) data 0.000 (0.014) loss 2.1079 (1.3939) lr 6.5451e-03 eta 0:09:38
epoch [13/30] batch [42/96] time 0.326 (0.342) data 0.000 (0.014) loss 0.9480 (1.3767) lr 6.5451e-03 eta 0:09:36
epoch [13/30] batch [44/96] time 0.438 (0.344) data 0.000 (0.013) loss 1.3046 (1.3721) lr 6.5451e-03 eta 0:09:38
epoch [13/30] batch [46/96] time 0.327 (0.343) data 0.000 (0.012) loss 2.7381 (1.4163) lr 6.5451e-03 eta 0:09:37
epoch [13/30] batch [48/96] time 0.327 (0.342) data 0.000 (0.012) loss 1.4686 (1.4174) lr 6.5451e-03 eta 0:09:35
epoch [13/30] batch [50/96] time 0.345 (0.342) data 0.000 (0.011) loss 1.6480 (1.4268) lr 6.5451e-03 eta 0:09:34
epoch [13/30] batch [52/96] time 0.332 (0.342) data 0.000 (0.011) loss 1.1300 (1.4185) lr 6.5451e-03 eta 0:09:33
epoch [13/30] batch [54/96] time 0.323 (0.341) data 0.000 (0.011) loss 1.7745 (1.4181) lr 6.5451e-03 eta 0:09:31
epoch [13/30] batch [56/96] time 0.324 (0.341) data 0.000 (0.010) loss 0.9273 (1.4120) lr 6.5451e-03 eta 0:09:30
epoch [13/30] batch [58/96] time 0.351 (0.341) data 0.000 (0.010) loss 1.6329 (1.4125) lr 6.5451e-03 eta 0:09:29
epoch [13/30] batch [60/96] time 0.326 (0.341) data 0.000 (0.010) loss 1.3275 (1.4096) lr 6.5451e-03 eta 0:09:28
epoch [13/30] batch [62/96] time 0.325 (0.340) data 0.000 (0.009) loss 1.1584 (1.4052) lr 6.5451e-03 eta 0:09:26
epoch [13/30] batch [64/96] time 0.334 (0.340) data 0.000 (0.009) loss 0.8446 (1.3898) lr 6.5451e-03 eta 0:09:25
epoch [13/30] batch [66/96] time 0.351 (0.340) data 0.000 (0.009) loss 1.4757 (1.3893) lr 6.5451e-03 eta 0:09:24
epoch [13/30] batch [68/96] time 0.341 (0.340) data 0.000 (0.009) loss 1.0825 (1.3836) lr 6.5451e-03 eta 0:09:24
epoch [13/30] batch [70/96] time 0.324 (0.340) data 0.000 (0.008) loss 2.1358 (1.4065) lr 6.5451e-03 eta 0:09:23
epoch [13/30] batch [72/96] time 0.328 (0.339) data 0.000 (0.008) loss 1.3366 (1.3977) lr 6.5451e-03 eta 0:09:21
epoch [13/30] batch [74/96] time 0.314 (0.339) data 0.000 (0.008) loss 1.6633 (1.3980) lr 6.5451e-03 eta 0:09:19
epoch [13/30] batch [76/96] time 0.308 (0.338) data 0.000 (0.008) loss 1.0215 (1.3858) lr 6.5451e-03 eta 0:09:17
epoch [13/30] batch [78/96] time 0.313 (0.337) data 0.000 (0.007) loss 1.3926 (1.3853) lr 6.5451e-03 eta 0:09:16
epoch [13/30] batch [80/96] time 0.315 (0.337) data 0.000 (0.007) loss 1.0609 (1.3773) lr 6.5451e-03 eta 0:09:14
epoch [13/30] batch [82/96] time 0.307 (0.336) data 0.000 (0.007) loss 1.0378 (1.3807) lr 6.5451e-03 eta 0:09:12
epoch [13/30] batch [84/96] time 0.308 (0.335) data 0.000 (0.007) loss 0.9923 (1.3765) lr 6.5451e-03 eta 0:09:11
epoch [13/30] batch [86/96] time 0.309 (0.335) data 0.000 (0.007) loss 1.4547 (1.3858) lr 6.5451e-03 eta 0:09:09
epoch [13/30] batch [88/96] time 0.311 (0.334) data 0.000 (0.007) loss 1.3345 (1.3836) lr 6.5451e-03 eta 0:09:07
epoch [13/30] batch [90/96] time 0.311 (0.333) data 0.000 (0.007) loss 1.6427 (1.3835) lr 6.5451e-03 eta 0:09:06
epoch [13/30] batch [92/96] time 0.311 (0.333) data 0.000 (0.006) loss 1.2144 (1.3779) lr 6.5451e-03 eta 0:09:04
epoch [13/30] batch [94/96] time 0.309 (0.332) data 0.000 (0.006) loss 1.4769 (1.3758) lr 6.5451e-03 eta 0:09:03
epoch [13/30] batch [96/96] time 0.308 (0.332) data 0.000 (0.006) loss 1.3028 (1.3714) lr 6.0396e-03 eta 0:09:01
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.91s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.37s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.20s/it]=> result
* total: 576
* correct: 424
* accuracy: 73.6%
* error: 26.4%
* macro_f1: 72.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [14/30] batch [2/96] time 0.324 (0.653) data 0.000 (0.296) loss 2.8106 (1.8331) lr 6.0396e-03 eta 0:17:45
epoch [14/30] batch [4/96] time 0.319 (0.485) data 0.000 (0.148) loss 1.2728 (1.5238) lr 6.0396e-03 eta 0:13:09
epoch [14/30] batch [6/96] time 0.351 (0.451) data 0.000 (0.099) loss 0.9121 (1.4025) lr 6.0396e-03 eta 0:12:13
epoch [14/30] batch [8/96] time 0.337 (0.420) data 0.000 (0.074) loss 1.6134 (1.3645) lr 6.0396e-03 eta 0:11:22
epoch [14/30] batch [10/96] time 0.340 (0.404) data 0.000 (0.059) loss 1.7905 (1.3658) lr 6.0396e-03 eta 0:10:55
epoch [14/30] batch [12/96] time 0.333 (0.394) data 0.000 (0.049) loss 2.4663 (1.4291) lr 6.0396e-03 eta 0:10:37
epoch [14/30] batch [14/96] time 0.339 (0.386) data 0.000 (0.042) loss 0.9106 (1.4180) lr 6.0396e-03 eta 0:10:23
epoch [14/30] batch [16/96] time 0.335 (0.379) data 0.000 (0.037) loss 1.2749 (1.4362) lr 6.0396e-03 eta 0:10:12
epoch [14/30] batch [18/96] time 0.327 (0.373) data 0.000 (0.033) loss 1.3326 (1.4116) lr 6.0396e-03 eta 0:10:02
epoch [14/30] batch [20/96] time 0.336 (0.370) data 0.000 (0.030) loss 1.0929 (1.3899) lr 6.0396e-03 eta 0:09:56
epoch [14/30] batch [22/96] time 0.330 (0.366) data 0.000 (0.027) loss 0.9526 (1.3520) lr 6.0396e-03 eta 0:09:49
epoch [14/30] batch [24/96] time 0.341 (0.364) data 0.000 (0.025) loss 1.5055 (1.3480) lr 6.0396e-03 eta 0:09:45
epoch [14/30] batch [26/96] time 0.336 (0.362) data 0.000 (0.023) loss 2.0340 (1.3899) lr 6.0396e-03 eta 0:09:41
epoch [14/30] batch [28/96] time 0.343 (0.361) data 0.000 (0.021) loss 1.1274 (1.3617) lr 6.0396e-03 eta 0:09:38
epoch [14/30] batch [30/96] time 0.340 (0.359) data 0.001 (0.020) loss 1.4772 (1.3622) lr 6.0396e-03 eta 0:09:35
epoch [14/30] batch [32/96] time 0.336 (0.358) data 0.000 (0.019) loss 1.0382 (1.3677) lr 6.0396e-03 eta 0:09:32
epoch [14/30] batch [34/96] time 0.329 (0.356) data 0.000 (0.018) loss 1.9040 (1.3790) lr 6.0396e-03 eta 0:09:29
epoch [14/30] batch [36/96] time 0.333 (0.355) data 0.000 (0.017) loss 1.6701 (1.4043) lr 6.0396e-03 eta 0:09:27
epoch [14/30] batch [38/96] time 0.336 (0.354) data 0.000 (0.016) loss 1.1329 (1.3963) lr 6.0396e-03 eta 0:09:24
epoch [14/30] batch [40/96] time 0.332 (0.353) data 0.000 (0.015) loss 1.1212 (1.3781) lr 6.0396e-03 eta 0:09:22
epoch [14/30] batch [42/96] time 0.338 (0.352) data 0.000 (0.014) loss 1.3005 (1.3871) lr 6.0396e-03 eta 0:09:20
epoch [14/30] batch [44/96] time 0.352 (0.352) data 0.000 (0.014) loss 1.6646 (1.3894) lr 6.0396e-03 eta 0:09:19
epoch [14/30] batch [46/96] time 0.345 (0.352) data 0.001 (0.013) loss 0.9116 (1.3777) lr 6.0396e-03 eta 0:09:17
epoch [14/30] batch [48/96] time 0.344 (0.352) data 0.000 (0.013) loss 1.8982 (1.3825) lr 6.0396e-03 eta 0:09:16
epoch [14/30] batch [50/96] time 0.343 (0.351) data 0.000 (0.012) loss 1.3724 (1.3749) lr 6.0396e-03 eta 0:09:16
epoch [14/30] batch [52/96] time 0.335 (0.351) data 0.000 (0.012) loss 1.8242 (1.3813) lr 6.0396e-03 eta 0:09:14
epoch [14/30] batch [54/96] time 0.338 (0.350) data 0.000 (0.011) loss 1.5750 (1.3898) lr 6.0396e-03 eta 0:09:12
epoch [14/30] batch [56/96] time 0.352 (0.350) data 0.000 (0.011) loss 1.1069 (1.3932) lr 6.0396e-03 eta 0:09:12
epoch [14/30] batch [58/96] time 0.344 (0.350) data 0.000 (0.011) loss 0.8575 (1.3943) lr 6.0396e-03 eta 0:09:11
epoch [14/30] batch [60/96] time 0.334 (0.350) data 0.000 (0.010) loss 1.2762 (1.3882) lr 6.0396e-03 eta 0:09:09
epoch [14/30] batch [62/96] time 0.335 (0.349) data 0.000 (0.010) loss 0.7657 (1.3788) lr 6.0396e-03 eta 0:09:08
epoch [14/30] batch [64/96] time 0.340 (0.350) data 0.000 (0.010) loss 1.0166 (1.3630) lr 6.0396e-03 eta 0:09:09
epoch [14/30] batch [66/96] time 0.324 (0.349) data 0.000 (0.009) loss 0.8710 (1.3565) lr 6.0396e-03 eta 0:09:07
epoch [14/30] batch [68/96] time 0.327 (0.349) data 0.000 (0.009) loss 1.4223 (1.3590) lr 6.0396e-03 eta 0:09:05
epoch [14/30] batch [70/96] time 0.325 (0.348) data 0.000 (0.009) loss 0.9194 (1.3709) lr 6.0396e-03 eta 0:09:03
epoch [14/30] batch [72/96] time 0.329 (0.348) data 0.000 (0.009) loss 0.8585 (1.3623) lr 6.0396e-03 eta 0:09:02
epoch [14/30] batch [74/96] time 0.310 (0.347) data 0.000 (0.008) loss 1.1272 (1.3569) lr 6.0396e-03 eta 0:09:00
epoch [14/30] batch [76/96] time 0.313 (0.346) data 0.000 (0.008) loss 1.0332 (1.3505) lr 6.0396e-03 eta 0:08:57
epoch [14/30] batch [78/96] time 0.311 (0.345) data 0.000 (0.008) loss 0.6604 (1.3429) lr 6.0396e-03 eta 0:08:55
epoch [14/30] batch [80/96] time 0.313 (0.344) data 0.000 (0.008) loss 1.7604 (1.3495) lr 6.0396e-03 eta 0:08:53
epoch [14/30] batch [82/96] time 0.309 (0.343) data 0.000 (0.008) loss 1.2387 (1.3499) lr 6.0396e-03 eta 0:08:51
epoch [14/30] batch [84/96] time 0.313 (0.342) data 0.000 (0.007) loss 1.3798 (1.3557) lr 6.0396e-03 eta 0:08:50
epoch [14/30] batch [86/96] time 0.307 (0.342) data 0.000 (0.007) loss 1.3039 (1.3519) lr 6.0396e-03 eta 0:08:48
epoch [14/30] batch [88/96] time 0.312 (0.341) data 0.000 (0.007) loss 1.1956 (1.3525) lr 6.0396e-03 eta 0:08:46
epoch [14/30] batch [90/96] time 0.308 (0.340) data 0.000 (0.007) loss 1.2455 (1.3473) lr 6.0396e-03 eta 0:08:44
epoch [14/30] batch [92/96] time 0.309 (0.339) data 0.000 (0.007) loss 1.1151 (1.3465) lr 6.0396e-03 eta 0:08:42
epoch [14/30] batch [94/96] time 0.312 (0.339) data 0.000 (0.007) loss 1.5728 (1.3508) lr 6.0396e-03 eta 0:08:41
epoch [14/30] batch [96/96] time 0.311 (0.338) data 0.000 (0.006) loss 1.5637 (1.3480) lr 5.5226e-03 eta 0:08:39
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.93s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.38s/it]100%|██████████| 3/3 [00:03<00:00,  1.14it/s]100%|██████████| 3/3 [00:03<00:00,  1.20s/it]=> result
* total: 576
* correct: 429
* accuracy: 74.5%
* error: 25.5%
* macro_f1: 73.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [15/30] batch [2/96] time 0.334 (0.669) data 0.001 (0.295) loss 1.1060 (1.1665) lr 5.5226e-03 eta 0:17:06
epoch [15/30] batch [4/96] time 0.341 (0.504) data 0.000 (0.147) loss 1.3921 (1.4513) lr 5.5226e-03 eta 0:12:51
epoch [15/30] batch [6/96] time 0.331 (0.449) data 0.000 (0.098) loss 0.9528 (1.2707) lr 5.5226e-03 eta 0:11:27
epoch [15/30] batch [8/96] time 0.336 (0.422) data 0.000 (0.074) loss 0.8394 (1.1509) lr 5.5226e-03 eta 0:10:44
epoch [15/30] batch [10/96] time 0.338 (0.406) data 0.000 (0.059) loss 1.4070 (1.2588) lr 5.5226e-03 eta 0:10:19
epoch [15/30] batch [12/96] time 0.322 (0.394) data 0.000 (0.049) loss 0.8366 (1.2753) lr 5.5226e-03 eta 0:09:59
epoch [15/30] batch [14/96] time 0.333 (0.385) data 0.000 (0.042) loss 1.1535 (1.2504) lr 5.5226e-03 eta 0:09:45
epoch [15/30] batch [16/96] time 0.334 (0.378) data 0.000 (0.037) loss 0.8656 (1.2256) lr 5.5226e-03 eta 0:09:34
epoch [15/30] batch [18/96] time 0.329 (0.373) data 0.001 (0.033) loss 1.1771 (1.2447) lr 5.5226e-03 eta 0:09:25
epoch [15/30] batch [20/96] time 0.326 (0.368) data 0.000 (0.030) loss 1.1935 (1.2374) lr 5.5226e-03 eta 0:09:18
epoch [15/30] batch [22/96] time 0.351 (0.366) data 0.000 (0.027) loss 1.0363 (1.2177) lr 5.5226e-03 eta 0:09:14
epoch [15/30] batch [24/96] time 0.332 (0.363) data 0.000 (0.025) loss 1.0299 (1.1948) lr 5.5226e-03 eta 0:09:09
epoch [15/30] batch [26/96] time 0.317 (0.360) data 0.000 (0.023) loss 1.3549 (1.2087) lr 5.5226e-03 eta 0:09:03
epoch [15/30] batch [28/96] time 0.350 (0.358) data 0.000 (0.021) loss 0.8180 (1.2090) lr 5.5226e-03 eta 0:09:00
epoch [15/30] batch [30/96] time 0.336 (0.357) data 0.000 (0.020) loss 0.8842 (1.1839) lr 5.5226e-03 eta 0:08:57
epoch [15/30] batch [32/96] time 0.324 (0.355) data 0.000 (0.019) loss 1.1055 (1.1926) lr 5.5226e-03 eta 0:08:53
epoch [15/30] batch [34/96] time 0.340 (0.354) data 0.000 (0.018) loss 1.4799 (1.2076) lr 5.5226e-03 eta 0:08:51
epoch [15/30] batch [36/96] time 0.365 (0.354) data 0.000 (0.017) loss 0.8501 (1.2037) lr 5.5226e-03 eta 0:08:50
epoch [15/30] batch [38/96] time 0.336 (0.353) data 0.000 (0.016) loss 2.5380 (1.2353) lr 5.5226e-03 eta 0:08:49
epoch [15/30] batch [40/96] time 0.336 (0.353) data 0.000 (0.015) loss 1.4804 (1.2546) lr 5.5226e-03 eta 0:08:47
epoch [15/30] batch [42/96] time 0.340 (0.352) data 0.000 (0.014) loss 0.9145 (1.2361) lr 5.5226e-03 eta 0:08:45
epoch [15/30] batch [44/96] time 0.326 (0.351) data 0.000 (0.014) loss 1.3496 (1.2256) lr 5.5226e-03 eta 0:08:43
epoch [15/30] batch [46/96] time 0.330 (0.352) data 0.000 (0.013) loss 1.0526 (1.2229) lr 5.5226e-03 eta 0:08:44
epoch [15/30] batch [48/96] time 0.336 (0.351) data 0.000 (0.013) loss 1.7773 (1.2339) lr 5.5226e-03 eta 0:08:42
epoch [15/30] batch [50/96] time 0.320 (0.350) data 0.000 (0.012) loss 1.5651 (1.2396) lr 5.5226e-03 eta 0:08:40
epoch [15/30] batch [52/96] time 0.323 (0.349) data 0.000 (0.012) loss 1.1341 (1.2349) lr 5.5226e-03 eta 0:08:38
epoch [15/30] batch [54/96] time 0.322 (0.348) data 0.000 (0.011) loss 1.4096 (1.2392) lr 5.5226e-03 eta 0:08:36
epoch [15/30] batch [56/96] time 0.339 (0.348) data 0.000 (0.011) loss 1.3545 (1.2343) lr 5.5226e-03 eta 0:08:35
epoch [15/30] batch [58/96] time 0.342 (0.348) data 0.000 (0.011) loss 1.6314 (1.2389) lr 5.5226e-03 eta 0:08:34
epoch [15/30] batch [60/96] time 0.338 (0.348) data 0.000 (0.010) loss 1.2485 (1.2321) lr 5.5226e-03 eta 0:08:33
epoch [15/30] batch [62/96] time 0.341 (0.348) data 0.000 (0.010) loss 1.0842 (1.2260) lr 5.5226e-03 eta 0:08:32
epoch [15/30] batch [64/96] time 0.347 (0.347) data 0.001 (0.010) loss 0.9156 (1.2361) lr 5.5226e-03 eta 0:08:31
epoch [15/30] batch [66/96] time 0.349 (0.347) data 0.000 (0.009) loss 0.8816 (1.2369) lr 5.5226e-03 eta 0:08:30
epoch [15/30] batch [68/96] time 0.347 (0.347) data 0.000 (0.009) loss 1.7107 (1.2401) lr 5.5226e-03 eta 0:08:30
epoch [15/30] batch [70/96] time 0.340 (0.347) data 0.001 (0.009) loss 1.2792 (1.2418) lr 5.5226e-03 eta 0:08:29
epoch [15/30] batch [72/96] time 0.347 (0.347) data 0.000 (0.009) loss 2.2312 (1.2607) lr 5.5226e-03 eta 0:08:28
epoch [15/30] batch [74/96] time 0.319 (0.347) data 0.000 (0.008) loss 1.3379 (1.2635) lr 5.5226e-03 eta 0:08:26
epoch [15/30] batch [76/96] time 0.322 (0.346) data 0.000 (0.008) loss 1.9672 (1.2772) lr 5.5226e-03 eta 0:08:25
epoch [15/30] batch [78/96] time 0.325 (0.346) data 0.000 (0.008) loss 1.4860 (1.2853) lr 5.5226e-03 eta 0:08:23
epoch [15/30] batch [80/96] time 0.323 (0.345) data 0.000 (0.008) loss 1.6437 (1.2865) lr 5.5226e-03 eta 0:08:22
epoch [15/30] batch [82/96] time 0.322 (0.344) data 0.000 (0.008) loss 2.0801 (1.2940) lr 5.5226e-03 eta 0:08:20
epoch [15/30] batch [84/96] time 0.323 (0.344) data 0.000 (0.007) loss 0.9470 (1.2922) lr 5.5226e-03 eta 0:08:19
epoch [15/30] batch [86/96] time 0.324 (0.343) data 0.000 (0.007) loss 2.1787 (1.3067) lr 5.5226e-03 eta 0:08:18
epoch [15/30] batch [88/96] time 0.321 (0.343) data 0.000 (0.007) loss 1.8021 (1.3118) lr 5.5226e-03 eta 0:08:16
epoch [15/30] batch [90/96] time 0.322 (0.343) data 0.000 (0.007) loss 0.8482 (1.3093) lr 5.5226e-03 eta 0:08:15
epoch [15/30] batch [92/96] time 0.326 (0.342) data 0.000 (0.007) loss 1.9115 (1.3116) lr 5.5226e-03 eta 0:08:14
epoch [15/30] batch [94/96] time 0.322 (0.342) data 0.000 (0.007) loss 1.7349 (1.3227) lr 5.5226e-03 eta 0:08:12
epoch [15/30] batch [96/96] time 0.322 (0.341) data 0.000 (0.006) loss 1.0980 (1.3244) lr 5.0000e-03 eta 0:08:11
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.89s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.37s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.19s/it]=> result
* total: 576
* correct: 429
* accuracy: 74.5%
* error: 25.5%
* macro_f1: 73.9%

epoch [16/30] batch [2/96] time 0.343 (0.670) data 0.001 (0.264) loss 1.2378 (1.3605) lr 5.0000e-03 eta 0:16:03
epoch [16/30] batch [4/96] time 0.342 (0.506) data 0.000 (0.132) loss 0.7493 (1.1586) lr 5.0000e-03 eta 0:12:06
epoch [16/30] batch [6/96] time 0.339 (0.450) data 0.000 (0.088) loss 1.4868 (1.2356) lr 5.0000e-03 eta 0:10:45
epoch [16/30] batch [8/96] time 0.348 (0.425) data 0.001 (0.066) loss 0.9345 (1.1652) lr 5.0000e-03 eta 0:10:08
epoch [16/30] batch [10/96] time 0.347 (0.409) data 0.000 (0.053) loss 2.0924 (1.2556) lr 5.0000e-03 eta 0:09:44
epoch [16/30] batch [12/96] time 0.346 (0.399) data 0.000 (0.044) loss 0.7651 (1.1838) lr 5.0000e-03 eta 0:09:30
epoch [16/30] batch [14/96] time 0.343 (0.391) data 0.000 (0.038) loss 1.4356 (1.1985) lr 5.0000e-03 eta 0:09:17
epoch [16/30] batch [16/96] time 0.348 (0.385) data 0.000 (0.033) loss 0.9430 (1.1858) lr 5.0000e-03 eta 0:09:08
epoch [16/30] batch [18/96] time 0.353 (0.381) data 0.000 (0.030) loss 1.5068 (1.2003) lr 5.0000e-03 eta 0:09:02
epoch [16/30] batch [20/96] time 0.341 (0.378) data 0.000 (0.027) loss 2.1869 (1.2415) lr 5.0000e-03 eta 0:08:56
epoch [16/30] batch [22/96] time 0.350 (0.375) data 0.000 (0.024) loss 1.4832 (1.2510) lr 5.0000e-03 eta 0:08:51
epoch [16/30] batch [24/96] time 0.339 (0.372) data 0.000 (0.022) loss 1.8014 (1.3041) lr 5.0000e-03 eta 0:08:46
epoch [16/30] batch [26/96] time 0.358 (0.370) data 0.000 (0.021) loss 1.3605 (1.3048) lr 5.0000e-03 eta 0:08:43
epoch [16/30] batch [28/96] time 0.344 (0.368) data 0.000 (0.019) loss 2.1790 (1.3315) lr 5.0000e-03 eta 0:08:39
epoch [16/30] batch [30/96] time 0.343 (0.366) data 0.001 (0.018) loss 1.4127 (1.3210) lr 5.0000e-03 eta 0:08:36
epoch [16/30] batch [32/96] time 0.340 (0.364) data 0.001 (0.017) loss 0.7418 (1.2954) lr 5.0000e-03 eta 0:08:33
epoch [16/30] batch [34/96] time 0.341 (0.363) data 0.000 (0.016) loss 1.3546 (1.3109) lr 5.0000e-03 eta 0:08:30
epoch [16/30] batch [36/96] time 0.343 (0.362) data 0.000 (0.015) loss 1.2380 (1.3001) lr 5.0000e-03 eta 0:08:28
epoch [16/30] batch [38/96] time 0.327 (0.360) data 0.000 (0.014) loss 0.9607 (1.2870) lr 5.0000e-03 eta 0:08:25
epoch [16/30] batch [40/96] time 0.320 (0.361) data 0.000 (0.014) loss 2.3165 (1.3120) lr 5.0000e-03 eta 0:08:25
epoch [16/30] batch [42/96] time 0.330 (0.359) data 0.000 (0.013) loss 0.7651 (1.2868) lr 5.0000e-03 eta 0:08:22
epoch [16/30] batch [44/96] time 0.321 (0.358) data 0.000 (0.012) loss 0.9983 (1.2820) lr 5.0000e-03 eta 0:08:19
epoch [16/30] batch [46/96] time 0.323 (0.356) data 0.000 (0.012) loss 1.6868 (1.3028) lr 5.0000e-03 eta 0:08:16
epoch [16/30] batch [48/96] time 0.326 (0.355) data 0.000 (0.011) loss 2.1396 (1.3185) lr 5.0000e-03 eta 0:08:14
epoch [16/30] batch [50/96] time 0.336 (0.354) data 0.000 (0.011) loss 0.8643 (1.3067) lr 5.0000e-03 eta 0:08:11
epoch [16/30] batch [52/96] time 0.335 (0.353) data 0.000 (0.011) loss 1.2200 (1.3124) lr 5.0000e-03 eta 0:08:10
epoch [16/30] batch [54/96] time 0.327 (0.352) data 0.000 (0.010) loss 3.5467 (1.3595) lr 5.0000e-03 eta 0:08:08
epoch [16/30] batch [56/96] time 0.339 (0.352) data 0.000 (0.010) loss 0.9390 (1.3654) lr 5.0000e-03 eta 0:08:06
epoch [16/30] batch [58/96] time 0.334 (0.351) data 0.000 (0.009) loss 1.0680 (1.3765) lr 5.0000e-03 eta 0:08:05
epoch [16/30] batch [60/96] time 0.325 (0.350) data 0.000 (0.009) loss 1.5585 (1.3699) lr 5.0000e-03 eta 0:08:03
epoch [16/30] batch [62/96] time 0.342 (0.350) data 0.000 (0.009) loss 1.3226 (1.3691) lr 5.0000e-03 eta 0:08:01
epoch [16/30] batch [64/96] time 0.332 (0.349) data 0.000 (0.009) loss 1.4385 (1.3777) lr 5.0000e-03 eta 0:08:00
epoch [16/30] batch [66/96] time 0.327 (0.349) data 0.000 (0.008) loss 1.4702 (1.3711) lr 5.0000e-03 eta 0:07:59
epoch [16/30] batch [68/96] time 0.338 (0.348) data 0.000 (0.008) loss 1.6692 (1.3720) lr 5.0000e-03 eta 0:07:57
epoch [16/30] batch [70/96] time 0.334 (0.348) data 0.000 (0.008) loss 0.8811 (1.3610) lr 5.0000e-03 eta 0:07:56
epoch [16/30] batch [72/96] time 0.326 (0.347) data 0.000 (0.008) loss 0.9210 (1.3598) lr 5.0000e-03 eta 0:07:54
epoch [16/30] batch [74/96] time 0.315 (0.346) data 0.000 (0.007) loss 0.9028 (1.3493) lr 5.0000e-03 eta 0:07:52
epoch [16/30] batch [76/96] time 0.312 (0.345) data 0.000 (0.007) loss 1.5775 (1.3572) lr 5.0000e-03 eta 0:07:51
epoch [16/30] batch [78/96] time 0.318 (0.345) data 0.000 (0.007) loss 0.7347 (1.3498) lr 5.0000e-03 eta 0:07:49
epoch [16/30] batch [80/96] time 0.312 (0.344) data 0.000 (0.007) loss 1.7467 (1.3557) lr 5.0000e-03 eta 0:07:47
epoch [16/30] batch [82/96] time 0.313 (0.343) data 0.000 (0.007) loss 0.9093 (1.3472) lr 5.0000e-03 eta 0:07:45
epoch [16/30] batch [84/96] time 0.316 (0.342) data 0.000 (0.007) loss 1.1021 (1.3433) lr 5.0000e-03 eta 0:07:44
epoch [16/30] batch [86/96] time 0.316 (0.342) data 0.000 (0.006) loss 1.0945 (1.3384) lr 5.0000e-03 eta 0:07:42
epoch [16/30] batch [88/96] time 0.312 (0.341) data 0.000 (0.006) loss 1.5028 (1.3434) lr 5.0000e-03 eta 0:07:41
epoch [16/30] batch [90/96] time 0.316 (0.341) data 0.000 (0.006) loss 0.9099 (1.3383) lr 5.0000e-03 eta 0:07:39
epoch [16/30] batch [92/96] time 0.311 (0.340) data 0.000 (0.006) loss 1.9648 (1.3421) lr 5.0000e-03 eta 0:07:38
epoch [16/30] batch [94/96] time 0.313 (0.339) data 0.000 (0.006) loss 1.6119 (1.3413) lr 5.0000e-03 eta 0:07:36
epoch [16/30] batch [96/96] time 0.310 (0.339) data 0.000 (0.006) loss 1.3486 (1.3444) lr 4.4774e-03 eta 0:07:35
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.92s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.38s/it]100%|██████████| 3/3 [00:03<00:00,  1.14it/s]100%|██████████| 3/3 [00:03<00:00,  1.20s/it]=> result
* total: 576
* correct: 426
* accuracy: 74.0%
* error: 26.0%
* macro_f1: 73.2%

epoch [17/30] batch [2/96] time 0.372 (0.705) data 0.000 (0.304) loss 0.8689 (0.9490) lr 4.4774e-03 eta 0:15:45
epoch [17/30] batch [4/96] time 0.331 (0.523) data 0.001 (0.152) loss 1.1715 (1.2066) lr 4.4774e-03 eta 0:11:40
epoch [17/30] batch [6/96] time 0.347 (0.461) data 0.000 (0.102) loss 0.6831 (1.1414) lr 4.4774e-03 eta 0:10:16
epoch [17/30] batch [8/96] time 0.328 (0.427) data 0.000 (0.076) loss 1.2757 (1.1255) lr 4.4774e-03 eta 0:09:30
epoch [17/30] batch [10/96] time 0.327 (0.407) data 0.000 (0.061) loss 3.0972 (1.2917) lr 4.4774e-03 eta 0:09:02
epoch [17/30] batch [12/96] time 0.325 (0.394) data 0.001 (0.051) loss 1.7938 (1.3529) lr 4.4774e-03 eta 0:08:44
epoch [17/30] batch [14/96] time 0.325 (0.384) data 0.000 (0.044) loss 1.5270 (1.3595) lr 4.4774e-03 eta 0:08:30
epoch [17/30] batch [16/96] time 0.326 (0.377) data 0.000 (0.038) loss 0.7932 (1.3629) lr 4.4774e-03 eta 0:08:21
epoch [17/30] batch [18/96] time 0.322 (0.372) data 0.000 (0.034) loss 1.0578 (1.3254) lr 4.4774e-03 eta 0:08:12
epoch [17/30] batch [20/96] time 0.334 (0.368) data 0.000 (0.031) loss 0.8995 (1.2834) lr 4.4774e-03 eta 0:08:06
epoch [17/30] batch [22/96] time 0.323 (0.364) data 0.000 (0.028) loss 1.5072 (1.3227) lr 4.4774e-03 eta 0:08:01
epoch [17/30] batch [24/96] time 0.328 (0.361) data 0.001 (0.026) loss 0.8264 (1.2851) lr 4.4774e-03 eta 0:07:56
epoch [17/30] batch [26/96] time 0.341 (0.359) data 0.000 (0.024) loss 0.8674 (1.2935) lr 4.4774e-03 eta 0:07:53
epoch [17/30] batch [28/96] time 0.324 (0.356) data 0.000 (0.022) loss 1.2235 (1.2842) lr 4.4774e-03 eta 0:07:49
epoch [17/30] batch [30/96] time 0.341 (0.355) data 0.000 (0.021) loss 1.9051 (1.3087) lr 4.4774e-03 eta 0:07:46
epoch [17/30] batch [32/96] time 0.329 (0.353) data 0.000 (0.019) loss 1.9041 (1.3398) lr 4.4774e-03 eta 0:07:43
epoch [17/30] batch [34/96] time 0.320 (0.351) data 0.000 (0.018) loss 1.4632 (1.3581) lr 4.4774e-03 eta 0:07:40
epoch [17/30] batch [36/96] time 0.435 (0.353) data 0.000 (0.017) loss 1.1137 (1.3549) lr 4.4774e-03 eta 0:07:41
epoch [17/30] batch [38/96] time 0.325 (0.352) data 0.000 (0.016) loss 1.3848 (1.3650) lr 4.4774e-03 eta 0:07:39
epoch [17/30] batch [40/96] time 0.327 (0.351) data 0.000 (0.016) loss 0.7841 (1.3573) lr 4.4774e-03 eta 0:07:37
epoch [17/30] batch [42/96] time 0.324 (0.349) data 0.000 (0.015) loss 1.8837 (1.3560) lr 4.4774e-03 eta 0:07:34
epoch [17/30] batch [44/96] time 0.339 (0.349) data 0.000 (0.014) loss 0.9476 (1.3387) lr 4.4774e-03 eta 0:07:33
epoch [17/30] batch [46/96] time 0.346 (0.348) data 0.000 (0.014) loss 1.3551 (1.3286) lr 4.4774e-03 eta 0:07:32
epoch [17/30] batch [48/96] time 0.331 (0.347) data 0.000 (0.013) loss 1.0319 (1.3128) lr 4.4774e-03 eta 0:07:30
epoch [17/30] batch [50/96] time 0.321 (0.347) data 0.000 (0.013) loss 1.1591 (1.3083) lr 4.4774e-03 eta 0:07:28
epoch [17/30] batch [52/96] time 0.326 (0.346) data 0.000 (0.012) loss 1.3032 (1.3002) lr 4.4774e-03 eta 0:07:26
epoch [17/30] batch [54/96] time 0.324 (0.345) data 0.001 (0.012) loss 1.1852 (1.2975) lr 4.4774e-03 eta 0:07:24
epoch [17/30] batch [56/96] time 0.320 (0.344) data 0.000 (0.011) loss 1.0230 (1.2986) lr 4.4774e-03 eta 0:07:23
epoch [17/30] batch [58/96] time 0.326 (0.343) data 0.000 (0.011) loss 1.5251 (1.3049) lr 4.4774e-03 eta 0:07:21
epoch [17/30] batch [60/96] time 0.321 (0.342) data 0.000 (0.010) loss 1.0743 (1.3025) lr 4.4774e-03 eta 0:07:19
epoch [17/30] batch [62/96] time 0.324 (0.342) data 0.000 (0.010) loss 1.5582 (1.3045) lr 4.4774e-03 eta 0:07:18
epoch [17/30] batch [64/96] time 0.326 (0.341) data 0.000 (0.010) loss 1.6419 (1.3092) lr 4.4774e-03 eta 0:07:16
epoch [17/30] batch [66/96] time 0.326 (0.341) data 0.000 (0.010) loss 1.0175 (1.2987) lr 4.4774e-03 eta 0:07:15
epoch [17/30] batch [68/96] time 0.332 (0.341) data 0.000 (0.009) loss 1.5129 (1.3187) lr 4.4774e-03 eta 0:07:14
epoch [17/30] batch [70/96] time 0.333 (0.340) data 0.000 (0.009) loss 1.1901 (1.3143) lr 4.4774e-03 eta 0:07:13
epoch [17/30] batch [72/96] time 0.332 (0.340) data 0.000 (0.009) loss 1.2490 (1.3208) lr 4.4774e-03 eta 0:07:12
epoch [17/30] batch [74/96] time 0.312 (0.339) data 0.000 (0.009) loss 1.6046 (1.3244) lr 4.4774e-03 eta 0:07:10
epoch [17/30] batch [76/96] time 0.310 (0.338) data 0.000 (0.008) loss 1.7811 (1.3370) lr 4.4774e-03 eta 0:07:09
epoch [17/30] batch [78/96] time 0.309 (0.338) data 0.000 (0.008) loss 0.9706 (1.3263) lr 4.4774e-03 eta 0:07:07
epoch [17/30] batch [80/96] time 0.313 (0.337) data 0.000 (0.008) loss 0.9605 (1.3180) lr 4.4774e-03 eta 0:07:06
epoch [17/30] batch [82/96] time 0.307 (0.336) data 0.000 (0.008) loss 1.2554 (1.3152) lr 4.4774e-03 eta 0:07:04
epoch [17/30] batch [84/96] time 0.310 (0.336) data 0.000 (0.008) loss 1.4320 (1.3138) lr 4.4774e-03 eta 0:07:03
epoch [17/30] batch [86/96] time 0.312 (0.335) data 0.000 (0.007) loss 1.0063 (1.3057) lr 4.4774e-03 eta 0:07:01
epoch [17/30] batch [88/96] time 0.308 (0.335) data 0.000 (0.007) loss 1.7498 (1.3071) lr 4.4774e-03 eta 0:07:00
epoch [17/30] batch [90/96] time 0.310 (0.334) data 0.000 (0.007) loss 1.7352 (1.3140) lr 4.4774e-03 eta 0:06:58
epoch [17/30] batch [92/96] time 0.309 (0.334) data 0.000 (0.007) loss 1.2241 (1.3161) lr 4.4774e-03 eta 0:06:57
epoch [17/30] batch [94/96] time 0.314 (0.333) data 0.000 (0.007) loss 1.2033 (1.3181) lr 4.4774e-03 eta 0:06:56
epoch [17/30] batch [96/96] time 0.312 (0.333) data 0.000 (0.007) loss 1.0892 (1.3204) lr 3.9604e-03 eta 0:06:55
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.90s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.37s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.19s/it]=> result
* total: 576
* correct: 432
* accuracy: 75.0%
* error: 25.0%
* macro_f1: 74.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [18/30] batch [2/96] time 0.330 (0.667) data 0.000 (0.279) loss 0.8882 (1.3294) lr 3.9604e-03 eta 0:13:51
epoch [18/30] batch [4/96] time 0.336 (0.502) data 0.000 (0.140) loss 1.1382 (1.4267) lr 3.9604e-03 eta 0:10:24
epoch [18/30] batch [6/96] time 0.335 (0.447) data 0.000 (0.093) loss 1.2390 (1.3474) lr 3.9604e-03 eta 0:09:14
epoch [18/30] batch [8/96] time 0.332 (0.419) data 0.000 (0.070) loss 1.3160 (1.3782) lr 3.9604e-03 eta 0:08:39
epoch [18/30] batch [10/96] time 0.325 (0.401) data 0.000 (0.056) loss 1.8022 (1.3954) lr 3.9604e-03 eta 0:08:16
epoch [18/30] batch [12/96] time 0.330 (0.389) data 0.000 (0.047) loss 1.4767 (1.3766) lr 3.9604e-03 eta 0:08:00
epoch [18/30] batch [14/96] time 0.330 (0.380) data 0.000 (0.040) loss 0.9504 (1.4001) lr 3.9604e-03 eta 0:07:48
epoch [18/30] batch [16/96] time 0.334 (0.373) data 0.000 (0.035) loss 0.8784 (1.3731) lr 3.9604e-03 eta 0:07:39
epoch [18/30] batch [18/96] time 0.333 (0.368) data 0.000 (0.031) loss 0.9769 (1.3587) lr 3.9604e-03 eta 0:07:33
epoch [18/30] batch [20/96] time 0.332 (0.364) data 0.000 (0.028) loss 1.2405 (1.3474) lr 3.9604e-03 eta 0:07:27
epoch [18/30] batch [22/96] time 0.325 (0.360) data 0.000 (0.026) loss 1.7739 (1.3713) lr 3.9604e-03 eta 0:07:21
epoch [18/30] batch [24/96] time 0.329 (0.358) data 0.000 (0.024) loss 1.1053 (1.3692) lr 3.9604e-03 eta 0:07:17
epoch [18/30] batch [26/96] time 0.323 (0.355) data 0.000 (0.022) loss 0.9432 (1.3336) lr 3.9604e-03 eta 0:07:14
epoch [18/30] batch [28/96] time 0.333 (0.353) data 0.000 (0.020) loss 1.2372 (1.3206) lr 3.9604e-03 eta 0:07:11
epoch [18/30] batch [30/96] time 0.333 (0.352) data 0.000 (0.019) loss 0.8330 (1.3034) lr 3.9604e-03 eta 0:07:08
epoch [18/30] batch [32/96] time 0.328 (0.350) data 0.000 (0.018) loss 1.8520 (1.3071) lr 3.9604e-03 eta 0:07:05
epoch [18/30] batch [34/96] time 0.320 (0.349) data 0.000 (0.017) loss 1.2431 (1.2970) lr 3.9604e-03 eta 0:07:03
epoch [18/30] batch [36/96] time 0.429 (0.350) data 0.001 (0.016) loss 0.7338 (1.2762) lr 3.9604e-03 eta 0:07:04
epoch [18/30] batch [38/96] time 0.324 (0.349) data 0.000 (0.015) loss 1.0325 (1.2641) lr 3.9604e-03 eta 0:07:02
epoch [18/30] batch [40/96] time 0.329 (0.348) data 0.000 (0.014) loss 1.2819 (1.2518) lr 3.9604e-03 eta 0:07:00
epoch [18/30] batch [42/96] time 0.326 (0.347) data 0.000 (0.014) loss 0.9496 (1.2375) lr 3.9604e-03 eta 0:06:58
epoch [18/30] batch [44/96] time 0.331 (0.346) data 0.000 (0.013) loss 1.3506 (1.2367) lr 3.9604e-03 eta 0:06:56
epoch [18/30] batch [46/96] time 0.335 (0.346) data 0.001 (0.012) loss 1.2973 (1.2372) lr 3.9604e-03 eta 0:06:55
epoch [18/30] batch [48/96] time 0.325 (0.345) data 0.000 (0.012) loss 2.2109 (1.2576) lr 3.9604e-03 eta 0:06:53
epoch [18/30] batch [50/96] time 0.334 (0.344) data 0.000 (0.011) loss 2.5423 (1.2813) lr 3.9604e-03 eta 0:06:52
epoch [18/30] batch [52/96] time 0.319 (0.343) data 0.000 (0.011) loss 1.4986 (1.2835) lr 3.9604e-03 eta 0:06:50
epoch [18/30] batch [54/96] time 0.320 (0.343) data 0.000 (0.011) loss 1.8075 (1.3112) lr 3.9604e-03 eta 0:06:48
epoch [18/30] batch [56/96] time 0.334 (0.342) data 0.000 (0.010) loss 1.3500 (1.3015) lr 3.9604e-03 eta 0:06:47
epoch [18/30] batch [58/96] time 0.321 (0.341) data 0.000 (0.010) loss 1.7234 (1.3029) lr 3.9604e-03 eta 0:06:46
epoch [18/30] batch [60/96] time 0.325 (0.341) data 0.000 (0.010) loss 2.0268 (1.3229) lr 3.9604e-03 eta 0:06:44
epoch [18/30] batch [62/96] time 0.326 (0.340) data 0.000 (0.009) loss 1.1514 (1.3207) lr 3.9604e-03 eta 0:06:43
epoch [18/30] batch [64/96] time 0.338 (0.340) data 0.000 (0.009) loss 1.7304 (1.3307) lr 3.9604e-03 eta 0:06:42
epoch [18/30] batch [66/96] time 0.327 (0.340) data 0.000 (0.009) loss 0.9278 (1.3178) lr 3.9604e-03 eta 0:06:41
epoch [18/30] batch [68/96] time 0.328 (0.340) data 0.000 (0.009) loss 1.1164 (1.3232) lr 3.9604e-03 eta 0:06:41
epoch [18/30] batch [70/96] time 0.328 (0.339) data 0.000 (0.008) loss 1.5753 (1.3353) lr 3.9604e-03 eta 0:06:39
epoch [18/30] batch [72/96] time 0.322 (0.339) data 0.000 (0.008) loss 0.9148 (1.3260) lr 3.9604e-03 eta 0:06:38
epoch [18/30] batch [74/96] time 0.312 (0.338) data 0.000 (0.008) loss 1.3455 (1.3321) lr 3.9604e-03 eta 0:06:37
epoch [18/30] batch [76/96] time 0.309 (0.338) data 0.000 (0.008) loss 1.8720 (1.3374) lr 3.9604e-03 eta 0:06:35
epoch [18/30] batch [78/96] time 0.312 (0.337) data 0.000 (0.007) loss 1.0771 (1.3342) lr 3.9604e-03 eta 0:06:34
epoch [18/30] batch [80/96] time 0.327 (0.337) data 0.000 (0.007) loss 1.5382 (1.3368) lr 3.9604e-03 eta 0:06:33
epoch [18/30] batch [82/96] time 0.324 (0.336) data 0.000 (0.007) loss 1.1760 (1.3317) lr 3.9604e-03 eta 0:06:32
epoch [18/30] batch [84/96] time 0.328 (0.336) data 0.000 (0.007) loss 1.3165 (1.3240) lr 3.9604e-03 eta 0:06:31
epoch [18/30] batch [86/96] time 0.333 (0.336) data 0.000 (0.007) loss 1.1164 (1.3206) lr 3.9604e-03 eta 0:06:30
epoch [18/30] batch [88/96] time 0.313 (0.335) data 0.000 (0.007) loss 0.8542 (1.3139) lr 3.9604e-03 eta 0:06:29
epoch [18/30] batch [90/96] time 0.312 (0.335) data 0.000 (0.007) loss 0.9467 (1.3062) lr 3.9604e-03 eta 0:06:27
epoch [18/30] batch [92/96] time 0.310 (0.334) data 0.000 (0.006) loss 0.8454 (1.2949) lr 3.9604e-03 eta 0:06:26
epoch [18/30] batch [94/96] time 0.310 (0.334) data 0.000 (0.006) loss 1.5110 (1.2923) lr 3.9604e-03 eta 0:06:25
epoch [18/30] batch [96/96] time 0.311 (0.333) data 0.000 (0.006) loss 1.2051 (1.2931) lr 3.4549e-03 eta 0:06:24
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.85s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 424
* accuracy: 73.6%
* error: 26.4%
* macro_f1: 73.1%

epoch [19/30] batch [2/96] time 0.341 (0.663) data 0.000 (0.280) loss 1.5419 (1.3879) lr 3.4549e-03 eta 0:12:42
epoch [19/30] batch [4/96] time 0.335 (0.501) data 0.000 (0.140) loss 1.4614 (1.5181) lr 3.4549e-03 eta 0:09:35
epoch [19/30] batch [6/96] time 0.335 (0.446) data 0.000 (0.094) loss 1.0314 (1.4156) lr 3.4549e-03 eta 0:08:31
epoch [19/30] batch [8/96] time 0.330 (0.418) data 0.000 (0.070) loss 1.0866 (1.3285) lr 3.4549e-03 eta 0:07:58
epoch [19/30] batch [10/96] time 0.339 (0.402) data 0.000 (0.056) loss 0.8281 (1.2240) lr 3.4549e-03 eta 0:07:39
epoch [19/30] batch [12/96] time 0.337 (0.390) data 0.000 (0.047) loss 1.2608 (1.2874) lr 3.4549e-03 eta 0:07:24
epoch [19/30] batch [14/96] time 0.338 (0.382) data 0.000 (0.040) loss 0.7971 (1.2537) lr 3.4549e-03 eta 0:07:15
epoch [19/30] batch [16/96] time 0.327 (0.375) data 0.000 (0.035) loss 1.3251 (1.2547) lr 3.4549e-03 eta 0:07:06
epoch [19/30] batch [18/96] time 0.327 (0.370) data 0.001 (0.031) loss 1.1214 (1.2456) lr 3.4549e-03 eta 0:06:59
epoch [19/30] batch [20/96] time 0.324 (0.366) data 0.000 (0.028) loss 1.0378 (1.2122) lr 3.4549e-03 eta 0:06:54
epoch [19/30] batch [22/96] time 0.326 (0.362) data 0.000 (0.026) loss 1.3172 (1.2198) lr 3.4549e-03 eta 0:06:49
epoch [19/30] batch [24/96] time 0.317 (0.359) data 0.000 (0.024) loss 1.1867 (1.2113) lr 3.4549e-03 eta 0:06:44
epoch [19/30] batch [26/96] time 0.327 (0.356) data 0.000 (0.022) loss 1.2912 (1.2141) lr 3.4549e-03 eta 0:06:41
epoch [19/30] batch [28/96] time 0.338 (0.355) data 0.000 (0.020) loss 1.5320 (1.2149) lr 3.4549e-03 eta 0:06:38
epoch [19/30] batch [30/96] time 0.333 (0.354) data 0.000 (0.019) loss 1.0402 (1.1975) lr 3.4549e-03 eta 0:06:36
epoch [19/30] batch [32/96] time 0.323 (0.352) data 0.000 (0.018) loss 1.4348 (1.2048) lr 3.4549e-03 eta 0:06:34
epoch [19/30] batch [34/96] time 0.322 (0.350) data 0.000 (0.017) loss 1.0949 (1.2017) lr 3.4549e-03 eta 0:06:31
epoch [19/30] batch [36/96] time 0.433 (0.352) data 0.000 (0.016) loss 2.0433 (1.2373) lr 3.4549e-03 eta 0:06:33
epoch [19/30] batch [38/96] time 0.333 (0.351) data 0.000 (0.015) loss 1.1262 (1.2278) lr 3.4549e-03 eta 0:06:31
epoch [19/30] batch [40/96] time 0.329 (0.350) data 0.000 (0.014) loss 0.7467 (1.2214) lr 3.4549e-03 eta 0:06:29
epoch [19/30] batch [42/96] time 0.331 (0.349) data 0.000 (0.014) loss 1.2480 (1.2158) lr 3.4549e-03 eta 0:06:27
epoch [19/30] batch [44/96] time 0.324 (0.348) data 0.000 (0.013) loss 1.6496 (1.2321) lr 3.4549e-03 eta 0:06:26
epoch [19/30] batch [46/96] time 0.335 (0.348) data 0.000 (0.013) loss 1.2970 (1.2236) lr 3.4549e-03 eta 0:06:24
epoch [19/30] batch [48/96] time 0.338 (0.347) data 0.000 (0.012) loss 0.7006 (1.2098) lr 3.4549e-03 eta 0:06:23
epoch [19/30] batch [50/96] time 0.337 (0.347) data 0.001 (0.012) loss 2.0655 (1.2186) lr 3.4549e-03 eta 0:06:22
epoch [19/30] batch [52/96] time 0.327 (0.346) data 0.000 (0.011) loss 2.1504 (1.2325) lr 3.4549e-03 eta 0:06:21
epoch [19/30] batch [54/96] time 0.332 (0.346) data 0.000 (0.011) loss 1.7377 (1.2480) lr 3.4549e-03 eta 0:06:19
epoch [19/30] batch [56/96] time 0.351 (0.346) data 0.000 (0.010) loss 0.8750 (1.2357) lr 3.4549e-03 eta 0:06:19
epoch [19/30] batch [58/96] time 0.349 (0.346) data 0.001 (0.010) loss 2.0378 (1.2629) lr 3.4549e-03 eta 0:06:18
epoch [19/30] batch [60/96] time 0.344 (0.346) data 0.000 (0.010) loss 1.7474 (1.2658) lr 3.4549e-03 eta 0:06:17
epoch [19/30] batch [62/96] time 0.346 (0.346) data 0.000 (0.009) loss 1.1913 (1.2688) lr 3.4549e-03 eta 0:06:17
epoch [19/30] batch [64/96] time 0.355 (0.346) data 0.000 (0.009) loss 1.3834 (1.2663) lr 3.4549e-03 eta 0:06:16
epoch [19/30] batch [66/96] time 0.345 (0.346) data 0.000 (0.009) loss 0.8886 (1.2584) lr 3.4549e-03 eta 0:06:16
epoch [19/30] batch [68/96] time 0.345 (0.346) data 0.001 (0.009) loss 0.8980 (1.2580) lr 3.4549e-03 eta 0:06:15
epoch [19/30] batch [70/96] time 0.344 (0.346) data 0.000 (0.008) loss 1.1822 (1.2535) lr 3.4549e-03 eta 0:06:14
epoch [19/30] batch [72/96] time 0.344 (0.346) data 0.000 (0.008) loss 1.8112 (1.2592) lr 3.4549e-03 eta 0:06:14
epoch [19/30] batch [74/96] time 0.328 (0.346) data 0.000 (0.008) loss 1.8801 (1.2663) lr 3.4549e-03 eta 0:06:12
epoch [19/30] batch [76/96] time 0.329 (0.345) data 0.000 (0.008) loss 1.0331 (1.2609) lr 3.4549e-03 eta 0:06:11
epoch [19/30] batch [78/96] time 0.327 (0.345) data 0.000 (0.008) loss 2.2983 (1.2709) lr 3.4549e-03 eta 0:06:10
epoch [19/30] batch [80/96] time 0.330 (0.345) data 0.000 (0.007) loss 1.1045 (1.2641) lr 3.4549e-03 eta 0:06:09
epoch [19/30] batch [82/96] time 0.333 (0.344) data 0.000 (0.007) loss 0.8012 (1.2664) lr 3.4549e-03 eta 0:06:08
epoch [19/30] batch [84/96] time 0.331 (0.344) data 0.000 (0.007) loss 1.4388 (1.2755) lr 3.4549e-03 eta 0:06:07
epoch [19/30] batch [86/96] time 0.328 (0.344) data 0.000 (0.007) loss 1.0547 (1.2710) lr 3.4549e-03 eta 0:06:06
epoch [19/30] batch [88/96] time 0.327 (0.343) data 0.000 (0.007) loss 0.9825 (1.2695) lr 3.4549e-03 eta 0:06:05
epoch [19/30] batch [90/96] time 0.330 (0.343) data 0.000 (0.007) loss 1.2549 (1.2647) lr 3.4549e-03 eta 0:06:04
epoch [19/30] batch [92/96] time 0.330 (0.343) data 0.000 (0.006) loss 2.1594 (1.2798) lr 3.4549e-03 eta 0:06:03
epoch [19/30] batch [94/96] time 0.331 (0.342) data 0.000 (0.006) loss 1.8055 (1.2802) lr 3.4549e-03 eta 0:06:02
epoch [19/30] batch [96/96] time 0.332 (0.342) data 0.000 (0.006) loss 1.4629 (1.2856) lr 2.9663e-03 eta 0:06:01
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.90s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.37s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.19s/it]=> result
* total: 576
* correct: 441
* accuracy: 76.6%
* error: 23.4%
* macro_f1: 75.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [20/30] batch [2/96] time 0.360 (0.694) data 0.000 (0.294) loss 1.6919 (1.2565) lr 2.9663e-03 eta 0:12:11
epoch [20/30] batch [4/96] time 0.346 (0.517) data 0.000 (0.147) loss 1.0669 (1.2348) lr 2.9663e-03 eta 0:09:03
epoch [20/30] batch [6/96] time 0.334 (0.454) data 0.000 (0.098) loss 1.4842 (1.3345) lr 2.9663e-03 eta 0:07:57
epoch [20/30] batch [8/96] time 0.329 (0.423) data 0.000 (0.074) loss 0.9034 (1.2384) lr 2.9663e-03 eta 0:07:23
epoch [20/30] batch [10/96] time 0.329 (0.404) data 0.000 (0.059) loss 1.5457 (1.3491) lr 2.9663e-03 eta 0:07:03
epoch [20/30] batch [12/96] time 0.338 (0.394) data 0.000 (0.049) loss 1.0068 (1.2724) lr 2.9663e-03 eta 0:06:51
epoch [20/30] batch [14/96] time 0.345 (0.387) data 0.000 (0.042) loss 1.6995 (1.2643) lr 2.9663e-03 eta 0:06:42
epoch [20/30] batch [16/96] time 0.356 (0.382) data 0.000 (0.037) loss 0.7762 (1.2000) lr 2.9663e-03 eta 0:06:37
epoch [20/30] batch [18/96] time 0.339 (0.377) data 0.000 (0.033) loss 1.3962 (1.2158) lr 2.9663e-03 eta 0:06:31
epoch [20/30] batch [20/96] time 0.351 (0.374) data 0.000 (0.030) loss 1.2503 (1.2149) lr 2.9663e-03 eta 0:06:27
epoch [20/30] batch [22/96] time 0.337 (0.370) data 0.000 (0.027) loss 0.8501 (1.2114) lr 2.9663e-03 eta 0:06:22
epoch [20/30] batch [24/96] time 0.340 (0.367) data 0.000 (0.025) loss 0.9968 (1.1870) lr 2.9663e-03 eta 0:06:19
epoch [20/30] batch [26/96] time 0.333 (0.365) data 0.000 (0.023) loss 1.4372 (1.2090) lr 2.9663e-03 eta 0:06:15
epoch [20/30] batch [28/96] time 0.341 (0.363) data 0.000 (0.021) loss 1.0893 (1.2475) lr 2.9663e-03 eta 0:06:13
epoch [20/30] batch [30/96] time 0.333 (0.361) data 0.000 (0.020) loss 1.2880 (1.2544) lr 2.9663e-03 eta 0:06:10
epoch [20/30] batch [32/96] time 0.341 (0.360) data 0.000 (0.019) loss 2.3093 (1.2830) lr 2.9663e-03 eta 0:06:09
epoch [20/30] batch [34/96] time 0.331 (0.359) data 0.000 (0.018) loss 1.3616 (1.2724) lr 2.9663e-03 eta 0:06:07
epoch [20/30] batch [36/96] time 0.438 (0.360) data 0.000 (0.017) loss 1.5271 (1.3086) lr 2.9663e-03 eta 0:06:07
epoch [20/30] batch [38/96] time 0.341 (0.359) data 0.000 (0.016) loss 1.6716 (1.3251) lr 2.9663e-03 eta 0:06:05
epoch [20/30] batch [40/96] time 0.324 (0.358) data 0.000 (0.015) loss 1.0811 (1.3143) lr 2.9663e-03 eta 0:06:03
epoch [20/30] batch [42/96] time 0.331 (0.356) data 0.000 (0.014) loss 1.5234 (1.3191) lr 2.9663e-03 eta 0:06:01
epoch [20/30] batch [44/96] time 0.337 (0.355) data 0.000 (0.014) loss 3.1222 (1.3520) lr 2.9663e-03 eta 0:05:59
epoch [20/30] batch [46/96] time 0.342 (0.355) data 0.000 (0.013) loss 1.0565 (1.3441) lr 2.9663e-03 eta 0:05:58
epoch [20/30] batch [48/96] time 0.340 (0.354) data 0.000 (0.013) loss 1.2509 (1.3388) lr 2.9663e-03 eta 0:05:57
epoch [20/30] batch [50/96] time 0.331 (0.354) data 0.001 (0.012) loss 1.1270 (1.3228) lr 2.9663e-03 eta 0:05:55
epoch [20/30] batch [52/96] time 0.331 (0.353) data 0.000 (0.012) loss 1.1152 (1.3169) lr 2.9663e-03 eta 0:05:54
epoch [20/30] batch [54/96] time 0.326 (0.352) data 0.000 (0.011) loss 1.0909 (1.3107) lr 2.9663e-03 eta 0:05:52
epoch [20/30] batch [56/96] time 0.340 (0.351) data 0.000 (0.011) loss 0.9014 (1.2973) lr 2.9663e-03 eta 0:05:51
epoch [20/30] batch [58/96] time 0.335 (0.350) data 0.000 (0.010) loss 1.9338 (1.3005) lr 2.9663e-03 eta 0:05:49
epoch [20/30] batch [60/96] time 0.326 (0.350) data 0.000 (0.010) loss 1.0325 (1.3002) lr 2.9663e-03 eta 0:05:48
epoch [20/30] batch [62/96] time 0.323 (0.349) data 0.000 (0.010) loss 0.9716 (1.2998) lr 2.9663e-03 eta 0:05:46
epoch [20/30] batch [64/96] time 0.322 (0.348) data 0.000 (0.010) loss 1.4427 (1.2955) lr 2.9663e-03 eta 0:05:45
epoch [20/30] batch [66/96] time 0.317 (0.347) data 0.000 (0.009) loss 0.9999 (1.2832) lr 2.9663e-03 eta 0:05:43
epoch [20/30] batch [68/96] time 0.327 (0.347) data 0.000 (0.009) loss 1.0891 (1.2851) lr 2.9663e-03 eta 0:05:42
epoch [20/30] batch [70/96] time 0.339 (0.346) data 0.000 (0.009) loss 2.2129 (1.2956) lr 2.9663e-03 eta 0:05:41
epoch [20/30] batch [72/96] time 0.332 (0.346) data 0.000 (0.008) loss 0.8134 (1.2966) lr 2.9663e-03 eta 0:05:40
epoch [20/30] batch [74/96] time 0.315 (0.345) data 0.000 (0.008) loss 1.4815 (1.3026) lr 2.9663e-03 eta 0:05:38
epoch [20/30] batch [76/96] time 0.306 (0.344) data 0.000 (0.008) loss 1.2856 (1.3050) lr 2.9663e-03 eta 0:05:37
epoch [20/30] batch [78/96] time 0.311 (0.343) data 0.000 (0.008) loss 2.2822 (1.3191) lr 2.9663e-03 eta 0:05:35
epoch [20/30] batch [80/96] time 0.314 (0.342) data 0.000 (0.008) loss 1.2375 (1.3095) lr 2.9663e-03 eta 0:05:34
epoch [20/30] batch [82/96] time 0.311 (0.342) data 0.000 (0.007) loss 1.0885 (1.3063) lr 2.9663e-03 eta 0:05:32
epoch [20/30] batch [84/96] time 0.311 (0.341) data 0.000 (0.007) loss 1.6669 (1.3078) lr 2.9663e-03 eta 0:05:31
epoch [20/30] batch [86/96] time 0.308 (0.340) data 0.000 (0.007) loss 1.0776 (1.3011) lr 2.9663e-03 eta 0:05:29
epoch [20/30] batch [88/96] time 0.312 (0.340) data 0.000 (0.007) loss 0.8568 (1.2968) lr 2.9663e-03 eta 0:05:28
epoch [20/30] batch [90/96] time 0.312 (0.339) data 0.000 (0.007) loss 1.2283 (1.2925) lr 2.9663e-03 eta 0:05:27
epoch [20/30] batch [92/96] time 0.309 (0.338) data 0.000 (0.007) loss 0.7264 (1.2884) lr 2.9663e-03 eta 0:05:26
epoch [20/30] batch [94/96] time 0.316 (0.338) data 0.000 (0.007) loss 0.9556 (1.2867) lr 2.9663e-03 eta 0:05:24
epoch [20/30] batch [96/96] time 0.313 (0.337) data 0.000 (0.006) loss 2.7410 (1.2979) lr 2.5000e-03 eta 0:05:23
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.88s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.20s/it]=> result
* total: 576
* correct: 439
* accuracy: 76.2%
* error: 23.8%
* macro_f1: 75.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-20

epoch [21/30] batch [2/96] time 0.320 (0.642) data 0.000 (0.262) loss 0.9551 (1.3039) lr 2.5000e-03 eta 0:10:15
epoch [21/30] batch [4/96] time 0.332 (0.481) data 0.000 (0.131) loss 1.2776 (1.3449) lr 2.5000e-03 eta 0:07:39
epoch [21/30] batch [6/96] time 0.333 (0.429) data 0.000 (0.087) loss 0.8182 (1.2099) lr 2.5000e-03 eta 0:06:49
epoch [21/30] batch [8/96] time 0.336 (0.405) data 0.000 (0.066) loss 1.1531 (1.1809) lr 2.5000e-03 eta 0:06:25
epoch [21/30] batch [10/96] time 0.324 (0.389) data 0.000 (0.053) loss 0.9195 (1.1230) lr 2.5000e-03 eta 0:06:09
epoch [21/30] batch [12/96] time 0.324 (0.379) data 0.000 (0.044) loss 0.9501 (1.0978) lr 2.5000e-03 eta 0:05:59
epoch [21/30] batch [14/96] time 0.333 (0.372) data 0.000 (0.038) loss 0.8581 (1.0937) lr 2.5000e-03 eta 0:05:51
epoch [21/30] batch [16/96] time 0.325 (0.366) data 0.000 (0.033) loss 1.6303 (1.1313) lr 2.5000e-03 eta 0:05:45
epoch [21/30] batch [18/96] time 0.340 (0.362) data 0.000 (0.029) loss 1.2935 (1.1307) lr 2.5000e-03 eta 0:05:41
epoch [21/30] batch [20/96] time 0.333 (0.359) data 0.000 (0.026) loss 1.0637 (1.1362) lr 2.5000e-03 eta 0:05:37
epoch [21/30] batch [22/96] time 0.322 (0.357) data 0.000 (0.024) loss 1.1858 (1.1248) lr 2.5000e-03 eta 0:05:34
epoch [21/30] batch [24/96] time 0.321 (0.354) data 0.000 (0.022) loss 0.6913 (1.1308) lr 2.5000e-03 eta 0:05:31
epoch [21/30] batch [26/96] time 0.327 (0.352) data 0.000 (0.020) loss 1.1328 (1.1468) lr 2.5000e-03 eta 0:05:28
epoch [21/30] batch [28/96] time 0.319 (0.350) data 0.000 (0.019) loss 0.7219 (1.1664) lr 2.5000e-03 eta 0:05:25
epoch [21/30] batch [30/96] time 0.328 (0.348) data 0.000 (0.018) loss 1.0905 (1.1502) lr 2.5000e-03 eta 0:05:23
epoch [21/30] batch [32/96] time 0.336 (0.347) data 0.000 (0.017) loss 0.7862 (1.1295) lr 2.5000e-03 eta 0:05:22
epoch [21/30] batch [34/96] time 0.327 (0.346) data 0.000 (0.016) loss 0.8879 (1.1154) lr 2.5000e-03 eta 0:05:20
epoch [21/30] batch [36/96] time 0.434 (0.348) data 0.000 (0.015) loss 0.8892 (1.1111) lr 2.5000e-03 eta 0:05:21
epoch [21/30] batch [38/96] time 0.330 (0.347) data 0.000 (0.014) loss 1.0837 (1.1099) lr 2.5000e-03 eta 0:05:20
epoch [21/30] batch [40/96] time 0.322 (0.347) data 0.000 (0.013) loss 1.3717 (1.1189) lr 2.5000e-03 eta 0:05:18
epoch [21/30] batch [42/96] time 0.334 (0.346) data 0.000 (0.013) loss 1.2910 (1.1176) lr 2.5000e-03 eta 0:05:17
epoch [21/30] batch [44/96] time 0.318 (0.344) data 0.000 (0.012) loss 1.4901 (1.1270) lr 2.5000e-03 eta 0:05:15
epoch [21/30] batch [46/96] time 0.328 (0.344) data 0.000 (0.012) loss 1.1776 (1.1411) lr 2.5000e-03 eta 0:05:14
epoch [21/30] batch [48/96] time 0.324 (0.343) data 0.000 (0.011) loss 1.7017 (1.1427) lr 2.5000e-03 eta 0:05:12
epoch [21/30] batch [50/96] time 0.330 (0.342) data 0.000 (0.011) loss 1.9040 (1.1688) lr 2.5000e-03 eta 0:05:11
epoch [21/30] batch [52/96] time 0.342 (0.342) data 0.000 (0.010) loss 0.8648 (1.1637) lr 2.5000e-03 eta 0:05:10
epoch [21/30] batch [54/96] time 0.336 (0.342) data 0.000 (0.010) loss 1.0274 (1.1612) lr 2.5000e-03 eta 0:05:09
epoch [21/30] batch [56/96] time 0.320 (0.341) data 0.000 (0.010) loss 0.6997 (1.1574) lr 2.5000e-03 eta 0:05:08
epoch [21/30] batch [58/96] time 0.321 (0.341) data 0.000 (0.009) loss 1.2930 (1.1604) lr 2.5000e-03 eta 0:05:07
epoch [21/30] batch [60/96] time 0.325 (0.340) data 0.000 (0.009) loss 0.9909 (1.1583) lr 2.5000e-03 eta 0:05:06
epoch [21/30] batch [62/96] time 0.337 (0.340) data 0.000 (0.009) loss 1.6329 (1.1655) lr 2.5000e-03 eta 0:05:05
epoch [21/30] batch [64/96] time 0.331 (0.340) data 0.000 (0.008) loss 1.3788 (1.1735) lr 2.5000e-03 eta 0:05:04
epoch [21/30] batch [66/96] time 0.320 (0.340) data 0.000 (0.008) loss 1.9913 (1.2049) lr 2.5000e-03 eta 0:05:03
epoch [21/30] batch [68/96] time 0.336 (0.339) data 0.000 (0.008) loss 1.0910 (1.2055) lr 2.5000e-03 eta 0:05:02
epoch [21/30] batch [70/96] time 0.324 (0.339) data 0.000 (0.008) loss 0.6949 (1.1981) lr 2.5000e-03 eta 0:05:01
epoch [21/30] batch [72/96] time 0.314 (0.338) data 0.000 (0.008) loss 0.8260 (1.1877) lr 2.5000e-03 eta 0:05:00
epoch [21/30] batch [74/96] time 0.309 (0.337) data 0.000 (0.007) loss 1.7021 (1.2020) lr 2.5000e-03 eta 0:04:58
epoch [21/30] batch [76/96] time 0.300 (0.337) data 0.000 (0.007) loss 1.1320 (1.2044) lr 2.5000e-03 eta 0:04:57
epoch [21/30] batch [78/96] time 0.302 (0.336) data 0.000 (0.007) loss 0.9393 (1.1976) lr 2.5000e-03 eta 0:04:56
epoch [21/30] batch [80/96] time 0.307 (0.335) data 0.000 (0.007) loss 1.2254 (1.2050) lr 2.5000e-03 eta 0:04:54
epoch [21/30] batch [82/96] time 0.301 (0.334) data 0.000 (0.007) loss 0.8875 (1.2033) lr 2.5000e-03 eta 0:04:53
epoch [21/30] batch [84/96] time 0.309 (0.333) data 0.000 (0.007) loss 0.8542 (1.2003) lr 2.5000e-03 eta 0:04:52
epoch [21/30] batch [86/96] time 0.309 (0.333) data 0.000 (0.006) loss 0.9789 (1.1994) lr 2.5000e-03 eta 0:04:50
epoch [21/30] batch [88/96] time 0.309 (0.332) data 0.000 (0.006) loss 1.5051 (1.2058) lr 2.5000e-03 eta 0:04:49
epoch [21/30] batch [90/96] time 0.307 (0.332) data 0.000 (0.006) loss 2.1984 (1.2201) lr 2.5000e-03 eta 0:04:48
epoch [21/30] batch [92/96] time 0.301 (0.331) data 0.000 (0.006) loss 1.6650 (1.2260) lr 2.5000e-03 eta 0:04:47
epoch [21/30] batch [94/96] time 0.306 (0.331) data 0.000 (0.006) loss 1.7878 (1.2310) lr 2.5000e-03 eta 0:04:46
epoch [21/30] batch [96/96] time 0.304 (0.330) data 0.000 (0.006) loss 1.9724 (1.2456) lr 2.0611e-03 eta 0:04:45
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.91s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.37s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.19s/it]=> result
* total: 576
* correct: 441
* accuracy: 76.6%
* error: 23.4%
* macro_f1: 76.2%

epoch [22/30] batch [2/96] time 0.325 (0.646) data 0.000 (0.275) loss 0.7616 (0.9757) lr 2.0611e-03 eta 0:09:16
epoch [22/30] batch [4/96] time 0.317 (0.483) data 0.000 (0.138) loss 1.2704 (1.1448) lr 2.0611e-03 eta 0:06:55
epoch [22/30] batch [6/96] time 0.320 (0.431) data 0.000 (0.092) loss 0.7887 (1.0774) lr 2.0611e-03 eta 0:06:09
epoch [22/30] batch [8/96] time 0.346 (0.406) data 0.000 (0.069) loss 1.2721 (1.0796) lr 2.0611e-03 eta 0:05:47
epoch [22/30] batch [10/96] time 0.324 (0.391) data 0.000 (0.055) loss 1.2428 (1.1590) lr 2.0611e-03 eta 0:05:33
epoch [22/30] batch [12/96] time 0.337 (0.381) data 0.000 (0.046) loss 1.8287 (1.2308) lr 2.0611e-03 eta 0:05:24
epoch [22/30] batch [14/96] time 0.330 (0.374) data 0.000 (0.040) loss 1.7375 (1.3139) lr 2.0611e-03 eta 0:05:17
epoch [22/30] batch [16/96] time 0.336 (0.369) data 0.000 (0.035) loss 1.2779 (1.3046) lr 2.0611e-03 eta 0:05:13
epoch [22/30] batch [18/96] time 0.356 (0.367) data 0.000 (0.031) loss 2.2566 (1.3363) lr 2.0611e-03 eta 0:05:10
epoch [22/30] batch [20/96] time 0.317 (0.363) data 0.000 (0.028) loss 1.3867 (1.3368) lr 2.0611e-03 eta 0:05:06
epoch [22/30] batch [22/96] time 0.320 (0.359) data 0.000 (0.025) loss 1.6466 (1.3413) lr 2.0611e-03 eta 0:05:02
epoch [22/30] batch [24/96] time 0.315 (0.356) data 0.000 (0.023) loss 1.2584 (1.3682) lr 2.0611e-03 eta 0:04:58
epoch [22/30] batch [26/96] time 0.314 (0.353) data 0.000 (0.021) loss 1.3961 (1.3689) lr 2.0611e-03 eta 0:04:55
epoch [22/30] batch [28/96] time 0.312 (0.351) data 0.000 (0.020) loss 1.5982 (1.3572) lr 2.0611e-03 eta 0:04:53
epoch [22/30] batch [30/96] time 0.333 (0.349) data 0.000 (0.019) loss 1.3263 (1.3415) lr 2.0611e-03 eta 0:04:51
epoch [22/30] batch [32/96] time 0.324 (0.347) data 0.000 (0.017) loss 1.6173 (1.3406) lr 2.0611e-03 eta 0:04:48
epoch [22/30] batch [34/96] time 0.327 (0.346) data 0.000 (0.016) loss 1.6088 (1.3520) lr 2.0611e-03 eta 0:04:47
epoch [22/30] batch [36/96] time 0.416 (0.347) data 0.000 (0.016) loss 2.0683 (1.3692) lr 2.0611e-03 eta 0:04:47
epoch [22/30] batch [38/96] time 0.320 (0.346) data 0.000 (0.015) loss 2.0428 (1.3875) lr 2.0611e-03 eta 0:04:45
epoch [22/30] batch [40/96] time 0.323 (0.345) data 0.000 (0.014) loss 0.7380 (1.3722) lr 2.0611e-03 eta 0:04:44
epoch [22/30] batch [42/96] time 0.323 (0.343) data 0.000 (0.013) loss 1.2049 (1.3613) lr 2.0611e-03 eta 0:04:42
epoch [22/30] batch [44/96] time 0.323 (0.343) data 0.000 (0.013) loss 0.8810 (1.3431) lr 2.0611e-03 eta 0:04:40
epoch [22/30] batch [46/96] time 0.327 (0.342) data 0.000 (0.012) loss 1.5175 (1.3420) lr 2.0611e-03 eta 0:04:39
epoch [22/30] batch [48/96] time 0.314 (0.341) data 0.000 (0.012) loss 1.0983 (1.3274) lr 2.0611e-03 eta 0:04:38
epoch [22/30] batch [50/96] time 0.329 (0.340) data 0.000 (0.011) loss 0.9468 (1.3073) lr 2.0611e-03 eta 0:04:37
epoch [22/30] batch [52/96] time 0.325 (0.340) data 0.000 (0.011) loss 0.8655 (1.3001) lr 2.0611e-03 eta 0:04:35
epoch [22/30] batch [54/96] time 0.312 (0.339) data 0.000 (0.010) loss 1.3866 (1.2977) lr 2.0611e-03 eta 0:04:34
epoch [22/30] batch [56/96] time 0.337 (0.339) data 0.000 (0.010) loss 1.0469 (1.2885) lr 2.0611e-03 eta 0:04:33
epoch [22/30] batch [58/96] time 0.324 (0.338) data 0.000 (0.010) loss 1.0867 (1.2878) lr 2.0611e-03 eta 0:04:32
epoch [22/30] batch [60/96] time 0.321 (0.338) data 0.000 (0.009) loss 1.3321 (1.2926) lr 2.0611e-03 eta 0:04:31
epoch [22/30] batch [62/96] time 0.321 (0.337) data 0.000 (0.009) loss 1.1983 (1.2824) lr 2.0611e-03 eta 0:04:30
epoch [22/30] batch [64/96] time 0.312 (0.336) data 0.000 (0.009) loss 1.4586 (1.2831) lr 2.0611e-03 eta 0:04:29
epoch [22/30] batch [66/96] time 0.327 (0.336) data 0.000 (0.009) loss 0.8639 (1.2756) lr 2.0611e-03 eta 0:04:28
epoch [22/30] batch [68/96] time 0.328 (0.336) data 0.000 (0.008) loss 0.8500 (1.2839) lr 2.0611e-03 eta 0:04:27
epoch [22/30] batch [70/96] time 0.320 (0.335) data 0.000 (0.008) loss 1.0407 (1.2769) lr 2.0611e-03 eta 0:04:26
epoch [22/30] batch [72/96] time 0.316 (0.335) data 0.000 (0.008) loss 1.0171 (1.2695) lr 2.0611e-03 eta 0:04:25
epoch [22/30] batch [74/96] time 0.307 (0.334) data 0.000 (0.008) loss 1.0654 (1.2711) lr 2.0611e-03 eta 0:04:24
epoch [22/30] batch [76/96] time 0.310 (0.334) data 0.000 (0.008) loss 0.9092 (1.2690) lr 2.0611e-03 eta 0:04:22
epoch [22/30] batch [78/96] time 0.311 (0.333) data 0.000 (0.007) loss 1.0476 (1.2670) lr 2.0611e-03 eta 0:04:21
epoch [22/30] batch [80/96] time 0.305 (0.332) data 0.000 (0.007) loss 0.9067 (1.2584) lr 2.0611e-03 eta 0:04:20
epoch [22/30] batch [82/96] time 0.311 (0.332) data 0.000 (0.007) loss 1.3074 (1.2583) lr 2.0611e-03 eta 0:04:19
epoch [22/30] batch [84/96] time 0.302 (0.331) data 0.000 (0.007) loss 1.0332 (1.2601) lr 2.0611e-03 eta 0:04:18
epoch [22/30] batch [86/96] time 0.306 (0.330) data 0.000 (0.007) loss 0.8173 (1.2569) lr 2.0611e-03 eta 0:04:17
epoch [22/30] batch [88/96] time 0.308 (0.330) data 0.000 (0.007) loss 0.9959 (1.2500) lr 2.0611e-03 eta 0:04:15
epoch [22/30] batch [90/96] time 0.304 (0.329) data 0.000 (0.006) loss 1.4264 (1.2480) lr 2.0611e-03 eta 0:04:14
epoch [22/30] batch [92/96] time 0.308 (0.329) data 0.000 (0.006) loss 1.0363 (1.2449) lr 2.0611e-03 eta 0:04:13
epoch [22/30] batch [94/96] time 0.301 (0.328) data 0.000 (0.006) loss 1.2983 (1.2532) lr 2.0611e-03 eta 0:04:12
epoch [22/30] batch [96/96] time 0.310 (0.328) data 0.000 (0.006) loss 0.9304 (1.2445) lr 1.6543e-03 eta 0:04:11
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.82s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.33s/it]100%|██████████| 3/3 [00:03<00:00,  1.18it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 440
* accuracy: 76.4%
* error: 23.6%
* macro_f1: 76.0%

epoch [23/30] batch [2/96] time 0.343 (0.665) data 0.000 (0.290) loss 0.8391 (0.8116) lr 1.6543e-03 eta 0:08:29
epoch [23/30] batch [4/96] time 0.325 (0.494) data 0.000 (0.145) loss 1.2885 (0.9783) lr 1.6543e-03 eta 0:06:17
epoch [23/30] batch [6/96] time 0.335 (0.441) data 0.000 (0.097) loss 1.1531 (1.0509) lr 1.6543e-03 eta 0:05:35
epoch [23/30] batch [8/96] time 0.340 (0.416) data 0.001 (0.073) loss 1.0268 (1.0561) lr 1.6543e-03 eta 0:05:15
epoch [23/30] batch [10/96] time 0.350 (0.402) data 0.000 (0.058) loss 1.1787 (1.0852) lr 1.6543e-03 eta 0:05:05
epoch [23/30] batch [12/96] time 0.329 (0.391) data 0.000 (0.049) loss 1.3690 (1.1243) lr 1.6543e-03 eta 0:04:55
epoch [23/30] batch [14/96] time 0.327 (0.381) data 0.000 (0.042) loss 1.1020 (1.1208) lr 1.6543e-03 eta 0:04:47
epoch [23/30] batch [16/96] time 0.342 (0.377) data 0.000 (0.037) loss 1.2602 (1.1346) lr 1.6543e-03 eta 0:04:43
epoch [23/30] batch [18/96] time 0.342 (0.372) data 0.000 (0.033) loss 1.4953 (1.1458) lr 1.6543e-03 eta 0:04:39
epoch [23/30] batch [20/96] time 0.319 (0.368) data 0.000 (0.029) loss 1.4259 (1.1592) lr 1.6543e-03 eta 0:04:35
epoch [23/30] batch [22/96] time 0.333 (0.365) data 0.000 (0.027) loss 0.7578 (1.1283) lr 1.6543e-03 eta 0:04:32
epoch [23/30] batch [24/96] time 0.333 (0.363) data 0.000 (0.024) loss 0.7844 (1.1520) lr 1.6543e-03 eta 0:04:29
epoch [23/30] batch [26/96] time 0.332 (0.360) data 0.000 (0.023) loss 0.7966 (1.1447) lr 1.6543e-03 eta 0:04:27
epoch [23/30] batch [28/96] time 0.340 (0.359) data 0.000 (0.021) loss 1.4328 (1.1555) lr 1.6543e-03 eta 0:04:25
epoch [23/30] batch [30/96] time 0.330 (0.358) data 0.000 (0.020) loss 0.8998 (1.1494) lr 1.6543e-03 eta 0:04:23
epoch [23/30] batch [32/96] time 0.335 (0.357) data 0.000 (0.018) loss 0.9474 (1.1445) lr 1.6543e-03 eta 0:04:22
epoch [23/30] batch [34/96] time 0.336 (0.355) data 0.000 (0.017) loss 0.6961 (1.1258) lr 1.6543e-03 eta 0:04:20
epoch [23/30] batch [36/96] time 0.424 (0.357) data 0.000 (0.016) loss 1.0831 (1.1228) lr 1.6543e-03 eta 0:04:21
epoch [23/30] batch [38/96] time 0.334 (0.356) data 0.000 (0.016) loss 1.2537 (1.1260) lr 1.6543e-03 eta 0:04:19
epoch [23/30] batch [40/96] time 0.347 (0.355) data 0.000 (0.015) loss 1.1926 (1.1287) lr 1.6543e-03 eta 0:04:18
epoch [23/30] batch [42/96] time 0.334 (0.354) data 0.000 (0.014) loss 1.0889 (1.1257) lr 1.6543e-03 eta 0:04:17
epoch [23/30] batch [44/96] time 0.325 (0.353) data 0.000 (0.013) loss 1.8021 (1.1739) lr 1.6543e-03 eta 0:04:15
epoch [23/30] batch [46/96] time 0.323 (0.352) data 0.000 (0.013) loss 0.7329 (1.1624) lr 1.6543e-03 eta 0:04:13
epoch [23/30] batch [48/96] time 0.324 (0.351) data 0.000 (0.012) loss 0.7936 (1.1696) lr 1.6543e-03 eta 0:04:12
epoch [23/30] batch [50/96] time 0.326 (0.350) data 0.000 (0.012) loss 0.9452 (1.1757) lr 1.6543e-03 eta 0:04:10
epoch [23/30] batch [52/96] time 0.324 (0.348) data 0.000 (0.011) loss 1.2333 (1.1719) lr 1.6543e-03 eta 0:04:09
epoch [23/30] batch [54/96] time 0.320 (0.348) data 0.000 (0.011) loss 1.4134 (1.1715) lr 1.6543e-03 eta 0:04:08
epoch [23/30] batch [56/96] time 0.329 (0.347) data 0.000 (0.011) loss 2.0380 (1.1925) lr 1.6543e-03 eta 0:04:07
epoch [23/30] batch [58/96] time 0.320 (0.346) data 0.000 (0.010) loss 0.9693 (1.2052) lr 1.6543e-03 eta 0:04:05
epoch [23/30] batch [60/96] time 0.319 (0.346) data 0.000 (0.010) loss 1.9396 (1.2230) lr 1.6543e-03 eta 0:04:04
epoch [23/30] batch [62/96] time 0.324 (0.345) data 0.000 (0.010) loss 0.9274 (1.2138) lr 1.6543e-03 eta 0:04:03
epoch [23/30] batch [64/96] time 0.318 (0.344) data 0.000 (0.009) loss 1.1419 (1.2198) lr 1.6543e-03 eta 0:04:02
epoch [23/30] batch [66/96] time 0.327 (0.344) data 0.000 (0.009) loss 0.9286 (1.2095) lr 1.6543e-03 eta 0:04:01
epoch [23/30] batch [68/96] time 0.331 (0.343) data 0.000 (0.009) loss 1.0952 (1.2171) lr 1.6543e-03 eta 0:04:00
epoch [23/30] batch [70/96] time 0.323 (0.342) data 0.000 (0.009) loss 1.6018 (1.2206) lr 1.6543e-03 eta 0:03:59
epoch [23/30] batch [72/96] time 0.327 (0.342) data 0.000 (0.008) loss 1.4382 (1.2298) lr 1.6543e-03 eta 0:03:57
epoch [23/30] batch [74/96] time 0.314 (0.341) data 0.000 (0.008) loss 1.1986 (1.2247) lr 1.6543e-03 eta 0:03:56
epoch [23/30] batch [76/96] time 0.310 (0.340) data 0.000 (0.008) loss 1.4794 (1.2257) lr 1.6543e-03 eta 0:03:55
epoch [23/30] batch [78/96] time 0.313 (0.339) data 0.000 (0.008) loss 1.2392 (1.2324) lr 1.6543e-03 eta 0:03:54
epoch [23/30] batch [80/96] time 0.307 (0.339) data 0.000 (0.008) loss 1.0197 (1.2358) lr 1.6543e-03 eta 0:03:53
epoch [23/30] batch [82/96] time 0.314 (0.338) data 0.000 (0.007) loss 1.1062 (1.2294) lr 1.6543e-03 eta 0:03:51
epoch [23/30] batch [84/96] time 0.311 (0.337) data 0.000 (0.007) loss 0.9839 (1.2315) lr 1.6543e-03 eta 0:03:50
epoch [23/30] batch [86/96] time 0.311 (0.337) data 0.000 (0.007) loss 0.9879 (1.2283) lr 1.6543e-03 eta 0:03:49
epoch [23/30] batch [88/96] time 0.311 (0.336) data 0.000 (0.007) loss 0.7581 (1.2229) lr 1.6543e-03 eta 0:03:48
epoch [23/30] batch [90/96] time 0.310 (0.336) data 0.000 (0.007) loss 0.9888 (1.2242) lr 1.6543e-03 eta 0:03:47
epoch [23/30] batch [92/96] time 0.312 (0.335) data 0.000 (0.007) loss 0.8428 (1.2170) lr 1.6543e-03 eta 0:03:46
epoch [23/30] batch [94/96] time 0.311 (0.335) data 0.000 (0.006) loss 1.2850 (1.2234) lr 1.6543e-03 eta 0:03:45
epoch [23/30] batch [96/96] time 0.332 (0.334) data 0.000 (0.006) loss 1.5778 (1.2301) lr 1.2843e-03 eta 0:03:44
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.86s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 442
* accuracy: 76.7%
* error: 23.3%
* macro_f1: 76.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [24/30] batch [2/96] time 0.322 (0.654) data 0.000 (0.283) loss 1.0104 (1.0285) lr 1.2843e-03 eta 0:07:18
epoch [24/30] batch [4/96] time 0.334 (0.492) data 0.000 (0.142) loss 1.4345 (1.3664) lr 1.2843e-03 eta 0:05:28
epoch [24/30] batch [6/96] time 0.342 (0.442) data 0.000 (0.095) loss 1.2928 (1.3536) lr 1.2843e-03 eta 0:04:54
epoch [24/30] batch [8/96] time 0.336 (0.414) data 0.000 (0.071) loss 1.4144 (1.3144) lr 1.2843e-03 eta 0:04:35
epoch [24/30] batch [10/96] time 0.319 (0.396) data 0.000 (0.057) loss 0.9799 (1.2482) lr 1.2843e-03 eta 0:04:22
epoch [24/30] batch [12/96] time 0.327 (0.385) data 0.000 (0.047) loss 1.3454 (1.2771) lr 1.2843e-03 eta 0:04:14
epoch [24/30] batch [14/96] time 0.323 (0.377) data 0.000 (0.041) loss 0.8797 (1.2927) lr 1.2843e-03 eta 0:04:07
epoch [24/30] batch [16/96] time 0.323 (0.370) data 0.000 (0.036) loss 1.1988 (1.3176) lr 1.2843e-03 eta 0:04:02
epoch [24/30] batch [18/96] time 0.330 (0.367) data 0.000 (0.032) loss 1.8172 (1.3341) lr 1.2843e-03 eta 0:03:59
epoch [24/30] batch [20/96] time 0.317 (0.362) data 0.000 (0.029) loss 0.7947 (1.3036) lr 1.2843e-03 eta 0:03:55
epoch [24/30] batch [22/96] time 0.319 (0.359) data 0.000 (0.026) loss 0.8040 (1.3416) lr 1.2843e-03 eta 0:03:53
epoch [24/30] batch [24/96] time 0.332 (0.357) data 0.000 (0.024) loss 1.0219 (1.3096) lr 1.2843e-03 eta 0:03:51
epoch [24/30] batch [26/96] time 0.324 (0.354) data 0.000 (0.022) loss 0.9752 (1.2904) lr 1.2843e-03 eta 0:03:48
epoch [24/30] batch [28/96] time 0.321 (0.352) data 0.000 (0.021) loss 0.8524 (1.2809) lr 1.2843e-03 eta 0:03:46
epoch [24/30] batch [30/96] time 0.341 (0.351) data 0.000 (0.019) loss 1.1204 (1.2677) lr 1.2843e-03 eta 0:03:45
epoch [24/30] batch [32/96] time 0.335 (0.350) data 0.000 (0.018) loss 1.0021 (1.2639) lr 1.2843e-03 eta 0:03:44
epoch [24/30] batch [34/96] time 0.344 (0.350) data 0.000 (0.017) loss 1.2016 (1.2595) lr 1.2843e-03 eta 0:03:43
epoch [24/30] batch [36/96] time 0.422 (0.351) data 0.000 (0.016) loss 0.8658 (1.2567) lr 1.2843e-03 eta 0:03:43
epoch [24/30] batch [38/96] time 0.320 (0.349) data 0.000 (0.015) loss 1.0799 (1.2489) lr 1.2843e-03 eta 0:03:41
epoch [24/30] batch [40/96] time 0.318 (0.348) data 0.000 (0.014) loss 0.7959 (1.2420) lr 1.2843e-03 eta 0:03:40
epoch [24/30] batch [42/96] time 0.338 (0.348) data 0.000 (0.014) loss 1.2676 (1.2595) lr 1.2843e-03 eta 0:03:38
epoch [24/30] batch [44/96] time 0.328 (0.347) data 0.000 (0.013) loss 1.8930 (1.2831) lr 1.2843e-03 eta 0:03:37
epoch [24/30] batch [46/96] time 0.326 (0.346) data 0.000 (0.013) loss 1.0737 (1.2817) lr 1.2843e-03 eta 0:03:36
epoch [24/30] batch [48/96] time 0.327 (0.345) data 0.000 (0.012) loss 1.0133 (1.2801) lr 1.2843e-03 eta 0:03:35
epoch [24/30] batch [50/96] time 0.323 (0.344) data 0.000 (0.012) loss 0.6882 (1.2824) lr 1.2843e-03 eta 0:03:34
epoch [24/30] batch [52/96] time 0.325 (0.343) data 0.000 (0.011) loss 1.2908 (1.2882) lr 1.2843e-03 eta 0:03:32
epoch [24/30] batch [54/96] time 0.320 (0.343) data 0.000 (0.011) loss 1.3980 (1.2958) lr 1.2843e-03 eta 0:03:31
epoch [24/30] batch [56/96] time 0.322 (0.342) data 0.000 (0.010) loss 1.1821 (1.2893) lr 1.2843e-03 eta 0:03:30
epoch [24/30] batch [58/96] time 0.326 (0.341) data 0.000 (0.010) loss 2.2117 (1.3106) lr 1.2843e-03 eta 0:03:29
epoch [24/30] batch [60/96] time 0.327 (0.341) data 0.000 (0.010) loss 1.0224 (1.3028) lr 1.2843e-03 eta 0:03:28
epoch [24/30] batch [62/96] time 0.327 (0.341) data 0.000 (0.009) loss 1.4544 (1.2984) lr 1.2843e-03 eta 0:03:27
epoch [24/30] batch [64/96] time 0.321 (0.340) data 0.000 (0.009) loss 0.6687 (1.2847) lr 1.2843e-03 eta 0:03:26
epoch [24/30] batch [66/96] time 0.337 (0.340) data 0.000 (0.009) loss 0.8234 (1.2702) lr 1.2843e-03 eta 0:03:25
epoch [24/30] batch [68/96] time 0.326 (0.339) data 0.000 (0.009) loss 1.1627 (1.2606) lr 1.2843e-03 eta 0:03:25
epoch [24/30] batch [70/96] time 0.329 (0.339) data 0.000 (0.008) loss 1.0063 (1.2542) lr 1.2843e-03 eta 0:03:24
epoch [24/30] batch [72/96] time 0.340 (0.339) data 0.000 (0.008) loss 1.8343 (1.2584) lr 1.2843e-03 eta 0:03:23
epoch [24/30] batch [74/96] time 0.313 (0.338) data 0.000 (0.008) loss 0.7669 (1.2561) lr 1.2843e-03 eta 0:03:22
epoch [24/30] batch [76/96] time 0.309 (0.338) data 0.000 (0.008) loss 0.9075 (1.2508) lr 1.2843e-03 eta 0:03:21
epoch [24/30] batch [78/96] time 0.310 (0.337) data 0.000 (0.008) loss 0.9550 (1.2577) lr 1.2843e-03 eta 0:03:20
epoch [24/30] batch [80/96] time 0.306 (0.336) data 0.000 (0.007) loss 1.2018 (1.2523) lr 1.2843e-03 eta 0:03:18
epoch [24/30] batch [82/96] time 0.308 (0.335) data 0.000 (0.007) loss 1.6269 (1.2603) lr 1.2843e-03 eta 0:03:17
epoch [24/30] batch [84/96] time 0.308 (0.335) data 0.000 (0.007) loss 1.0071 (1.2551) lr 1.2843e-03 eta 0:03:16
epoch [24/30] batch [86/96] time 0.309 (0.334) data 0.000 (0.007) loss 0.9715 (1.2493) lr 1.2843e-03 eta 0:03:15
epoch [24/30] batch [88/96] time 0.309 (0.333) data 0.000 (0.007) loss 1.2071 (1.2478) lr 1.2843e-03 eta 0:03:14
epoch [24/30] batch [90/96] time 0.308 (0.333) data 0.000 (0.007) loss 0.9132 (1.2463) lr 1.2843e-03 eta 0:03:13
epoch [24/30] batch [92/96] time 0.308 (0.332) data 0.000 (0.006) loss 1.4781 (1.2574) lr 1.2843e-03 eta 0:03:12
epoch [24/30] batch [94/96] time 0.304 (0.332) data 0.000 (0.006) loss 0.6848 (1.2505) lr 1.2843e-03 eta 0:03:11
epoch [24/30] batch [96/96] time 0.308 (0.331) data 0.000 (0.006) loss 1.7311 (1.2539) lr 9.5492e-04 eta 0:03:10
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.85s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 439
* accuracy: 76.2%
* error: 23.8%
* macro_f1: 75.8%

epoch [25/30] batch [2/96] time 0.335 (0.670) data 0.001 (0.305) loss 0.8628 (0.9684) lr 9.5492e-04 eta 0:06:24
epoch [25/30] batch [4/96] time 0.317 (0.499) data 0.000 (0.153) loss 1.4073 (1.0847) lr 9.5492e-04 eta 0:04:45
epoch [25/30] batch [6/96] time 0.381 (0.450) data 0.000 (0.102) loss 1.0969 (1.1014) lr 9.5492e-04 eta 0:04:16
epoch [25/30] batch [8/96] time 0.325 (0.422) data 0.000 (0.076) loss 0.8021 (1.0976) lr 9.5492e-04 eta 0:03:59
epoch [25/30] batch [10/96] time 0.313 (0.402) data 0.000 (0.061) loss 0.9695 (1.0815) lr 9.5492e-04 eta 0:03:47
epoch [25/30] batch [12/96] time 0.325 (0.389) data 0.000 (0.051) loss 1.6334 (1.1176) lr 9.5492e-04 eta 0:03:39
epoch [25/30] batch [14/96] time 0.338 (0.381) data 0.000 (0.044) loss 0.8282 (1.1456) lr 9.5492e-04 eta 0:03:34
epoch [25/30] batch [16/96] time 0.328 (0.375) data 0.000 (0.038) loss 1.2194 (1.1838) lr 9.5492e-04 eta 0:03:29
epoch [25/30] batch [18/96] time 0.324 (0.369) data 0.000 (0.034) loss 1.0772 (1.1863) lr 9.5492e-04 eta 0:03:25
epoch [25/30] batch [20/96] time 0.318 (0.364) data 0.000 (0.031) loss 1.1634 (1.1782) lr 9.5492e-04 eta 0:03:22
epoch [25/30] batch [22/96] time 0.338 (0.361) data 0.000 (0.028) loss 1.0084 (1.1813) lr 9.5492e-04 eta 0:03:19
epoch [25/30] batch [24/96] time 0.326 (0.358) data 0.000 (0.026) loss 1.7466 (1.2542) lr 9.5492e-04 eta 0:03:17
epoch [25/30] batch [26/96] time 0.319 (0.355) data 0.000 (0.024) loss 1.4096 (1.2710) lr 9.5492e-04 eta 0:03:15
epoch [25/30] batch [28/96] time 0.330 (0.353) data 0.000 (0.022) loss 1.0891 (1.2569) lr 9.5492e-04 eta 0:03:13
epoch [25/30] batch [30/96] time 0.322 (0.351) data 0.000 (0.021) loss 1.8579 (1.2859) lr 9.5492e-04 eta 0:03:11
epoch [25/30] batch [32/96] time 0.321 (0.349) data 0.000 (0.019) loss 0.9108 (1.2618) lr 9.5492e-04 eta 0:03:09
epoch [25/30] batch [34/96] time 0.335 (0.348) data 0.000 (0.018) loss 1.1831 (1.2579) lr 9.5492e-04 eta 0:03:08
epoch [25/30] batch [36/96] time 0.428 (0.349) data 0.000 (0.017) loss 0.7830 (1.2449) lr 9.5492e-04 eta 0:03:08
epoch [25/30] batch [38/96] time 0.335 (0.349) data 0.000 (0.016) loss 1.0366 (1.2467) lr 9.5492e-04 eta 0:03:07
epoch [25/30] batch [40/96] time 0.327 (0.348) data 0.000 (0.016) loss 1.1937 (1.2527) lr 9.5492e-04 eta 0:03:06
epoch [25/30] batch [42/96] time 0.324 (0.347) data 0.000 (0.015) loss 0.9438 (1.2355) lr 9.5492e-04 eta 0:03:05
epoch [25/30] batch [44/96] time 0.321 (0.346) data 0.000 (0.014) loss 0.7553 (1.2237) lr 9.5492e-04 eta 0:03:04
epoch [25/30] batch [46/96] time 0.328 (0.345) data 0.000 (0.014) loss 2.2131 (1.2494) lr 9.5492e-04 eta 0:03:02
epoch [25/30] batch [48/96] time 0.315 (0.344) data 0.000 (0.013) loss 1.6784 (1.2558) lr 9.5492e-04 eta 0:03:01
epoch [25/30] batch [50/96] time 0.331 (0.343) data 0.000 (0.012) loss 1.2289 (1.2565) lr 9.5492e-04 eta 0:03:00
epoch [25/30] batch [52/96] time 0.320 (0.343) data 0.000 (0.012) loss 1.1619 (1.2488) lr 9.5492e-04 eta 0:02:59
epoch [25/30] batch [54/96] time 0.322 (0.342) data 0.000 (0.012) loss 1.6859 (1.2495) lr 9.5492e-04 eta 0:02:58
epoch [25/30] batch [56/96] time 0.336 (0.342) data 0.000 (0.011) loss 0.9055 (1.2401) lr 9.5492e-04 eta 0:02:57
epoch [25/30] batch [58/96] time 0.327 (0.341) data 0.000 (0.011) loss 1.1853 (1.2330) lr 9.5492e-04 eta 0:02:56
epoch [25/30] batch [60/96] time 0.326 (0.340) data 0.000 (0.010) loss 1.0672 (1.2396) lr 9.5492e-04 eta 0:02:55
epoch [25/30] batch [62/96] time 0.325 (0.340) data 0.001 (0.010) loss 1.2287 (1.2414) lr 9.5492e-04 eta 0:02:54
epoch [25/30] batch [64/96] time 0.325 (0.340) data 0.000 (0.010) loss 1.5460 (1.2456) lr 9.5492e-04 eta 0:02:53
epoch [25/30] batch [66/96] time 0.321 (0.339) data 0.000 (0.010) loss 1.3890 (1.2530) lr 9.5492e-04 eta 0:02:53
epoch [25/30] batch [68/96] time 0.326 (0.339) data 0.000 (0.009) loss 0.7421 (1.2516) lr 9.5492e-04 eta 0:02:52
epoch [25/30] batch [70/96] time 0.335 (0.339) data 0.000 (0.009) loss 1.2910 (1.2476) lr 9.5492e-04 eta 0:02:51
epoch [25/30] batch [72/96] time 0.319 (0.339) data 0.000 (0.009) loss 1.8373 (1.2512) lr 9.5492e-04 eta 0:02:50
epoch [25/30] batch [74/96] time 0.312 (0.338) data 0.000 (0.009) loss 2.0577 (1.2615) lr 9.5492e-04 eta 0:02:49
epoch [25/30] batch [76/96] time 0.312 (0.337) data 0.000 (0.008) loss 1.4801 (1.2645) lr 9.5492e-04 eta 0:02:48
epoch [25/30] batch [78/96] time 0.313 (0.337) data 0.000 (0.008) loss 1.4965 (1.2678) lr 9.5492e-04 eta 0:02:47
epoch [25/30] batch [80/96] time 0.306 (0.336) data 0.000 (0.008) loss 1.1662 (1.2704) lr 9.5492e-04 eta 0:02:46
epoch [25/30] batch [82/96] time 0.314 (0.335) data 0.000 (0.008) loss 1.1188 (1.2661) lr 9.5492e-04 eta 0:02:45
epoch [25/30] batch [84/96] time 0.308 (0.335) data 0.000 (0.008) loss 1.1670 (1.2683) lr 9.5492e-04 eta 0:02:44
epoch [25/30] batch [86/96] time 0.312 (0.334) data 0.000 (0.007) loss 1.5196 (1.2694) lr 9.5492e-04 eta 0:02:43
epoch [25/30] batch [88/96] time 0.308 (0.334) data 0.000 (0.007) loss 0.9006 (1.2621) lr 9.5492e-04 eta 0:02:42
epoch [25/30] batch [90/96] time 0.306 (0.333) data 0.000 (0.007) loss 0.8257 (1.2581) lr 9.5492e-04 eta 0:02:41
epoch [25/30] batch [92/96] time 0.315 (0.333) data 0.000 (0.007) loss 1.1282 (1.2515) lr 9.5492e-04 eta 0:02:41
epoch [25/30] batch [94/96] time 0.308 (0.332) data 0.000 (0.007) loss 0.9451 (1.2458) lr 9.5492e-04 eta 0:02:40
epoch [25/30] batch [96/96] time 0.312 (0.332) data 0.000 (0.007) loss 1.2790 (1.2447) lr 6.6987e-04 eta 0:02:39
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.86s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]100%|██████████| 3/3 [00:03<00:00,  1.16it/s]100%|██████████| 3/3 [00:03<00:00,  1.18s/it]=> result
* total: 576
* correct: 443
* accuracy: 76.9%
* error: 23.1%
* macro_f1: 76.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [26/30] batch [2/96] time 0.325 (0.649) data 0.001 (0.281) loss 0.8560 (1.2342) lr 6.6987e-04 eta 0:05:10
epoch [26/30] batch [4/96] time 0.321 (0.485) data 0.000 (0.141) loss 0.7001 (1.2282) lr 6.6987e-04 eta 0:03:50
epoch [26/30] batch [6/96] time 0.325 (0.433) data 0.000 (0.094) loss 1.4186 (1.2187) lr 6.6987e-04 eta 0:03:25
epoch [26/30] batch [8/96] time 0.335 (0.406) data 0.000 (0.071) loss 1.3996 (1.2166) lr 6.6987e-04 eta 0:03:11
epoch [26/30] batch [10/96] time 0.311 (0.387) data 0.000 (0.057) loss 1.0264 (1.2339) lr 6.6987e-04 eta 0:03:02
epoch [26/30] batch [12/96] time 0.331 (0.378) data 0.001 (0.047) loss 0.8661 (1.2322) lr 6.6987e-04 eta 0:02:56
epoch [26/30] batch [14/96] time 0.318 (0.370) data 0.000 (0.040) loss 1.7538 (1.2632) lr 6.6987e-04 eta 0:02:52
epoch [26/30] batch [16/96] time 0.328 (0.364) data 0.000 (0.035) loss 0.8842 (1.2519) lr 6.6987e-04 eta 0:02:49
epoch [26/30] batch [18/96] time 0.323 (0.360) data 0.000 (0.032) loss 1.1919 (1.2404) lr 6.6987e-04 eta 0:02:46
epoch [26/30] batch [20/96] time 0.324 (0.356) data 0.000 (0.028) loss 0.9383 (1.2080) lr 6.6987e-04 eta 0:02:43
epoch [26/30] batch [22/96] time 0.333 (0.353) data 0.001 (0.026) loss 1.3840 (1.2364) lr 6.6987e-04 eta 0:02:41
epoch [26/30] batch [24/96] time 0.316 (0.351) data 0.000 (0.024) loss 1.9538 (1.2656) lr 6.6987e-04 eta 0:02:40
epoch [26/30] batch [26/96] time 0.315 (0.348) data 0.000 (0.022) loss 0.6773 (1.2355) lr 6.6987e-04 eta 0:02:38
epoch [26/30] batch [28/96] time 0.337 (0.347) data 0.001 (0.020) loss 1.1023 (1.2193) lr 6.6987e-04 eta 0:02:36
epoch [26/30] batch [30/96] time 0.320 (0.346) data 0.000 (0.019) loss 0.9488 (1.1947) lr 6.6987e-04 eta 0:02:35
epoch [26/30] batch [32/96] time 0.334 (0.345) data 0.000 (0.018) loss 1.9993 (1.2189) lr 6.6987e-04 eta 0:02:34
epoch [26/30] batch [34/96] time 0.334 (0.343) data 0.000 (0.017) loss 1.6964 (1.2272) lr 6.6987e-04 eta 0:02:33
epoch [26/30] batch [36/96] time 0.406 (0.345) data 0.000 (0.016) loss 0.9568 (1.2140) lr 6.6987e-04 eta 0:02:33
epoch [26/30] batch [38/96] time 0.332 (0.344) data 0.000 (0.015) loss 1.0068 (1.2075) lr 6.6987e-04 eta 0:02:31
epoch [26/30] batch [40/96] time 0.328 (0.342) data 0.000 (0.014) loss 1.2011 (1.2011) lr 6.6987e-04 eta 0:02:30
epoch [26/30] batch [42/96] time 0.327 (0.342) data 0.000 (0.014) loss 1.1179 (1.1872) lr 6.6987e-04 eta 0:02:29
epoch [26/30] batch [44/96] time 0.323 (0.341) data 0.000 (0.013) loss 1.0925 (1.1756) lr 6.6987e-04 eta 0:02:28
epoch [26/30] batch [46/96] time 0.321 (0.340) data 0.000 (0.013) loss 1.6632 (1.1842) lr 6.6987e-04 eta 0:02:27
epoch [26/30] batch [48/96] time 0.318 (0.340) data 0.000 (0.012) loss 0.8890 (1.1862) lr 6.6987e-04 eta 0:02:26
epoch [26/30] batch [50/96] time 0.329 (0.339) data 0.000 (0.012) loss 0.9832 (1.1844) lr 6.6987e-04 eta 0:02:25
epoch [26/30] batch [52/96] time 0.325 (0.338) data 0.000 (0.011) loss 0.8295 (1.1688) lr 6.6987e-04 eta 0:02:24
epoch [26/30] batch [54/96] time 0.325 (0.338) data 0.000 (0.011) loss 0.9022 (1.1570) lr 6.6987e-04 eta 0:02:23
epoch [26/30] batch [56/96] time 0.335 (0.338) data 0.000 (0.010) loss 0.9986 (1.1467) lr 6.6987e-04 eta 0:02:23
epoch [26/30] batch [58/96] time 0.315 (0.337) data 0.000 (0.010) loss 1.6420 (1.1580) lr 6.6987e-04 eta 0:02:22
epoch [26/30] batch [60/96] time 0.316 (0.336) data 0.000 (0.010) loss 1.3754 (1.1667) lr 6.6987e-04 eta 0:02:21
epoch [26/30] batch [62/96] time 0.321 (0.336) data 0.000 (0.009) loss 1.6835 (1.1770) lr 6.6987e-04 eta 0:02:20
epoch [26/30] batch [64/96] time 0.317 (0.335) data 0.000 (0.009) loss 1.2774 (1.1767) lr 6.6987e-04 eta 0:02:19
epoch [26/30] batch [66/96] time 0.329 (0.335) data 0.000 (0.009) loss 1.3939 (1.1756) lr 6.6987e-04 eta 0:02:18
epoch [26/30] batch [68/96] time 0.328 (0.335) data 0.000 (0.009) loss 1.1923 (1.1770) lr 6.6987e-04 eta 0:02:18
epoch [26/30] batch [70/96] time 0.332 (0.335) data 0.000 (0.008) loss 0.7257 (1.1722) lr 6.6987e-04 eta 0:02:17
epoch [26/30] batch [72/96] time 0.337 (0.335) data 0.000 (0.008) loss 1.3153 (1.1711) lr 6.6987e-04 eta 0:02:16
epoch [26/30] batch [74/96] time 0.311 (0.334) data 0.000 (0.008) loss 1.9583 (1.1777) lr 6.6987e-04 eta 0:02:15
epoch [26/30] batch [76/96] time 0.319 (0.334) data 0.000 (0.008) loss 1.4372 (1.1805) lr 6.6987e-04 eta 0:02:14
epoch [26/30] batch [78/96] time 0.314 (0.334) data 0.000 (0.008) loss 0.7865 (1.1692) lr 6.6987e-04 eta 0:02:14
epoch [26/30] batch [80/96] time 0.312 (0.333) data 0.000 (0.007) loss 1.6027 (1.1907) lr 6.6987e-04 eta 0:02:13
epoch [26/30] batch [82/96] time 0.317 (0.333) data 0.000 (0.007) loss 1.4335 (1.1952) lr 6.6987e-04 eta 0:02:12
epoch [26/30] batch [84/96] time 0.313 (0.332) data 0.000 (0.007) loss 0.9535 (1.1896) lr 6.6987e-04 eta 0:02:11
epoch [26/30] batch [86/96] time 0.318 (0.332) data 0.000 (0.007) loss 0.8436 (1.1852) lr 6.6987e-04 eta 0:02:10
epoch [26/30] batch [88/96] time 0.312 (0.331) data 0.000 (0.007) loss 0.8045 (1.1794) lr 6.6987e-04 eta 0:02:09
epoch [26/30] batch [90/96] time 0.317 (0.331) data 0.000 (0.007) loss 0.9530 (1.1824) lr 6.6987e-04 eta 0:02:09
epoch [26/30] batch [92/96] time 0.320 (0.331) data 0.000 (0.006) loss 1.6244 (1.1870) lr 6.6987e-04 eta 0:02:08
epoch [26/30] batch [94/96] time 0.309 (0.330) data 0.000 (0.006) loss 1.2816 (1.1857) lr 6.6987e-04 eta 0:02:07
epoch [26/30] batch [96/96] time 0.316 (0.330) data 0.000 (0.006) loss 1.3209 (1.1916) lr 4.3227e-04 eta 0:02:06
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.80s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.33s/it]100%|██████████| 3/3 [00:03<00:00,  1.18it/s]100%|██████████| 3/3 [00:03<00:00,  1.16s/it]=> result
* total: 576
* correct: 444
* accuracy: 77.1%
* error: 22.9%
* macro_f1: 76.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar

epoch [27/30] batch [2/96] time 0.316 (0.633) data 0.000 (0.276) loss 1.2364 (1.2575) lr 4.3227e-04 eta 0:04:01
epoch [27/30] batch [4/96] time 0.315 (0.475) data 0.000 (0.138) loss 0.9278 (1.1419) lr 4.3227e-04 eta 0:03:00
epoch [27/30] batch [6/96] time 0.311 (0.422) data 0.000 (0.092) loss 1.8730 (1.3078) lr 4.3227e-04 eta 0:02:39
epoch [27/30] batch [8/96] time 0.332 (0.400) data 0.000 (0.069) loss 1.5504 (1.3864) lr 4.3227e-04 eta 0:02:30
epoch [27/30] batch [10/96] time 0.330 (0.386) data 0.000 (0.055) loss 1.0379 (1.3382) lr 4.3227e-04 eta 0:02:24
epoch [27/30] batch [12/96] time 0.325 (0.375) data 0.000 (0.046) loss 0.8614 (1.2702) lr 4.3227e-04 eta 0:02:19
epoch [27/30] batch [14/96] time 0.320 (0.368) data 0.000 (0.040) loss 1.0403 (1.2590) lr 4.3227e-04 eta 0:02:16
epoch [27/30] batch [16/96] time 0.328 (0.363) data 0.000 (0.035) loss 1.4180 (1.2530) lr 4.3227e-04 eta 0:02:13
epoch [27/30] batch [18/96] time 0.345 (0.360) data 0.000 (0.031) loss 1.2519 (1.2522) lr 4.3227e-04 eta 0:02:11
epoch [27/30] batch [20/96] time 0.334 (0.357) data 0.000 (0.028) loss 1.2626 (1.2778) lr 4.3227e-04 eta 0:02:09
epoch [27/30] batch [22/96] time 0.323 (0.353) data 0.000 (0.025) loss 1.3209 (1.2602) lr 4.3227e-04 eta 0:02:07
epoch [27/30] batch [24/96] time 0.326 (0.352) data 0.000 (0.023) loss 2.0336 (1.2848) lr 4.3227e-04 eta 0:02:06
epoch [27/30] batch [26/96] time 0.330 (0.350) data 0.000 (0.022) loss 1.0098 (1.2721) lr 4.3227e-04 eta 0:02:05
epoch [27/30] batch [28/96] time 0.324 (0.348) data 0.000 (0.020) loss 0.9553 (1.2600) lr 4.3227e-04 eta 0:02:03
epoch [27/30] batch [30/96] time 0.315 (0.346) data 0.000 (0.019) loss 2.2052 (1.2969) lr 4.3227e-04 eta 0:02:02
epoch [27/30] batch [32/96] time 0.321 (0.344) data 0.000 (0.018) loss 0.8051 (1.2721) lr 4.3227e-04 eta 0:02:01
epoch [27/30] batch [34/96] time 0.321 (0.343) data 0.000 (0.017) loss 0.9242 (1.2493) lr 4.3227e-04 eta 0:01:59
epoch [27/30] batch [36/96] time 0.421 (0.344) data 0.000 (0.016) loss 1.1506 (1.2373) lr 4.3227e-04 eta 0:01:59
epoch [27/30] batch [38/96] time 0.365 (0.345) data 0.000 (0.015) loss 1.2706 (1.2287) lr 4.3227e-04 eta 0:01:59
epoch [27/30] batch [40/96] time 0.333 (0.345) data 0.000 (0.014) loss 0.8249 (1.2114) lr 4.3227e-04 eta 0:01:58
epoch [27/30] batch [42/96] time 0.321 (0.344) data 0.000 (0.013) loss 1.4336 (1.2123) lr 4.3227e-04 eta 0:01:57
epoch [27/30] batch [44/96] time 0.317 (0.343) data 0.000 (0.013) loss 2.1536 (1.2477) lr 4.3227e-04 eta 0:01:56
epoch [27/30] batch [46/96] time 0.321 (0.342) data 0.000 (0.012) loss 1.1488 (1.2490) lr 4.3227e-04 eta 0:01:55
epoch [27/30] batch [48/96] time 0.326 (0.342) data 0.000 (0.012) loss 0.7626 (1.2310) lr 4.3227e-04 eta 0:01:54
epoch [27/30] batch [50/96] time 0.328 (0.341) data 0.000 (0.011) loss 1.4376 (1.2379) lr 4.3227e-04 eta 0:01:53
epoch [27/30] batch [52/96] time 0.316 (0.340) data 0.000 (0.011) loss 0.8127 (1.2269) lr 4.3227e-04 eta 0:01:52
epoch [27/30] batch [54/96] time 0.344 (0.340) data 0.000 (0.011) loss 1.4097 (1.2294) lr 4.3227e-04 eta 0:01:52
epoch [27/30] batch [56/96] time 0.320 (0.339) data 0.000 (0.010) loss 1.0367 (1.2220) lr 4.3227e-04 eta 0:01:51
epoch [27/30] batch [58/96] time 0.321 (0.338) data 0.000 (0.010) loss 1.4438 (1.2272) lr 4.3227e-04 eta 0:01:50
epoch [27/30] batch [60/96] time 0.323 (0.338) data 0.000 (0.010) loss 1.0428 (1.2197) lr 4.3227e-04 eta 0:01:49
epoch [27/30] batch [62/96] time 0.319 (0.338) data 0.000 (0.009) loss 1.3125 (1.2273) lr 4.3227e-04 eta 0:01:48
epoch [27/30] batch [64/96] time 0.325 (0.337) data 0.000 (0.009) loss 1.4072 (1.2294) lr 4.3227e-04 eta 0:01:47
epoch [27/30] batch [66/96] time 0.322 (0.337) data 0.000 (0.009) loss 0.9102 (1.2331) lr 4.3227e-04 eta 0:01:47
epoch [27/30] batch [68/96] time 0.325 (0.336) data 0.000 (0.008) loss 1.6719 (1.2345) lr 4.3227e-04 eta 0:01:46
epoch [27/30] batch [70/96] time 0.317 (0.336) data 0.000 (0.008) loss 1.0419 (1.2417) lr 4.3227e-04 eta 0:01:45
epoch [27/30] batch [72/96] time 0.325 (0.336) data 0.000 (0.008) loss 1.5654 (1.2449) lr 4.3227e-04 eta 0:01:44
epoch [27/30] batch [74/96] time 0.307 (0.335) data 0.000 (0.008) loss 1.1528 (1.2422) lr 4.3227e-04 eta 0:01:43
epoch [27/30] batch [76/96] time 0.307 (0.334) data 0.000 (0.008) loss 1.1355 (1.2470) lr 4.3227e-04 eta 0:01:42
epoch [27/30] batch [78/96] time 0.306 (0.333) data 0.000 (0.007) loss 1.1727 (1.2404) lr 4.3227e-04 eta 0:01:42
epoch [27/30] batch [80/96] time 0.311 (0.333) data 0.000 (0.007) loss 1.3713 (1.2378) lr 4.3227e-04 eta 0:01:41
epoch [27/30] batch [82/96] time 0.303 (0.332) data 0.000 (0.007) loss 1.3485 (1.2397) lr 4.3227e-04 eta 0:01:40
epoch [27/30] batch [84/96] time 0.310 (0.332) data 0.000 (0.007) loss 0.6985 (1.2307) lr 4.3227e-04 eta 0:01:39
epoch [27/30] batch [86/96] time 0.310 (0.331) data 0.000 (0.007) loss 1.3895 (1.2343) lr 4.3227e-04 eta 0:01:38
epoch [27/30] batch [88/96] time 0.305 (0.331) data 0.000 (0.007) loss 1.4518 (1.2445) lr 4.3227e-04 eta 0:01:37
epoch [27/30] batch [90/96] time 0.308 (0.330) data 0.000 (0.006) loss 0.7196 (1.2347) lr 4.3227e-04 eta 0:01:37
epoch [27/30] batch [92/96] time 0.300 (0.329) data 0.000 (0.006) loss 0.7641 (1.2316) lr 4.3227e-04 eta 0:01:36
epoch [27/30] batch [94/96] time 0.303 (0.329) data 0.000 (0.006) loss 1.3569 (1.2287) lr 4.3227e-04 eta 0:01:35
epoch [27/30] batch [96/96] time 0.306 (0.328) data 0.000 (0.006) loss 1.0423 (1.2255) lr 2.4472e-04 eta 0:01:34
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.90s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.37s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.20s/it]=> result
* total: 576
* correct: 443
* accuracy: 76.9%
* error: 23.1%
* macro_f1: 76.5%

epoch [28/30] batch [2/96] time 0.318 (0.634) data 0.000 (0.274) loss 0.9841 (1.1209) lr 2.4472e-04 eta 0:03:01
epoch [28/30] batch [4/96] time 0.343 (0.486) data 0.000 (0.137) loss 0.7048 (1.0764) lr 2.4472e-04 eta 0:02:18
epoch [28/30] batch [6/96] time 0.328 (0.435) data 0.000 (0.092) loss 1.5981 (1.1299) lr 2.4472e-04 eta 0:02:02
epoch [28/30] batch [8/96] time 0.325 (0.406) data 0.000 (0.069) loss 1.7627 (1.1720) lr 2.4472e-04 eta 0:01:53
epoch [28/30] batch [10/96] time 0.318 (0.389) data 0.000 (0.055) loss 0.7559 (1.1076) lr 2.4472e-04 eta 0:01:48
epoch [28/30] batch [12/96] time 0.317 (0.378) data 0.000 (0.046) loss 1.2641 (1.1158) lr 2.4472e-04 eta 0:01:44
epoch [28/30] batch [14/96] time 0.343 (0.373) data 0.000 (0.039) loss 1.2343 (1.1584) lr 2.4472e-04 eta 0:01:42
epoch [28/30] batch [16/96] time 0.319 (0.366) data 0.000 (0.035) loss 1.1090 (1.1370) lr 2.4472e-04 eta 0:01:39
epoch [28/30] batch [18/96] time 0.351 (0.364) data 0.000 (0.031) loss 0.8313 (1.1164) lr 2.4472e-04 eta 0:01:38
epoch [28/30] batch [20/96] time 0.345 (0.362) data 0.000 (0.028) loss 1.0238 (1.1130) lr 2.4472e-04 eta 0:01:36
epoch [28/30] batch [22/96] time 0.325 (0.358) data 0.000 (0.025) loss 0.8141 (1.1274) lr 2.4472e-04 eta 0:01:35
epoch [28/30] batch [24/96] time 0.327 (0.356) data 0.000 (0.023) loss 1.5881 (1.1353) lr 2.4472e-04 eta 0:01:33
epoch [28/30] batch [26/96] time 0.332 (0.354) data 0.000 (0.021) loss 0.9829 (1.1554) lr 2.4472e-04 eta 0:01:32
epoch [28/30] batch [28/96] time 0.325 (0.352) data 0.000 (0.020) loss 2.1247 (1.1989) lr 2.4472e-04 eta 0:01:31
epoch [28/30] batch [30/96] time 0.332 (0.350) data 0.000 (0.019) loss 1.9790 (1.2234) lr 2.4472e-04 eta 0:01:30
epoch [28/30] batch [32/96] time 0.348 (0.349) data 0.000 (0.017) loss 0.8350 (1.1988) lr 2.4472e-04 eta 0:01:29
epoch [28/30] batch [34/96] time 0.329 (0.348) data 0.000 (0.016) loss 1.0735 (1.2014) lr 2.4472e-04 eta 0:01:28
epoch [28/30] batch [36/96] time 0.434 (0.350) data 0.000 (0.016) loss 0.6869 (1.1935) lr 2.4472e-04 eta 0:01:28
epoch [28/30] batch [38/96] time 0.342 (0.349) data 0.000 (0.015) loss 1.7595 (1.2026) lr 2.4472e-04 eta 0:01:27
epoch [28/30] batch [40/96] time 0.332 (0.348) data 0.000 (0.014) loss 1.0390 (1.1959) lr 2.4472e-04 eta 0:01:26
epoch [28/30] batch [42/96] time 0.325 (0.347) data 0.000 (0.013) loss 1.3623 (1.1906) lr 2.4472e-04 eta 0:01:25
epoch [28/30] batch [44/96] time 0.323 (0.346) data 0.000 (0.013) loss 1.3274 (1.1975) lr 2.4472e-04 eta 0:01:24
epoch [28/30] batch [46/96] time 0.323 (0.346) data 0.000 (0.012) loss 1.3544 (1.1985) lr 2.4472e-04 eta 0:01:23
epoch [28/30] batch [48/96] time 0.328 (0.344) data 0.000 (0.012) loss 1.2416 (1.1962) lr 2.4472e-04 eta 0:01:22
epoch [28/30] batch [50/96] time 0.342 (0.344) data 0.000 (0.011) loss 0.7868 (1.1825) lr 2.4472e-04 eta 0:01:21
epoch [28/30] batch [52/96] time 0.336 (0.344) data 0.000 (0.011) loss 0.9496 (1.1729) lr 2.4472e-04 eta 0:01:21
epoch [28/30] batch [54/96] time 0.327 (0.343) data 0.000 (0.010) loss 0.9426 (1.1805) lr 2.4472e-04 eta 0:01:20
epoch [28/30] batch [56/96] time 0.328 (0.342) data 0.000 (0.010) loss 1.1450 (1.1853) lr 2.4472e-04 eta 0:01:19
epoch [28/30] batch [58/96] time 0.334 (0.342) data 0.000 (0.010) loss 1.4354 (1.1830) lr 2.4472e-04 eta 0:01:18
epoch [28/30] batch [60/96] time 0.350 (0.342) data 0.000 (0.009) loss 1.3247 (1.1953) lr 2.4472e-04 eta 0:01:17
epoch [28/30] batch [62/96] time 0.337 (0.342) data 0.000 (0.009) loss 0.8495 (1.1839) lr 2.4472e-04 eta 0:01:17
epoch [28/30] batch [64/96] time 0.334 (0.342) data 0.000 (0.009) loss 1.0876 (1.1761) lr 2.4472e-04 eta 0:01:16
epoch [28/30] batch [66/96] time 0.341 (0.342) data 0.000 (0.009) loss 1.8485 (1.1818) lr 2.4472e-04 eta 0:01:15
epoch [28/30] batch [68/96] time 0.340 (0.342) data 0.000 (0.008) loss 1.1471 (1.1774) lr 2.4472e-04 eta 0:01:15
epoch [28/30] batch [70/96] time 0.353 (0.342) data 0.000 (0.008) loss 1.3102 (1.1775) lr 2.4472e-04 eta 0:01:14
epoch [28/30] batch [72/96] time 0.340 (0.342) data 0.000 (0.008) loss 0.9805 (1.1717) lr 2.4472e-04 eta 0:01:13
epoch [28/30] batch [74/96] time 0.323 (0.342) data 0.000 (0.008) loss 1.0515 (1.1782) lr 2.4472e-04 eta 0:01:13
epoch [28/30] batch [76/96] time 0.321 (0.341) data 0.000 (0.008) loss 1.0839 (1.1745) lr 2.4472e-04 eta 0:01:12
epoch [28/30] batch [78/96] time 0.320 (0.341) data 0.000 (0.007) loss 0.9014 (1.1689) lr 2.4472e-04 eta 0:01:11
epoch [28/30] batch [80/96] time 0.323 (0.340) data 0.000 (0.007) loss 1.2928 (1.1732) lr 2.4472e-04 eta 0:01:10
epoch [28/30] batch [82/96] time 0.326 (0.340) data 0.000 (0.007) loss 1.1550 (1.1714) lr 2.4472e-04 eta 0:01:09
epoch [28/30] batch [84/96] time 0.319 (0.339) data 0.000 (0.007) loss 1.5340 (1.1793) lr 2.4472e-04 eta 0:01:09
epoch [28/30] batch [86/96] time 0.322 (0.339) data 0.000 (0.007) loss 1.3435 (1.1859) lr 2.4472e-04 eta 0:01:08
epoch [28/30] batch [88/96] time 0.316 (0.338) data 0.000 (0.007) loss 1.1995 (1.1932) lr 2.4472e-04 eta 0:01:07
epoch [28/30] batch [90/96] time 0.324 (0.338) data 0.000 (0.006) loss 1.4718 (1.2002) lr 2.4472e-04 eta 0:01:06
epoch [28/30] batch [92/96] time 0.325 (0.338) data 0.000 (0.006) loss 1.1371 (1.1996) lr 2.4472e-04 eta 0:01:06
epoch [28/30] batch [94/96] time 0.320 (0.337) data 0.000 (0.006) loss 1.2076 (1.1951) lr 2.4472e-04 eta 0:01:05
epoch [28/30] batch [96/96] time 0.325 (0.337) data 0.000 (0.006) loss 1.3105 (1.1958) lr 1.0926e-04 eta 0:01:04
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.84s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.34s/it]100%|██████████| 3/3 [00:03<00:00,  1.17it/s]100%|██████████| 3/3 [00:03<00:00,  1.17s/it]=> result
* total: 576
* correct: 442
* accuracy: 76.7%
* error: 23.3%
* macro_f1: 76.3%

epoch [29/30] batch [2/96] time 0.318 (0.647) data 0.000 (0.281) loss 1.0198 (0.8920) lr 1.0926e-04 eta 0:02:03
epoch [29/30] batch [4/96] time 0.322 (0.485) data 0.000 (0.141) loss 1.2018 (1.0759) lr 1.0926e-04 eta 0:01:31
epoch [29/30] batch [6/96] time 0.326 (0.433) data 0.000 (0.094) loss 1.2136 (1.0969) lr 1.0926e-04 eta 0:01:20
epoch [29/30] batch [8/96] time 0.329 (0.409) data 0.000 (0.071) loss 1.0173 (1.0871) lr 1.0926e-04 eta 0:01:15
epoch [29/30] batch [10/96] time 0.329 (0.393) data 0.000 (0.056) loss 1.0100 (1.0726) lr 1.0926e-04 eta 0:01:11
epoch [29/30] batch [12/96] time 0.334 (0.382) data 0.000 (0.047) loss 1.0175 (1.0604) lr 1.0926e-04 eta 0:01:08
epoch [29/30] batch [14/96] time 0.323 (0.375) data 0.000 (0.040) loss 0.8574 (1.0414) lr 1.0926e-04 eta 0:01:06
epoch [29/30] batch [16/96] time 0.327 (0.369) data 0.000 (0.035) loss 0.8994 (1.0313) lr 1.0926e-04 eta 0:01:04
epoch [29/30] batch [18/96] time 0.320 (0.363) data 0.000 (0.032) loss 1.1163 (1.0664) lr 1.0926e-04 eta 0:01:03
epoch [29/30] batch [20/96] time 0.327 (0.359) data 0.000 (0.028) loss 0.8648 (1.0536) lr 1.0926e-04 eta 0:01:01
epoch [29/30] batch [22/96] time 0.337 (0.357) data 0.000 (0.026) loss 1.2865 (1.0649) lr 1.0926e-04 eta 0:01:00
epoch [29/30] batch [24/96] time 0.326 (0.355) data 0.000 (0.024) loss 1.3328 (1.0793) lr 1.0926e-04 eta 0:00:59
epoch [29/30] batch [26/96] time 0.334 (0.353) data 0.000 (0.022) loss 1.2772 (1.0888) lr 1.0926e-04 eta 0:00:58
epoch [29/30] batch [28/96] time 0.359 (0.353) data 0.000 (0.020) loss 1.1822 (1.1027) lr 1.0926e-04 eta 0:00:57
epoch [29/30] batch [30/96] time 0.333 (0.352) data 0.000 (0.019) loss 1.2346 (1.0997) lr 1.0926e-04 eta 0:00:57
epoch [29/30] batch [32/96] time 0.323 (0.350) data 0.000 (0.018) loss 1.7154 (1.1178) lr 1.0926e-04 eta 0:00:56
epoch [29/30] batch [34/96] time 0.326 (0.349) data 0.000 (0.017) loss 1.2296 (1.1319) lr 1.0926e-04 eta 0:00:55
epoch [29/30] batch [36/96] time 0.418 (0.350) data 0.000 (0.016) loss 1.0065 (1.1346) lr 1.0926e-04 eta 0:00:54
epoch [29/30] batch [38/96] time 0.327 (0.349) data 0.000 (0.015) loss 1.7046 (1.1396) lr 1.0926e-04 eta 0:00:53
epoch [29/30] batch [40/96] time 0.339 (0.348) data 0.000 (0.014) loss 1.2203 (1.1339) lr 1.0926e-04 eta 0:00:52
epoch [29/30] batch [42/96] time 0.331 (0.348) data 0.000 (0.014) loss 0.9951 (1.1402) lr 1.0926e-04 eta 0:00:52
epoch [29/30] batch [44/96] time 0.343 (0.347) data 0.000 (0.013) loss 1.3779 (1.1430) lr 1.0926e-04 eta 0:00:51
epoch [29/30] batch [46/96] time 0.340 (0.346) data 0.000 (0.013) loss 1.0648 (1.1571) lr 1.0926e-04 eta 0:00:50
epoch [29/30] batch [48/96] time 0.333 (0.346) data 0.000 (0.012) loss 1.1529 (1.1681) lr 1.0926e-04 eta 0:00:49
epoch [29/30] batch [50/96] time 0.336 (0.345) data 0.000 (0.012) loss 1.0906 (1.1644) lr 1.0926e-04 eta 0:00:49
epoch [29/30] batch [52/96] time 0.318 (0.344) data 0.000 (0.011) loss 1.9421 (1.1761) lr 1.0926e-04 eta 0:00:48
epoch [29/30] batch [54/96] time 0.332 (0.344) data 0.000 (0.011) loss 1.6054 (1.1782) lr 1.0926e-04 eta 0:00:47
epoch [29/30] batch [56/96] time 0.345 (0.344) data 0.000 (0.010) loss 1.0843 (1.1713) lr 1.0926e-04 eta 0:00:46
epoch [29/30] batch [58/96] time 0.326 (0.343) data 0.000 (0.010) loss 1.6141 (1.1731) lr 1.0926e-04 eta 0:00:45
epoch [29/30] batch [60/96] time 0.334 (0.343) data 0.000 (0.010) loss 1.0679 (1.1698) lr 1.0926e-04 eta 0:00:45
epoch [29/30] batch [62/96] time 0.333 (0.342) data 0.000 (0.009) loss 1.4844 (1.1854) lr 1.0926e-04 eta 0:00:44
epoch [29/30] batch [64/96] time 0.335 (0.342) data 0.000 (0.009) loss 1.5746 (1.1924) lr 1.0926e-04 eta 0:00:43
epoch [29/30] batch [66/96] time 0.334 (0.342) data 0.000 (0.009) loss 1.4058 (1.1941) lr 1.0926e-04 eta 0:00:43
epoch [29/30] batch [68/96] time 0.334 (0.341) data 0.000 (0.009) loss 1.3154 (1.2114) lr 1.0926e-04 eta 0:00:42
epoch [29/30] batch [70/96] time 0.328 (0.341) data 0.000 (0.008) loss 1.2273 (1.2129) lr 1.0926e-04 eta 0:00:41
epoch [29/30] batch [72/96] time 0.344 (0.341) data 0.000 (0.008) loss 0.8649 (1.2066) lr 1.0926e-04 eta 0:00:40
epoch [29/30] batch [74/96] time 0.317 (0.340) data 0.000 (0.008) loss 1.5510 (1.2105) lr 1.0926e-04 eta 0:00:40
epoch [29/30] batch [76/96] time 0.318 (0.340) data 0.000 (0.008) loss 1.0911 (1.2103) lr 1.0926e-04 eta 0:00:39
epoch [29/30] batch [78/96] time 0.322 (0.339) data 0.000 (0.008) loss 0.8277 (1.2018) lr 1.0926e-04 eta 0:00:38
epoch [29/30] batch [80/96] time 0.318 (0.339) data 0.000 (0.007) loss 1.2115 (1.2021) lr 1.0926e-04 eta 0:00:37
epoch [29/30] batch [82/96] time 0.323 (0.338) data 0.000 (0.007) loss 0.7832 (1.1912) lr 1.0926e-04 eta 0:00:37
epoch [29/30] batch [84/96] time 0.321 (0.338) data 0.000 (0.007) loss 0.6721 (1.1825) lr 1.0926e-04 eta 0:00:36
epoch [29/30] batch [86/96] time 0.319 (0.337) data 0.000 (0.007) loss 1.2740 (1.1841) lr 1.0926e-04 eta 0:00:35
epoch [29/30] batch [88/96] time 0.317 (0.337) data 0.000 (0.007) loss 1.0741 (1.1856) lr 1.0926e-04 eta 0:00:35
epoch [29/30] batch [90/96] time 0.318 (0.337) data 0.000 (0.007) loss 1.3165 (1.1864) lr 1.0926e-04 eta 0:00:34
epoch [29/30] batch [92/96] time 0.322 (0.336) data 0.000 (0.006) loss 1.0240 (1.1840) lr 1.0926e-04 eta 0:00:33
epoch [29/30] batch [94/96] time 0.321 (0.336) data 0.000 (0.006) loss 0.8252 (1.1857) lr 1.0926e-04 eta 0:00:32
epoch [29/30] batch [96/96] time 0.318 (0.335) data 0.000 (0.006) loss 0.8431 (1.1832) lr 2.7391e-05 eta 0:00:32
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.89s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.36s/it]100%|██████████| 3/3 [00:03<00:00,  1.15it/s]100%|██████████| 3/3 [00:03<00:00,  1.19s/it]=> result
* total: 576
* correct: 442
* accuracy: 76.7%
* error: 23.3%
* macro_f1: 76.3%

epoch [30/30] batch [2/96] time 0.346 (0.677) data 0.000 (0.274) loss 0.9046 (0.7901) lr 2.7391e-05 eta 0:01:03
epoch [30/30] batch [4/96] time 0.338 (0.504) data 0.000 (0.137) loss 1.2390 (1.0051) lr 2.7391e-05 eta 0:00:46
epoch [30/30] batch [6/96] time 0.323 (0.444) data 0.000 (0.092) loss 0.8150 (0.9678) lr 2.7391e-05 eta 0:00:39
epoch [30/30] batch [8/96] time 0.343 (0.415) data 0.000 (0.069) loss 1.0422 (1.0251) lr 2.7391e-05 eta 0:00:36
epoch [30/30] batch [10/96] time 0.333 (0.399) data 0.000 (0.055) loss 1.3865 (1.0887) lr 2.7391e-05 eta 0:00:34
epoch [30/30] batch [12/96] time 0.327 (0.388) data 0.000 (0.046) loss 1.1599 (1.0896) lr 2.7391e-05 eta 0:00:32
epoch [30/30] batch [14/96] time 0.351 (0.381) data 0.000 (0.039) loss 0.8405 (1.0640) lr 2.7391e-05 eta 0:00:31
epoch [30/30] batch [16/96] time 0.332 (0.376) data 0.000 (0.035) loss 1.9480 (1.1123) lr 2.7391e-05 eta 0:00:30
epoch [30/30] batch [18/96] time 0.338 (0.372) data 0.000 (0.031) loss 0.9562 (1.1023) lr 2.7391e-05 eta 0:00:28
epoch [30/30] batch [20/96] time 0.339 (0.368) data 0.000 (0.028) loss 0.6823 (1.0830) lr 2.7391e-05 eta 0:00:27
epoch [30/30] batch [22/96] time 0.323 (0.365) data 0.000 (0.025) loss 0.8194 (1.0987) lr 2.7391e-05 eta 0:00:27
epoch [30/30] batch [24/96] time 0.332 (0.362) data 0.000 (0.023) loss 1.4145 (1.1278) lr 2.7391e-05 eta 0:00:26
epoch [30/30] batch [26/96] time 0.326 (0.360) data 0.000 (0.021) loss 1.2678 (1.1158) lr 2.7391e-05 eta 0:00:25
epoch [30/30] batch [28/96] time 0.325 (0.358) data 0.000 (0.020) loss 1.2767 (1.1189) lr 2.7391e-05 eta 0:00:24
epoch [30/30] batch [30/96] time 0.340 (0.356) data 0.000 (0.019) loss 1.5004 (1.1218) lr 2.7391e-05 eta 0:00:23
epoch [30/30] batch [32/96] time 0.325 (0.354) data 0.000 (0.017) loss 2.0236 (1.1530) lr 2.7391e-05 eta 0:00:22
epoch [30/30] batch [34/96] time 0.337 (0.354) data 0.000 (0.016) loss 1.1798 (1.1502) lr 2.7391e-05 eta 0:00:21
epoch [30/30] batch [36/96] time 0.430 (0.355) data 0.000 (0.016) loss 1.9106 (1.1635) lr 2.7391e-05 eta 0:00:21
epoch [30/30] batch [38/96] time 0.325 (0.354) data 0.000 (0.015) loss 1.4570 (1.1830) lr 2.7391e-05 eta 0:00:20
epoch [30/30] batch [40/96] time 0.322 (0.352) data 0.000 (0.014) loss 1.3407 (1.1915) lr 2.7391e-05 eta 0:00:19
epoch [30/30] batch [42/96] time 0.346 (0.351) data 0.000 (0.013) loss 1.0329 (1.1852) lr 2.7391e-05 eta 0:00:18
epoch [30/30] batch [44/96] time 0.321 (0.350) data 0.000 (0.013) loss 1.0090 (1.1945) lr 2.7391e-05 eta 0:00:18
epoch [30/30] batch [46/96] time 0.338 (0.349) data 0.000 (0.012) loss 1.3362 (1.2000) lr 2.7391e-05 eta 0:00:17
epoch [30/30] batch [48/96] time 0.334 (0.349) data 0.000 (0.012) loss 1.5363 (1.2036) lr 2.7391e-05 eta 0:00:16
epoch [30/30] batch [50/96] time 0.310 (0.347) data 0.000 (0.011) loss 1.2884 (1.2009) lr 2.7391e-05 eta 0:00:15
epoch [30/30] batch [52/96] time 0.332 (0.347) data 0.000 (0.011) loss 1.1444 (1.1927) lr 2.7391e-05 eta 0:00:15
epoch [30/30] batch [54/96] time 0.330 (0.346) data 0.000 (0.010) loss 1.0275 (1.1858) lr 2.7391e-05 eta 0:00:14
epoch [30/30] batch [56/96] time 0.331 (0.346) data 0.000 (0.010) loss 1.1444 (1.1902) lr 2.7391e-05 eta 0:00:13
epoch [30/30] batch [58/96] time 0.325 (0.345) data 0.000 (0.010) loss 1.2038 (1.1883) lr 2.7391e-05 eta 0:00:13
epoch [30/30] batch [60/96] time 0.313 (0.344) data 0.000 (0.009) loss 0.9314 (1.1828) lr 2.7391e-05 eta 0:00:12
epoch [30/30] batch [62/96] time 0.319 (0.343) data 0.000 (0.009) loss 1.4527 (1.1928) lr 2.7391e-05 eta 0:00:11
epoch [30/30] batch [64/96] time 0.325 (0.343) data 0.000 (0.009) loss 1.6582 (1.1940) lr 2.7391e-05 eta 0:00:10
epoch [30/30] batch [66/96] time 0.325 (0.342) data 0.000 (0.009) loss 0.9470 (1.1923) lr 2.7391e-05 eta 0:00:10
epoch [30/30] batch [68/96] time 0.338 (0.342) data 0.000 (0.008) loss 1.3130 (1.1874) lr 2.7391e-05 eta 0:00:09
epoch [30/30] batch [70/96] time 0.358 (0.342) data 0.000 (0.008) loss 0.8061 (1.1746) lr 2.7391e-05 eta 0:00:08
epoch [30/30] batch [72/96] time 0.314 (0.341) data 0.000 (0.008) loss 1.5390 (1.1793) lr 2.7391e-05 eta 0:00:08
epoch [30/30] batch [74/96] time 0.311 (0.340) data 0.000 (0.008) loss 0.9939 (1.1834) lr 2.7391e-05 eta 0:00:07
epoch [30/30] batch [76/96] time 0.305 (0.340) data 0.000 (0.008) loss 1.0578 (1.1759) lr 2.7391e-05 eta 0:00:06
epoch [30/30] batch [78/96] time 0.306 (0.339) data 0.000 (0.007) loss 1.3603 (1.1729) lr 2.7391e-05 eta 0:00:06
epoch [30/30] batch [80/96] time 0.308 (0.338) data 0.000 (0.007) loss 1.2588 (1.1748) lr 2.7391e-05 eta 0:00:05
epoch [30/30] batch [82/96] time 0.304 (0.337) data 0.000 (0.007) loss 0.6942 (1.1688) lr 2.7391e-05 eta 0:00:04
epoch [30/30] batch [84/96] time 0.313 (0.337) data 0.000 (0.007) loss 0.8258 (1.1601) lr 2.7391e-05 eta 0:00:04
epoch [30/30] batch [86/96] time 0.312 (0.336) data 0.000 (0.007) loss 1.7871 (1.1643) lr 2.7391e-05 eta 0:00:03
epoch [30/30] batch [88/96] time 0.311 (0.336) data 0.000 (0.007) loss 1.0455 (1.1601) lr 2.7391e-05 eta 0:00:02
epoch [30/30] batch [90/96] time 0.310 (0.335) data 0.000 (0.006) loss 1.0619 (1.1655) lr 2.7391e-05 eta 0:00:02
epoch [30/30] batch [92/96] time 0.310 (0.334) data 0.000 (0.006) loss 1.6847 (1.1815) lr 2.7391e-05 eta 0:00:01
epoch [30/30] batch [94/96] time 0.314 (0.334) data 0.000 (0.006) loss 1.1657 (1.1847) lr 2.7391e-05 eta 0:00:00
epoch [30/30] batch [96/96] time 0.304 (0.333) data 0.000 (0.006) loss 1.4684 (1.1875) lr 0.0000e+00 eta 0:00:00
Evaluate on the *val* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:02<00:05,  2.92s/it] 67%|██████▋   | 2/3 [00:03<00:01,  1.37s/it]100%|██████████| 3/3 [00:03<00:00,  1.14it/s]100%|██████████| 3/3 [00:03<00:00,  1.20s/it]
=> result
* total: 576
* correct: 442
* accuracy: 76.7%
* error: 23.3%
* macro_f1: 76.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 26)
Evaluate on the *test* set
  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [00:03<00:12,  3.16s/it] 40%|████      | 2/5 [00:03<00:04,  1.47s/it] 60%|██████    | 3/5 [00:03<00:01,  1.07it/s] 80%|████████  | 4/5 [00:04<00:00,  1.46it/s]100%|██████████| 5/5 [00:04<00:00,  2.05it/s]100%|██████████| 5/5 [00:04<00:00,  1.16it/s]
=> result
* total: 864
* correct: 680
* accuracy: 78.7%
* error: 21.3%
* macro_f1: 78.2%
Elapsed: 0:17:59
+ sh scripts/rpo_prime/base2new_test_sdl.sh dtd 3 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:DescribableTextures
Loading dataset: DescribableTextures
Reading split from /shared/s2/lab01/dataset/clip/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/dtd/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
368 552 828
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  23
# train_x  368
# val      552
# test     828
---------  -------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/dtd/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 26)
Evaluate on the *test* set
  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [00:05<00:20,  5.16s/it] 40%|████      | 2/5 [00:05<00:06,  2.30s/it] 60%|██████    | 3/5 [00:05<00:02,  1.38s/it] 80%|████████  | 4/5 [00:06<00:00,  1.05it/s]100%|██████████| 5/5 [00:06<00:00,  1.24s/it]
=> result
* total: 828
* correct: 531
* accuracy: 64.1%
* error: 35.9%
* macro_f1: 63.5%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh fgvc_aircraft 1 0 main_tmp1_0.1sdl 16
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
800 1667 1666
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      1,667
# test     1,666
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model.pth.tar-50"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 50
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/tensorboard)
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar" (epoch = 40)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:08<01:11,  8.94s/it] 22%|██▏       | 2/9 [00:09<00:26,  3.86s/it] 33%|███▎      | 3/9 [00:09<00:13,  2.23s/it] 44%|████▍     | 4/9 [00:09<00:07,  1.47s/it] 56%|█████▌    | 5/9 [00:10<00:04,  1.04s/it] 67%|██████▋   | 6/9 [00:10<00:02,  1.26it/s] 78%|███████▊  | 7/9 [00:10<00:01,  1.59it/s] 89%|████████▉ | 8/9 [00:11<00:00,  1.91it/s]100%|██████████| 9/9 [00:11<00:00,  2.43it/s]100%|██████████| 9/9 [00:11<00:00,  1.25s/it]
=> result
* total: 1,666
* correct: 604
* accuracy: 36.3%
* error: 63.7%
* macro_f1: 33.1%
Elapsed: 0:00:11
+ sh scripts/rpo_prime/base2new_test_sdl.sh fgvc_aircraft 1 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
800 1666 1667
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      1,666
# test     1,667
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar" (epoch = 40)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:09<01:13,  9.17s/it] 22%|██▏       | 2/9 [00:09<00:27,  3.95s/it] 33%|███▎      | 3/9 [00:09<00:13,  2.28s/it] 44%|████▍     | 4/9 [00:10<00:07,  1.50s/it] 56%|█████▌    | 5/9 [00:10<00:04,  1.07s/it] 67%|██████▋   | 6/9 [00:10<00:02,  1.24it/s] 78%|███████▊  | 7/9 [00:10<00:01,  1.56it/s] 89%|████████▉ | 8/9 [00:11<00:00,  1.88it/s]100%|██████████| 9/9 [00:11<00:00,  2.40it/s]100%|██████████| 9/9 [00:11<00:00,  1.28s/it]
=> result
* total: 1,667
* correct: 548
* accuracy: 32.9%
* error: 67.1%
* macro_f1: 29.9%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh fgvc_aircraft 2 0 main_tmp1_0.1sdl 16
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
800 1667 1666
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      1,667
# test     1,666
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-50"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 50
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/tensorboard)
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 37)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:08<01:10,  8.84s/it] 22%|██▏       | 2/9 [00:09<00:26,  3.81s/it] 33%|███▎      | 3/9 [00:09<00:13,  2.21s/it] 44%|████▍     | 4/9 [00:09<00:07,  1.45s/it] 56%|█████▌    | 5/9 [00:10<00:04,  1.04s/it] 67%|██████▋   | 6/9 [00:10<00:02,  1.27it/s] 78%|███████▊  | 7/9 [00:10<00:01,  1.60it/s] 89%|████████▉ | 8/9 [00:10<00:00,  1.92it/s]100%|██████████| 9/9 [00:11<00:00,  2.44it/s]100%|██████████| 9/9 [00:11<00:00,  1.24s/it]
=> result
* total: 1,666
* correct: 639
* accuracy: 38.4%
* error: 61.6%
* macro_f1: 35.0%
Elapsed: 0:00:11
+ sh scripts/rpo_prime/base2new_test_sdl.sh fgvc_aircraft 2 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
800 1666 1667
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      1,666
# test     1,667
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 37)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:08<01:10,  8.82s/it] 22%|██▏       | 2/9 [00:09<00:26,  3.81s/it] 33%|███▎      | 3/9 [00:09<00:13,  2.20s/it] 44%|████▍     | 4/9 [00:09<00:07,  1.45s/it] 56%|█████▌    | 5/9 [00:10<00:04,  1.03s/it] 67%|██████▋   | 6/9 [00:10<00:02,  1.28it/s] 78%|███████▊  | 7/9 [00:10<00:01,  1.60it/s] 89%|████████▉ | 8/9 [00:10<00:00,  1.92it/s]100%|██████████| 9/9 [00:11<00:00,  2.44it/s]100%|██████████| 9/9 [00:11<00:00,  1.24s/it]
=> result
* total: 1,667
* correct: 591
* accuracy: 35.5%
* error: 64.5%
* macro_f1: 32.1%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh fgvc_aircraft 3 0 main_tmp1_0.1sdl 16
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
800 1667 1666
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      1,667
# test     1,666
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-50"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 50
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/tensorboard)
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 49)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:09<01:14,  9.35s/it] 22%|██▏       | 2/9 [00:09<00:28,  4.02s/it] 33%|███▎      | 3/9 [00:09<00:13,  2.32s/it] 44%|████▍     | 4/9 [00:10<00:07,  1.52s/it] 56%|█████▌    | 5/9 [00:10<00:04,  1.08s/it] 67%|██████▋   | 6/9 [00:10<00:02,  1.23it/s] 78%|███████▊  | 7/9 [00:11<00:01,  1.55it/s] 89%|████████▉ | 8/9 [00:11<00:00,  1.87it/s]100%|██████████| 9/9 [00:11<00:00,  2.38it/s]100%|██████████| 9/9 [00:11<00:00,  1.30s/it]
=> result
* total: 1,666
* correct: 642
* accuracy: 38.5%
* error: 61.5%
* macro_f1: 35.6%
Elapsed: 0:00:12
+ sh scripts/rpo_prime/base2new_test_sdl.sh fgvc_aircraft 3 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:FGVCAircraft
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/fgvc-aircraft/data/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
800 1666 1667
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      1,666
# test     1,667
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/fgvc_aircraft/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 49)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:08<01:10,  8.87s/it] 22%|██▏       | 2/9 [00:09<00:26,  3.83s/it] 33%|███▎      | 3/9 [00:09<00:13,  2.22s/it] 44%|████▍     | 4/9 [00:09<00:07,  1.46s/it] 56%|█████▌    | 5/9 [00:10<00:04,  1.04s/it] 67%|██████▋   | 6/9 [00:10<00:02,  1.27it/s] 78%|███████▊  | 7/9 [00:10<00:01,  1.60it/s] 89%|████████▉ | 8/9 [00:10<00:00,  1.92it/s]100%|██████████| 9/9 [00:11<00:00,  2.43it/s]100%|██████████| 9/9 [00:11<00:00,  1.25s/it]
=> result
* total: 1,667
* correct: 575
* accuracy: 34.5%
* error: 65.5%
* macro_f1: 31.5%
+ for dataset in eurosat dtd fgvc_aircraft oxford_flowers
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh oxford_flowers 1 0 main_tmp1_0.1sdl 16
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
816 696 1053
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      696
# test     1,053
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model.pth.tar-50"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 50
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/tensorboard)
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar" (epoch = 46)
Evaluate on the *test* set
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:05<00:27,  5.41s/it] 33%|███▎      | 2/6 [00:05<00:09,  2.40s/it] 50%|█████     | 3/6 [00:05<00:04,  1.44s/it] 67%|██████▋   | 4/6 [00:06<00:01,  1.01it/s] 83%|████████▎ | 5/6 [00:06<00:00,  1.35it/s]100%|██████████| 6/6 [00:06<00:00,  1.88it/s]100%|██████████| 6/6 [00:06<00:00,  1.14s/it]
=> result
* total: 1,053
* correct: 991
* accuracy: 94.1%
* error: 5.9%
* macro_f1: 93.8%
Elapsed: 0:00:07
+ sh scripts/rpo_prime/base2new_test_sdl.sh oxford_flowers 1 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
816 937 1410
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      937
# test     1,410
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed1/prompt_learner/model-best.pth.tar" (epoch = 46)
Evaluate on the *test* set
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:05<00:39,  5.65s/it] 25%|██▌       | 2/8 [00:05<00:15,  2.50s/it] 38%|███▊      | 3/8 [00:06<00:07,  1.50s/it] 50%|█████     | 4/8 [00:06<00:04,  1.02s/it] 62%|██████▎   | 5/8 [00:06<00:02,  1.31it/s] 75%|███████▌  | 6/8 [00:07<00:01,  1.66it/s] 88%|████████▊ | 7/8 [00:07<00:00,  1.99it/s]100%|██████████| 8/8 [00:07<00:00,  1.05it/s]
=> result
* total: 1,410
* correct: 1,047
* accuracy: 74.3%
* error: 25.7%
* macro_f1: 69.2%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh oxford_flowers 2 0 main_tmp1_0.1sdl 16
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
816 696 1053
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      696
# test     1,053
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-50"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 50
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/tensorboard)
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 45)
Evaluate on the *test* set
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:05<00:26,  5.27s/it] 33%|███▎      | 2/6 [00:05<00:09,  2.35s/it] 50%|█████     | 3/6 [00:05<00:04,  1.41s/it] 67%|██████▋   | 4/6 [00:06<00:01,  1.03it/s] 83%|████████▎ | 5/6 [00:06<00:00,  1.37it/s]100%|██████████| 6/6 [00:06<00:00,  1.90it/s]100%|██████████| 6/6 [00:06<00:00,  1.12s/it]
=> result
* total: 1,053
* correct: 975
* accuracy: 92.6%
* error: 7.4%
* macro_f1: 91.9%
Elapsed: 0:00:07
+ sh scripts/rpo_prime/base2new_test_sdl.sh oxford_flowers 2 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
816 937 1410
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      937
# test     1,410
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 45)
Evaluate on the *test* set
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:05<00:38,  5.53s/it] 25%|██▌       | 2/8 [00:05<00:14,  2.45s/it] 38%|███▊      | 3/8 [00:06<00:07,  1.47s/it] 50%|█████     | 4/8 [00:06<00:04,  1.01s/it] 62%|██████▎   | 5/8 [00:06<00:02,  1.33it/s] 75%|███████▌  | 6/8 [00:07<00:01,  1.68it/s] 88%|████████▊ | 7/8 [00:07<00:00,  2.01it/s]100%|██████████| 8/8 [00:07<00:00,  1.07it/s]
=> result
* total: 1,410
* correct: 1,053
* accuracy: 74.7%
* error: 25.3%
* macro_f1: 69.9%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train_sdl.sh oxford_flowers 3 0 main_tmp1_0.1sdl 16
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
816 696 1053
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      696
# test     1,053
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-40"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 40
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/tensorboard)
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 40)
Evaluate on the *test* set
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:05<00:26,  5.39s/it] 33%|███▎      | 2/6 [00:05<00:09,  2.40s/it] 50%|█████     | 3/6 [00:05<00:04,  1.44s/it] 67%|██████▋   | 4/6 [00:06<00:01,  1.01it/s] 83%|████████▎ | 5/6 [00:06<00:00,  1.35it/s]100%|██████████| 6/6 [00:06<00:00,  1.86it/s]100%|██████████| 6/6 [00:06<00:00,  1.14s/it]
=> result
* total: 1,053
* correct: 971
* accuracy: 92.2%
* error: 7.8%
* macro_f1: 91.4%
Elapsed: 0:00:07
+ sh scripts/rpo_prime/base2new_test_sdl.sh oxford_flowers 3 0 main_tmp1_0.1sdl 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 12
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 196
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 2
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 8
    K2: 24
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA TITAN RTX
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:OxfordFlowers
Loading dataset: OxfordFlowers
Reading split from /shared/s2/lab01/dataset/clip/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/oxford_flowers/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
816 937 1410
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      937
# test     1,410
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/oxford_flowers/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 40)
Evaluate on the *test* set
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:05<00:38,  5.56s/it] 25%|██▌       | 2/8 [00:05<00:14,  2.46s/it] 38%|███▊      | 3/8 [00:06<00:07,  1.47s/it] 50%|█████     | 4/8 [00:06<00:04,  1.01s/it] 62%|██████▎   | 5/8 [00:06<00:02,  1.33it/s] 75%|███████▌  | 6/8 [00:07<00:01,  1.67it/s] 88%|████████▊ | 7/8 [00:07<00:00,  2.00it/s]100%|██████████| 8/8 [00:07<00:00,  1.06it/s]
=> result
* total: 1,410
* correct: 1,062
* accuracy: 75.3%
* error: 24.7%
* macro_f1: 70.2%
