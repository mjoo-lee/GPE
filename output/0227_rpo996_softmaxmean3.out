set -e
set -x

eval "$(conda shell.bash hook)"
++ conda shell.bash hook
+ eval 'export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
export CONDA_EXE='/home/s2/mjoolee/anaconda/bin/conda'
++ export CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
++ CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
export _CE_M=''
++ export _CE_M=
++ _CE_M=
export _CE_CONDA=''
++ export _CE_CONDA=
++ _CE_CONDA=
export CONDA_PYTHON_EXE='/home/s2/mjoolee/anaconda/bin/python'
++ export CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
++ CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We're not allowing PS1 to be unbound. It must at least be set.
    # However, we're not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi
++ '[' -z x ']'

conda activate base
++ conda activate base
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate base
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate base
+++ /home/s2/mjoolee/anaconda/bin/conda shell.posix activate base
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\'''
PS1='(base) '
+++ PS1='(base) '
export PATH='/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'
+++ export PATH=/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
+++ PATH=/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
export CONDA_SHLVL='1'
+++ export CONDA_SHLVL=1
+++ CONDA_SHLVL=1
export CONDA_PROMPT_MODIFIER='(base) '
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
conda activate dassl
+ conda activate dassl
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate dassl
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate dassl
++ /home/s2/mjoolee/anaconda/bin/conda shell.posix activate dassl
+ ask_conda='PS1='\''(dassl) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_PREFIX='\''/home/s2/mjoolee/anaconda/envs/dassl'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''dassl'\''
export CONDA_PROMPT_MODIFIER='\''(dassl) '\''
export CONDA_PREFIX_1='\''/home/s2/mjoolee/anaconda'\''
export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\'''
+ eval 'PS1='\''(dassl) '\''
export PATH='\''/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\''
export CONDA_PREFIX='\''/home/s2/mjoolee/anaconda/envs/dassl'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''dassl'\''
export CONDA_PROMPT_MODIFIER='\''(dassl) '\''
export CONDA_PREFIX_1='\''/home/s2/mjoolee/anaconda'\''
export CONDA_EXE='\''/home/s2/mjoolee/anaconda/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/home/s2/mjoolee/anaconda/bin/python'\'''
PS1='(dassl) '
++ PS1='(dassl) '
export PATH='/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'
++ export PATH=/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
++ PATH=/home/s2/mjoolee/.vscode-server/cli/servers/Stable-903b1e9d8990623e3d7da1df3d33db3e42d80eda/server/bin/remote-cli:/home/s2/mjoolee/anaconda/envs/dassl/bin:/home/s2/mjoolee/anaconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
export CONDA_PREFIX='/home/s2/mjoolee/anaconda/envs/dassl'
++ export CONDA_PREFIX=/home/s2/mjoolee/anaconda/envs/dassl
++ CONDA_PREFIX=/home/s2/mjoolee/anaconda/envs/dassl
export CONDA_SHLVL='2'
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
export CONDA_DEFAULT_ENV='dassl'
++ export CONDA_DEFAULT_ENV=dassl
++ CONDA_DEFAULT_ENV=dassl
export CONDA_PROMPT_MODIFIER='(dassl) '
++ export 'CONDA_PROMPT_MODIFIER=(dassl) '
++ CONDA_PROMPT_MODIFIER='(dassl) '
export CONDA_PREFIX_1='/home/s2/mjoolee/anaconda'
++ export CONDA_PREFIX_1=/home/s2/mjoolee/anaconda
++ CONDA_PREFIX_1=/home/s2/mjoolee/anaconda
export CONDA_EXE='/home/s2/mjoolee/anaconda/bin/conda'
++ export CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
++ CONDA_EXE=/home/s2/mjoolee/anaconda/bin/conda
export _CE_M=''
++ export _CE_M=
++ _CE_M=
export _CE_CONDA=''
++ export _CE_CONDA=
++ _CE_CONDA=
export CONDA_PYTHON_EXE='/home/s2/mjoolee/anaconda/bin/python'
++ export CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
++ CONDA_PYTHON_EXE=/home/s2/mjoolee/anaconda/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r

GPU=0
+ GPU=0
SHOT=16
+ SHOT=16

for dataset in caltech101 sun397 imagenet
#for dataset in caltech101 imagenet
do
    for seed in 1 2 3
    do
    sh scripts/rpo_prime/base2new_train.sh ${dataset} ${seed} ${GPU} main_tmp ${SHOT}
    #sh scripts/rpo_prime/base2new_test.sh ${dataset} ${seed} ${GPU} main_9_9 ${SHOT} base
    sh scripts/rpo_prime/base2new_test.sh ${dataset} ${seed} ${GPU} main_tmp ${SHOT} new
    done
done
+ for dataset in caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train.sh caltech101 1 0 main_tmp 16
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:Caltech101
Loading dataset: Caltech101
Reading split from /shared/s2/lab01/dataset/clip/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/caltech-101/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
800 1036 1549
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      1,036
# test     1,549
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed1 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed1/prompt_learner/model.pth.tar-30"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 30
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed1/tensorboard)
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed1/prompt_learner/model-best.pth.tar" (epoch = 18)
Evaluate on the *test* set
  0%|          | 0/16 [00:00<?, ?it/s]  6%|▋         | 1/16 [00:03<00:59,  3.94s/it] 12%|█▎        | 2/16 [00:04<00:23,  1.70s/it] 19%|█▉        | 3/16 [00:04<00:12,  1.01it/s] 25%|██▌       | 4/16 [00:04<00:07,  1.53it/s] 31%|███▏      | 5/16 [00:04<00:05,  2.14it/s] 38%|███▊      | 6/16 [00:04<00:03,  2.81it/s] 44%|████▍     | 7/16 [00:04<00:02,  3.52it/s] 50%|█████     | 8/16 [00:04<00:01,  4.21it/s] 56%|█████▋    | 9/16 [00:05<00:01,  4.84it/s] 62%|██████▎   | 10/16 [00:05<00:01,  5.39it/s] 69%|██████▉   | 11/16 [00:05<00:00,  5.85it/s] 75%|███████▌  | 12/16 [00:05<00:00,  6.22it/s] 81%|████████▏ | 13/16 [00:05<00:00,  6.50it/s] 88%|████████▊ | 14/16 [00:05<00:00,  6.71it/s] 94%|█████████▍| 15/16 [00:05<00:00,  6.86it/s]100%|██████████| 16/16 [00:06<00:00,  2.64it/s]
=> result
* total: 1,549
* correct: 1,523
* accuracy: 98.3%
* error: 1.7%
* macro_f1: 96.7%
Elapsed: 0:00:06
+ sh scripts/rpo_prime/base2new_test.sh caltech101 1 0 main_tmp 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/caltech101/shots_16/RPO_prime/main_tmp/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/caltech101/shots_16/RPO_prime/main_tmp/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:Caltech101
Loading dataset: Caltech101
Reading split from /shared/s2/lab01/dataset/clip/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/caltech-101/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
800 613 916
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      613
# test     916
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed1/prompt_learner/model-best.pth.tar" (epoch = 18)
Evaluate on the *test* set
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:03<00:30,  3.44s/it] 20%|██        | 2/10 [00:03<00:11,  1.50s/it] 30%|███       | 3/10 [00:03<00:06,  1.14it/s] 40%|████      | 4/10 [00:03<00:03,  1.72it/s] 50%|█████     | 5/10 [00:03<00:02,  2.37it/s] 60%|██████    | 6/10 [00:04<00:01,  3.09it/s] 70%|███████   | 7/10 [00:04<00:00,  3.82it/s] 80%|████████  | 8/10 [00:04<00:00,  4.53it/s] 90%|█████████ | 9/10 [00:04<00:00,  5.16it/s]100%|██████████| 10/10 [00:04<00:00,  2.14it/s]
=> result
* total: 916
* correct: 864
* accuracy: 94.3%
* error: 5.7%
* macro_f1: 94.5%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train.sh caltech101 2 0 main_tmp 16
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:Caltech101
Loading dataset: Caltech101
Reading split from /shared/s2/lab01/dataset/clip/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/caltech-101/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
800 1036 1549
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      1,036
# test     1,549
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed2 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model.pth.tar-30"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 30
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed2/tensorboard)
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar" (epoch = 9)
Evaluate on the *test* set
  0%|          | 0/16 [00:00<?, ?it/s]  6%|▋         | 1/16 [00:03<00:57,  3.80s/it] 12%|█▎        | 2/16 [00:03<00:23,  1.65s/it] 19%|█▉        | 3/16 [00:04<00:12,  1.04it/s] 25%|██▌       | 4/16 [00:04<00:07,  1.58it/s] 31%|███▏      | 5/16 [00:04<00:05,  2.20it/s] 38%|███▊      | 6/16 [00:04<00:03,  2.89it/s] 44%|████▍     | 7/16 [00:04<00:02,  3.60it/s] 50%|█████     | 8/16 [00:04<00:01,  4.29it/s] 56%|█████▋    | 9/16 [00:04<00:01,  4.91it/s] 62%|██████▎   | 10/16 [00:05<00:01,  5.46it/s] 69%|██████▉   | 11/16 [00:05<00:00,  5.91it/s] 75%|███████▌  | 12/16 [00:05<00:00,  6.27it/s] 81%|████████▏ | 13/16 [00:05<00:00,  6.54it/s] 88%|████████▊ | 14/16 [00:05<00:00,  6.74it/s] 94%|█████████▍| 15/16 [00:05<00:00,  6.89it/s]100%|██████████| 16/16 [00:05<00:00,  2.70it/s]
=> result
* total: 1,549
* correct: 1,521
* accuracy: 98.2%
* error: 1.8%
* macro_f1: 96.3%
Elapsed: 0:00:06
+ sh scripts/rpo_prime/base2new_test.sh caltech101 2 0 main_tmp 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/caltech101/shots_16/RPO_prime/main_tmp/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/caltech101/shots_16/RPO_prime/main_tmp/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:Caltech101
Loading dataset: Caltech101
Reading split from /shared/s2/lab01/dataset/clip/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/caltech-101/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
800 613 916
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      613
# test     916
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar" (epoch = 9)
Evaluate on the *test* set
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:03<00:30,  3.44s/it] 20%|██        | 2/10 [00:03<00:12,  1.50s/it] 30%|███       | 3/10 [00:03<00:06,  1.14it/s] 40%|████      | 4/10 [00:03<00:03,  1.71it/s] 50%|█████     | 5/10 [00:03<00:02,  2.36it/s] 60%|██████    | 6/10 [00:04<00:01,  3.06it/s] 70%|███████   | 7/10 [00:04<00:00,  3.77it/s] 80%|████████  | 8/10 [00:04<00:00,  4.45it/s] 90%|█████████ | 9/10 [00:04<00:00,  5.07it/s]100%|██████████| 10/10 [00:04<00:00,  2.13it/s]
=> result
* total: 916
* correct: 862
* accuracy: 94.1%
* error: 5.9%
* macro_f1: 94.2%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train.sh caltech101 3 0 main_tmp 16
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:Caltech101
Loading dataset: Caltech101
Reading split from /shared/s2/lab01/dataset/clip/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/caltech-101/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
800 1036 1549
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      1,036
# test     1,549
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed3 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model.pth.tar-30"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 30
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed3/tensorboard)
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar" (epoch = 17)
Evaluate on the *test* set
  0%|          | 0/16 [00:00<?, ?it/s]  6%|▋         | 1/16 [00:03<00:58,  3.93s/it] 12%|█▎        | 2/16 [00:04<00:23,  1.70s/it] 19%|█▉        | 3/16 [00:04<00:12,  1.01it/s] 25%|██▌       | 4/16 [00:04<00:07,  1.54it/s] 31%|███▏      | 5/16 [00:04<00:05,  2.15it/s] 38%|███▊      | 6/16 [00:04<00:03,  2.82it/s] 44%|████▍     | 7/16 [00:04<00:02,  3.53it/s] 50%|█████     | 8/16 [00:04<00:01,  4.21it/s] 56%|█████▋    | 9/16 [00:05<00:01,  4.85it/s] 62%|██████▎   | 10/16 [00:05<00:01,  5.40it/s] 69%|██████▉   | 11/16 [00:05<00:00,  5.86it/s] 75%|███████▌  | 12/16 [00:05<00:00,  6.23it/s] 81%|████████▏ | 13/16 [00:05<00:00,  6.51it/s] 88%|████████▊ | 14/16 [00:05<00:00,  6.69it/s] 94%|█████████▍| 15/16 [00:05<00:00,  6.85it/s]100%|██████████| 16/16 [00:06<00:00,  2.65it/s]
=> result
* total: 1,549
* correct: 1,526
* accuracy: 98.5%
* error: 1.5%
* macro_f1: 97.1%
Elapsed: 0:00:06
+ sh scripts/rpo_prime/base2new_test.sh caltech101 3 0 main_tmp 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/caltech101/shots_16/RPO_prime/main_tmp/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/caltech101/shots_16/RPO_prime/main_tmp/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:Caltech101
Loading dataset: Caltech101
Reading split from /shared/s2/lab01/dataset/clip/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/caltech-101/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
800 613 916
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      613
# test     916
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/caltech101/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar" (epoch = 17)
Evaluate on the *test* set
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:03<00:28,  3.19s/it] 20%|██        | 2/10 [00:03<00:11,  1.39s/it] 30%|███       | 3/10 [00:03<00:05,  1.22it/s] 40%|████      | 4/10 [00:03<00:03,  1.81it/s] 50%|█████     | 5/10 [00:03<00:02,  2.49it/s] 60%|██████    | 6/10 [00:03<00:01,  3.20it/s] 70%|███████   | 7/10 [00:04<00:00,  3.92it/s] 80%|████████  | 8/10 [00:04<00:00,  4.59it/s] 90%|█████████ | 9/10 [00:04<00:00,  5.19it/s]100%|██████████| 10/10 [00:04<00:00,  2.25it/s]
=> result
* total: 916
* correct: 862
* accuracy: 94.1%
* error: 5.9%
* macro_f1: 94.1%
+ for dataset in caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train.sh sun397 1 0 main_tmp 16
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:SUN397
Loading dataset: SUN397
Reading split from /shared/s2/lab01/dataset/clip/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/sun397/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
3184 1990 9950
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  199
# train_x  3,184
# val      1,990
# test     9,950
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Found checkpoint at output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1 (will resume training)
Loading checkpoint from "output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1/prompt_learner/model-best.pth.tar"
Loaded model weights
Loaded optimizer
Loaded scheduler
Previous epoch: 13
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [14/30] batch [20/796] time 0.427 (0.529) data 0.000 (0.058) loss 1.5205 (0.8476) lr 6.5451e-03 eta 1:59:06
epoch [14/30] batch [40/796] time 0.419 (0.464) data 0.000 (0.029) loss 0.5454 (0.8582) lr 6.5451e-03 eta 1:44:23
epoch [14/30] batch [60/796] time 0.373 (0.440) data 0.000 (0.019) loss 1.2832 (0.8196) lr 6.5451e-03 eta 1:38:50
epoch [14/30] batch [80/796] time 0.353 (0.425) data 0.000 (0.015) loss 1.3438 (0.8433) lr 6.5451e-03 eta 1:35:13
epoch [14/30] batch [100/796] time 0.349 (0.414) data 0.000 (0.012) loss 0.4863 (0.8214) lr 6.5451e-03 eta 1:32:47
epoch [14/30] batch [120/796] time 0.409 (0.408) data 0.000 (0.010) loss 0.4639 (0.8162) lr 6.5451e-03 eta 1:31:09
epoch [14/30] batch [140/796] time 0.354 (0.405) data 0.000 (0.009) loss 1.2656 (0.8363) lr 6.5451e-03 eta 1:30:26
epoch [14/30] batch [160/796] time 0.368 (0.402) data 0.000 (0.008) loss 0.5298 (0.8271) lr 6.5451e-03 eta 1:29:38
epoch [14/30] batch [180/796] time 0.429 (0.401) data 0.000 (0.007) loss 0.3379 (0.8446) lr 6.5451e-03 eta 1:29:15
epoch [14/30] batch [200/796] time 0.398 (0.400) data 0.000 (0.006) loss 0.3110 (0.8403) lr 6.5451e-03 eta 1:28:53
epoch [14/30] batch [220/796] time 0.413 (0.400) data 0.000 (0.006) loss 2.9473 (0.8457) lr 6.5451e-03 eta 1:28:39
epoch [14/30] batch [240/796] time 0.428 (0.399) data 0.000 (0.005) loss 0.3687 (0.8423) lr 6.5451e-03 eta 1:28:20
epoch [14/30] batch [260/796] time 0.420 (0.398) data 0.000 (0.005) loss 0.7969 (0.8398) lr 6.5451e-03 eta 1:28:00
epoch [14/30] batch [280/796] time 0.367 (0.397) data 0.000 (0.004) loss 1.1836 (0.8354) lr 6.5451e-03 eta 1:27:35
epoch [14/30] batch [300/796] time 0.405 (0.396) data 0.000 (0.004) loss 0.4485 (0.8398) lr 6.5451e-03 eta 1:27:22
epoch [14/30] batch [320/796] time 0.391 (0.395) data 0.000 (0.004) loss 0.3213 (0.8525) lr 6.5451e-03 eta 1:27:00
epoch [14/30] batch [340/796] time 0.376 (0.394) data 0.000 (0.004) loss 0.0775 (0.8526) lr 6.5451e-03 eta 1:26:40
epoch [14/30] batch [360/796] time 0.394 (0.394) data 0.000 (0.004) loss 1.6025 (0.8557) lr 6.5451e-03 eta 1:26:24
epoch [14/30] batch [380/796] time 0.350 (0.393) data 0.000 (0.003) loss 0.5869 (0.8485) lr 6.5451e-03 eta 1:26:08
epoch [14/30] batch [400/796] time 0.386 (0.393) data 0.000 (0.003) loss 1.2881 (0.8554) lr 6.5451e-03 eta 1:25:55
epoch [14/30] batch [420/796] time 0.392 (0.392) data 0.000 (0.003) loss 0.6333 (0.8559) lr 6.5451e-03 eta 1:25:45
epoch [14/30] batch [440/796] time 0.402 (0.392) data 0.000 (0.003) loss 0.3821 (0.8541) lr 6.5451e-03 eta 1:25:34
epoch [14/30] batch [460/796] time 0.361 (0.391) data 0.000 (0.003) loss 0.2627 (0.8496) lr 6.5451e-03 eta 1:25:12
epoch [14/30] batch [480/796] time 0.379 (0.390) data 0.000 (0.003) loss 2.3242 (0.8501) lr 6.5451e-03 eta 1:24:54
epoch [14/30] batch [500/796] time 0.388 (0.390) data 0.000 (0.003) loss 1.0117 (0.8516) lr 6.5451e-03 eta 1:24:40
epoch [14/30] batch [520/796] time 0.408 (0.390) data 0.000 (0.003) loss 1.6230 (0.8522) lr 6.5451e-03 eta 1:24:28
epoch [14/30] batch [540/796] time 0.371 (0.389) data 0.000 (0.002) loss 1.2236 (0.8544) lr 6.5451e-03 eta 1:24:17
epoch [14/30] batch [560/796] time 0.352 (0.389) data 0.000 (0.002) loss 0.2556 (0.8477) lr 6.5451e-03 eta 1:24:01
epoch [14/30] batch [580/796] time 0.368 (0.388) data 0.000 (0.002) loss 0.9131 (0.8496) lr 6.5451e-03 eta 1:23:51
epoch [14/30] batch [600/796] time 0.351 (0.388) data 0.000 (0.002) loss 0.4487 (0.8513) lr 6.5451e-03 eta 1:23:37
epoch [14/30] batch [620/796] time 0.400 (0.388) data 0.000 (0.002) loss 1.1992 (0.8603) lr 6.5451e-03 eta 1:23:27
epoch [14/30] batch [640/796] time 0.392 (0.388) data 0.000 (0.002) loss 1.5469 (0.8691) lr 6.5451e-03 eta 1:23:18
epoch [14/30] batch [660/796] time 0.375 (0.388) data 0.000 (0.002) loss 2.3477 (0.8724) lr 6.5451e-03 eta 1:23:10
epoch [14/30] batch [680/796] time 0.383 (0.388) data 0.000 (0.002) loss 0.4434 (0.8796) lr 6.5451e-03 eta 1:23:00
epoch [14/30] batch [700/796] time 0.352 (0.387) data 0.000 (0.002) loss 0.4080 (0.8830) lr 6.5451e-03 eta 1:22:48
epoch [14/30] batch [720/796] time 0.395 (0.387) data 0.001 (0.002) loss 1.0166 (0.8849) lr 6.5451e-03 eta 1:22:37
epoch [14/30] batch [740/796] time 0.387 (0.387) data 0.000 (0.002) loss 0.6362 (0.8904) lr 6.5451e-03 eta 1:22:24
epoch [14/30] batch [760/796] time 0.399 (0.386) data 0.001 (0.002) loss 1.4023 (0.8925) lr 6.5451e-03 eta 1:22:14
epoch [14/30] batch [780/796] time 0.461 (0.385) data 0.000 (0.002) loss 0.2443 (0.9003) lr 6.5451e-03 eta 1:21:55
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:51,  5.86s/it] 10%|█         | 2/20 [00:06<00:50,  2.78s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.66s/it] 20%|██        | 4/20 [00:07<00:17,  1.12s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.22it/s] 30%|███       | 6/20 [00:07<00:09,  1.55it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.91it/s] 40%|████      | 8/20 [00:08<00:05,  2.20it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.48it/s] 50%|█████     | 10/20 [00:08<00:03,  2.70it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.94it/s] 60%|██████    | 12/20 [00:09<00:02,  3.20it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.42it/s] 70%|███████   | 14/20 [00:09<00:01,  3.51it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.61it/s] 80%|████████  | 16/20 [00:10<00:01,  3.68it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.82it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.14it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.42it/s]100%|██████████| 20/20 [00:11<00:00,  4.71it/s]100%|██████████| 20/20 [00:11<00:00,  1.76it/s]=> result
* total: 1,990
* correct: 1,577
* accuracy: 79.2%
* error: 20.8%
* macro_f1: 78.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1/prompt_learner/model-best.pth.tar

epoch [15/30] batch [20/796] time 0.345 (0.429) data 0.000 (0.041) loss 0.9722 (0.9125) lr 6.4914e-03 eta 1:30:58
epoch [15/30] batch [40/796] time 0.335 (0.404) data 0.000 (0.021) loss 0.9160 (0.8697) lr 6.4914e-03 eta 1:25:31
epoch [15/30] batch [60/796] time 0.365 (0.394) data 0.000 (0.014) loss 0.5454 (0.9074) lr 6.4914e-03 eta 1:23:16
epoch [15/30] batch [80/796] time 0.348 (0.389) data 0.000 (0.010) loss 0.3071 (0.9128) lr 6.4914e-03 eta 1:22:03
epoch [15/30] batch [100/796] time 0.395 (0.388) data 0.000 (0.008) loss 0.3120 (0.8826) lr 6.4914e-03 eta 1:21:37
epoch [15/30] batch [120/796] time 0.407 (0.385) data 0.000 (0.007) loss 0.3835 (0.9177) lr 6.4914e-03 eta 1:20:57
epoch [15/30] batch [140/796] time 0.346 (0.385) data 0.000 (0.006) loss 1.0967 (0.9056) lr 6.4914e-03 eta 1:20:52
epoch [15/30] batch [160/796] time 0.386 (0.385) data 0.001 (0.005) loss 0.9307 (0.9021) lr 6.4914e-03 eta 1:20:38
epoch [15/30] batch [180/796] time 0.408 (0.384) data 0.000 (0.005) loss 1.7832 (0.8991) lr 6.4914e-03 eta 1:20:24
epoch [15/30] batch [200/796] time 0.355 (0.383) data 0.000 (0.004) loss 0.1262 (0.8795) lr 6.4914e-03 eta 1:19:59
epoch [15/30] batch [220/796] time 0.386 (0.382) data 0.000 (0.004) loss 1.1787 (0.8898) lr 6.4914e-03 eta 1:19:43
epoch [15/30] batch [240/796] time 0.390 (0.382) data 0.000 (0.004) loss 2.8262 (0.8757) lr 6.4914e-03 eta 1:19:30
epoch [15/30] batch [260/796] time 0.399 (0.382) data 0.000 (0.003) loss 0.6030 (0.8708) lr 6.4914e-03 eta 1:19:25
epoch [15/30] batch [280/796] time 0.405 (0.382) data 0.000 (0.003) loss 1.4482 (0.9128) lr 6.4914e-03 eta 1:19:13
epoch [15/30] batch [300/796] time 0.357 (0.381) data 0.000 (0.003) loss 1.4814 (0.9025) lr 6.4914e-03 eta 1:18:59
epoch [15/30] batch [320/796] time 0.400 (0.381) data 0.000 (0.003) loss 0.8555 (0.8953) lr 6.4914e-03 eta 1:18:55
epoch [15/30] batch [340/796] time 0.405 (0.381) data 0.000 (0.003) loss 0.4419 (0.9092) lr 6.4914e-03 eta 1:18:46
epoch [15/30] batch [360/796] time 0.387 (0.381) data 0.001 (0.003) loss 0.5459 (0.9053) lr 6.4914e-03 eta 1:18:31
epoch [15/30] batch [380/796] time 0.390 (0.380) data 0.000 (0.002) loss 1.0107 (0.9120) lr 6.4914e-03 eta 1:18:20
epoch [15/30] batch [400/796] time 0.371 (0.380) data 0.000 (0.002) loss 0.4780 (0.9072) lr 6.4914e-03 eta 1:18:06
epoch [15/30] batch [420/796] time 0.387 (0.380) data 0.000 (0.002) loss 1.9111 (0.9204) lr 6.4914e-03 eta 1:17:58
epoch [15/30] batch [440/796] time 0.341 (0.380) data 0.000 (0.002) loss 0.4866 (0.9190) lr 6.4914e-03 eta 1:17:46
epoch [15/30] batch [460/796] time 0.362 (0.379) data 0.000 (0.002) loss 1.4365 (0.9136) lr 6.4914e-03 eta 1:17:34
epoch [15/30] batch [480/796] time 0.347 (0.379) data 0.000 (0.002) loss 0.6279 (0.9105) lr 6.4914e-03 eta 1:17:21
epoch [15/30] batch [500/796] time 0.391 (0.378) data 0.000 (0.002) loss 0.2494 (0.9098) lr 6.4914e-03 eta 1:17:10
epoch [15/30] batch [520/796] time 0.396 (0.378) data 0.000 (0.002) loss 0.4409 (0.9062) lr 6.4914e-03 eta 1:16:57
epoch [15/30] batch [540/796] time 0.408 (0.378) data 0.000 (0.002) loss 0.4651 (0.8998) lr 6.4914e-03 eta 1:16:49
epoch [15/30] batch [560/796] time 0.368 (0.378) data 0.000 (0.002) loss 0.5781 (0.8937) lr 6.4914e-03 eta 1:16:39
epoch [15/30] batch [580/796] time 0.384 (0.378) data 0.000 (0.002) loss 2.0117 (0.8969) lr 6.4914e-03 eta 1:16:33
epoch [15/30] batch [600/796] time 0.393 (0.378) data 0.000 (0.002) loss 1.0039 (0.8945) lr 6.4914e-03 eta 1:16:26
epoch [15/30] batch [620/796] time 0.408 (0.378) data 0.000 (0.002) loss 0.4580 (0.9000) lr 6.4914e-03 eta 1:16:21
epoch [15/30] batch [640/796] time 0.387 (0.378) data 0.000 (0.002) loss 0.6899 (0.9016) lr 6.4914e-03 eta 1:16:12
epoch [15/30] batch [660/796] time 0.342 (0.378) data 0.000 (0.002) loss 0.6675 (0.8972) lr 6.4914e-03 eta 1:16:03
epoch [15/30] batch [680/796] time 0.368 (0.378) data 0.000 (0.002) loss 0.2930 (0.8997) lr 6.4914e-03 eta 1:15:57
epoch [15/30] batch [700/796] time 0.391 (0.378) data 0.000 (0.001) loss 0.3311 (0.8955) lr 6.4914e-03 eta 1:15:48
epoch [15/30] batch [720/796] time 0.369 (0.378) data 0.000 (0.001) loss 0.1823 (0.8953) lr 6.4914e-03 eta 1:15:41
epoch [15/30] batch [740/796] time 0.396 (0.378) data 0.000 (0.001) loss 0.8257 (0.9009) lr 6.4914e-03 eta 1:15:35
epoch [15/30] batch [760/796] time 0.388 (0.378) data 0.000 (0.001) loss 0.0229 (0.8945) lr 6.4914e-03 eta 1:15:26
epoch [15/30] batch [780/796] time 0.335 (0.377) data 0.000 (0.001) loss 0.2998 (0.8975) lr 6.4914e-03 eta 1:15:12
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:44,  5.50s/it] 10%|█         | 2/20 [00:06<00:51,  2.85s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.68s/it] 20%|██        | 4/20 [00:07<00:18,  1.13s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.21it/s] 30%|███       | 6/20 [00:07<00:08,  1.56it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.91it/s] 40%|████      | 8/20 [00:08<00:05,  2.25it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.54it/s] 50%|█████     | 10/20 [00:08<00:03,  2.80it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.01it/s] 60%|██████    | 12/20 [00:09<00:02,  3.23it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.36it/s] 70%|███████   | 14/20 [00:09<00:01,  3.51it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.64it/s] 80%|████████  | 16/20 [00:10<00:01,  3.69it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.73it/s] 90%|█████████ | 18/20 [00:10<00:00,  3.76it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.91it/s]100%|██████████| 20/20 [00:11<00:00,  4.34it/s]100%|██████████| 20/20 [00:11<00:00,  1.75it/s]=> result
* total: 1,990
* correct: 1,573
* accuracy: 79.0%
* error: 21.0%
* macro_f1: 78.4%

epoch [16/30] batch [20/796] time 0.369 (0.435) data 0.000 (0.044) loss 1.2656 (1.0560) lr 6.4025e-03 eta 1:26:28
epoch [16/30] batch [40/796] time 0.384 (0.404) data 0.000 (0.022) loss 1.3008 (0.9826) lr 6.4025e-03 eta 1:20:05
epoch [16/30] batch [60/796] time 0.369 (0.395) data 0.000 (0.015) loss 1.0762 (0.9115) lr 6.4025e-03 eta 1:18:12
epoch [16/30] batch [80/796] time 0.331 (0.389) data 0.000 (0.011) loss 1.4775 (0.8900) lr 6.4025e-03 eta 1:16:50
epoch [16/30] batch [100/796] time 0.339 (0.384) data 0.000 (0.009) loss 1.0439 (0.8687) lr 6.4025e-03 eta 1:15:48
epoch [16/30] batch [120/796] time 0.388 (0.382) data 0.000 (0.007) loss 0.8174 (0.8907) lr 6.4025e-03 eta 1:15:14
epoch [16/30] batch [140/796] time 0.374 (0.382) data 0.000 (0.006) loss 0.5918 (0.8863) lr 6.4025e-03 eta 1:15:04
epoch [16/30] batch [160/796] time 0.389 (0.381) data 0.000 (0.006) loss 0.4771 (0.8872) lr 6.4025e-03 eta 1:14:52
epoch [16/30] batch [180/796] time 0.388 (0.381) data 0.000 (0.005) loss 0.5605 (0.8939) lr 6.4025e-03 eta 1:14:45
epoch [16/30] batch [200/796] time 0.363 (0.380) data 0.000 (0.005) loss 0.3625 (0.8933) lr 6.4025e-03 eta 1:14:22
epoch [16/30] batch [220/796] time 0.366 (0.380) data 0.000 (0.004) loss 1.1816 (0.8740) lr 6.4025e-03 eta 1:14:07
epoch [16/30] batch [240/796] time 0.340 (0.379) data 0.001 (0.004) loss 0.1407 (0.8687) lr 6.4025e-03 eta 1:13:56
epoch [16/30] batch [260/796] time 0.370 (0.380) data 0.000 (0.004) loss 2.0547 (0.8707) lr 6.4025e-03 eta 1:13:53
epoch [16/30] batch [280/796] time 0.394 (0.380) data 0.000 (0.003) loss 0.1958 (0.8667) lr 6.4025e-03 eta 1:13:47
epoch [16/30] batch [300/796] time 0.395 (0.380) data 0.000 (0.003) loss 0.5366 (0.8558) lr 6.4025e-03 eta 1:13:38
epoch [16/30] batch [320/796] time 0.362 (0.380) data 0.000 (0.003) loss 1.3164 (0.8485) lr 6.4025e-03 eta 1:13:32
epoch [16/30] batch [340/796] time 0.367 (0.380) data 0.000 (0.003) loss 0.6313 (0.8608) lr 6.4025e-03 eta 1:13:24
epoch [16/30] batch [360/796] time 0.398 (0.379) data 0.000 (0.003) loss 1.3691 (0.8601) lr 6.4025e-03 eta 1:13:12
epoch [16/30] batch [380/796] time 0.398 (0.380) data 0.000 (0.003) loss 0.7998 (0.8552) lr 6.4025e-03 eta 1:13:08
epoch [16/30] batch [400/796] time 0.382 (0.379) data 0.001 (0.002) loss 0.2222 (0.8432) lr 6.4025e-03 eta 1:12:54
epoch [16/30] batch [420/796] time 0.343 (0.379) data 0.000 (0.002) loss 0.8433 (0.8445) lr 6.4025e-03 eta 1:12:43
epoch [16/30] batch [440/796] time 0.376 (0.378) data 0.000 (0.002) loss 0.8774 (0.8448) lr 6.4025e-03 eta 1:12:29
epoch [16/30] batch [460/796] time 0.387 (0.378) data 0.001 (0.002) loss 1.5117 (0.8594) lr 6.4025e-03 eta 1:12:21
epoch [16/30] batch [480/796] time 0.370 (0.378) data 0.000 (0.002) loss 0.2008 (0.8654) lr 6.4025e-03 eta 1:12:14
epoch [16/30] batch [500/796] time 0.388 (0.378) data 0.000 (0.002) loss 1.2031 (0.8685) lr 6.4025e-03 eta 1:12:00
epoch [16/30] batch [520/796] time 0.363 (0.377) data 0.000 (0.002) loss 0.9722 (0.8705) lr 6.4025e-03 eta 1:11:50
epoch [16/30] batch [540/796] time 0.390 (0.378) data 0.000 (0.002) loss 0.2014 (0.8684) lr 6.4025e-03 eta 1:11:44
epoch [16/30] batch [560/796] time 0.357 (0.377) data 0.000 (0.002) loss 0.2734 (0.8648) lr 6.4025e-03 eta 1:11:33
epoch [16/30] batch [580/796] time 0.378 (0.377) data 0.000 (0.002) loss 0.6484 (0.8674) lr 6.4025e-03 eta 1:11:25
epoch [16/30] batch [600/796] time 0.364 (0.377) data 0.000 (0.002) loss 0.2573 (0.8605) lr 6.4025e-03 eta 1:11:16
epoch [16/30] batch [620/796] time 0.395 (0.377) data 0.001 (0.002) loss 0.2617 (0.8581) lr 6.4025e-03 eta 1:11:08
epoch [16/30] batch [640/796] time 0.367 (0.377) data 0.000 (0.002) loss 0.9263 (0.8582) lr 6.4025e-03 eta 1:11:00
epoch [16/30] batch [660/796] time 0.400 (0.377) data 0.001 (0.002) loss 2.9844 (0.8575) lr 6.4025e-03 eta 1:10:57
epoch [16/30] batch [680/796] time 0.390 (0.378) data 0.000 (0.002) loss 0.1472 (0.8567) lr 6.4025e-03 eta 1:10:52
epoch [16/30] batch [700/796] time 0.382 (0.378) data 0.000 (0.002) loss 0.6685 (0.8639) lr 6.4025e-03 eta 1:10:43
epoch [16/30] batch [720/796] time 0.371 (0.378) data 0.000 (0.001) loss 1.1162 (0.8599) lr 6.4025e-03 eta 1:10:35
epoch [16/30] batch [740/796] time 0.393 (0.378) data 0.001 (0.001) loss 1.7178 (0.8591) lr 6.4025e-03 eta 1:10:29
epoch [16/30] batch [760/796] time 0.364 (0.377) data 0.000 (0.001) loss 0.1942 (0.8621) lr 6.4025e-03 eta 1:10:18
epoch [16/30] batch [780/796] time 0.329 (0.377) data 0.000 (0.001) loss 0.3918 (0.8608) lr 6.4025e-03 eta 1:10:03
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:43,  5.44s/it] 10%|█         | 2/20 [00:06<00:50,  2.79s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.65s/it] 20%|██        | 4/20 [00:06<00:17,  1.11s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.23it/s] 30%|███       | 6/20 [00:07<00:08,  1.58it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.92it/s] 40%|████      | 8/20 [00:08<00:05,  2.25it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.53it/s] 50%|█████     | 10/20 [00:08<00:03,  2.79it/s] 55%|█████▌    | 11/20 [00:08<00:02,  3.03it/s] 60%|██████    | 12/20 [00:09<00:02,  3.20it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.34it/s] 70%|███████   | 14/20 [00:09<00:01,  3.44it/s] 75%|███████▌  | 15/20 [00:09<00:01,  3.79it/s] 80%|████████  | 16/20 [00:10<00:01,  3.86it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.89it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.22it/s] 95%|█████████▌| 19/20 [00:10<00:00,  4.53it/s]100%|██████████| 20/20 [00:10<00:00,  4.86it/s]100%|██████████| 20/20 [00:11<00:00,  1.80it/s]=> result
* total: 1,990
* correct: 1,577
* accuracy: 79.2%
* error: 20.8%
* macro_f1: 78.6%

epoch [17/30] batch [20/796] time 0.390 (0.425) data 0.000 (0.040) loss 1.7119 (1.0060) lr 6.2794e-03 eta 1:18:46
epoch [17/30] batch [40/796] time 0.373 (0.399) data 0.000 (0.020) loss 1.8545 (1.0451) lr 6.2794e-03 eta 1:13:51
epoch [17/30] batch [60/796] time 0.395 (0.391) data 0.000 (0.013) loss 1.0098 (0.8953) lr 6.2794e-03 eta 1:12:17
epoch [17/30] batch [80/796] time 0.354 (0.388) data 0.000 (0.010) loss 0.3804 (0.8680) lr 6.2794e-03 eta 1:11:30
epoch [17/30] batch [100/796] time 0.391 (0.386) data 0.001 (0.008) loss 0.7388 (0.8575) lr 6.2794e-03 eta 1:11:01
epoch [17/30] batch [120/796] time 0.385 (0.384) data 0.000 (0.007) loss 0.5679 (0.8667) lr 6.2794e-03 eta 1:10:29
epoch [17/30] batch [140/796] time 0.394 (0.383) data 0.000 (0.006) loss 0.2822 (0.8917) lr 6.2794e-03 eta 1:10:10
epoch [17/30] batch [160/796] time 0.358 (0.382) data 0.000 (0.005) loss 1.6797 (0.8979) lr 6.2794e-03 eta 1:09:55
epoch [17/30] batch [180/796] time 0.411 (0.381) data 0.000 (0.005) loss 0.8623 (0.8980) lr 6.2794e-03 eta 1:09:41
epoch [17/30] batch [200/796] time 0.391 (0.381) data 0.000 (0.004) loss 0.7568 (0.8759) lr 6.2794e-03 eta 1:09:28
epoch [17/30] batch [220/796] time 0.386 (0.380) data 0.000 (0.004) loss 1.2139 (0.8729) lr 6.2794e-03 eta 1:09:16
epoch [17/30] batch [240/796] time 0.409 (0.381) data 0.000 (0.004) loss 1.7607 (0.8716) lr 6.2794e-03 eta 1:09:15
epoch [17/30] batch [260/796] time 0.396 (0.382) data 0.000 (0.003) loss 1.1348 (0.8626) lr 6.2794e-03 eta 1:09:13
epoch [17/30] batch [280/796] time 0.356 (0.382) data 0.001 (0.003) loss 2.4746 (0.8701) lr 6.2794e-03 eta 1:09:04
epoch [17/30] batch [300/796] time 0.389 (0.381) data 0.000 (0.003) loss 0.7397 (0.8829) lr 6.2794e-03 eta 1:08:50
epoch [17/30] batch [320/796] time 0.378 (0.380) data 0.000 (0.003) loss 0.5698 (0.8834) lr 6.2794e-03 eta 1:08:36
epoch [17/30] batch [340/796] time 0.364 (0.380) data 0.000 (0.003) loss 1.4385 (0.8971) lr 6.2794e-03 eta 1:08:25
epoch [17/30] batch [360/796] time 0.395 (0.380) data 0.000 (0.002) loss 0.6860 (0.9080) lr 6.2794e-03 eta 1:08:17
epoch [17/30] batch [380/796] time 0.367 (0.380) data 0.000 (0.002) loss 0.1747 (0.9068) lr 6.2794e-03 eta 1:08:08
epoch [17/30] batch [400/796] time 0.368 (0.380) data 0.001 (0.002) loss 1.0400 (0.9064) lr 6.2794e-03 eta 1:08:00
epoch [17/30] batch [420/796] time 0.364 (0.379) data 0.000 (0.002) loss 0.2600 (0.8999) lr 6.2794e-03 eta 1:07:48
epoch [17/30] batch [440/796] time 0.389 (0.379) data 0.001 (0.002) loss 1.9199 (0.8944) lr 6.2794e-03 eta 1:07:38
epoch [17/30] batch [460/796] time 0.388 (0.379) data 0.000 (0.002) loss 0.2874 (0.8947) lr 6.2794e-03 eta 1:07:28
epoch [17/30] batch [480/796] time 0.373 (0.379) data 0.000 (0.002) loss 0.4185 (0.8971) lr 6.2794e-03 eta 1:07:18
epoch [17/30] batch [500/796] time 0.385 (0.379) data 0.000 (0.002) loss 0.9370 (0.8946) lr 6.2794e-03 eta 1:07:11
epoch [17/30] batch [520/796] time 0.382 (0.378) data 0.000 (0.002) loss 1.1221 (0.8971) lr 6.2794e-03 eta 1:07:00
epoch [17/30] batch [540/796] time 0.395 (0.378) data 0.000 (0.002) loss 1.6445 (0.8948) lr 6.2794e-03 eta 1:06:52
epoch [17/30] batch [560/796] time 0.385 (0.378) data 0.000 (0.002) loss 0.1948 (0.8869) lr 6.2794e-03 eta 1:06:42
epoch [17/30] batch [580/796] time 0.367 (0.378) data 0.000 (0.002) loss 1.0449 (0.8799) lr 6.2794e-03 eta 1:06:30
epoch [17/30] batch [600/796] time 0.388 (0.378) data 0.000 (0.002) loss 0.2035 (0.8889) lr 6.2794e-03 eta 1:06:26
epoch [17/30] batch [620/796] time 0.381 (0.378) data 0.000 (0.002) loss 0.4651 (0.8872) lr 6.2794e-03 eta 1:06:20
epoch [17/30] batch [640/796] time 0.360 (0.378) data 0.000 (0.002) loss 2.0527 (0.8936) lr 6.2794e-03 eta 1:06:10
epoch [17/30] batch [660/796] time 0.362 (0.378) data 0.000 (0.001) loss 1.7217 (0.8922) lr 6.2794e-03 eta 1:05:59
epoch [17/30] batch [680/796] time 0.387 (0.378) data 0.000 (0.001) loss 0.3723 (0.8892) lr 6.2794e-03 eta 1:05:54
epoch [17/30] batch [700/796] time 0.387 (0.378) data 0.000 (0.001) loss 1.2148 (0.8883) lr 6.2794e-03 eta 1:05:43
epoch [17/30] batch [720/796] time 0.365 (0.377) data 0.000 (0.001) loss 0.1887 (0.8884) lr 6.2794e-03 eta 1:05:33
epoch [17/30] batch [740/796] time 0.366 (0.377) data 0.000 (0.001) loss 0.0508 (0.8817) lr 6.2794e-03 eta 1:05:24
epoch [17/30] batch [760/796] time 0.394 (0.377) data 0.000 (0.001) loss 0.5000 (0.8820) lr 6.2794e-03 eta 1:05:13
epoch [17/30] batch [780/796] time 0.336 (0.376) data 0.000 (0.001) loss 0.4131 (0.8793) lr 6.2794e-03 eta 1:04:57
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:50,  5.82s/it] 10%|█         | 2/20 [00:06<00:52,  2.91s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.70s/it] 20%|██        | 4/20 [00:07<00:18,  1.15s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.20it/s] 30%|███       | 6/20 [00:07<00:09,  1.55it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.91it/s] 40%|████      | 8/20 [00:08<00:05,  2.25it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.53it/s] 50%|█████     | 10/20 [00:08<00:03,  2.80it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.98it/s] 60%|██████    | 12/20 [00:09<00:02,  3.24it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.40it/s] 70%|███████   | 14/20 [00:09<00:01,  3.49it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.59it/s] 80%|████████  | 16/20 [00:10<00:01,  3.69it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.75it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.25it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.70it/s]100%|██████████| 20/20 [00:11<00:00,  4.16it/s]100%|██████████| 20/20 [00:11<00:00,  1.72it/s]=> result
* total: 1,990
* correct: 1,575
* accuracy: 79.1%
* error: 20.9%
* macro_f1: 78.5%

epoch [18/30] batch [20/796] time 0.358 (0.434) data 0.000 (0.046) loss 1.3672 (0.8507) lr 6.1234e-03 eta 1:14:47
epoch [18/30] batch [40/796] time 0.356 (0.405) data 0.000 (0.023) loss 0.7568 (0.9218) lr 6.1234e-03 eta 1:09:34
epoch [18/30] batch [60/796] time 0.384 (0.395) data 0.000 (0.015) loss 0.6323 (0.9966) lr 6.1234e-03 eta 1:07:46
epoch [18/30] batch [80/796] time 0.386 (0.393) data 0.000 (0.012) loss 0.9336 (0.9893) lr 6.1234e-03 eta 1:07:13
epoch [18/30] batch [100/796] time 0.394 (0.390) data 0.000 (0.009) loss 1.2500 (0.9488) lr 6.1234e-03 eta 1:06:32
epoch [18/30] batch [120/796] time 0.390 (0.387) data 0.000 (0.008) loss 0.2656 (0.9322) lr 6.1234e-03 eta 1:05:53
epoch [18/30] batch [140/796] time 0.385 (0.385) data 0.000 (0.007) loss 0.6875 (0.9026) lr 6.1234e-03 eta 1:05:24
epoch [18/30] batch [160/796] time 0.389 (0.383) data 0.000 (0.006) loss 0.6455 (0.8872) lr 6.1234e-03 eta 1:05:01
epoch [18/30] batch [180/796] time 0.389 (0.383) data 0.000 (0.005) loss 1.6074 (0.8781) lr 6.1234e-03 eta 1:04:55
epoch [18/30] batch [200/796] time 0.375 (0.383) data 0.000 (0.005) loss 0.3896 (0.8709) lr 6.1234e-03 eta 1:04:43
epoch [18/30] batch [220/796] time 0.398 (0.381) data 0.000 (0.004) loss 1.2451 (0.8765) lr 6.1234e-03 eta 1:04:22
epoch [18/30] batch [240/796] time 0.379 (0.381) data 0.000 (0.004) loss 0.0593 (0.8657) lr 6.1234e-03 eta 1:04:14
epoch [18/30] batch [260/796] time 0.374 (0.380) data 0.000 (0.004) loss 0.3528 (0.8613) lr 6.1234e-03 eta 1:03:57
epoch [18/30] batch [280/796] time 0.364 (0.380) data 0.000 (0.003) loss 0.4929 (0.8431) lr 6.1234e-03 eta 1:03:46
epoch [18/30] batch [300/796] time 0.395 (0.380) data 0.000 (0.003) loss 0.2196 (0.8372) lr 6.1234e-03 eta 1:03:36
epoch [18/30] batch [320/796] time 0.382 (0.380) data 0.000 (0.003) loss 1.3291 (0.8361) lr 6.1234e-03 eta 1:03:26
epoch [18/30] batch [340/796] time 0.352 (0.380) data 0.000 (0.003) loss 0.8135 (0.8263) lr 6.1234e-03 eta 1:03:23
epoch [18/30] batch [360/796] time 0.354 (0.380) data 0.000 (0.003) loss 0.3818 (0.8274) lr 6.1234e-03 eta 1:03:14
epoch [18/30] batch [380/796] time 0.408 (0.380) data 0.000 (0.003) loss 0.1587 (0.8301) lr 6.1234e-03 eta 1:03:03
epoch [18/30] batch [400/796] time 0.344 (0.379) data 0.000 (0.003) loss 0.0513 (0.8334) lr 6.1234e-03 eta 1:02:52
epoch [18/30] batch [420/796] time 0.369 (0.379) data 0.000 (0.002) loss 1.2734 (0.8312) lr 6.1234e-03 eta 1:02:45
epoch [18/30] batch [440/796] time 0.375 (0.379) data 0.000 (0.002) loss 1.0186 (0.8329) lr 6.1234e-03 eta 1:02:34
epoch [18/30] batch [460/796] time 0.363 (0.379) data 0.000 (0.002) loss 0.2981 (0.8354) lr 6.1234e-03 eta 1:02:23
epoch [18/30] batch [480/796] time 0.356 (0.378) data 0.000 (0.002) loss 1.3369 (0.8428) lr 6.1234e-03 eta 1:02:14
epoch [18/30] batch [500/796] time 0.355 (0.378) data 0.000 (0.002) loss 1.1123 (0.8459) lr 6.1234e-03 eta 1:02:02
epoch [18/30] batch [520/796] time 0.390 (0.378) data 0.000 (0.002) loss 0.7456 (0.8471) lr 6.1234e-03 eta 1:01:54
epoch [18/30] batch [540/796] time 0.387 (0.378) data 0.000 (0.002) loss 0.5010 (0.8475) lr 6.1234e-03 eta 1:01:46
epoch [18/30] batch [560/796] time 0.383 (0.378) data 0.000 (0.002) loss 2.3750 (0.8501) lr 6.1234e-03 eta 1:01:37
epoch [18/30] batch [580/796] time 0.381 (0.378) data 0.000 (0.002) loss 0.9087 (0.8488) lr 6.1234e-03 eta 1:01:28
epoch [18/30] batch [600/796] time 0.384 (0.378) data 0.000 (0.002) loss 1.0635 (0.8508) lr 6.1234e-03 eta 1:01:21
epoch [18/30] batch [620/796] time 0.350 (0.378) data 0.000 (0.002) loss 2.5527 (0.8544) lr 6.1234e-03 eta 1:01:14
epoch [18/30] batch [640/796] time 0.381 (0.378) data 0.000 (0.002) loss 1.4541 (0.8489) lr 6.1234e-03 eta 1:01:05
epoch [18/30] batch [660/796] time 0.340 (0.377) data 0.000 (0.002) loss 2.7754 (0.8449) lr 6.1234e-03 eta 1:00:56
epoch [18/30] batch [680/796] time 0.345 (0.377) data 0.000 (0.002) loss 0.6270 (0.8441) lr 6.1234e-03 eta 1:00:44
epoch [18/30] batch [700/796] time 0.353 (0.377) data 0.000 (0.002) loss 0.2041 (0.8413) lr 6.1234e-03 eta 1:00:36
epoch [18/30] batch [720/796] time 0.394 (0.377) data 0.000 (0.002) loss 0.3049 (0.8412) lr 6.1234e-03 eta 1:00:29
epoch [18/30] batch [740/796] time 0.363 (0.377) data 0.000 (0.001) loss 0.4492 (0.8469) lr 6.1234e-03 eta 1:00:19
epoch [18/30] batch [760/796] time 0.368 (0.377) data 0.000 (0.001) loss 0.8999 (0.8417) lr 6.1234e-03 eta 1:00:11
epoch [18/30] batch [780/796] time 0.328 (0.376) data 0.000 (0.001) loss 1.6650 (0.8497) lr 6.1234e-03 eta 0:59:54
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:50,  5.83s/it] 10%|█         | 2/20 [00:06<00:54,  3.05s/it] 15%|█▌        | 3/20 [00:07<00:30,  1.78s/it] 20%|██        | 4/20 [00:07<00:19,  1.19s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.17it/s] 30%|███       | 6/20 [00:08<00:09,  1.52it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.87it/s] 40%|████      | 8/20 [00:08<00:05,  2.21it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.47it/s] 50%|█████     | 10/20 [00:09<00:03,  2.70it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.90it/s] 60%|██████    | 12/20 [00:09<00:02,  3.13it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.41it/s] 70%|███████   | 14/20 [00:10<00:01,  3.55it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.62it/s] 80%|████████  | 16/20 [00:10<00:01,  3.71it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.80it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.62it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.84it/s]100%|██████████| 20/20 [00:11<00:00,  4.27it/s]100%|██████████| 20/20 [00:11<00:00,  1.69it/s]=> result
* total: 1,990
* correct: 1,581
* accuracy: 79.4%
* error: 20.6%
* macro_f1: 78.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1/prompt_learner/model-best.pth.tar

epoch [19/30] batch [20/796] time 0.347 (0.426) data 0.000 (0.048) loss 0.8677 (0.6989) lr 5.9363e-03 eta 1:07:44
epoch [19/30] batch [40/796] time 0.376 (0.403) data 0.000 (0.024) loss 0.1951 (0.7952) lr 5.9363e-03 eta 1:03:51
epoch [19/30] batch [60/796] time 0.385 (0.396) data 0.000 (0.016) loss 0.2091 (0.8258) lr 5.9363e-03 eta 1:02:39
epoch [19/30] batch [80/796] time 0.385 (0.391) data 0.000 (0.012) loss 0.4253 (0.7906) lr 5.9363e-03 eta 1:01:42
epoch [19/30] batch [100/796] time 0.384 (0.388) data 0.000 (0.010) loss 0.5752 (0.7699) lr 5.9363e-03 eta 1:01:05
epoch [19/30] batch [120/796] time 0.356 (0.385) data 0.000 (0.008) loss 0.1909 (0.7643) lr 5.9363e-03 eta 1:00:32
epoch [19/30] batch [140/796] time 0.347 (0.384) data 0.000 (0.007) loss 0.4077 (0.7742) lr 5.9363e-03 eta 1:00:15
epoch [19/30] batch [160/796] time 0.381 (0.383) data 0.000 (0.006) loss 0.7642 (0.7715) lr 5.9363e-03 eta 1:00:01
epoch [19/30] batch [180/796] time 0.389 (0.383) data 0.000 (0.006) loss 0.1501 (0.7524) lr 5.9363e-03 eta 0:59:45
epoch [19/30] batch [200/796] time 0.340 (0.382) data 0.000 (0.005) loss 0.2281 (0.7611) lr 5.9363e-03 eta 0:59:29
epoch [19/30] batch [220/796] time 0.396 (0.381) data 0.000 (0.005) loss 0.8374 (0.7629) lr 5.9363e-03 eta 0:59:15
epoch [19/30] batch [240/796] time 0.350 (0.380) data 0.000 (0.004) loss 1.1572 (0.7684) lr 5.9363e-03 eta 0:59:02
epoch [19/30] batch [260/796] time 0.388 (0.380) data 0.000 (0.004) loss 1.6104 (0.7692) lr 5.9363e-03 eta 0:58:52
epoch [19/30] batch [280/796] time 0.388 (0.380) data 0.000 (0.004) loss 3.0039 (0.7948) lr 5.9363e-03 eta 0:58:41
epoch [19/30] batch [300/796] time 0.364 (0.380) data 0.000 (0.003) loss 0.2087 (0.7893) lr 5.9363e-03 eta 0:58:34
epoch [19/30] batch [320/796] time 0.367 (0.379) data 0.000 (0.003) loss 0.6875 (0.7943) lr 5.9363e-03 eta 0:58:18
epoch [19/30] batch [340/796] time 0.357 (0.378) data 0.000 (0.003) loss 0.5151 (0.8038) lr 5.9363e-03 eta 0:58:05
epoch [19/30] batch [360/796] time 0.382 (0.378) data 0.000 (0.003) loss 0.2505 (0.8004) lr 5.9363e-03 eta 0:57:57
epoch [19/30] batch [380/796] time 0.383 (0.379) data 0.000 (0.003) loss 1.1436 (0.7976) lr 5.9363e-03 eta 0:57:54
epoch [19/30] batch [400/796] time 0.347 (0.379) data 0.001 (0.003) loss 0.5234 (0.8032) lr 5.9363e-03 eta 0:57:48
epoch [19/30] batch [420/796] time 0.340 (0.379) data 0.000 (0.003) loss 1.9775 (0.8061) lr 5.9363e-03 eta 0:57:38
epoch [19/30] batch [440/796] time 0.391 (0.378) data 0.000 (0.002) loss 0.5181 (0.8219) lr 5.9363e-03 eta 0:57:28
epoch [19/30] batch [460/796] time 0.352 (0.378) data 0.000 (0.002) loss 0.4724 (0.8179) lr 5.9363e-03 eta 0:57:16
epoch [19/30] batch [480/796] time 0.372 (0.378) data 0.000 (0.002) loss 1.4668 (0.8211) lr 5.9363e-03 eta 0:57:08
epoch [19/30] batch [500/796] time 0.391 (0.378) data 0.000 (0.002) loss 0.3252 (0.8212) lr 5.9363e-03 eta 0:57:00
epoch [19/30] batch [520/796] time 0.367 (0.378) data 0.000 (0.002) loss 0.2217 (0.8243) lr 5.9363e-03 eta 0:56:52
epoch [19/30] batch [540/796] time 0.388 (0.378) data 0.000 (0.002) loss 1.1045 (0.8218) lr 5.9363e-03 eta 0:56:43
epoch [19/30] batch [560/796] time 0.365 (0.377) data 0.000 (0.002) loss 1.1846 (0.8265) lr 5.9363e-03 eta 0:56:34
epoch [19/30] batch [580/796] time 0.401 (0.377) data 0.000 (0.002) loss 1.1592 (0.8227) lr 5.9363e-03 eta 0:56:24
epoch [19/30] batch [600/796] time 0.381 (0.377) data 0.000 (0.002) loss 0.4407 (0.8185) lr 5.9363e-03 eta 0:56:15
epoch [19/30] batch [620/796] time 0.376 (0.377) data 0.000 (0.002) loss 0.1407 (0.8202) lr 5.9363e-03 eta 0:56:06
epoch [19/30] batch [640/796] time 0.364 (0.377) data 0.000 (0.002) loss 0.3921 (0.8271) lr 5.9363e-03 eta 0:55:58
epoch [19/30] batch [660/796] time 0.378 (0.377) data 0.000 (0.002) loss 0.5317 (0.8222) lr 5.9363e-03 eta 0:55:51
epoch [19/30] batch [680/796] time 0.383 (0.377) data 0.000 (0.002) loss 0.7861 (0.8247) lr 5.9363e-03 eta 0:55:42
epoch [19/30] batch [700/796] time 0.358 (0.377) data 0.000 (0.002) loss 1.8535 (0.8252) lr 5.9363e-03 eta 0:55:36
epoch [19/30] batch [720/796] time 0.379 (0.377) data 0.000 (0.002) loss 0.4836 (0.8277) lr 5.9363e-03 eta 0:55:27
epoch [19/30] batch [740/796] time 0.357 (0.377) data 0.000 (0.002) loss 1.5420 (0.8300) lr 5.9363e-03 eta 0:55:19
epoch [19/30] batch [760/796] time 0.361 (0.377) data 0.000 (0.002) loss 1.2715 (0.8318) lr 5.9363e-03 eta 0:55:10
epoch [19/30] batch [780/796] time 0.335 (0.376) data 0.000 (0.001) loss 0.4905 (0.8300) lr 5.9363e-03 eta 0:54:55
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:44,  5.52s/it] 10%|█         | 2/20 [00:06<00:51,  2.84s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.68s/it] 20%|██        | 4/20 [00:07<00:18,  1.14s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.21it/s] 30%|███       | 6/20 [00:07<00:08,  1.56it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.92it/s] 40%|████      | 8/20 [00:08<00:05,  2.23it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.52it/s] 50%|█████     | 10/20 [00:08<00:03,  2.73it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.92it/s] 60%|██████    | 12/20 [00:09<00:02,  3.16it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.42it/s] 70%|███████   | 14/20 [00:09<00:01,  3.58it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.93it/s] 80%|████████  | 16/20 [00:10<00:00,  4.14it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.10it/s] 90%|█████████ | 18/20 [00:10<00:00,  3.95it/s] 95%|█████████▌| 19/20 [00:10<00:00,  4.31it/s]100%|██████████| 20/20 [00:11<00:00,  4.68it/s]100%|██████████| 20/20 [00:11<00:00,  1.77it/s]=> result
* total: 1,990
* correct: 1,584
* accuracy: 79.6%
* error: 20.4%
* macro_f1: 79.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1/prompt_learner/model-best.pth.tar

epoch [20/30] batch [20/796] time 0.350 (0.425) data 0.000 (0.042) loss 1.0537 (0.8219) lr 5.7202e-03 eta 1:01:50
epoch [20/30] batch [40/796] time 0.351 (0.400) data 0.000 (0.021) loss 1.2568 (0.8852) lr 5.7202e-03 eta 0:58:04
epoch [20/30] batch [60/796] time 0.349 (0.392) data 0.000 (0.014) loss 0.4133 (0.8964) lr 5.7202e-03 eta 0:56:44
epoch [20/30] batch [80/796] time 0.349 (0.386) data 0.000 (0.011) loss 1.3916 (0.8635) lr 5.7202e-03 eta 0:55:47
epoch [20/30] batch [100/796] time 0.355 (0.384) data 0.000 (0.009) loss 2.5996 (0.8861) lr 5.7202e-03 eta 0:55:21
epoch [20/30] batch [120/796] time 0.345 (0.381) data 0.000 (0.007) loss 0.1036 (0.8486) lr 5.7202e-03 eta 0:54:49
epoch [20/30] batch [140/796] time 0.347 (0.379) data 0.000 (0.006) loss 1.3936 (0.8984) lr 5.7202e-03 eta 0:54:22
epoch [20/30] batch [160/796] time 0.402 (0.377) data 0.000 (0.005) loss 0.3052 (0.8899) lr 5.7202e-03 eta 0:54:03
epoch [20/30] batch [180/796] time 0.344 (0.377) data 0.000 (0.005) loss 0.6758 (0.8925) lr 5.7202e-03 eta 0:53:54
epoch [20/30] batch [200/796] time 0.397 (0.377) data 0.000 (0.004) loss 0.2542 (0.8561) lr 5.7202e-03 eta 0:53:48
epoch [20/30] batch [220/796] time 0.342 (0.377) data 0.000 (0.004) loss 1.1865 (0.8765) lr 5.7202e-03 eta 0:53:34
epoch [20/30] batch [240/796] time 0.384 (0.376) data 0.000 (0.004) loss 2.1719 (0.8771) lr 5.7202e-03 eta 0:53:23
epoch [20/30] batch [260/796] time 0.366 (0.376) data 0.000 (0.003) loss 1.0303 (0.8951) lr 5.7202e-03 eta 0:53:12
epoch [20/30] batch [280/796] time 0.391 (0.376) data 0.000 (0.003) loss 0.4165 (0.8974) lr 5.7202e-03 eta 0:53:03
epoch [20/30] batch [300/796] time 0.380 (0.375) data 0.000 (0.003) loss 0.6807 (0.9010) lr 5.7202e-03 eta 0:52:52
epoch [20/30] batch [320/796] time 0.370 (0.375) data 0.000 (0.003) loss 0.8804 (0.9145) lr 5.7202e-03 eta 0:52:43
epoch [20/30] batch [340/796] time 0.370 (0.375) data 0.000 (0.003) loss 0.7549 (0.9214) lr 5.7202e-03 eta 0:52:32
epoch [20/30] batch [360/796] time 0.388 (0.375) data 0.000 (0.003) loss 0.9648 (0.9089) lr 5.7202e-03 eta 0:52:24
epoch [20/30] batch [380/796] time 0.374 (0.375) data 0.000 (0.002) loss 4.2383 (0.9069) lr 5.7202e-03 eta 0:52:17
epoch [20/30] batch [400/796] time 0.396 (0.375) data 0.000 (0.002) loss 1.1699 (0.9101) lr 5.7202e-03 eta 0:52:10
epoch [20/30] batch [420/796] time 0.379 (0.375) data 0.000 (0.002) loss 1.5137 (0.9077) lr 5.7202e-03 eta 0:52:04
epoch [20/30] batch [440/796] time 0.366 (0.375) data 0.000 (0.002) loss 0.2217 (0.8996) lr 5.7202e-03 eta 0:52:01
epoch [20/30] batch [460/796] time 0.353 (0.375) data 0.000 (0.002) loss 1.8564 (0.8991) lr 5.7202e-03 eta 0:51:52
epoch [20/30] batch [480/796] time 0.386 (0.375) data 0.000 (0.002) loss 0.5103 (0.8925) lr 5.7202e-03 eta 0:51:41
epoch [20/30] batch [500/796] time 0.375 (0.375) data 0.000 (0.002) loss 0.1360 (0.8893) lr 5.7202e-03 eta 0:51:35
epoch [20/30] batch [520/796] time 0.415 (0.375) data 0.000 (0.002) loss 0.8125 (0.8839) lr 5.7202e-03 eta 0:51:27
epoch [20/30] batch [540/796] time 0.387 (0.375) data 0.000 (0.002) loss 1.4834 (0.8865) lr 5.7202e-03 eta 0:51:21
epoch [20/30] batch [560/796] time 0.386 (0.375) data 0.000 (0.002) loss 1.3604 (0.8832) lr 5.7202e-03 eta 0:51:13
epoch [20/30] batch [580/796] time 0.390 (0.375) data 0.000 (0.002) loss 0.4053 (0.8792) lr 5.7202e-03 eta 0:51:08
epoch [20/30] batch [600/796] time 0.338 (0.375) data 0.000 (0.002) loss 0.7212 (0.8760) lr 5.7202e-03 eta 0:50:59
epoch [20/30] batch [620/796] time 0.370 (0.375) data 0.000 (0.002) loss 0.5928 (0.8735) lr 5.7202e-03 eta 0:50:50
epoch [20/30] batch [640/796] time 0.361 (0.375) data 0.000 (0.002) loss 1.2715 (0.8719) lr 5.7202e-03 eta 0:50:41
epoch [20/30] batch [660/796] time 0.357 (0.375) data 0.000 (0.002) loss 0.3433 (0.8680) lr 5.7202e-03 eta 0:50:32
epoch [20/30] batch [680/796] time 0.343 (0.374) data 0.000 (0.001) loss 0.1852 (0.8688) lr 5.7202e-03 eta 0:50:23
epoch [20/30] batch [700/796] time 0.354 (0.374) data 0.000 (0.001) loss 1.1670 (0.8633) lr 5.7202e-03 eta 0:50:15
epoch [20/30] batch [720/796] time 0.382 (0.374) data 0.000 (0.001) loss 0.7339 (0.8612) lr 5.7202e-03 eta 0:50:06
epoch [20/30] batch [740/796] time 0.355 (0.374) data 0.000 (0.001) loss 1.2041 (0.8644) lr 5.7202e-03 eta 0:49:57
epoch [20/30] batch [760/796] time 0.339 (0.374) data 0.000 (0.001) loss 0.3701 (0.8649) lr 5.7202e-03 eta 0:49:47
epoch [20/30] batch [780/796] time 0.332 (0.373) data 0.000 (0.001) loss 1.5371 (0.8682) lr 5.7202e-03 eta 0:49:32
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:48,  5.71s/it] 10%|█         | 2/20 [00:06<00:52,  2.93s/it] 15%|█▌        | 3/20 [00:06<00:29,  1.73s/it] 20%|██        | 4/20 [00:07<00:18,  1.16s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.18it/s] 30%|███       | 6/20 [00:07<00:09,  1.52it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.86it/s] 40%|████      | 8/20 [00:08<00:05,  2.21it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.52it/s] 50%|█████     | 10/20 [00:08<00:03,  2.77it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.03it/s] 60%|██████    | 12/20 [00:09<00:02,  3.26it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.41it/s] 70%|███████   | 14/20 [00:10<00:01,  3.55it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.67it/s] 80%|████████  | 16/20 [00:10<00:01,  3.74it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.81it/s] 90%|█████████ | 18/20 [00:10<00:00,  3.98it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.34it/s]100%|██████████| 20/20 [00:11<00:00,  4.71it/s]100%|██████████| 20/20 [00:11<00:00,  1.74it/s]=> result
* total: 1,990
* correct: 1,586
* accuracy: 79.7%
* error: 20.3%
* macro_f1: 79.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1/prompt_learner/model.pth.tar-20

epoch [21/30] batch [20/796] time 0.386 (0.429) data 0.000 (0.044) loss 0.4275 (0.8935) lr 5.4773e-03 eta 0:56:46
epoch [21/30] batch [40/796] time 0.368 (0.403) data 0.000 (0.022) loss 0.3931 (0.8767) lr 5.4773e-03 eta 0:53:11
epoch [21/30] batch [60/796] time 0.389 (0.395) data 0.000 (0.015) loss 0.1641 (0.8498) lr 5.4773e-03 eta 0:52:02
epoch [21/30] batch [80/796] time 0.362 (0.390) data 0.000 (0.011) loss 1.3369 (0.8209) lr 5.4773e-03 eta 0:51:10
epoch [21/30] batch [100/796] time 0.401 (0.389) data 0.000 (0.009) loss 0.9468 (0.7974) lr 5.4773e-03 eta 0:50:54
epoch [21/30] batch [120/796] time 0.384 (0.388) data 0.001 (0.008) loss 0.3450 (0.7521) lr 5.4773e-03 eta 0:50:38
epoch [21/30] batch [140/796] time 0.370 (0.386) data 0.000 (0.007) loss 1.1689 (0.7628) lr 5.4773e-03 eta 0:50:20
epoch [21/30] batch [160/796] time 0.378 (0.386) data 0.000 (0.006) loss 0.7856 (0.7848) lr 5.4773e-03 eta 0:50:08
epoch [21/30] batch [180/796] time 0.394 (0.385) data 0.000 (0.005) loss 0.5366 (0.8198) lr 5.4773e-03 eta 0:49:54
epoch [21/30] batch [200/796] time 0.393 (0.383) data 0.000 (0.005) loss 0.7725 (0.8038) lr 5.4773e-03 eta 0:49:34
epoch [21/30] batch [220/796] time 0.409 (0.383) data 0.000 (0.004) loss 0.5200 (0.7860) lr 5.4773e-03 eta 0:49:23
epoch [21/30] batch [240/796] time 0.373 (0.383) data 0.000 (0.004) loss 1.3809 (0.8004) lr 5.4773e-03 eta 0:49:16
epoch [21/30] batch [260/796] time 0.400 (0.383) data 0.000 (0.004) loss 1.2266 (0.8049) lr 5.4773e-03 eta 0:49:09
epoch [21/30] batch [280/796] time 0.368 (0.383) data 0.000 (0.003) loss 0.3635 (0.8003) lr 5.4773e-03 eta 0:48:58
epoch [21/30] batch [300/796] time 0.345 (0.382) data 0.000 (0.003) loss 1.0605 (0.7979) lr 5.4773e-03 eta 0:48:47
epoch [21/30] batch [320/796] time 0.369 (0.382) data 0.000 (0.003) loss 0.3000 (0.7990) lr 5.4773e-03 eta 0:48:40
epoch [21/30] batch [340/796] time 0.430 (0.382) data 0.000 (0.003) loss 0.6895 (0.8063) lr 5.4773e-03 eta 0:48:33
epoch [21/30] batch [360/796] time 0.370 (0.382) data 0.000 (0.003) loss 0.3943 (0.8062) lr 5.4773e-03 eta 0:48:24
epoch [21/30] batch [380/796] time 0.385 (0.382) data 0.000 (0.003) loss 0.3335 (0.8117) lr 5.4773e-03 eta 0:48:14
epoch [21/30] batch [400/796] time 0.381 (0.382) data 0.000 (0.002) loss 0.5132 (0.8154) lr 5.4773e-03 eta 0:48:04
epoch [21/30] batch [420/796] time 0.379 (0.381) data 0.000 (0.002) loss 3.3457 (0.8234) lr 5.4773e-03 eta 0:47:54
epoch [21/30] batch [440/796] time 0.391 (0.381) data 0.000 (0.002) loss 1.5078 (0.8305) lr 5.4773e-03 eta 0:47:45
epoch [21/30] batch [460/796] time 0.380 (0.381) data 0.000 (0.002) loss 0.7227 (0.8384) lr 5.4773e-03 eta 0:47:40
epoch [21/30] batch [480/796] time 0.351 (0.381) data 0.000 (0.002) loss 1.0225 (0.8314) lr 5.4773e-03 eta 0:47:32
epoch [21/30] batch [500/796] time 0.374 (0.382) data 0.000 (0.002) loss 0.5566 (0.8293) lr 5.4773e-03 eta 0:47:26
epoch [21/30] batch [520/796] time 0.355 (0.381) data 0.000 (0.002) loss 0.9844 (0.8289) lr 5.4773e-03 eta 0:47:17
epoch [21/30] batch [540/796] time 0.348 (0.381) data 0.000 (0.002) loss 1.2568 (0.8347) lr 5.4773e-03 eta 0:47:07
epoch [21/30] batch [560/796] time 0.350 (0.381) data 0.000 (0.002) loss 1.7441 (0.8372) lr 5.4773e-03 eta 0:46:58
epoch [21/30] batch [580/796] time 0.470 (0.381) data 0.000 (0.002) loss 0.1608 (0.8327) lr 5.4773e-03 eta 0:46:50
epoch [21/30] batch [600/796] time 0.381 (0.381) data 0.000 (0.002) loss 0.1740 (0.8255) lr 5.4773e-03 eta 0:46:42
epoch [21/30] batch [620/796] time 0.406 (0.381) data 0.000 (0.002) loss 0.3801 (0.8143) lr 5.4773e-03 eta 0:46:35
epoch [21/30] batch [640/796] time 0.341 (0.381) data 0.000 (0.002) loss 0.1360 (0.8193) lr 5.4773e-03 eta 0:46:27
epoch [21/30] batch [660/796] time 0.365 (0.381) data 0.000 (0.002) loss 0.4639 (0.8238) lr 5.4773e-03 eta 0:46:18
epoch [21/30] batch [680/796] time 0.394 (0.380) data 0.000 (0.002) loss 0.4282 (0.8235) lr 5.4773e-03 eta 0:46:09
epoch [21/30] batch [700/796] time 0.396 (0.380) data 0.000 (0.002) loss 0.9419 (0.8254) lr 5.4773e-03 eta 0:46:00
epoch [21/30] batch [720/796] time 0.344 (0.380) data 0.000 (0.002) loss 0.1995 (0.8222) lr 5.4773e-03 eta 0:45:51
epoch [21/30] batch [740/796] time 0.388 (0.380) data 0.000 (0.001) loss 0.4065 (0.8260) lr 5.4773e-03 eta 0:45:42
epoch [21/30] batch [760/796] time 0.363 (0.380) data 0.000 (0.001) loss 2.3145 (0.8298) lr 5.4773e-03 eta 0:45:35
epoch [21/30] batch [780/796] time 0.333 (0.379) data 0.000 (0.001) loss 0.5156 (0.8309) lr 5.4773e-03 eta 0:45:20
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:51,  5.86s/it] 10%|█         | 2/20 [00:06<00:55,  3.08s/it] 15%|█▌        | 3/20 [00:07<00:30,  1.80s/it] 20%|██        | 4/20 [00:07<00:19,  1.20s/it] 25%|██▌       | 5/20 [00:07<00:13,  1.15it/s] 30%|███       | 6/20 [00:08<00:09,  1.50it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.86it/s] 40%|████      | 8/20 [00:08<00:05,  2.19it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.50it/s] 50%|█████     | 10/20 [00:09<00:03,  2.77it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.02it/s] 60%|██████    | 12/20 [00:09<00:02,  3.26it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.42it/s] 70%|███████   | 14/20 [00:10<00:01,  3.55it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.66it/s] 80%|████████  | 16/20 [00:10<00:01,  3.68it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.92it/s] 90%|█████████ | 18/20 [00:11<00:00,  4.16it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.49it/s]100%|██████████| 20/20 [00:11<00:00,  4.82it/s]100%|██████████| 20/20 [00:11<00:00,  1.71it/s]=> result
* total: 1,990
* correct: 1,598
* accuracy: 80.3%
* error: 19.7%
* macro_f1: 79.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1/prompt_learner/model-best.pth.tar

epoch [22/30] batch [20/796] time 0.387 (0.423) data 0.000 (0.042) loss 0.7939 (0.7509) lr 5.2104e-03 eta 0:50:24
epoch [22/30] batch [40/796] time 0.367 (0.403) data 0.000 (0.021) loss 1.1572 (0.7594) lr 5.2104e-03 eta 0:47:52
epoch [22/30] batch [60/796] time 0.390 (0.393) data 0.000 (0.014) loss 0.8887 (0.7371) lr 5.2104e-03 eta 0:46:34
epoch [22/30] batch [80/796] time 0.403 (0.389) data 0.000 (0.011) loss 0.5557 (0.7474) lr 5.2104e-03 eta 0:45:56
epoch [22/30] batch [100/796] time 0.377 (0.386) data 0.000 (0.009) loss 0.2286 (0.7533) lr 5.2104e-03 eta 0:45:25
epoch [22/30] batch [120/796] time 0.412 (0.383) data 0.000 (0.007) loss 1.4463 (0.7579) lr 5.2104e-03 eta 0:44:58
epoch [22/30] batch [140/796] time 0.385 (0.382) data 0.000 (0.006) loss 2.0469 (0.7614) lr 5.2104e-03 eta 0:44:43
epoch [22/30] batch [160/796] time 0.404 (0.382) data 0.000 (0.005) loss 1.1865 (0.7767) lr 5.2104e-03 eta 0:44:36
epoch [22/30] batch [180/796] time 0.382 (0.381) data 0.000 (0.005) loss 0.7964 (0.7838) lr 5.2104e-03 eta 0:44:20
epoch [22/30] batch [200/796] time 0.384 (0.380) data 0.000 (0.004) loss 0.1598 (0.7785) lr 5.2104e-03 eta 0:44:06
epoch [22/30] batch [220/796] time 0.331 (0.380) data 0.000 (0.004) loss 1.1533 (0.8022) lr 5.2104e-03 eta 0:43:57
epoch [22/30] batch [240/796] time 0.377 (0.380) data 0.000 (0.004) loss 0.7993 (0.8208) lr 5.2104e-03 eta 0:43:47
epoch [22/30] batch [260/796] time 0.361 (0.379) data 0.000 (0.003) loss 0.6738 (0.8104) lr 5.2104e-03 eta 0:43:36
epoch [22/30] batch [280/796] time 0.364 (0.380) data 0.000 (0.003) loss 1.0176 (0.8064) lr 5.2104e-03 eta 0:43:33
epoch [22/30] batch [300/796] time 0.383 (0.379) data 0.000 (0.003) loss 0.9097 (0.8186) lr 5.2104e-03 eta 0:43:24
epoch [22/30] batch [320/796] time 0.396 (0.380) data 0.000 (0.003) loss 0.3508 (0.8112) lr 5.2104e-03 eta 0:43:18
epoch [22/30] batch [340/796] time 0.368 (0.379) data 0.000 (0.003) loss 0.5430 (0.8154) lr 5.2104e-03 eta 0:43:08
epoch [22/30] batch [360/796] time 0.353 (0.379) data 0.001 (0.003) loss 0.4509 (0.8048) lr 5.2104e-03 eta 0:42:56
epoch [22/30] batch [380/796] time 0.369 (0.379) data 0.000 (0.002) loss 0.6157 (0.8157) lr 5.2104e-03 eta 0:42:49
epoch [22/30] batch [400/796] time 0.375 (0.378) data 0.000 (0.002) loss 0.6089 (0.8179) lr 5.2104e-03 eta 0:42:40
epoch [22/30] batch [420/796] time 0.383 (0.379) data 0.000 (0.002) loss 0.0632 (0.8159) lr 5.2104e-03 eta 0:42:34
epoch [22/30] batch [440/796] time 0.391 (0.379) data 0.000 (0.002) loss 0.5322 (0.8192) lr 5.2104e-03 eta 0:42:25
epoch [22/30] batch [460/796] time 0.397 (0.378) data 0.000 (0.002) loss 1.2949 (0.8267) lr 5.2104e-03 eta 0:42:17
epoch [22/30] batch [480/796] time 0.367 (0.378) data 0.000 (0.002) loss 1.3262 (0.8292) lr 5.2104e-03 eta 0:42:06
epoch [22/30] batch [500/796] time 0.398 (0.378) data 0.000 (0.002) loss 0.2137 (0.8261) lr 5.2104e-03 eta 0:41:59
epoch [22/30] batch [520/796] time 0.381 (0.378) data 0.000 (0.002) loss 1.4814 (0.8304) lr 5.2104e-03 eta 0:41:53
epoch [22/30] batch [540/796] time 0.353 (0.378) data 0.000 (0.002) loss 2.1562 (0.8254) lr 5.2104e-03 eta 0:41:44
epoch [22/30] batch [560/796] time 0.340 (0.378) data 0.000 (0.002) loss 0.8691 (0.8308) lr 5.2104e-03 eta 0:41:34
epoch [22/30] batch [580/796] time 0.407 (0.378) data 0.000 (0.002) loss 0.6519 (0.8264) lr 5.2104e-03 eta 0:41:26
epoch [22/30] batch [600/796] time 0.413 (0.378) data 0.000 (0.002) loss 0.6631 (0.8253) lr 5.2104e-03 eta 0:41:17
epoch [22/30] batch [620/796] time 0.386 (0.377) data 0.000 (0.002) loss 0.7949 (0.8224) lr 5.2104e-03 eta 0:41:10
epoch [22/30] batch [640/796] time 0.386 (0.378) data 0.000 (0.002) loss 0.1156 (0.8236) lr 5.2104e-03 eta 0:41:03
epoch [22/30] batch [660/796] time 0.378 (0.377) data 0.000 (0.002) loss 1.7773 (0.8266) lr 5.2104e-03 eta 0:40:54
epoch [22/30] batch [680/796] time 0.345 (0.377) data 0.000 (0.002) loss 0.6553 (0.8310) lr 5.2104e-03 eta 0:40:45
epoch [22/30] batch [700/796] time 0.383 (0.377) data 0.000 (0.001) loss 1.0312 (0.8401) lr 5.2104e-03 eta 0:40:38
epoch [22/30] batch [720/796] time 0.384 (0.377) data 0.000 (0.001) loss 0.6328 (0.8444) lr 5.2104e-03 eta 0:40:32
epoch [22/30] batch [740/796] time 0.400 (0.378) data 0.000 (0.001) loss 0.2365 (0.8398) lr 5.2104e-03 eta 0:40:25
epoch [22/30] batch [760/796] time 0.355 (0.378) data 0.000 (0.001) loss 0.3499 (0.8366) lr 5.2104e-03 eta 0:40:17
epoch [22/30] batch [780/796] time 0.331 (0.377) data 0.000 (0.001) loss 0.4421 (0.8326) lr 5.2104e-03 eta 0:40:03
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:51,  5.88s/it] 10%|█         | 2/20 [00:06<00:50,  2.82s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.66s/it] 20%|██        | 4/20 [00:07<00:17,  1.11s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.23it/s] 30%|███       | 6/20 [00:07<00:08,  1.59it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.95it/s] 40%|████      | 8/20 [00:08<00:05,  2.29it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.59it/s] 50%|█████     | 10/20 [00:08<00:03,  2.84it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.07it/s] 60%|██████    | 12/20 [00:09<00:02,  3.33it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.49it/s] 70%|███████   | 14/20 [00:09<00:01,  3.59it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.65it/s] 80%|████████  | 16/20 [00:10<00:01,  3.73it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.80it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.19it/s] 95%|█████████▌| 19/20 [00:10<00:00,  4.52it/s]100%|██████████| 20/20 [00:11<00:00,  4.86it/s]100%|██████████| 20/20 [00:11<00:00,  1.78it/s]=> result
* total: 1,990
* correct: 1,581
* accuracy: 79.4%
* error: 20.6%
* macro_f1: 78.8%

epoch [23/30] batch [20/796] time 0.381 (0.426) data 0.000 (0.048) loss 0.7100 (0.7311) lr 4.9223e-03 eta 0:45:05
epoch [23/30] batch [40/796] time 0.383 (0.400) data 0.000 (0.024) loss 0.3457 (0.6886) lr 4.9223e-03 eta 0:42:09
epoch [23/30] batch [60/796] time 0.398 (0.391) data 0.000 (0.016) loss 0.2673 (0.7502) lr 4.9223e-03 eta 0:41:03
epoch [23/30] batch [80/796] time 0.370 (0.387) data 0.000 (0.012) loss 0.6255 (0.7863) lr 4.9223e-03 eta 0:40:33
epoch [23/30] batch [100/796] time 0.377 (0.385) data 0.000 (0.010) loss 0.7104 (0.7803) lr 4.9223e-03 eta 0:40:10
epoch [23/30] batch [120/796] time 0.360 (0.384) data 0.000 (0.008) loss 1.3750 (0.7710) lr 4.9223e-03 eta 0:40:00
epoch [23/30] batch [140/796] time 0.387 (0.383) data 0.000 (0.007) loss 0.5122 (0.7822) lr 4.9223e-03 eta 0:39:46
epoch [23/30] batch [160/796] time 0.377 (0.382) data 0.000 (0.006) loss 1.4180 (0.7730) lr 4.9223e-03 eta 0:39:32
epoch [23/30] batch [180/796] time 0.380 (0.381) data 0.000 (0.006) loss 0.4722 (0.7665) lr 4.9223e-03 eta 0:39:14
epoch [23/30] batch [200/796] time 0.339 (0.380) data 0.000 (0.005) loss 0.6738 (0.7646) lr 4.9223e-03 eta 0:39:05
epoch [23/30] batch [220/796] time 0.361 (0.380) data 0.000 (0.005) loss 0.7041 (0.7799) lr 4.9223e-03 eta 0:38:57
epoch [23/30] batch [240/796] time 0.383 (0.380) data 0.000 (0.004) loss 1.8994 (0.7839) lr 4.9223e-03 eta 0:38:45
epoch [23/30] batch [260/796] time 0.386 (0.379) data 0.000 (0.004) loss 0.7656 (0.7852) lr 4.9223e-03 eta 0:38:36
epoch [23/30] batch [280/796] time 0.388 (0.379) data 0.000 (0.004) loss 0.8716 (0.7884) lr 4.9223e-03 eta 0:38:26
epoch [23/30] batch [300/796] time 0.386 (0.379) data 0.000 (0.003) loss 1.1504 (0.7864) lr 4.9223e-03 eta 0:38:19
epoch [23/30] batch [320/796] time 0.346 (0.378) data 0.000 (0.003) loss 1.9287 (0.7890) lr 4.9223e-03 eta 0:38:08
epoch [23/30] batch [340/796] time 0.386 (0.379) data 0.000 (0.003) loss 0.3826 (0.7784) lr 4.9223e-03 eta 0:38:01
epoch [23/30] batch [360/796] time 0.361 (0.378) data 0.000 (0.003) loss 0.8613 (0.7819) lr 4.9223e-03 eta 0:37:51
epoch [23/30] batch [380/796] time 0.384 (0.378) data 0.000 (0.003) loss 0.4570 (0.7877) lr 4.9223e-03 eta 0:37:41
epoch [23/30] batch [400/796] time 0.379 (0.377) data 0.000 (0.003) loss 0.1614 (0.7846) lr 4.9223e-03 eta 0:37:32
epoch [23/30] batch [420/796] time 0.385 (0.377) data 0.000 (0.003) loss 1.2285 (0.7857) lr 4.9223e-03 eta 0:37:24
epoch [23/30] batch [440/796] time 0.393 (0.377) data 0.000 (0.002) loss 0.9814 (0.7878) lr 4.9223e-03 eta 0:37:14
epoch [23/30] batch [460/796] time 0.386 (0.377) data 0.000 (0.002) loss 1.3232 (0.7914) lr 4.9223e-03 eta 0:37:06
epoch [23/30] batch [480/796] time 0.341 (0.377) data 0.000 (0.002) loss 0.6309 (0.7901) lr 4.9223e-03 eta 0:36:58
epoch [23/30] batch [500/796] time 0.386 (0.377) data 0.000 (0.002) loss 0.6104 (0.7977) lr 4.9223e-03 eta 0:36:51
epoch [23/30] batch [520/796] time 0.392 (0.377) data 0.000 (0.002) loss 0.3574 (0.7904) lr 4.9223e-03 eta 0:36:44
epoch [23/30] batch [540/796] time 0.372 (0.377) data 0.000 (0.002) loss 0.3918 (0.7881) lr 4.9223e-03 eta 0:36:35
epoch [23/30] batch [560/796] time 0.387 (0.377) data 0.000 (0.002) loss 0.7969 (0.7896) lr 4.9223e-03 eta 0:36:29
epoch [23/30] batch [580/796] time 0.376 (0.377) data 0.000 (0.002) loss 1.3057 (0.7903) lr 4.9223e-03 eta 0:36:21
epoch [23/30] batch [600/796] time 0.370 (0.377) data 0.000 (0.002) loss 0.5020 (0.7925) lr 4.9223e-03 eta 0:36:13
epoch [23/30] batch [620/796] time 0.344 (0.377) data 0.000 (0.002) loss 0.3713 (0.7931) lr 4.9223e-03 eta 0:36:06
epoch [23/30] batch [640/796] time 0.375 (0.377) data 0.000 (0.002) loss 0.2542 (0.7994) lr 4.9223e-03 eta 0:35:57
epoch [23/30] batch [660/796] time 0.363 (0.376) data 0.000 (0.002) loss 0.4663 (0.7979) lr 4.9223e-03 eta 0:35:48
epoch [23/30] batch [680/796] time 0.353 (0.376) data 0.000 (0.002) loss 1.5859 (0.7963) lr 4.9223e-03 eta 0:35:41
epoch [23/30] batch [700/796] time 0.390 (0.377) data 0.000 (0.002) loss 0.2981 (0.8012) lr 4.9223e-03 eta 0:35:34
epoch [23/30] batch [720/796] time 0.389 (0.377) data 0.001 (0.002) loss 1.4346 (0.8000) lr 4.9223e-03 eta 0:35:27
epoch [23/30] batch [740/796] time 0.396 (0.377) data 0.000 (0.002) loss 1.2021 (0.8054) lr 4.9223e-03 eta 0:35:19
epoch [23/30] batch [760/796] time 0.401 (0.377) data 0.000 (0.002) loss 0.5742 (0.8057) lr 4.9223e-03 eta 0:35:11
epoch [23/30] batch [780/796] time 0.328 (0.376) data 0.000 (0.002) loss 0.2036 (0.8012) lr 4.9223e-03 eta 0:34:58
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<01:54,  6.03s/it] 10%|█         | 2/20 [00:06<00:52,  2.93s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.72s/it] 20%|██        | 4/20 [00:07<00:18,  1.15s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.20it/s] 30%|███       | 6/20 [00:07<00:09,  1.55it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.90it/s] 40%|████      | 8/20 [00:08<00:05,  2.25it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.56it/s] 50%|█████     | 10/20 [00:08<00:03,  2.84it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.10it/s] 60%|██████    | 12/20 [00:09<00:02,  3.32it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.50it/s] 70%|███████   | 14/20 [00:10<00:01,  3.59it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.75it/s] 80%|████████  | 16/20 [00:10<00:01,  3.78it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.83it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.90it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.28it/s]100%|██████████| 20/20 [00:11<00:00,  4.66it/s]100%|██████████| 20/20 [00:11<00:00,  1.74it/s]=> result
* total: 1,990
* correct: 1,583
* accuracy: 79.5%
* error: 20.5%
* macro_f1: 79.0%

epoch [24/30] batch [20/796] time 0.367 (0.427) data 0.000 (0.045) loss 1.8604 (0.6702) lr 4.6162e-03 eta 0:39:29
epoch [24/30] batch [40/796] time 0.384 (0.397) data 0.000 (0.022) loss 0.2114 (0.6957) lr 4.6162e-03 eta 0:36:34
epoch [24/30] batch [60/796] time 0.363 (0.386) data 0.000 (0.015) loss 0.7979 (0.6759) lr 4.6162e-03 eta 0:35:25
epoch [24/30] batch [80/796] time 0.376 (0.381) data 0.000 (0.011) loss 0.9502 (0.7317) lr 4.6162e-03 eta 0:34:51
epoch [24/30] batch [100/796] time 0.392 (0.377) data 0.000 (0.009) loss 0.1979 (0.7460) lr 4.6162e-03 eta 0:34:24
epoch [24/30] batch [120/796] time 0.389 (0.377) data 0.000 (0.008) loss 0.3901 (0.7464) lr 4.6162e-03 eta 0:34:15
epoch [24/30] batch [140/796] time 0.348 (0.377) data 0.000 (0.007) loss 0.4624 (0.7467) lr 4.6162e-03 eta 0:34:06
epoch [24/30] batch [160/796] time 0.352 (0.376) data 0.000 (0.006) loss 1.0049 (0.7429) lr 4.6162e-03 eta 0:33:55
epoch [24/30] batch [180/796] time 0.352 (0.376) data 0.000 (0.005) loss 0.8916 (0.7403) lr 4.6162e-03 eta 0:33:46
epoch [24/30] batch [200/796] time 0.385 (0.375) data 0.000 (0.005) loss 0.4285 (0.7558) lr 4.6162e-03 eta 0:33:34
epoch [24/30] batch [220/796] time 0.357 (0.375) data 0.000 (0.004) loss 0.1869 (0.7476) lr 4.6162e-03 eta 0:33:25
epoch [24/30] batch [240/796] time 0.365 (0.375) data 0.000 (0.004) loss 0.2913 (0.7415) lr 4.6162e-03 eta 0:33:19
epoch [24/30] batch [260/796] time 0.393 (0.374) data 0.000 (0.004) loss 1.8828 (0.7544) lr 4.6162e-03 eta 0:33:07
epoch [24/30] batch [280/796] time 0.388 (0.374) data 0.001 (0.003) loss 0.8286 (0.7605) lr 4.6162e-03 eta 0:33:01
epoch [24/30] batch [300/796] time 0.398 (0.374) data 0.000 (0.003) loss 0.3884 (0.7559) lr 4.6162e-03 eta 0:32:50
epoch [24/30] batch [320/796] time 0.407 (0.374) data 0.000 (0.003) loss 0.5103 (0.7569) lr 4.6162e-03 eta 0:32:42
epoch [24/30] batch [340/796] time 0.384 (0.374) data 0.000 (0.003) loss 0.3799 (0.7625) lr 4.6162e-03 eta 0:32:36
epoch [24/30] batch [360/796] time 0.384 (0.374) data 0.000 (0.003) loss 1.0049 (0.7661) lr 4.6162e-03 eta 0:32:30
epoch [24/30] batch [380/796] time 0.380 (0.374) data 0.000 (0.003) loss 1.6270 (0.7746) lr 4.6162e-03 eta 0:32:23
epoch [24/30] batch [400/796] time 0.361 (0.375) data 0.000 (0.002) loss 0.5107 (0.7738) lr 4.6162e-03 eta 0:32:17
epoch [24/30] batch [420/796] time 0.385 (0.375) data 0.000 (0.002) loss 0.6973 (0.7785) lr 4.6162e-03 eta 0:32:10
epoch [24/30] batch [440/796] time 0.390 (0.375) data 0.000 (0.002) loss 1.3281 (0.7759) lr 4.6162e-03 eta 0:32:05
epoch [24/30] batch [460/796] time 0.340 (0.375) data 0.000 (0.002) loss 1.0254 (0.7818) lr 4.6162e-03 eta 0:31:57
epoch [24/30] batch [480/796] time 0.350 (0.375) data 0.000 (0.002) loss 1.9570 (0.7890) lr 4.6162e-03 eta 0:31:50
epoch [24/30] batch [500/796] time 0.350 (0.375) data 0.000 (0.002) loss 0.5786 (0.7979) lr 4.6162e-03 eta 0:31:40
epoch [24/30] batch [520/796] time 0.390 (0.375) data 0.000 (0.002) loss 1.6279 (0.8020) lr 4.6162e-03 eta 0:31:32
epoch [24/30] batch [540/796] time 0.368 (0.375) data 0.000 (0.002) loss 1.5977 (0.8082) lr 4.6162e-03 eta 0:31:25
epoch [24/30] batch [560/796] time 0.399 (0.375) data 0.000 (0.002) loss 1.0557 (0.8103) lr 4.6162e-03 eta 0:31:17
epoch [24/30] batch [580/796] time 0.389 (0.375) data 0.000 (0.002) loss 1.0254 (0.8114) lr 4.6162e-03 eta 0:31:10
epoch [24/30] batch [600/796] time 0.343 (0.375) data 0.000 (0.002) loss 0.6831 (0.8059) lr 4.6162e-03 eta 0:31:03
epoch [24/30] batch [620/796] time 0.393 (0.375) data 0.000 (0.002) loss 0.6660 (0.8153) lr 4.6162e-03 eta 0:30:56
epoch [24/30] batch [640/796] time 0.345 (0.375) data 0.000 (0.002) loss 1.7734 (0.8215) lr 4.6162e-03 eta 0:30:47
epoch [24/30] batch [660/796] time 0.378 (0.375) data 0.000 (0.002) loss 0.4487 (0.8221) lr 4.6162e-03 eta 0:30:39
epoch [24/30] batch [680/796] time 0.343 (0.375) data 0.000 (0.002) loss 0.2815 (0.8201) lr 4.6162e-03 eta 0:30:32
epoch [24/30] batch [700/796] time 0.384 (0.375) data 0.000 (0.002) loss 0.4543 (0.8181) lr 4.6162e-03 eta 0:30:24
epoch [24/30] batch [720/796] time 0.379 (0.375) data 0.000 (0.002) loss 1.6152 (0.8197) lr 4.6162e-03 eta 0:30:18
epoch [24/30] batch [740/796] time 0.384 (0.375) data 0.000 (0.001) loss 0.2837 (0.8164) lr 4.6162e-03 eta 0:30:09
epoch [24/30] batch [760/796] time 0.376 (0.374) data 0.000 (0.001) loss 1.2119 (0.8137) lr 4.6162e-03 eta 0:30:00
epoch [24/30] batch [780/796] time 0.328 (0.373) data 0.000 (0.001) loss 1.0684 (0.8163) lr 4.6162e-03 eta 0:29:49
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<01:55,  6.07s/it] 10%|█         | 2/20 [00:06<00:52,  2.93s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.72s/it] 20%|██        | 4/20 [00:07<00:18,  1.15s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.20it/s] 30%|███       | 6/20 [00:07<00:09,  1.55it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.91it/s] 40%|████      | 8/20 [00:08<00:05,  2.24it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.54it/s] 50%|█████     | 10/20 [00:09<00:03,  2.79it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.99it/s] 60%|██████    | 12/20 [00:09<00:02,  3.23it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.41it/s] 70%|███████   | 14/20 [00:10<00:01,  3.57it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.65it/s] 80%|████████  | 16/20 [00:10<00:01,  3.69it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.74it/s] 90%|█████████ | 18/20 [00:11<00:00,  4.12it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.45it/s]100%|██████████| 20/20 [00:11<00:00,  4.79it/s]100%|██████████| 20/20 [00:11<00:00,  1.74it/s]=> result
* total: 1,990
* correct: 1,589
* accuracy: 79.8%
* error: 20.2%
* macro_f1: 79.3%

epoch [25/30] batch [20/796] time 0.375 (0.425) data 0.000 (0.041) loss 0.2313 (0.7422) lr 4.2956e-03 eta 0:33:40
epoch [25/30] batch [40/796] time 0.348 (0.396) data 0.000 (0.020) loss 0.2198 (0.7620) lr 4.2956e-03 eta 0:31:15
epoch [25/30] batch [60/796] time 0.390 (0.388) data 0.000 (0.014) loss 0.6929 (0.7996) lr 4.2956e-03 eta 0:30:31
epoch [25/30] batch [80/796] time 0.400 (0.387) data 0.000 (0.010) loss 1.7588 (0.7698) lr 4.2956e-03 eta 0:30:16
epoch [25/30] batch [100/796] time 0.348 (0.384) data 0.000 (0.008) loss 0.5503 (0.7441) lr 4.2956e-03 eta 0:29:53
epoch [25/30] batch [120/796] time 0.387 (0.382) data 0.000 (0.007) loss 0.2330 (0.7336) lr 4.2956e-03 eta 0:29:38
epoch [25/30] batch [140/796] time 0.374 (0.380) data 0.000 (0.006) loss 0.7075 (0.7292) lr 4.2956e-03 eta 0:29:22
epoch [25/30] batch [160/796] time 0.388 (0.380) data 0.000 (0.005) loss 0.4900 (0.7319) lr 4.2956e-03 eta 0:29:13
epoch [25/30] batch [180/796] time 0.387 (0.380) data 0.000 (0.005) loss 1.5762 (0.7413) lr 4.2956e-03 eta 0:29:04
epoch [25/30] batch [200/796] time 0.339 (0.380) data 0.000 (0.004) loss 0.3164 (0.7515) lr 4.2956e-03 eta 0:28:57
epoch [25/30] batch [220/796] time 0.386 (0.379) data 0.000 (0.004) loss 0.2930 (0.7547) lr 4.2956e-03 eta 0:28:48
epoch [25/30] batch [240/796] time 0.356 (0.378) data 0.000 (0.004) loss 0.1967 (0.7478) lr 4.2956e-03 eta 0:28:36
epoch [25/30] batch [260/796] time 0.357 (0.378) data 0.001 (0.003) loss 0.2137 (0.7710) lr 4.2956e-03 eta 0:28:27
epoch [25/30] batch [280/796] time 0.371 (0.377) data 0.000 (0.003) loss 1.1719 (0.7714) lr 4.2956e-03 eta 0:28:16
epoch [25/30] batch [300/796] time 0.397 (0.377) data 0.000 (0.003) loss 0.5186 (0.7733) lr 4.2956e-03 eta 0:28:09
epoch [25/30] batch [320/796] time 0.351 (0.378) data 0.000 (0.003) loss 0.8032 (0.7889) lr 4.2956e-03 eta 0:28:02
epoch [25/30] batch [340/796] time 0.352 (0.377) data 0.000 (0.003) loss 0.4934 (0.7915) lr 4.2956e-03 eta 0:27:52
epoch [25/30] batch [360/796] time 0.367 (0.377) data 0.000 (0.003) loss 0.2759 (0.7889) lr 4.2956e-03 eta 0:27:44
epoch [25/30] batch [380/796] time 0.376 (0.377) data 0.000 (0.002) loss 0.6475 (0.7825) lr 4.2956e-03 eta 0:27:36
epoch [25/30] batch [400/796] time 0.335 (0.376) data 0.000 (0.002) loss 0.7349 (0.7876) lr 4.2956e-03 eta 0:27:27
epoch [25/30] batch [420/796] time 0.398 (0.377) data 0.000 (0.002) loss 0.8164 (0.7819) lr 4.2956e-03 eta 0:27:20
epoch [25/30] batch [440/796] time 0.376 (0.377) data 0.000 (0.002) loss 1.0283 (0.7810) lr 4.2956e-03 eta 0:27:12
epoch [25/30] batch [460/796] time 0.343 (0.377) data 0.000 (0.002) loss 1.4609 (0.7802) lr 4.2956e-03 eta 0:27:05
epoch [25/30] batch [480/796] time 0.370 (0.376) data 0.000 (0.002) loss 1.0918 (0.7914) lr 4.2956e-03 eta 0:26:56
epoch [25/30] batch [500/796] time 0.390 (0.376) data 0.000 (0.002) loss 0.2969 (0.7846) lr 4.2956e-03 eta 0:26:49
epoch [25/30] batch [520/796] time 0.365 (0.376) data 0.000 (0.002) loss 0.7349 (0.7857) lr 4.2956e-03 eta 0:26:41
epoch [25/30] batch [540/796] time 0.400 (0.376) data 0.000 (0.002) loss 0.3213 (0.7844) lr 4.2956e-03 eta 0:26:33
epoch [25/30] batch [560/796] time 0.380 (0.376) data 0.000 (0.002) loss 2.5254 (0.7922) lr 4.2956e-03 eta 0:26:24
epoch [25/30] batch [580/796] time 0.360 (0.376) data 0.000 (0.002) loss 1.3018 (0.7973) lr 4.2956e-03 eta 0:26:16
epoch [25/30] batch [600/796] time 0.406 (0.375) data 0.001 (0.002) loss 0.5029 (0.8010) lr 4.2956e-03 eta 0:26:07
epoch [25/30] batch [620/796] time 0.341 (0.375) data 0.000 (0.002) loss 1.5742 (0.8066) lr 4.2956e-03 eta 0:25:59
epoch [25/30] batch [640/796] time 0.374 (0.375) data 0.000 (0.002) loss 0.2229 (0.8067) lr 4.2956e-03 eta 0:25:49
epoch [25/30] batch [660/796] time 0.377 (0.375) data 0.000 (0.001) loss 0.4060 (0.8012) lr 4.2956e-03 eta 0:25:42
epoch [25/30] batch [680/796] time 0.384 (0.375) data 0.000 (0.001) loss 1.2490 (0.8003) lr 4.2956e-03 eta 0:25:34
epoch [25/30] batch [700/796] time 0.399 (0.374) data 0.000 (0.001) loss 0.7637 (0.8027) lr 4.2956e-03 eta 0:25:26
epoch [25/30] batch [720/796] time 0.386 (0.374) data 0.000 (0.001) loss 0.7349 (0.8000) lr 4.2956e-03 eta 0:25:18
epoch [25/30] batch [740/796] time 0.353 (0.374) data 0.000 (0.001) loss 0.4055 (0.8006) lr 4.2956e-03 eta 0:25:10
epoch [25/30] batch [760/796] time 0.361 (0.374) data 0.000 (0.001) loss 0.3647 (0.8012) lr 4.2956e-03 eta 0:25:03
epoch [25/30] batch [780/796] time 0.330 (0.374) data 0.000 (0.001) loss 0.5322 (0.8036) lr 4.2956e-03 eta 0:24:52
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:50,  5.84s/it] 10%|█         | 2/20 [00:06<00:52,  2.91s/it] 15%|█▌        | 3/20 [00:06<00:29,  1.71s/it] 20%|██        | 4/20 [00:07<00:18,  1.14s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.20it/s] 30%|███       | 6/20 [00:07<00:09,  1.55it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.90it/s] 40%|████      | 8/20 [00:08<00:05,  2.25it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.54it/s] 50%|█████     | 10/20 [00:08<00:03,  2.78it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.98it/s] 60%|██████    | 12/20 [00:09<00:02,  3.18it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.38it/s] 70%|███████   | 14/20 [00:09<00:01,  3.52it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.60it/s] 80%|████████  | 16/20 [00:10<00:01,  3.72it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.76it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.47it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.84it/s]100%|██████████| 20/20 [00:11<00:00,  4.28it/s]100%|██████████| 20/20 [00:11<00:00,  1.72it/s]=> result
* total: 1,990
* correct: 1,599
* accuracy: 80.4%
* error: 19.6%
* macro_f1: 79.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1/prompt_learner/model-best.pth.tar

epoch [26/30] batch [20/796] time 0.371 (0.423) data 0.000 (0.042) loss 1.4512 (0.8273) lr 3.9638e-03 eta 0:27:56
epoch [26/30] batch [40/796] time 0.359 (0.402) data 0.000 (0.021) loss 1.1338 (0.7725) lr 3.9638e-03 eta 0:26:23
epoch [26/30] batch [60/796] time 0.395 (0.394) data 0.000 (0.014) loss 0.5986 (0.7188) lr 3.9638e-03 eta 0:25:45
epoch [26/30] batch [80/796] time 0.395 (0.389) data 0.000 (0.011) loss 0.9160 (0.6921) lr 3.9638e-03 eta 0:25:18
epoch [26/30] batch [100/796] time 0.391 (0.385) data 0.000 (0.009) loss 0.4668 (0.7147) lr 3.9638e-03 eta 0:24:54
epoch [26/30] batch [120/796] time 0.391 (0.384) data 0.000 (0.007) loss 0.3596 (0.7449) lr 3.9638e-03 eta 0:24:43
epoch [26/30] batch [140/796] time 0.381 (0.384) data 0.000 (0.006) loss 0.3359 (0.7145) lr 3.9638e-03 eta 0:24:32
epoch [26/30] batch [160/796] time 0.385 (0.382) data 0.000 (0.005) loss 2.3945 (0.7244) lr 3.9638e-03 eta 0:24:19
epoch [26/30] batch [180/796] time 0.391 (0.383) data 0.000 (0.005) loss 0.1477 (0.7250) lr 3.9638e-03 eta 0:24:14
epoch [26/30] batch [200/796] time 0.392 (0.382) data 0.000 (0.004) loss 0.5078 (0.7200) lr 3.9638e-03 eta 0:24:02
epoch [26/30] batch [220/796] time 0.358 (0.381) data 0.000 (0.004) loss 0.6538 (0.7229) lr 3.9638e-03 eta 0:23:52
epoch [26/30] batch [240/796] time 0.375 (0.380) data 0.000 (0.004) loss 0.9302 (0.7157) lr 3.9638e-03 eta 0:23:42
epoch [26/30] batch [260/796] time 0.376 (0.381) data 0.000 (0.003) loss 0.2292 (0.7100) lr 3.9638e-03 eta 0:23:36
epoch [26/30] batch [280/796] time 0.395 (0.381) data 0.000 (0.003) loss 0.7349 (0.7194) lr 3.9638e-03 eta 0:23:28
epoch [26/30] batch [300/796] time 0.411 (0.381) data 0.000 (0.003) loss 0.5566 (0.7130) lr 3.9638e-03 eta 0:23:21
epoch [26/30] batch [320/796] time 0.375 (0.381) data 0.000 (0.003) loss 2.1465 (0.7243) lr 3.9638e-03 eta 0:23:12
epoch [26/30] batch [340/796] time 0.395 (0.381) data 0.000 (0.003) loss 0.6279 (0.7216) lr 3.9638e-03 eta 0:23:05
epoch [26/30] batch [360/796] time 0.387 (0.380) data 0.000 (0.003) loss 0.9087 (0.7233) lr 3.9638e-03 eta 0:22:56
epoch [26/30] batch [380/796] time 0.355 (0.380) data 0.000 (0.002) loss 0.1099 (0.7317) lr 3.9638e-03 eta 0:22:46
epoch [26/30] batch [400/796] time 0.380 (0.379) data 0.000 (0.002) loss 0.8159 (0.7357) lr 3.9638e-03 eta 0:22:38
epoch [26/30] batch [420/796] time 0.383 (0.379) data 0.000 (0.002) loss 1.0234 (0.7346) lr 3.9638e-03 eta 0:22:28
epoch [26/30] batch [440/796] time 0.376 (0.379) data 0.000 (0.002) loss 0.2157 (0.7304) lr 3.9638e-03 eta 0:22:20
epoch [26/30] batch [460/796] time 0.348 (0.378) data 0.000 (0.002) loss 1.0596 (0.7348) lr 3.9638e-03 eta 0:22:11
epoch [26/30] batch [480/796] time 0.375 (0.378) data 0.000 (0.002) loss 0.6211 (0.7382) lr 3.9638e-03 eta 0:22:02
epoch [26/30] batch [500/796] time 0.397 (0.378) data 0.000 (0.002) loss 1.9990 (0.7445) lr 3.9638e-03 eta 0:21:54
epoch [26/30] batch [520/796] time 0.350 (0.378) data 0.000 (0.002) loss 0.2305 (0.7486) lr 3.9638e-03 eta 0:21:47
epoch [26/30] batch [540/796] time 0.374 (0.378) data 0.000 (0.002) loss 0.3340 (0.7525) lr 3.9638e-03 eta 0:21:40
epoch [26/30] batch [560/796] time 0.392 (0.378) data 0.000 (0.002) loss 1.2783 (0.7557) lr 3.9638e-03 eta 0:21:31
epoch [26/30] batch [580/796] time 0.389 (0.377) data 0.000 (0.002) loss 0.7476 (0.7619) lr 3.9638e-03 eta 0:21:23
epoch [26/30] batch [600/796] time 0.390 (0.377) data 0.000 (0.002) loss 1.4043 (0.7672) lr 3.9638e-03 eta 0:21:15
epoch [26/30] batch [620/796] time 0.345 (0.377) data 0.000 (0.002) loss 1.0059 (0.7712) lr 3.9638e-03 eta 0:21:06
epoch [26/30] batch [640/796] time 0.388 (0.377) data 0.000 (0.002) loss 0.5630 (0.7668) lr 3.9638e-03 eta 0:20:58
epoch [26/30] batch [660/796] time 0.393 (0.377) data 0.000 (0.002) loss 1.2695 (0.7697) lr 3.9638e-03 eta 0:20:50
epoch [26/30] batch [680/796] time 0.403 (0.377) data 0.000 (0.001) loss 0.1498 (0.7677) lr 3.9638e-03 eta 0:20:43
epoch [26/30] batch [700/796] time 0.363 (0.377) data 0.000 (0.001) loss 0.7900 (0.7782) lr 3.9638e-03 eta 0:20:35
epoch [26/30] batch [720/796] time 0.342 (0.377) data 0.000 (0.001) loss 0.3306 (0.7728) lr 3.9638e-03 eta 0:20:27
epoch [26/30] batch [740/796] time 0.382 (0.377) data 0.000 (0.001) loss 0.1467 (0.7732) lr 3.9638e-03 eta 0:20:19
epoch [26/30] batch [760/796] time 0.372 (0.376) data 0.000 (0.001) loss 0.3506 (0.7713) lr 3.9638e-03 eta 0:20:12
epoch [26/30] batch [780/796] time 0.327 (0.375) data 0.000 (0.001) loss 1.4443 (0.7763) lr 3.9638e-03 eta 0:20:01
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:42,  5.40s/it] 10%|█         | 2/20 [00:06<00:53,  2.99s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.76s/it] 20%|██        | 4/20 [00:07<00:18,  1.18s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.17it/s] 30%|███       | 6/20 [00:07<00:09,  1.50it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.86it/s] 40%|████      | 8/20 [00:08<00:05,  2.20it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.50it/s] 50%|█████     | 10/20 [00:08<00:03,  2.76it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.01it/s] 60%|██████    | 12/20 [00:09<00:02,  3.20it/s] 65%|██████▌   | 13/20 [00:09<00:01,  3.50it/s] 70%|███████   | 14/20 [00:09<00:01,  3.62it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.82it/s] 80%|████████  | 16/20 [00:10<00:01,  3.95it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.90it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.32it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.50it/s]100%|██████████| 20/20 [00:11<00:00,  3.66it/s]100%|██████████| 20/20 [00:11<00:00,  1.70it/s]=> result
* total: 1,990
* correct: 1,594
* accuracy: 80.1%
* error: 19.9%
* macro_f1: 79.5%

epoch [27/30] batch [20/796] time 0.373 (0.442) data 0.000 (0.060) loss 1.1494 (0.7742) lr 3.6245e-03 eta 0:23:17
epoch [27/30] batch [40/796] time 0.381 (0.410) data 0.000 (0.030) loss 0.6943 (0.6659) lr 3.6245e-03 eta 0:21:29
epoch [27/30] batch [60/796] time 0.393 (0.399) data 0.000 (0.020) loss 1.0381 (0.7477) lr 3.6245e-03 eta 0:20:45
epoch [27/30] batch [80/796] time 0.388 (0.392) data 0.000 (0.015) loss 1.2031 (0.7119) lr 3.6245e-03 eta 0:20:17
epoch [27/30] batch [100/796] time 0.373 (0.387) data 0.000 (0.012) loss 0.8662 (0.7695) lr 3.6245e-03 eta 0:19:52
epoch [27/30] batch [120/796] time 0.374 (0.385) data 0.000 (0.010) loss 0.3042 (0.7816) lr 3.6245e-03 eta 0:19:38
epoch [27/30] batch [140/796] time 0.369 (0.383) data 0.000 (0.009) loss 0.5176 (0.7862) lr 3.6245e-03 eta 0:19:25
epoch [27/30] batch [160/796] time 0.356 (0.382) data 0.000 (0.008) loss 0.3247 (0.7763) lr 3.6245e-03 eta 0:19:14
epoch [27/30] batch [180/796] time 0.379 (0.381) data 0.000 (0.007) loss 0.5713 (0.7792) lr 3.6245e-03 eta 0:19:04
epoch [27/30] batch [200/796] time 0.369 (0.381) data 0.000 (0.006) loss 0.5239 (0.7798) lr 3.6245e-03 eta 0:18:57
epoch [27/30] batch [220/796] time 0.391 (0.381) data 0.000 (0.006) loss 0.8770 (0.7857) lr 3.6245e-03 eta 0:18:50
epoch [27/30] batch [240/796] time 0.373 (0.380) data 0.000 (0.005) loss 0.4934 (0.7697) lr 3.6245e-03 eta 0:18:40
epoch [27/30] batch [260/796] time 0.388 (0.380) data 0.000 (0.005) loss 0.7134 (0.7691) lr 3.6245e-03 eta 0:18:30
epoch [27/30] batch [280/796] time 0.390 (0.379) data 0.000 (0.004) loss 1.8193 (0.7569) lr 3.6245e-03 eta 0:18:22
epoch [27/30] batch [300/796] time 0.346 (0.379) data 0.000 (0.004) loss 1.1543 (0.7610) lr 3.6245e-03 eta 0:18:12
epoch [27/30] batch [320/796] time 0.369 (0.378) data 0.000 (0.004) loss 0.8613 (0.7555) lr 3.6245e-03 eta 0:18:03
epoch [27/30] batch [340/796] time 0.380 (0.378) data 0.000 (0.004) loss 0.6123 (0.7596) lr 3.6245e-03 eta 0:17:53
epoch [27/30] batch [360/796] time 0.384 (0.377) data 0.000 (0.004) loss 1.0713 (0.7637) lr 3.6245e-03 eta 0:17:46
epoch [27/30] batch [380/796] time 0.388 (0.377) data 0.000 (0.003) loss 0.4492 (0.7617) lr 3.6245e-03 eta 0:17:37
epoch [27/30] batch [400/796] time 0.391 (0.377) data 0.000 (0.003) loss 1.8369 (0.7608) lr 3.6245e-03 eta 0:17:30
epoch [27/30] batch [420/796] time 0.377 (0.377) data 0.000 (0.003) loss 0.8950 (0.7574) lr 3.6245e-03 eta 0:17:22
epoch [27/30] batch [440/796] time 0.394 (0.377) data 0.000 (0.003) loss 1.3145 (0.7612) lr 3.6245e-03 eta 0:17:15
epoch [27/30] batch [460/796] time 0.377 (0.377) data 0.000 (0.003) loss 0.6113 (0.7656) lr 3.6245e-03 eta 0:17:05
epoch [27/30] batch [480/796] time 0.382 (0.377) data 0.000 (0.003) loss 1.1523 (0.7653) lr 3.6245e-03 eta 0:16:58
epoch [27/30] batch [500/796] time 0.378 (0.377) data 0.000 (0.003) loss 1.0508 (0.7695) lr 3.6245e-03 eta 0:16:50
epoch [27/30] batch [520/796] time 0.389 (0.377) data 0.000 (0.003) loss 1.3398 (0.7690) lr 3.6245e-03 eta 0:16:43
epoch [27/30] batch [540/796] time 0.392 (0.377) data 0.000 (0.002) loss 0.6826 (0.7645) lr 3.6245e-03 eta 0:16:35
epoch [27/30] batch [560/796] time 0.382 (0.376) data 0.000 (0.002) loss 0.4299 (0.7645) lr 3.6245e-03 eta 0:16:27
epoch [27/30] batch [580/796] time 0.409 (0.376) data 0.000 (0.002) loss 0.3232 (0.7672) lr 3.6245e-03 eta 0:16:19
epoch [27/30] batch [600/796] time 0.383 (0.376) data 0.001 (0.002) loss 0.6792 (0.7732) lr 3.6245e-03 eta 0:16:12
epoch [27/30] batch [620/796] time 0.390 (0.376) data 0.000 (0.002) loss 0.7964 (0.7671) lr 3.6245e-03 eta 0:16:04
epoch [27/30] batch [640/796] time 0.382 (0.376) data 0.000 (0.002) loss 0.7993 (0.7841) lr 3.6245e-03 eta 0:15:56
epoch [27/30] batch [660/796] time 0.371 (0.376) data 0.000 (0.002) loss 0.4375 (0.7778) lr 3.6245e-03 eta 0:15:48
epoch [27/30] batch [680/796] time 0.373 (0.376) data 0.000 (0.002) loss 0.9243 (0.7778) lr 3.6245e-03 eta 0:15:40
epoch [27/30] batch [700/796] time 0.364 (0.376) data 0.000 (0.002) loss 2.4863 (0.7769) lr 3.6245e-03 eta 0:15:33
epoch [27/30] batch [720/796] time 0.364 (0.375) data 0.000 (0.002) loss 0.2141 (0.7744) lr 3.6245e-03 eta 0:15:25
epoch [27/30] batch [740/796] time 0.376 (0.375) data 0.000 (0.002) loss 0.6660 (0.7719) lr 3.6245e-03 eta 0:15:17
epoch [27/30] batch [760/796] time 0.382 (0.375) data 0.000 (0.002) loss 0.4048 (0.7707) lr 3.6245e-03 eta 0:15:09
epoch [27/30] batch [780/796] time 0.328 (0.375) data 0.000 (0.002) loss 1.2236 (0.7762) lr 3.6245e-03 eta 0:15:00
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:47,  5.64s/it] 10%|█         | 2/20 [00:06<00:53,  2.99s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.75s/it] 20%|██        | 4/20 [00:07<00:18,  1.17s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.18it/s] 30%|███       | 6/20 [00:07<00:09,  1.53it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.88it/s] 40%|████      | 8/20 [00:08<00:05,  2.23it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.54it/s] 50%|█████     | 10/20 [00:08<00:03,  2.82it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.08it/s] 60%|██████    | 12/20 [00:09<00:02,  3.28it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.45it/s] 70%|███████   | 14/20 [00:09<00:01,  3.57it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.69it/s] 80%|████████  | 16/20 [00:10<00:01,  3.78it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.88it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.24it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.54it/s]100%|██████████| 20/20 [00:11<00:00,  4.87it/s]100%|██████████| 20/20 [00:11<00:00,  1.75it/s]=> result
* total: 1,990
* correct: 1,597
* accuracy: 80.3%
* error: 19.7%
* macro_f1: 79.7%

epoch [28/30] batch [20/796] time 0.342 (0.426) data 0.000 (0.045) loss 0.8027 (0.6312) lr 3.2815e-03 eta 0:16:49
epoch [28/30] batch [40/796] time 0.377 (0.402) data 0.000 (0.023) loss 0.8916 (0.5793) lr 3.2815e-03 eta 0:15:43
epoch [28/30] batch [60/796] time 0.359 (0.392) data 0.000 (0.015) loss 0.9214 (0.6255) lr 3.2815e-03 eta 0:15:13
epoch [28/30] batch [80/796] time 0.349 (0.387) data 0.000 (0.011) loss 0.7085 (0.7177) lr 3.2815e-03 eta 0:14:53
epoch [28/30] batch [100/796] time 0.342 (0.383) data 0.000 (0.009) loss 0.6099 (0.7774) lr 3.2815e-03 eta 0:14:35
epoch [28/30] batch [120/796] time 0.349 (0.382) data 0.000 (0.008) loss 0.2795 (0.7844) lr 3.2815e-03 eta 0:14:25
epoch [28/30] batch [140/796] time 0.402 (0.381) data 0.000 (0.007) loss 0.8188 (0.7863) lr 3.2815e-03 eta 0:14:15
epoch [28/30] batch [160/796] time 0.394 (0.379) data 0.000 (0.006) loss 0.3127 (0.7954) lr 3.2815e-03 eta 0:14:05
epoch [28/30] batch [180/796] time 0.385 (0.380) data 0.000 (0.005) loss 0.5864 (0.8027) lr 3.2815e-03 eta 0:13:58
epoch [28/30] batch [200/796] time 0.415 (0.379) data 0.000 (0.005) loss 0.7612 (0.7935) lr 3.2815e-03 eta 0:13:49
epoch [28/30] batch [220/796] time 0.350 (0.378) data 0.000 (0.004) loss 0.4536 (0.8009) lr 3.2815e-03 eta 0:13:39
epoch [28/30] batch [240/796] time 0.379 (0.378) data 0.000 (0.004) loss 0.8423 (0.7886) lr 3.2815e-03 eta 0:13:31
epoch [28/30] batch [260/796] time 0.381 (0.378) data 0.000 (0.004) loss 1.3672 (0.7927) lr 3.2815e-03 eta 0:13:24
epoch [28/30] batch [280/796] time 0.385 (0.377) data 0.000 (0.003) loss 0.1329 (0.7971) lr 3.2815e-03 eta 0:13:14
epoch [28/30] batch [300/796] time 0.338 (0.376) data 0.000 (0.003) loss 0.7319 (0.7944) lr 3.2815e-03 eta 0:13:05
epoch [28/30] batch [320/796] time 0.351 (0.376) data 0.000 (0.003) loss 1.1094 (0.7929) lr 3.2815e-03 eta 0:12:57
epoch [28/30] batch [340/796] time 0.386 (0.376) data 0.000 (0.003) loss 0.6558 (0.7891) lr 3.2815e-03 eta 0:12:49
epoch [28/30] batch [360/796] time 0.392 (0.375) data 0.000 (0.003) loss 0.4902 (0.7957) lr 3.2815e-03 eta 0:12:41
epoch [28/30] batch [380/796] time 0.364 (0.375) data 0.000 (0.003) loss 0.1172 (0.8100) lr 3.2815e-03 eta 0:12:32
epoch [28/30] batch [400/796] time 0.361 (0.375) data 0.000 (0.002) loss 1.4346 (0.8044) lr 3.2815e-03 eta 0:12:25
epoch [28/30] batch [420/796] time 0.393 (0.375) data 0.000 (0.002) loss 1.0674 (0.8193) lr 3.2815e-03 eta 0:12:18
epoch [28/30] batch [440/796] time 0.334 (0.375) data 0.000 (0.002) loss 1.2373 (0.8176) lr 3.2815e-03 eta 0:12:10
epoch [28/30] batch [460/796] time 0.378 (0.375) data 0.000 (0.002) loss 0.1599 (0.8177) lr 3.2815e-03 eta 0:12:03
epoch [28/30] batch [480/796] time 0.457 (0.375) data 0.000 (0.002) loss 1.5605 (0.8173) lr 3.2815e-03 eta 0:11:56
epoch [28/30] batch [500/796] time 0.351 (0.376) data 0.000 (0.002) loss 0.7163 (0.8218) lr 3.2815e-03 eta 0:11:49
epoch [28/30] batch [520/796] time 0.398 (0.375) data 0.000 (0.002) loss 1.3291 (0.8210) lr 3.2815e-03 eta 0:11:41
epoch [28/30] batch [540/796] time 0.383 (0.375) data 0.000 (0.002) loss 0.5000 (0.8205) lr 3.2815e-03 eta 0:11:33
epoch [28/30] batch [560/796] time 0.401 (0.375) data 0.000 (0.002) loss 0.3005 (0.8193) lr 3.2815e-03 eta 0:11:25
epoch [28/30] batch [580/796] time 0.376 (0.375) data 0.000 (0.002) loss 0.1287 (0.8186) lr 3.2815e-03 eta 0:11:18
epoch [28/30] batch [600/796] time 0.397 (0.375) data 0.000 (0.002) loss 0.3308 (0.8185) lr 3.2815e-03 eta 0:11:10
epoch [28/30] batch [620/796] time 0.403 (0.375) data 0.000 (0.002) loss 0.2188 (0.8209) lr 3.2815e-03 eta 0:11:03
epoch [28/30] batch [640/796] time 0.393 (0.375) data 0.000 (0.002) loss 1.2266 (0.8187) lr 3.2815e-03 eta 0:10:55
epoch [28/30] batch [660/796] time 0.396 (0.375) data 0.000 (0.002) loss 0.5601 (0.8186) lr 3.2815e-03 eta 0:10:47
epoch [28/30] batch [680/796] time 0.403 (0.375) data 0.000 (0.002) loss 0.7324 (0.8212) lr 3.2815e-03 eta 0:10:40
epoch [28/30] batch [700/796] time 0.386 (0.375) data 0.000 (0.002) loss 0.5674 (0.8210) lr 3.2815e-03 eta 0:10:32
epoch [28/30] batch [720/796] time 0.388 (0.375) data 0.000 (0.001) loss 0.7646 (0.8206) lr 3.2815e-03 eta 0:10:25
epoch [28/30] batch [740/796] time 0.387 (0.375) data 0.000 (0.001) loss 0.3984 (0.8236) lr 3.2815e-03 eta 0:10:17
epoch [28/30] batch [760/796] time 0.366 (0.375) data 0.000 (0.001) loss 1.1055 (0.8210) lr 3.2815e-03 eta 0:10:10
epoch [28/30] batch [780/796] time 0.327 (0.374) data 0.000 (0.001) loss 0.2708 (0.8243) lr 3.2815e-03 eta 0:10:01
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:47,  5.67s/it] 10%|█         | 2/20 [00:07<00:56,  3.16s/it] 15%|█▌        | 3/20 [00:07<00:31,  1.85s/it] 20%|██        | 4/20 [00:07<00:19,  1.23s/it] 25%|██▌       | 5/20 [00:07<00:13,  1.13it/s] 30%|███       | 6/20 [00:08<00:09,  1.48it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.84it/s] 40%|████      | 8/20 [00:08<00:05,  2.19it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.53it/s] 50%|█████     | 10/20 [00:09<00:03,  2.87it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.23it/s] 60%|██████    | 12/20 [00:09<00:02,  3.39it/s] 65%|██████▌   | 13/20 [00:09<00:01,  3.52it/s] 70%|███████   | 14/20 [00:10<00:01,  3.66it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.76it/s] 80%|████████  | 16/20 [00:10<00:01,  3.81it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.85it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.88it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.09it/s]100%|██████████| 20/20 [00:11<00:00,  4.50it/s]100%|██████████| 20/20 [00:11<00:00,  1.70it/s]=> result
* total: 1,990
* correct: 1,612
* accuracy: 81.0%
* error: 19.0%
* macro_f1: 80.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1/prompt_learner/model-best.pth.tar

epoch [29/30] batch [20/796] time 0.391 (0.427) data 0.000 (0.046) loss 0.8560 (0.8055) lr 2.9385e-03 eta 0:11:11
epoch [29/30] batch [40/796] time 0.385 (0.407) data 0.000 (0.023) loss 2.4863 (0.9873) lr 2.9385e-03 eta 0:10:30
epoch [29/30] batch [60/796] time 0.399 (0.396) data 0.000 (0.016) loss 0.4365 (0.9890) lr 2.9385e-03 eta 0:10:06
epoch [29/30] batch [80/796] time 0.364 (0.389) data 0.000 (0.012) loss 1.1523 (0.9561) lr 2.9385e-03 eta 0:09:47
epoch [29/30] batch [100/796] time 0.397 (0.387) data 0.000 (0.009) loss 0.4946 (0.9047) lr 2.9385e-03 eta 0:09:37
epoch [29/30] batch [120/796] time 0.391 (0.386) data 0.000 (0.008) loss 0.1444 (0.8939) lr 2.9385e-03 eta 0:09:28
epoch [29/30] batch [140/796] time 0.363 (0.384) data 0.000 (0.007) loss 1.0459 (0.8744) lr 2.9385e-03 eta 0:09:18
epoch [29/30] batch [160/796] time 0.381 (0.383) data 0.000 (0.006) loss 0.6924 (0.8702) lr 2.9385e-03 eta 0:09:08
epoch [29/30] batch [180/796] time 0.369 (0.383) data 0.000 (0.005) loss 0.3679 (0.8761) lr 2.9385e-03 eta 0:09:00
epoch [29/30] batch [200/796] time 0.339 (0.382) data 0.000 (0.005) loss 0.2974 (0.8749) lr 2.9385e-03 eta 0:08:52
epoch [29/30] batch [220/796] time 0.382 (0.382) data 0.000 (0.004) loss 0.4175 (0.8693) lr 2.9385e-03 eta 0:08:44
epoch [29/30] batch [240/796] time 0.422 (0.382) data 0.000 (0.004) loss 0.2922 (0.8753) lr 2.9385e-03 eta 0:08:36
epoch [29/30] batch [260/796] time 0.365 (0.381) data 0.000 (0.004) loss 0.5088 (0.8713) lr 2.9385e-03 eta 0:08:27
epoch [29/30] batch [280/796] time 0.386 (0.380) data 0.000 (0.004) loss 2.5566 (0.8589) lr 2.9385e-03 eta 0:08:18
epoch [29/30] batch [300/796] time 0.385 (0.380) data 0.000 (0.003) loss 0.2837 (0.8619) lr 2.9385e-03 eta 0:08:10
epoch [29/30] batch [320/796] time 0.373 (0.379) data 0.000 (0.003) loss 0.7451 (0.8442) lr 2.9385e-03 eta 0:08:02
epoch [29/30] batch [340/796] time 0.387 (0.379) data 0.000 (0.003) loss 0.2800 (0.8391) lr 2.9385e-03 eta 0:07:54
epoch [29/30] batch [360/796] time 0.375 (0.379) data 0.000 (0.003) loss 0.6489 (0.8366) lr 2.9385e-03 eta 0:07:46
epoch [29/30] batch [380/796] time 0.359 (0.379) data 0.000 (0.003) loss 0.3291 (0.8397) lr 2.9385e-03 eta 0:07:39
epoch [29/30] batch [400/796] time 0.381 (0.378) data 0.000 (0.003) loss 0.1615 (0.8328) lr 2.9385e-03 eta 0:07:30
epoch [29/30] batch [420/796] time 0.397 (0.378) data 0.000 (0.002) loss 1.0713 (0.8391) lr 2.9385e-03 eta 0:07:23
epoch [29/30] batch [440/796] time 0.391 (0.378) data 0.000 (0.002) loss 0.2424 (0.8274) lr 2.9385e-03 eta 0:07:15
epoch [29/30] batch [460/796] time 0.390 (0.378) data 0.000 (0.002) loss 0.8042 (0.8296) lr 2.9385e-03 eta 0:07:07
epoch [29/30] batch [480/796] time 0.348 (0.378) data 0.000 (0.002) loss 0.9341 (0.8239) lr 2.9385e-03 eta 0:07:00
epoch [29/30] batch [500/796] time 0.385 (0.378) data 0.000 (0.002) loss 1.3613 (0.8331) lr 2.9385e-03 eta 0:06:52
epoch [29/30] batch [520/796] time 0.385 (0.379) data 0.000 (0.002) loss 0.9951 (0.8353) lr 2.9385e-03 eta 0:06:45
epoch [29/30] batch [540/796] time 0.389 (0.378) data 0.000 (0.002) loss 0.8584 (0.8355) lr 2.9385e-03 eta 0:06:38
epoch [29/30] batch [560/796] time 0.358 (0.378) data 0.000 (0.002) loss 0.4373 (0.8318) lr 2.9385e-03 eta 0:06:30
epoch [29/30] batch [580/796] time 0.393 (0.378) data 0.000 (0.002) loss 0.6235 (0.8311) lr 2.9385e-03 eta 0:06:22
epoch [29/30] batch [600/796] time 0.374 (0.378) data 0.000 (0.002) loss 0.4590 (0.8375) lr 2.9385e-03 eta 0:06:14
epoch [29/30] batch [620/796] time 0.398 (0.378) data 0.000 (0.002) loss 0.0849 (0.8406) lr 2.9385e-03 eta 0:06:07
epoch [29/30] batch [640/796] time 0.343 (0.378) data 0.000 (0.002) loss 1.1367 (0.8426) lr 2.9385e-03 eta 0:05:59
epoch [29/30] batch [660/796] time 0.394 (0.378) data 0.000 (0.002) loss 1.5234 (0.8412) lr 2.9385e-03 eta 0:05:52
epoch [29/30] batch [680/796] time 0.374 (0.378) data 0.000 (0.002) loss 0.1764 (0.8369) lr 2.9385e-03 eta 0:05:44
epoch [29/30] batch [700/796] time 0.384 (0.378) data 0.000 (0.002) loss 0.7505 (0.8425) lr 2.9385e-03 eta 0:05:37
epoch [29/30] batch [720/796] time 0.384 (0.378) data 0.000 (0.002) loss 0.2947 (0.8440) lr 2.9385e-03 eta 0:05:29
epoch [29/30] batch [740/796] time 0.386 (0.378) data 0.000 (0.001) loss 0.5430 (0.8416) lr 2.9385e-03 eta 0:05:21
epoch [29/30] batch [760/796] time 0.397 (0.378) data 0.000 (0.001) loss 1.3066 (0.8464) lr 2.9385e-03 eta 0:05:14
epoch [29/30] batch [780/796] time 0.328 (0.377) data 0.000 (0.001) loss 0.6802 (0.8402) lr 2.9385e-03 eta 0:05:06
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:49,  5.75s/it] 10%|█         | 2/20 [00:06<00:54,  3.04s/it] 15%|█▌        | 3/20 [00:07<00:30,  1.79s/it] 20%|██        | 4/20 [00:07<00:19,  1.20s/it] 25%|██▌       | 5/20 [00:07<00:13,  1.15it/s] 30%|███       | 6/20 [00:08<00:09,  1.48it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.81it/s] 40%|████      | 8/20 [00:08<00:05,  2.12it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.45it/s] 50%|█████     | 10/20 [00:09<00:03,  2.74it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.01it/s] 60%|██████    | 12/20 [00:09<00:02,  3.28it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.45it/s] 70%|███████   | 14/20 [00:10<00:01,  3.63it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.86it/s] 80%|████████  | 16/20 [00:10<00:01,  4.00it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.99it/s] 90%|█████████ | 18/20 [00:11<00:00,  4.35it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.64it/s]100%|██████████| 20/20 [00:11<00:00,  4.95it/s]100%|██████████| 20/20 [00:11<00:00,  1.72it/s]=> result
* total: 1,990
* correct: 1,595
* accuracy: 80.2%
* error: 19.8%
* macro_f1: 79.5%

epoch [30/30] batch [20/796] time 0.379 (0.429) data 0.000 (0.042) loss 1.5361 (0.8984) lr 2.5993e-03 eta 0:05:33
epoch [30/30] batch [40/796] time 0.337 (0.401) data 0.000 (0.021) loss 0.5376 (0.8187) lr 2.5993e-03 eta 0:05:03
epoch [30/30] batch [60/796] time 0.364 (0.391) data 0.000 (0.014) loss 0.1785 (0.7614) lr 2.5993e-03 eta 0:04:47
epoch [30/30] batch [80/796] time 0.396 (0.386) data 0.000 (0.011) loss 1.3613 (0.8195) lr 2.5993e-03 eta 0:04:36
epoch [30/30] batch [100/796] time 0.394 (0.384) data 0.001 (0.009) loss 0.2467 (0.7764) lr 2.5993e-03 eta 0:04:27
epoch [30/30] batch [120/796] time 0.389 (0.383) data 0.000 (0.007) loss 0.5137 (0.7480) lr 2.5993e-03 eta 0:04:18
epoch [30/30] batch [140/796] time 0.352 (0.383) data 0.000 (0.006) loss 0.3064 (0.7255) lr 2.5993e-03 eta 0:04:11
epoch [30/30] batch [160/796] time 0.354 (0.381) data 0.000 (0.006) loss 0.7471 (0.7475) lr 2.5993e-03 eta 0:04:02
epoch [30/30] batch [180/796] time 0.349 (0.380) data 0.000 (0.005) loss 0.8340 (0.7587) lr 2.5993e-03 eta 0:03:54
epoch [30/30] batch [200/796] time 0.367 (0.380) data 0.000 (0.004) loss 0.5107 (0.7625) lr 2.5993e-03 eta 0:03:46
epoch [30/30] batch [220/796] time 0.347 (0.380) data 0.000 (0.004) loss 0.9668 (0.7639) lr 2.5993e-03 eta 0:03:38
epoch [30/30] batch [240/796] time 0.379 (0.379) data 0.000 (0.004) loss 0.3745 (0.7560) lr 2.5993e-03 eta 0:03:30
epoch [30/30] batch [260/796] time 0.358 (0.379) data 0.000 (0.003) loss 1.2861 (0.7662) lr 2.5993e-03 eta 0:03:23
epoch [30/30] batch [280/796] time 0.373 (0.379) data 0.000 (0.003) loss 0.2114 (0.7625) lr 2.5993e-03 eta 0:03:15
epoch [30/30] batch [300/796] time 0.344 (0.379) data 0.000 (0.003) loss 1.5029 (0.7619) lr 2.5993e-03 eta 0:03:07
epoch [30/30] batch [320/796] time 0.396 (0.378) data 0.000 (0.003) loss 0.5308 (0.7726) lr 2.5993e-03 eta 0:02:59
epoch [30/30] batch [340/796] time 0.389 (0.378) data 0.000 (0.003) loss 0.5391 (0.7764) lr 2.5993e-03 eta 0:02:52
epoch [30/30] batch [360/796] time 0.396 (0.378) data 0.000 (0.003) loss 1.3213 (0.7692) lr 2.5993e-03 eta 0:02:44
epoch [30/30] batch [380/796] time 0.397 (0.378) data 0.000 (0.002) loss 0.0124 (0.7604) lr 2.5993e-03 eta 0:02:37
epoch [30/30] batch [400/796] time 0.376 (0.378) data 0.000 (0.002) loss 1.7002 (0.7753) lr 2.5993e-03 eta 0:02:29
epoch [30/30] batch [420/796] time 0.391 (0.378) data 0.000 (0.002) loss 0.6274 (0.7720) lr 2.5993e-03 eta 0:02:22
epoch [30/30] batch [440/796] time 0.387 (0.378) data 0.000 (0.002) loss 1.0254 (0.7696) lr 2.5993e-03 eta 0:02:14
epoch [30/30] batch [460/796] time 0.335 (0.378) data 0.000 (0.002) loss 0.6338 (0.7736) lr 2.5993e-03 eta 0:02:06
epoch [30/30] batch [480/796] time 0.353 (0.378) data 0.000 (0.002) loss 1.1328 (0.7846) lr 2.5993e-03 eta 0:01:59
epoch [30/30] batch [500/796] time 0.376 (0.377) data 0.000 (0.002) loss 1.1084 (0.7877) lr 2.5993e-03 eta 0:01:51
epoch [30/30] batch [520/796] time 0.388 (0.377) data 0.000 (0.002) loss 1.3818 (0.7987) lr 2.5993e-03 eta 0:01:44
epoch [30/30] batch [540/796] time 0.367 (0.377) data 0.000 (0.002) loss 0.4297 (0.7993) lr 2.5993e-03 eta 0:01:36
epoch [30/30] batch [560/796] time 0.388 (0.377) data 0.000 (0.002) loss 0.8750 (0.8080) lr 2.5993e-03 eta 0:01:28
epoch [30/30] batch [580/796] time 0.385 (0.377) data 0.000 (0.002) loss 0.4348 (0.8015) lr 2.5993e-03 eta 0:01:21
epoch [30/30] batch [600/796] time 0.390 (0.377) data 0.000 (0.002) loss 0.2585 (0.8033) lr 2.5993e-03 eta 0:01:13
epoch [30/30] batch [620/796] time 0.347 (0.377) data 0.000 (0.002) loss 0.3721 (0.7994) lr 2.5993e-03 eta 0:01:06
epoch [30/30] batch [640/796] time 0.380 (0.377) data 0.000 (0.002) loss 1.2588 (0.8008) lr 2.5993e-03 eta 0:00:58
epoch [30/30] batch [660/796] time 0.394 (0.377) data 0.000 (0.002) loss 1.5000 (0.8020) lr 2.5993e-03 eta 0:00:51
epoch [30/30] batch [680/796] time 0.367 (0.377) data 0.000 (0.001) loss 0.6602 (0.8052) lr 2.5993e-03 eta 0:00:43
epoch [30/30] batch [700/796] time 0.337 (0.377) data 0.000 (0.001) loss 0.4338 (0.8076) lr 2.5993e-03 eta 0:00:36
epoch [30/30] batch [720/796] time 0.373 (0.377) data 0.000 (0.001) loss 0.7549 (0.8018) lr 2.5993e-03 eta 0:00:28
epoch [30/30] batch [740/796] time 0.362 (0.377) data 0.000 (0.001) loss 0.4829 (0.7995) lr 2.5993e-03 eta 0:00:21
epoch [30/30] batch [760/796] time 0.396 (0.377) data 0.000 (0.001) loss 0.4370 (0.8008) lr 2.5993e-03 eta 0:00:13
epoch [30/30] batch [780/796] time 0.331 (0.376) data 0.000 (0.001) loss 0.3225 (0.8072) lr 2.5993e-03 eta 0:00:06
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:50,  5.80s/it] 10%|█         | 2/20 [00:06<00:50,  2.79s/it] 15%|█▌        | 3/20 [00:06<00:27,  1.64s/it] 20%|██        | 4/20 [00:07<00:17,  1.10s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.24it/s] 30%|███       | 6/20 [00:07<00:08,  1.59it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.96it/s] 40%|████      | 8/20 [00:08<00:05,  2.29it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.56it/s] 50%|█████     | 10/20 [00:08<00:03,  2.80it/s] 55%|█████▌    | 11/20 [00:08<00:03,  2.99it/s] 60%|██████    | 12/20 [00:09<00:02,  3.13it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.33it/s] 70%|███████   | 14/20 [00:09<00:01,  3.49it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.60it/s] 80%|████████  | 16/20 [00:10<00:01,  3.61it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.69it/s] 90%|█████████ | 18/20 [00:10<00:00,  3.41it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.58it/s]100%|██████████| 20/20 [00:11<00:00,  3.74it/s]100%|██████████| 20/20 [00:11<00:00,  1.73it/s]
=> result
* total: 1,990
* correct: 1,602
* accuracy: 80.5%
* error: 19.5%
* macro_f1: 79.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1/prompt_learner/model-best.pth.tar" (epoch = 28)
Evaluate on the *test* set
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:05<09:22,  5.68s/it]  2%|▏         | 2/100 [00:06<04:17,  2.63s/it]  3%|▎         | 3/100 [00:07<03:25,  2.12s/it]  4%|▍         | 4/100 [00:08<02:20,  1.47s/it]  5%|▌         | 5/100 [00:08<01:45,  1.11s/it]  6%|▌         | 6/100 [00:09<01:25,  1.10it/s]  7%|▋         | 7/100 [00:09<01:13,  1.27it/s]  8%|▊         | 8/100 [00:10<01:03,  1.45it/s]  9%|▉         | 9/100 [00:10<00:56,  1.60it/s] 10%|█         | 10/100 [00:11<00:51,  1.73it/s] 11%|█         | 11/100 [00:11<00:50,  1.77it/s] 12%|█▏        | 12/100 [00:12<00:47,  1.84it/s] 13%|█▎        | 13/100 [00:12<00:43,  1.98it/s] 14%|█▍        | 14/100 [00:13<00:45,  1.90it/s] 15%|█▌        | 15/100 [00:13<00:44,  1.92it/s] 16%|█▌        | 16/100 [00:14<00:40,  2.07it/s] 17%|█▋        | 17/100 [00:14<00:39,  2.11it/s] 18%|█▊        | 18/100 [00:14<00:38,  2.11it/s] 19%|█▉        | 19/100 [00:15<00:38,  2.10it/s] 20%|██        | 20/100 [00:15<00:39,  2.03it/s] 21%|██        | 21/100 [00:16<00:38,  2.07it/s] 22%|██▏       | 22/100 [00:16<00:36,  2.13it/s] 23%|██▎       | 23/100 [00:17<00:35,  2.18it/s] 24%|██▍       | 24/100 [00:17<00:34,  2.22it/s] 25%|██▌       | 25/100 [00:18<00:32,  2.28it/s] 26%|██▌       | 26/100 [00:18<00:33,  2.23it/s] 27%|██▋       | 27/100 [00:19<00:34,  2.13it/s] 28%|██▊       | 28/100 [00:19<00:34,  2.08it/s] 29%|██▉       | 29/100 [00:20<00:34,  2.06it/s] 30%|███       | 30/100 [00:20<00:33,  2.07it/s] 31%|███       | 31/100 [00:21<00:32,  2.11it/s] 32%|███▏      | 32/100 [00:21<00:31,  2.13it/s] 33%|███▎      | 33/100 [00:22<00:32,  2.08it/s] 34%|███▍      | 34/100 [00:22<00:31,  2.08it/s] 35%|███▌      | 35/100 [00:22<00:30,  2.12it/s] 36%|███▌      | 36/100 [00:23<00:30,  2.10it/s] 37%|███▋      | 37/100 [00:24<00:31,  2.01it/s] 38%|███▊      | 38/100 [00:24<00:29,  2.10it/s] 39%|███▉      | 39/100 [00:24<00:27,  2.25it/s] 40%|████      | 40/100 [00:25<00:26,  2.28it/s] 41%|████      | 41/100 [00:25<00:24,  2.41it/s] 42%|████▏     | 42/100 [00:25<00:23,  2.47it/s] 43%|████▎     | 43/100 [00:26<00:21,  2.61it/s] 44%|████▍     | 44/100 [00:26<00:20,  2.67it/s] 45%|████▌     | 45/100 [00:27<00:20,  2.66it/s] 46%|████▌     | 46/100 [00:27<00:20,  2.63it/s] 47%|████▋     | 47/100 [00:27<00:20,  2.59it/s] 48%|████▊     | 48/100 [00:28<00:20,  2.52it/s] 49%|████▉     | 49/100 [00:28<00:20,  2.47it/s] 50%|█████     | 50/100 [00:29<00:20,  2.48it/s] 51%|█████     | 51/100 [00:29<00:19,  2.54it/s] 52%|█████▏    | 52/100 [00:29<00:18,  2.55it/s] 53%|█████▎    | 53/100 [00:30<00:18,  2.56it/s] 54%|█████▍    | 54/100 [00:30<00:18,  2.45it/s] 55%|█████▌    | 55/100 [00:31<00:18,  2.38it/s] 56%|█████▌    | 56/100 [00:31<00:18,  2.37it/s] 57%|█████▋    | 57/100 [00:31<00:18,  2.36it/s] 58%|█████▊    | 58/100 [00:32<00:18,  2.31it/s] 59%|█████▉    | 59/100 [00:32<00:17,  2.37it/s] 60%|██████    | 60/100 [00:33<00:17,  2.31it/s] 61%|██████    | 61/100 [00:33<00:17,  2.24it/s] 62%|██████▏   | 62/100 [00:34<00:16,  2.36it/s] 63%|██████▎   | 63/100 [00:34<00:14,  2.52it/s] 64%|██████▍   | 64/100 [00:34<00:13,  2.62it/s] 65%|██████▌   | 65/100 [00:35<00:13,  2.59it/s] 66%|██████▌   | 66/100 [00:35<00:13,  2.52it/s] 67%|██████▋   | 67/100 [00:36<00:13,  2.45it/s] 68%|██████▊   | 68/100 [00:36<00:14,  2.21it/s] 69%|██████▉   | 69/100 [00:36<00:13,  2.31it/s] 70%|███████   | 70/100 [00:37<00:12,  2.35it/s] 71%|███████   | 71/100 [00:37<00:12,  2.37it/s] 72%|███████▏  | 72/100 [00:38<00:10,  2.56it/s] 73%|███████▎  | 73/100 [00:38<00:10,  2.59it/s] 74%|███████▍  | 74/100 [00:38<00:09,  2.63it/s] 75%|███████▌  | 75/100 [00:39<00:09,  2.71it/s] 76%|███████▌  | 76/100 [00:39<00:08,  2.88it/s] 77%|███████▋  | 77/100 [00:39<00:07,  3.05it/s] 78%|███████▊  | 78/100 [00:40<00:06,  3.23it/s] 79%|███████▉  | 79/100 [00:40<00:06,  3.40it/s] 80%|████████  | 80/100 [00:40<00:05,  3.64it/s] 81%|████████  | 81/100 [00:40<00:04,  4.03it/s] 82%|████████▏ | 82/100 [00:40<00:04,  4.38it/s] 83%|████████▎ | 83/100 [00:41<00:03,  4.67it/s] 84%|████████▍ | 84/100 [00:41<00:03,  4.89it/s] 85%|████████▌ | 85/100 [00:41<00:02,  5.06it/s] 86%|████████▌ | 86/100 [00:41<00:02,  5.18it/s] 87%|████████▋ | 87/100 [00:41<00:02,  5.27it/s] 88%|████████▊ | 88/100 [00:42<00:02,  5.32it/s] 89%|████████▉ | 89/100 [00:42<00:02,  5.37it/s] 90%|█████████ | 90/100 [00:42<00:01,  5.40it/s] 91%|█████████ | 91/100 [00:42<00:01,  5.43it/s] 92%|█████████▏| 92/100 [00:42<00:01,  5.44it/s] 93%|█████████▎| 93/100 [00:42<00:01,  5.44it/s] 94%|█████████▍| 94/100 [00:43<00:01,  5.45it/s] 95%|█████████▌| 95/100 [00:43<00:00,  5.46it/s] 96%|█████████▌| 96/100 [00:43<00:00,  5.47it/s] 97%|█████████▋| 97/100 [00:43<00:00,  5.47it/s] 98%|█████████▊| 98/100 [00:43<00:00,  5.48it/s] 99%|█████████▉| 99/100 [00:44<00:00,  5.47it/s]100%|██████████| 100/100 [00:44<00:00,  6.01it/s]100%|██████████| 100/100 [00:44<00:00,  2.26it/s]
=> result
* total: 9,950
* correct: 7,933
* accuracy: 79.7%
* error: 20.3%
* macro_f1: 79.4%
Elapsed: 1:28:44
+ sh scripts/rpo_prime/base2new_test.sh sun397 1 0 main_tmp 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/sun397/shots_16/RPO_prime/main_tmp/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/sun397/shots_16/RPO_prime/main_tmp/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:SUN397
Loading dataset: SUN397
Reading split from /shared/s2/lab01/dataset/clip/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/sun397/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
3168 1980 9900
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  198
# train_x  3,168
# val      1,980
# test     9,900
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed1/prompt_learner/model-best.pth.tar" (epoch = 28)
Evaluate on the *test* set
  0%|          | 0/99 [00:00<?, ?it/s]  1%|          | 1/99 [00:10<16:38, 10.19s/it]  2%|▏         | 2/99 [00:10<07:08,  4.41s/it]  3%|▎         | 3/99 [00:10<04:03,  2.54s/it]  4%|▍         | 4/99 [00:11<02:38,  1.67s/it]  5%|▌         | 5/99 [00:11<01:52,  1.19s/it]  6%|▌         | 6/99 [00:11<01:24,  1.10it/s]  7%|▋         | 7/99 [00:12<01:06,  1.38it/s]  8%|▊         | 8/99 [00:12<00:54,  1.67it/s]  9%|▉         | 9/99 [00:12<00:46,  1.95it/s] 10%|█         | 10/99 [00:13<00:40,  2.20it/s] 11%|█         | 11/99 [00:13<00:38,  2.27it/s] 12%|█▏        | 12/99 [00:14<00:38,  2.29it/s] 13%|█▎        | 13/99 [00:14<00:38,  2.25it/s] 14%|█▍        | 14/99 [00:15<00:38,  2.19it/s] 15%|█▌        | 15/99 [00:15<00:38,  2.18it/s] 16%|█▌        | 16/99 [00:15<00:39,  2.11it/s] 17%|█▋        | 17/99 [00:16<00:38,  2.11it/s] 18%|█▊        | 18/99 [00:16<00:38,  2.11it/s] 19%|█▉        | 19/99 [00:17<00:36,  2.19it/s] 20%|██        | 20/99 [00:17<00:37,  2.12it/s] 21%|██        | 21/99 [00:18<00:34,  2.23it/s] 22%|██▏       | 22/99 [00:18<00:34,  2.24it/s] 23%|██▎       | 23/99 [00:19<00:34,  2.20it/s] 24%|██▍       | 24/99 [00:19<00:33,  2.26it/s] 25%|██▌       | 25/99 [00:19<00:31,  2.33it/s] 26%|██▋       | 26/99 [00:20<00:29,  2.51it/s] 27%|██▋       | 27/99 [00:20<00:26,  2.67it/s] 28%|██▊       | 28/99 [00:20<00:24,  2.86it/s] 29%|██▉       | 29/99 [00:21<00:23,  3.00it/s] 30%|███       | 30/99 [00:21<00:22,  3.01it/s] 31%|███▏      | 31/99 [00:21<00:22,  3.00it/s] 32%|███▏      | 32/99 [00:22<00:22,  2.92it/s] 33%|███▎      | 33/99 [00:22<00:22,  2.87it/s] 34%|███▍      | 34/99 [00:23<00:23,  2.71it/s] 35%|███▌      | 35/99 [00:23<00:26,  2.46it/s] 36%|███▋      | 36/99 [00:23<00:26,  2.42it/s] 37%|███▋      | 37/99 [00:24<00:26,  2.33it/s] 38%|███▊      | 38/99 [00:24<00:27,  2.25it/s] 39%|███▉      | 39/99 [00:25<00:27,  2.22it/s] 40%|████      | 40/99 [00:25<00:25,  2.33it/s] 41%|████▏     | 41/99 [00:26<00:24,  2.39it/s] 42%|████▏     | 42/99 [00:26<00:23,  2.46it/s] 43%|████▎     | 43/99 [00:26<00:22,  2.48it/s] 44%|████▍     | 44/99 [00:27<00:21,  2.58it/s] 45%|████▌     | 45/99 [00:27<00:21,  2.53it/s] 46%|████▋     | 46/99 [00:28<00:22,  2.40it/s] 47%|████▋     | 47/99 [00:28<00:22,  2.36it/s] 48%|████▊     | 48/99 [00:29<00:21,  2.33it/s] 49%|████▉     | 49/99 [00:29<00:21,  2.32it/s] 51%|█████     | 50/99 [00:29<00:20,  2.36it/s] 52%|█████▏    | 51/99 [00:30<00:20,  2.34it/s] 53%|█████▎    | 52/99 [00:30<00:20,  2.34it/s] 54%|█████▎    | 53/99 [00:31<00:20,  2.26it/s] 55%|█████▍    | 54/99 [00:31<00:19,  2.28it/s] 56%|█████▌    | 55/99 [00:31<00:17,  2.45it/s] 57%|█████▋    | 56/99 [00:32<00:17,  2.50it/s] 58%|█████▊    | 57/99 [00:32<00:16,  2.53it/s] 59%|█████▊    | 58/99 [00:33<00:16,  2.51it/s] 60%|█████▉    | 59/99 [00:33<00:16,  2.46it/s] 61%|██████    | 60/99 [00:34<00:16,  2.37it/s] 62%|██████▏   | 61/99 [00:34<00:16,  2.32it/s] 63%|██████▎   | 62/99 [00:34<00:15,  2.34it/s] 64%|██████▎   | 63/99 [00:35<00:14,  2.40it/s] 65%|██████▍   | 64/99 [00:35<00:14,  2.39it/s] 66%|██████▌   | 65/99 [00:36<00:13,  2.55it/s] 67%|██████▋   | 66/99 [00:36<00:12,  2.58it/s] 68%|██████▊   | 67/99 [00:36<00:12,  2.50it/s] 69%|██████▊   | 68/99 [00:37<00:12,  2.52it/s] 70%|██████▉   | 69/99 [00:37<00:11,  2.60it/s] 71%|███████   | 70/99 [00:37<00:10,  2.71it/s] 72%|███████▏  | 71/99 [00:38<00:09,  2.81it/s] 73%|███████▎  | 72/99 [00:38<00:09,  2.94it/s] 74%|███████▎  | 73/99 [00:38<00:08,  3.10it/s] 75%|███████▍  | 74/99 [00:39<00:07,  3.27it/s] 76%|███████▌  | 75/99 [00:39<00:07,  3.39it/s] 77%|███████▋  | 76/99 [00:39<00:06,  3.46it/s] 78%|███████▊  | 77/99 [00:39<00:06,  3.52it/s] 79%|███████▉  | 78/99 [00:40<00:05,  3.62it/s] 80%|███████▉  | 79/99 [00:40<00:05,  3.81it/s] 81%|████████  | 80/99 [00:40<00:04,  3.97it/s] 82%|████████▏ | 81/99 [00:40<00:04,  4.00it/s] 83%|████████▎ | 82/99 [00:41<00:03,  4.37it/s] 84%|████████▍ | 83/99 [00:41<00:03,  4.66it/s] 85%|████████▍ | 84/99 [00:41<00:03,  4.90it/s] 86%|████████▌ | 85/99 [00:41<00:02,  5.07it/s] 87%|████████▋ | 86/99 [00:41<00:02,  5.21it/s] 88%|████████▊ | 87/99 [00:41<00:02,  5.30it/s] 89%|████████▉ | 88/99 [00:42<00:02,  5.37it/s] 90%|████████▉ | 89/99 [00:42<00:01,  5.43it/s] 91%|█████████ | 90/99 [00:42<00:01,  5.46it/s] 92%|█████████▏| 91/99 [00:42<00:01,  5.48it/s] 93%|█████████▎| 92/99 [00:42<00:01,  5.50it/s] 94%|█████████▍| 93/99 [00:43<00:01,  5.51it/s] 95%|█████████▍| 94/99 [00:43<00:00,  5.52it/s] 96%|█████████▌| 95/99 [00:43<00:00,  5.53it/s] 97%|█████████▋| 96/99 [00:43<00:00,  5.53it/s] 98%|█████████▊| 97/99 [00:43<00:00,  5.54it/s] 99%|█████████▉| 98/99 [00:43<00:00,  5.54it/s]100%|██████████| 99/99 [00:44<00:00,  5.54it/s]100%|██████████| 99/99 [00:44<00:00,  2.24it/s]
=> result
* total: 9,900
* correct: 7,773
* accuracy: 78.5%
* error: 21.5%
* macro_f1: 77.6%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train.sh sun397 2 0 main_tmp 16
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:SUN397
Loading dataset: SUN397
Reading split from /shared/s2/lab01/dataset/clip/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/sun397/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
3184 1990 9950
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  199
# train_x  3,184
# val      1,990
# test     9,950
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/30] batch [20/796] time 0.369 (0.518) data 0.000 (0.067) loss 0.4658 (1.2628) lr 1.0000e-02 eta 3:26:08
epoch [1/30] batch [40/796] time 0.352 (0.440) data 0.000 (0.033) loss 0.7480 (1.0759) lr 1.0000e-02 eta 2:54:44
epoch [1/30] batch [60/796] time 0.342 (0.419) data 0.000 (0.022) loss 0.5830 (1.1540) lr 1.0000e-02 eta 2:46:12
epoch [1/30] batch [80/796] time 0.394 (0.407) data 0.000 (0.017) loss 0.0553 (1.1607) lr 1.0000e-02 eta 2:41:38
epoch [1/30] batch [100/796] time 0.384 (0.399) data 0.000 (0.013) loss 2.3789 (1.2155) lr 1.0000e-02 eta 2:38:17
epoch [1/30] batch [120/796] time 0.388 (0.394) data 0.000 (0.011) loss 0.9458 (1.1931) lr 1.0000e-02 eta 2:35:59
epoch [1/30] batch [140/796] time 0.372 (0.390) data 0.000 (0.010) loss 1.5488 (1.1940) lr 1.0000e-02 eta 2:34:29
epoch [1/30] batch [160/796] time 0.388 (0.388) data 0.000 (0.009) loss 0.5288 (1.1897) lr 1.0000e-02 eta 2:33:29
epoch [1/30] batch [180/796] time 0.391 (0.386) data 0.000 (0.008) loss 0.6660 (1.1654) lr 1.0000e-02 eta 2:32:26
epoch [1/30] batch [200/796] time 0.396 (0.385) data 0.000 (0.007) loss 0.1151 (1.1562) lr 1.0000e-02 eta 2:32:00
epoch [1/30] batch [220/796] time 0.373 (0.384) data 0.000 (0.006) loss 1.0723 (1.1560) lr 1.0000e-02 eta 2:31:26
epoch [1/30] batch [240/796] time 0.335 (0.383) data 0.000 (0.006) loss 2.1348 (1.1557) lr 1.0000e-02 eta 2:31:02
epoch [1/30] batch [260/796] time 0.359 (0.382) data 0.000 (0.005) loss 0.8491 (1.1604) lr 1.0000e-02 eta 2:30:31
epoch [1/30] batch [280/796] time 0.395 (0.382) data 0.000 (0.005) loss 0.8457 (1.1386) lr 1.0000e-02 eta 2:30:11
epoch [1/30] batch [300/796] time 0.406 (0.381) data 0.000 (0.005) loss 0.4429 (1.1514) lr 1.0000e-02 eta 2:29:52
epoch [1/30] batch [320/796] time 0.352 (0.381) data 0.000 (0.004) loss 0.4998 (1.1347) lr 1.0000e-02 eta 2:29:26
epoch [1/30] batch [340/796] time 0.351 (0.380) data 0.000 (0.004) loss 0.7954 (1.1330) lr 1.0000e-02 eta 2:29:02
epoch [1/30] batch [360/796] time 0.368 (0.380) data 0.000 (0.004) loss 0.1390 (1.1375) lr 1.0000e-02 eta 2:28:50
epoch [1/30] batch [380/796] time 0.366 (0.379) data 0.000 (0.004) loss 1.0947 (1.1442) lr 1.0000e-02 eta 2:28:25
epoch [1/30] batch [400/796] time 0.359 (0.379) data 0.000 (0.004) loss 0.8247 (1.1349) lr 1.0000e-02 eta 2:28:09
epoch [1/30] batch [420/796] time 0.338 (0.378) data 0.000 (0.003) loss 2.3340 (1.1380) lr 1.0000e-02 eta 2:27:51
epoch [1/30] batch [440/796] time 0.344 (0.379) data 0.000 (0.003) loss 3.7402 (1.1395) lr 1.0000e-02 eta 2:27:54
epoch [1/30] batch [460/796] time 0.389 (0.378) data 0.000 (0.003) loss 0.6343 (1.1316) lr 1.0000e-02 eta 2:27:35
epoch [1/30] batch [480/796] time 0.345 (0.378) data 0.000 (0.003) loss 1.1562 (1.1186) lr 1.0000e-02 eta 2:27:29
epoch [1/30] batch [500/796] time 0.383 (0.378) data 0.000 (0.003) loss 1.9727 (1.1200) lr 1.0000e-02 eta 2:27:23
epoch [1/30] batch [520/796] time 0.381 (0.378) data 0.000 (0.003) loss 0.2803 (1.1079) lr 1.0000e-02 eta 2:27:09
epoch [1/30] batch [540/796] time 0.366 (0.378) data 0.000 (0.003) loss 0.1797 (1.1023) lr 1.0000e-02 eta 2:27:03
epoch [1/30] batch [560/796] time 0.338 (0.378) data 0.000 (0.003) loss 0.7651 (1.1075) lr 1.0000e-02 eta 2:26:49
epoch [1/30] batch [580/796] time 0.378 (0.377) data 0.000 (0.003) loss 1.6611 (1.1095) lr 1.0000e-02 eta 2:26:35
epoch [1/30] batch [600/796] time 0.378 (0.377) data 0.000 (0.002) loss 2.3281 (1.1106) lr 1.0000e-02 eta 2:26:23
epoch [1/30] batch [620/796] time 0.410 (0.377) data 0.000 (0.002) loss 0.2266 (1.1120) lr 1.0000e-02 eta 2:26:11
epoch [1/30] batch [640/796] time 0.377 (0.377) data 0.000 (0.002) loss 1.5898 (1.1117) lr 1.0000e-02 eta 2:25:55
epoch [1/30] batch [660/796] time 0.378 (0.377) data 0.000 (0.002) loss 0.7305 (1.1099) lr 1.0000e-02 eta 2:25:46
epoch [1/30] batch [680/796] time 0.338 (0.376) data 0.000 (0.002) loss 0.6846 (1.1054) lr 1.0000e-02 eta 2:25:33
epoch [1/30] batch [700/796] time 0.379 (0.376) data 0.000 (0.002) loss 1.0068 (1.1018) lr 1.0000e-02 eta 2:25:22
epoch [1/30] batch [720/796] time 0.377 (0.376) data 0.000 (0.002) loss 1.7002 (1.1017) lr 1.0000e-02 eta 2:25:07
epoch [1/30] batch [740/796] time 0.404 (0.376) data 0.000 (0.002) loss 1.3896 (1.1009) lr 1.0000e-02 eta 2:24:56
epoch [1/30] batch [760/796] time 0.400 (0.376) data 0.000 (0.002) loss 0.4900 (1.1021) lr 1.0000e-02 eta 2:24:44
epoch [1/30] batch [780/796] time 0.328 (0.375) data 0.000 (0.002) loss 1.5049 (1.1065) lr 1.0000e-02 eta 2:24:21
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<01:55,  6.06s/it] 10%|█         | 2/20 [00:06<00:47,  2.66s/it] 15%|█▌        | 3/20 [00:06<00:27,  1.59s/it] 20%|██        | 4/20 [00:06<00:17,  1.08s/it] 25%|██▌       | 5/20 [00:07<00:11,  1.26it/s] 30%|███       | 6/20 [00:07<00:08,  1.62it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.99it/s] 40%|████      | 8/20 [00:08<00:05,  2.32it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.57it/s] 50%|█████     | 10/20 [00:08<00:03,  2.80it/s] 55%|█████▌    | 11/20 [00:08<00:02,  3.01it/s] 60%|██████    | 12/20 [00:09<00:02,  3.18it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.33it/s] 70%|███████   | 14/20 [00:09<00:01,  3.78it/s] 75%|███████▌  | 15/20 [00:09<00:01,  4.17it/s] 80%|████████  | 16/20 [00:10<00:00,  4.50it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.76it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.96it/s] 95%|█████████▌| 19/20 [00:10<00:00,  5.11it/s]100%|██████████| 20/20 [00:10<00:00,  5.31it/s]100%|██████████| 20/20 [00:10<00:00,  1.84it/s]=> result
* total: 1,990
* correct: 1,507
* accuracy: 75.7%
* error: 24.3%
* macro_f1: 74.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar

epoch [2/30] batch [20/796] time 0.343 (0.428) data 0.000 (0.046) loss 0.1687 (0.8587) lr 9.9726e-03 eta 2:44:37
epoch [2/30] batch [40/796] time 0.367 (0.395) data 0.000 (0.023) loss 0.2267 (0.9829) lr 9.9726e-03 eta 2:31:46
epoch [2/30] batch [60/796] time 0.374 (0.389) data 0.000 (0.015) loss 0.4563 (0.9745) lr 9.9726e-03 eta 2:29:15
epoch [2/30] batch [80/796] time 0.391 (0.386) data 0.000 (0.012) loss 0.2196 (0.9972) lr 9.9726e-03 eta 2:27:48
epoch [2/30] batch [100/796] time 0.397 (0.381) data 0.001 (0.009) loss 1.5869 (1.0267) lr 9.9726e-03 eta 2:25:55
epoch [2/30] batch [120/796] time 0.380 (0.380) data 0.000 (0.008) loss 0.9722 (1.0225) lr 9.9726e-03 eta 2:25:29
epoch [2/30] batch [140/796] time 0.373 (0.379) data 0.000 (0.007) loss 0.1724 (1.0068) lr 9.9726e-03 eta 2:24:53
epoch [2/30] batch [160/796] time 0.378 (0.378) data 0.000 (0.006) loss 0.6309 (1.0056) lr 9.9726e-03 eta 2:24:22
epoch [2/30] batch [180/796] time 0.368 (0.377) data 0.000 (0.005) loss 1.3301 (0.9870) lr 9.9726e-03 eta 2:23:45
epoch [2/30] batch [200/796] time 0.362 (0.375) data 0.000 (0.005) loss 0.2583 (0.9989) lr 9.9726e-03 eta 2:22:59
epoch [2/30] batch [220/796] time 0.448 (0.374) data 0.000 (0.004) loss 1.1787 (0.9998) lr 9.9726e-03 eta 2:22:40
epoch [2/30] batch [240/796] time 0.383 (0.375) data 0.000 (0.004) loss 0.7598 (1.0113) lr 9.9726e-03 eta 2:22:37
epoch [2/30] batch [260/796] time 0.367 (0.374) data 0.000 (0.004) loss 1.7715 (1.0101) lr 9.9726e-03 eta 2:22:18
epoch [2/30] batch [280/796] time 0.354 (0.374) data 0.000 (0.003) loss 1.2090 (0.9937) lr 9.9726e-03 eta 2:22:08
epoch [2/30] batch [300/796] time 0.389 (0.374) data 0.000 (0.003) loss 0.3008 (0.9954) lr 9.9726e-03 eta 2:21:55
epoch [2/30] batch [320/796] time 0.392 (0.374) data 0.000 (0.003) loss 0.6450 (0.9896) lr 9.9726e-03 eta 2:21:46
epoch [2/30] batch [340/796] time 0.376 (0.373) data 0.000 (0.003) loss 1.1191 (0.9639) lr 9.9726e-03 eta 2:21:27
epoch [2/30] batch [360/796] time 0.343 (0.373) data 0.000 (0.003) loss 1.2139 (0.9884) lr 9.9726e-03 eta 2:21:11
epoch [2/30] batch [380/796] time 0.344 (0.373) data 0.000 (0.003) loss 0.2549 (1.0060) lr 9.9726e-03 eta 2:21:01
epoch [2/30] batch [400/796] time 0.384 (0.373) data 0.000 (0.003) loss 0.9248 (1.0022) lr 9.9726e-03 eta 2:20:51
epoch [2/30] batch [420/796] time 0.379 (0.372) data 0.000 (0.002) loss 1.5586 (0.9981) lr 9.9726e-03 eta 2:20:37
epoch [2/30] batch [440/796] time 0.342 (0.373) data 0.000 (0.002) loss 1.5518 (0.9912) lr 9.9726e-03 eta 2:20:41
epoch [2/30] batch [460/796] time 0.458 (0.373) data 0.000 (0.002) loss 1.0527 (1.0064) lr 9.9726e-03 eta 2:20:41
epoch [2/30] batch [480/796] time 0.389 (0.373) data 0.000 (0.002) loss 0.7661 (1.0055) lr 9.9726e-03 eta 2:20:31
epoch [2/30] batch [500/796] time 0.380 (0.373) data 0.000 (0.002) loss 0.9780 (1.0037) lr 9.9726e-03 eta 2:20:21
epoch [2/30] batch [520/796] time 0.382 (0.373) data 0.000 (0.002) loss 0.7622 (1.0119) lr 9.9726e-03 eta 2:20:13
epoch [2/30] batch [540/796] time 0.356 (0.373) data 0.000 (0.002) loss 0.4568 (1.0167) lr 9.9726e-03 eta 2:20:10
epoch [2/30] batch [560/796] time 0.373 (0.373) data 0.000 (0.002) loss 1.6729 (1.0142) lr 9.9726e-03 eta 2:20:03
epoch [2/30] batch [580/796] time 0.409 (0.373) data 0.000 (0.002) loss 0.7246 (1.0130) lr 9.9726e-03 eta 2:19:55
epoch [2/30] batch [600/796] time 0.370 (0.373) data 0.000 (0.002) loss 0.9922 (1.0193) lr 9.9726e-03 eta 2:19:50
epoch [2/30] batch [620/796] time 0.391 (0.373) data 0.000 (0.002) loss 2.3887 (1.0183) lr 9.9726e-03 eta 2:19:44
epoch [2/30] batch [640/796] time 0.341 (0.373) data 0.000 (0.002) loss 0.9609 (1.0215) lr 9.9726e-03 eta 2:19:31
epoch [2/30] batch [660/796] time 0.346 (0.373) data 0.000 (0.002) loss 0.7969 (1.0242) lr 9.9726e-03 eta 2:19:26
epoch [2/30] batch [680/796] time 0.371 (0.373) data 0.000 (0.002) loss 1.2803 (1.0246) lr 9.9726e-03 eta 2:19:18
epoch [2/30] batch [700/796] time 0.348 (0.373) data 0.000 (0.002) loss 0.8896 (1.0200) lr 9.9726e-03 eta 2:19:12
epoch [2/30] batch [720/796] time 0.387 (0.373) data 0.000 (0.002) loss 0.7788 (1.0215) lr 9.9726e-03 eta 2:19:06
epoch [2/30] batch [740/796] time 0.366 (0.373) data 0.000 (0.001) loss 1.5400 (1.0166) lr 9.9726e-03 eta 2:18:56
epoch [2/30] batch [760/796] time 0.349 (0.373) data 0.001 (0.001) loss 0.8931 (1.0163) lr 9.9726e-03 eta 2:18:47
epoch [2/30] batch [780/796] time 0.336 (0.372) data 0.000 (0.001) loss 2.1816 (1.0245) lr 9.9726e-03 eta 2:18:19
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:48,  5.71s/it] 10%|█         | 2/20 [00:06<00:53,  2.94s/it] 15%|█▌        | 3/20 [00:06<00:29,  1.73s/it] 20%|██        | 4/20 [00:07<00:18,  1.16s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.19it/s] 30%|███       | 6/20 [00:07<00:09,  1.55it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.91it/s] 40%|████      | 8/20 [00:08<00:05,  2.25it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.57it/s] 50%|█████     | 10/20 [00:08<00:03,  2.86it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.09it/s] 60%|██████    | 12/20 [00:09<00:02,  3.30it/s] 65%|██████▌   | 13/20 [00:09<00:01,  3.55it/s] 70%|███████   | 14/20 [00:09<00:01,  3.68it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.75it/s] 80%|████████  | 16/20 [00:10<00:01,  3.79it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.89it/s] 90%|█████████ | 18/20 [00:10<00:00,  3.87it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.25it/s]100%|██████████| 20/20 [00:11<00:00,  4.63it/s]100%|██████████| 20/20 [00:11<00:00,  1.75it/s]=> result
* total: 1,990
* correct: 1,512
* accuracy: 76.0%
* error: 24.0%
* macro_f1: 74.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar

epoch [3/30] batch [20/796] time 0.387 (0.427) data 0.000 (0.044) loss 0.7246 (1.0044) lr 9.8907e-03 eta 2:38:21
epoch [3/30] batch [40/796] time 0.359 (0.398) data 0.000 (0.022) loss 0.6377 (1.0113) lr 9.8907e-03 eta 2:27:41
epoch [3/30] batch [60/796] time 0.384 (0.392) data 0.000 (0.015) loss 0.3726 (0.9838) lr 9.8907e-03 eta 2:25:23
epoch [3/30] batch [80/796] time 0.364 (0.386) data 0.000 (0.011) loss 1.3018 (0.9581) lr 9.8907e-03 eta 2:22:42
epoch [3/30] batch [100/796] time 0.380 (0.383) data 0.000 (0.009) loss 1.0781 (0.9283) lr 9.8907e-03 eta 2:21:32
epoch [3/30] batch [120/796] time 0.334 (0.381) data 0.000 (0.008) loss 0.3877 (0.9217) lr 9.8907e-03 eta 2:20:54
epoch [3/30] batch [140/796] time 0.343 (0.379) data 0.000 (0.007) loss 1.4600 (0.9087) lr 9.8907e-03 eta 2:19:59
epoch [3/30] batch [160/796] time 0.383 (0.379) data 0.000 (0.006) loss 2.3516 (0.9182) lr 9.8907e-03 eta 2:19:45
epoch [3/30] batch [180/796] time 0.386 (0.379) data 0.000 (0.005) loss 0.3330 (0.9265) lr 9.8907e-03 eta 2:19:35
epoch [3/30] batch [200/796] time 0.381 (0.379) data 0.000 (0.005) loss 0.1909 (0.9338) lr 9.8907e-03 eta 2:19:29
epoch [3/30] batch [220/796] time 0.390 (0.378) data 0.000 (0.004) loss 0.7178 (0.9458) lr 9.8907e-03 eta 2:19:07
epoch [3/30] batch [240/796] time 0.350 (0.377) data 0.000 (0.004) loss 1.2139 (0.9692) lr 9.8907e-03 eta 2:18:38
epoch [3/30] batch [260/796] time 0.381 (0.376) data 0.000 (0.004) loss 0.8052 (0.9683) lr 9.8907e-03 eta 2:18:08
epoch [3/30] batch [280/796] time 0.369 (0.376) data 0.000 (0.003) loss 1.2637 (0.9697) lr 9.8907e-03 eta 2:17:55
epoch [3/30] batch [300/796] time 0.386 (0.376) data 0.000 (0.003) loss 0.5430 (0.9792) lr 9.8907e-03 eta 2:17:52
epoch [3/30] batch [320/796] time 0.339 (0.376) data 0.000 (0.003) loss 1.7568 (0.9859) lr 9.8907e-03 eta 2:17:33
epoch [3/30] batch [340/796] time 0.348 (0.376) data 0.000 (0.003) loss 0.4397 (1.0104) lr 9.8907e-03 eta 2:17:22
epoch [3/30] batch [360/796] time 0.346 (0.376) data 0.000 (0.003) loss 1.5273 (1.0133) lr 9.8907e-03 eta 2:17:14
epoch [3/30] batch [380/796] time 0.402 (0.376) data 0.000 (0.003) loss 1.3916 (1.0241) lr 9.8907e-03 eta 2:17:06
epoch [3/30] batch [400/796] time 0.361 (0.375) data 0.000 (0.002) loss 0.3157 (1.0247) lr 9.8907e-03 eta 2:16:58
epoch [3/30] batch [420/796] time 0.393 (0.375) data 0.000 (0.002) loss 0.6050 (1.0261) lr 9.8907e-03 eta 2:16:45
epoch [3/30] batch [440/796] time 0.334 (0.375) data 0.000 (0.002) loss 0.2808 (1.0297) lr 9.8907e-03 eta 2:16:38
epoch [3/30] batch [460/796] time 0.352 (0.375) data 0.000 (0.002) loss 0.6812 (1.0191) lr 9.8907e-03 eta 2:16:28
epoch [3/30] batch [480/796] time 0.363 (0.375) data 0.000 (0.002) loss 0.2781 (1.0250) lr 9.8907e-03 eta 2:16:21
epoch [3/30] batch [500/796] time 0.374 (0.375) data 0.000 (0.002) loss 0.3350 (1.0150) lr 9.8907e-03 eta 2:16:12
epoch [3/30] batch [520/796] time 0.355 (0.375) data 0.000 (0.002) loss 1.6182 (1.0129) lr 9.8907e-03 eta 2:16:02
epoch [3/30] batch [540/796] time 0.339 (0.375) data 0.000 (0.002) loss 1.9082 (1.0226) lr 9.8907e-03 eta 2:15:52
epoch [3/30] batch [560/796] time 0.371 (0.374) data 0.001 (0.002) loss 1.3857 (1.0309) lr 9.8907e-03 eta 2:15:35
epoch [3/30] batch [580/796] time 0.387 (0.374) data 0.000 (0.002) loss 1.4111 (1.0318) lr 9.8907e-03 eta 2:15:22
epoch [3/30] batch [600/796] time 0.368 (0.374) data 0.000 (0.002) loss 0.7266 (1.0294) lr 9.8907e-03 eta 2:15:10
epoch [3/30] batch [620/796] time 0.359 (0.374) data 0.000 (0.002) loss 0.1486 (1.0240) lr 9.8907e-03 eta 2:15:06
epoch [3/30] batch [640/796] time 0.398 (0.374) data 0.000 (0.002) loss 1.2793 (1.0174) lr 9.8907e-03 eta 2:15:03
epoch [3/30] batch [660/796] time 0.380 (0.374) data 0.000 (0.002) loss 0.1342 (1.0180) lr 9.8907e-03 eta 2:14:57
epoch [3/30] batch [680/796] time 0.380 (0.375) data 0.000 (0.002) loss 1.3096 (1.0158) lr 9.8907e-03 eta 2:14:53
epoch [3/30] batch [700/796] time 0.379 (0.375) data 0.000 (0.002) loss 0.0297 (1.0116) lr 9.8907e-03 eta 2:14:48
epoch [3/30] batch [720/796] time 0.384 (0.375) data 0.000 (0.001) loss 1.5225 (1.0065) lr 9.8907e-03 eta 2:14:44
epoch [3/30] batch [740/796] time 0.346 (0.375) data 0.000 (0.001) loss 0.4512 (1.0050) lr 9.8907e-03 eta 2:14:30
epoch [3/30] batch [760/796] time 0.343 (0.375) data 0.000 (0.001) loss 0.7007 (0.9986) lr 9.8907e-03 eta 2:14:22
epoch [3/30] batch [780/796] time 0.326 (0.373) data 0.000 (0.001) loss 1.7236 (1.0008) lr 9.8907e-03 eta 2:13:53
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<01:55,  6.06s/it] 10%|█         | 2/20 [00:06<00:54,  3.05s/it] 15%|█▌        | 3/20 [00:07<00:30,  1.78s/it] 20%|██        | 4/20 [00:07<00:18,  1.19s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.17it/s] 30%|███       | 6/20 [00:08<00:09,  1.51it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.87it/s] 40%|████      | 8/20 [00:08<00:05,  2.22it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.50it/s] 50%|█████     | 10/20 [00:09<00:03,  2.75it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.02it/s] 60%|██████    | 12/20 [00:09<00:02,  3.26it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.44it/s] 70%|███████   | 14/20 [00:10<00:01,  3.56it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.63it/s] 80%|████████  | 16/20 [00:10<00:01,  3.75it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.91it/s] 90%|█████████ | 18/20 [00:11<00:00,  4.28it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.59it/s]100%|██████████| 20/20 [00:11<00:00,  4.90it/s]100%|██████████| 20/20 [00:11<00:00,  1.71it/s]=> result
* total: 1,990
* correct: 1,526
* accuracy: 76.7%
* error: 23.3%
* macro_f1: 75.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar

epoch [4/30] batch [20/796] time 0.371 (0.425) data 0.000 (0.042) loss 1.2080 (1.0700) lr 9.7553e-03 eta 2:31:57
epoch [4/30] batch [40/796] time 0.385 (0.400) data 0.000 (0.021) loss 0.1514 (0.9477) lr 9.7553e-03 eta 2:22:50
epoch [4/30] batch [60/796] time 0.366 (0.391) data 0.000 (0.014) loss 0.9180 (1.0533) lr 9.7553e-03 eta 2:19:46
epoch [4/30] batch [80/796] time 0.395 (0.388) data 0.000 (0.011) loss 0.4937 (1.0422) lr 9.7553e-03 eta 2:18:33
epoch [4/30] batch [100/796] time 0.377 (0.388) data 0.000 (0.009) loss 1.1523 (1.0943) lr 9.7553e-03 eta 2:18:12
epoch [4/30] batch [120/796] time 0.397 (0.386) data 0.000 (0.007) loss 1.1260 (1.0700) lr 9.7553e-03 eta 2:17:36
epoch [4/30] batch [140/796] time 0.394 (0.385) data 0.000 (0.006) loss 0.8560 (1.0523) lr 9.7553e-03 eta 2:17:07
epoch [4/30] batch [160/796] time 0.372 (0.385) data 0.000 (0.005) loss 0.7441 (1.0437) lr 9.7553e-03 eta 2:16:44
epoch [4/30] batch [180/796] time 0.407 (0.384) data 0.001 (0.005) loss 1.0762 (1.0429) lr 9.7553e-03 eta 2:16:14
epoch [4/30] batch [200/796] time 0.354 (0.382) data 0.000 (0.004) loss 2.7559 (1.0388) lr 9.7553e-03 eta 2:15:34
epoch [4/30] batch [220/796] time 0.342 (0.382) data 0.000 (0.004) loss 0.2954 (1.0182) lr 9.7553e-03 eta 2:15:18
epoch [4/30] batch [240/796] time 0.388 (0.381) data 0.000 (0.004) loss 2.4785 (1.0037) lr 9.7553e-03 eta 2:14:49
epoch [4/30] batch [260/796] time 0.383 (0.380) data 0.000 (0.003) loss 0.1693 (0.9964) lr 9.7553e-03 eta 2:14:33
epoch [4/30] batch [280/796] time 0.371 (0.379) data 0.000 (0.003) loss 1.4648 (0.9926) lr 9.7553e-03 eta 2:14:06
epoch [4/30] batch [300/796] time 0.404 (0.379) data 0.000 (0.003) loss 0.4844 (0.9873) lr 9.7553e-03 eta 2:13:58
epoch [4/30] batch [320/796] time 0.401 (0.379) data 0.000 (0.003) loss 0.9424 (0.9735) lr 9.7553e-03 eta 2:13:38
epoch [4/30] batch [340/796] time 0.375 (0.379) data 0.000 (0.003) loss 0.0715 (0.9579) lr 9.7553e-03 eta 2:13:28
epoch [4/30] batch [360/796] time 0.372 (0.378) data 0.000 (0.003) loss 2.0254 (0.9585) lr 9.7553e-03 eta 2:13:11
epoch [4/30] batch [380/796] time 0.384 (0.378) data 0.000 (0.002) loss 1.0176 (0.9551) lr 9.7553e-03 eta 2:12:55
epoch [4/30] batch [400/796] time 0.366 (0.378) data 0.000 (0.002) loss 1.4932 (0.9540) lr 9.7553e-03 eta 2:12:44
epoch [4/30] batch [420/796] time 0.348 (0.377) data 0.000 (0.002) loss 0.8125 (0.9661) lr 9.7553e-03 eta 2:12:29
epoch [4/30] batch [440/796] time 0.390 (0.377) data 0.000 (0.002) loss 2.6953 (0.9635) lr 9.7553e-03 eta 2:12:17
epoch [4/30] batch [460/796] time 0.367 (0.377) data 0.000 (0.002) loss 0.9189 (0.9566) lr 9.7553e-03 eta 2:12:11
epoch [4/30] batch [480/796] time 0.370 (0.377) data 0.000 (0.002) loss 1.0195 (0.9566) lr 9.7553e-03 eta 2:12:02
epoch [4/30] batch [500/796] time 0.377 (0.377) data 0.000 (0.002) loss 1.0371 (0.9600) lr 9.7553e-03 eta 2:11:54
epoch [4/30] batch [520/796] time 0.376 (0.377) data 0.000 (0.002) loss 1.6904 (0.9647) lr 9.7553e-03 eta 2:11:43
epoch [4/30] batch [540/796] time 0.388 (0.377) data 0.000 (0.002) loss 1.1680 (0.9690) lr 9.7553e-03 eta 2:11:32
epoch [4/30] batch [560/796] time 0.392 (0.377) data 0.000 (0.002) loss 1.3311 (0.9690) lr 9.7553e-03 eta 2:11:22
epoch [4/30] batch [580/796] time 0.389 (0.376) data 0.000 (0.002) loss 0.4792 (0.9736) lr 9.7553e-03 eta 2:11:13
epoch [4/30] batch [600/796] time 0.349 (0.376) data 0.000 (0.002) loss 0.2703 (0.9724) lr 9.7553e-03 eta 2:11:02
epoch [4/30] batch [620/796] time 0.391 (0.376) data 0.000 (0.002) loss 2.5645 (0.9768) lr 9.7553e-03 eta 2:10:48
epoch [4/30] batch [640/796] time 0.372 (0.376) data 0.000 (0.002) loss 0.5615 (0.9730) lr 9.7553e-03 eta 2:10:36
epoch [4/30] batch [660/796] time 0.374 (0.376) data 0.000 (0.002) loss 2.5156 (0.9750) lr 9.7553e-03 eta 2:10:23
epoch [4/30] batch [680/796] time 0.398 (0.376) data 0.000 (0.001) loss 2.3008 (0.9800) lr 9.7553e-03 eta 2:10:17
epoch [4/30] batch [700/796] time 0.392 (0.376) data 0.000 (0.001) loss 2.2012 (0.9927) lr 9.7553e-03 eta 2:10:10
epoch [4/30] batch [720/796] time 0.372 (0.376) data 0.000 (0.001) loss 1.7070 (0.9951) lr 9.7553e-03 eta 2:10:07
epoch [4/30] batch [740/796] time 0.386 (0.376) data 0.000 (0.001) loss 0.6382 (0.9994) lr 9.7553e-03 eta 2:09:55
epoch [4/30] batch [760/796] time 0.382 (0.375) data 0.000 (0.001) loss 1.9727 (1.0043) lr 9.7553e-03 eta 2:09:42
epoch [4/30] batch [780/796] time 0.329 (0.375) data 0.000 (0.001) loss 0.5381 (0.9993) lr 9.7553e-03 eta 2:09:18
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:50,  5.80s/it] 10%|█         | 2/20 [00:06<00:51,  2.83s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.67s/it] 20%|██        | 4/20 [00:07<00:17,  1.12s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.23it/s] 30%|███       | 6/20 [00:07<00:08,  1.58it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.94it/s] 40%|████      | 8/20 [00:08<00:05,  2.28it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.58it/s] 50%|█████     | 10/20 [00:08<00:03,  2.85it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.13it/s] 60%|██████    | 12/20 [00:09<00:02,  3.32it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.49it/s] 70%|███████   | 14/20 [00:09<00:01,  3.62it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.71it/s] 80%|████████  | 16/20 [00:10<00:01,  3.75it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.89it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.26it/s] 95%|█████████▌| 19/20 [00:10<00:00,  4.57it/s]100%|██████████| 20/20 [00:11<00:00,  4.89it/s]100%|██████████| 20/20 [00:11<00:00,  1.78it/s]=> result
* total: 1,990
* correct: 1,525
* accuracy: 76.6%
* error: 23.4%
* macro_f1: 75.6%

epoch [5/30] batch [20/796] time 0.380 (0.429) data 0.000 (0.041) loss 1.0713 (1.0174) lr 9.5677e-03 eta 2:27:57
epoch [5/30] batch [40/796] time 0.420 (0.405) data 0.000 (0.021) loss 0.2334 (1.0580) lr 9.5677e-03 eta 2:19:20
epoch [5/30] batch [60/796] time 0.355 (0.393) data 0.000 (0.014) loss 0.7144 (1.0925) lr 9.5677e-03 eta 2:15:14
epoch [5/30] batch [80/796] time 0.380 (0.386) data 0.000 (0.010) loss 0.3293 (1.0620) lr 9.5677e-03 eta 2:12:37
epoch [5/30] batch [100/796] time 0.372 (0.383) data 0.000 (0.008) loss 0.2988 (1.0324) lr 9.5677e-03 eta 2:11:29
epoch [5/30] batch [120/796] time 0.378 (0.380) data 0.000 (0.007) loss 1.3281 (1.0516) lr 9.5677e-03 eta 2:10:26
epoch [5/30] batch [140/796] time 0.347 (0.379) data 0.000 (0.006) loss 1.4521 (1.0425) lr 9.5677e-03 eta 2:09:41
epoch [5/30] batch [160/796] time 0.339 (0.378) data 0.000 (0.005) loss 0.3508 (1.0374) lr 9.5677e-03 eta 2:09:19
epoch [5/30] batch [180/796] time 0.400 (0.378) data 0.000 (0.005) loss 1.3818 (1.0077) lr 9.5677e-03 eta 2:09:13
epoch [5/30] batch [200/796] time 0.373 (0.378) data 0.000 (0.004) loss 1.1191 (1.0074) lr 9.5677e-03 eta 2:09:01
epoch [5/30] batch [220/796] time 0.384 (0.377) data 0.000 (0.004) loss 0.5029 (1.0178) lr 9.5677e-03 eta 2:08:29
epoch [5/30] batch [240/796] time 0.343 (0.376) data 0.000 (0.004) loss 0.7261 (1.0160) lr 9.5677e-03 eta 2:08:04
epoch [5/30] batch [260/796] time 0.370 (0.375) data 0.000 (0.003) loss 0.5425 (1.0064) lr 9.5677e-03 eta 2:07:53
epoch [5/30] batch [280/796] time 0.354 (0.375) data 0.000 (0.003) loss 2.4219 (0.9927) lr 9.5677e-03 eta 2:07:35
epoch [5/30] batch [300/796] time 0.358 (0.374) data 0.000 (0.003) loss 1.4629 (0.9867) lr 9.5677e-03 eta 2:07:14
epoch [5/30] batch [320/796] time 0.349 (0.374) data 0.000 (0.003) loss 1.6846 (0.9873) lr 9.5677e-03 eta 2:07:04
epoch [5/30] batch [340/796] time 0.366 (0.374) data 0.000 (0.003) loss 0.7681 (0.9852) lr 9.5677e-03 eta 2:06:43
epoch [5/30] batch [360/796] time 0.354 (0.373) data 0.000 (0.003) loss 0.0853 (0.9850) lr 9.5677e-03 eta 2:06:25
epoch [5/30] batch [380/796] time 0.371 (0.373) data 0.000 (0.002) loss 0.0459 (0.9772) lr 9.5677e-03 eta 2:06:13
epoch [5/30] batch [400/796] time 0.372 (0.373) data 0.000 (0.002) loss 0.8481 (0.9647) lr 9.5677e-03 eta 2:06:03
epoch [5/30] batch [420/796] time 0.344 (0.372) data 0.000 (0.002) loss 3.0977 (0.9777) lr 9.5677e-03 eta 2:05:52
epoch [5/30] batch [440/796] time 0.392 (0.372) data 0.000 (0.002) loss 1.7529 (0.9924) lr 9.5677e-03 eta 2:05:41
epoch [5/30] batch [460/796] time 0.378 (0.372) data 0.000 (0.002) loss 1.1445 (1.0006) lr 9.5677e-03 eta 2:05:35
epoch [5/30] batch [480/796] time 0.392 (0.372) data 0.000 (0.002) loss 1.5420 (1.0107) lr 9.5677e-03 eta 2:05:28
epoch [5/30] batch [500/796] time 0.399 (0.373) data 0.000 (0.002) loss 0.7305 (1.0109) lr 9.5677e-03 eta 2:05:27
epoch [5/30] batch [520/796] time 0.376 (0.373) data 0.000 (0.002) loss 0.3057 (1.0143) lr 9.5677e-03 eta 2:05:27
epoch [5/30] batch [540/796] time 0.343 (0.373) data 0.000 (0.002) loss 0.4724 (1.0091) lr 9.5677e-03 eta 2:05:17
epoch [5/30] batch [560/796] time 0.362 (0.373) data 0.000 (0.002) loss 1.1553 (1.0110) lr 9.5677e-03 eta 2:05:13
epoch [5/30] batch [580/796] time 0.384 (0.373) data 0.000 (0.002) loss 2.4004 (1.0118) lr 9.5677e-03 eta 2:05:09
epoch [5/30] batch [600/796] time 0.354 (0.373) data 0.000 (0.002) loss 0.8359 (1.0014) lr 9.5677e-03 eta 2:05:03
epoch [5/30] batch [620/796] time 0.385 (0.373) data 0.000 (0.002) loss 1.3457 (1.0034) lr 9.5677e-03 eta 2:04:57
epoch [5/30] batch [640/796] time 0.375 (0.374) data 0.000 (0.002) loss 1.7178 (1.0103) lr 9.5677e-03 eta 2:04:50
epoch [5/30] batch [660/796] time 0.389 (0.374) data 0.000 (0.001) loss 2.3516 (1.0083) lr 9.5677e-03 eta 2:04:45
epoch [5/30] batch [680/796] time 0.387 (0.373) data 0.000 (0.001) loss 0.6274 (1.0054) lr 9.5677e-03 eta 2:04:33
epoch [5/30] batch [700/796] time 0.336 (0.373) data 0.000 (0.001) loss 0.5122 (1.0071) lr 9.5677e-03 eta 2:04:20
epoch [5/30] batch [720/796] time 0.390 (0.373) data 0.000 (0.001) loss 0.8975 (1.0057) lr 9.5677e-03 eta 2:04:09
epoch [5/30] batch [740/796] time 0.353 (0.373) data 0.000 (0.001) loss 1.3438 (1.0102) lr 9.5677e-03 eta 2:03:58
epoch [5/30] batch [760/796] time 0.374 (0.373) data 0.000 (0.001) loss 1.3604 (1.0162) lr 9.5677e-03 eta 2:03:47
epoch [5/30] batch [780/796] time 0.330 (0.372) data 0.000 (0.001) loss 0.5479 (1.0119) lr 9.5677e-03 eta 2:03:20
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:48,  5.69s/it] 10%|█         | 2/20 [00:07<00:57,  3.19s/it] 15%|█▌        | 3/20 [00:07<00:31,  1.87s/it] 20%|██        | 4/20 [00:07<00:19,  1.24s/it] 25%|██▌       | 5/20 [00:07<00:13,  1.13it/s] 30%|███       | 6/20 [00:08<00:09,  1.47it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.82it/s] 40%|████      | 8/20 [00:08<00:05,  2.17it/s] 45%|████▌     | 9/20 [00:09<00:04,  2.50it/s] 50%|█████     | 10/20 [00:09<00:03,  2.82it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.19it/s] 60%|██████    | 12/20 [00:09<00:02,  3.46it/s] 65%|██████▌   | 13/20 [00:10<00:01,  3.61it/s] 70%|███████   | 14/20 [00:10<00:01,  3.72it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.87it/s] 80%|████████  | 16/20 [00:10<00:00,  4.20it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.52it/s] 90%|█████████ | 18/20 [00:11<00:00,  2.97it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.45it/s]100%|██████████| 20/20 [00:11<00:00,  3.94it/s]100%|██████████| 20/20 [00:11<00:00,  1.67it/s]=> result
* total: 1,990
* correct: 1,529
* accuracy: 76.8%
* error: 23.2%
* macro_f1: 75.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar

epoch [6/30] batch [20/796] time 0.340 (0.423) data 0.001 (0.048) loss 0.3682 (0.7850) lr 9.3301e-03 eta 2:20:11
epoch [6/30] batch [40/796] time 0.344 (0.393) data 0.000 (0.024) loss 0.9639 (0.7627) lr 9.3301e-03 eta 2:10:05
epoch [6/30] batch [60/796] time 0.359 (0.388) data 0.000 (0.016) loss 0.6865 (0.9055) lr 9.3301e-03 eta 2:08:08
epoch [6/30] batch [80/796] time 0.373 (0.384) data 0.000 (0.012) loss 0.2179 (0.8886) lr 9.3301e-03 eta 2:06:52
epoch [6/30] batch [100/796] time 0.387 (0.383) data 0.000 (0.010) loss 0.2966 (0.9227) lr 9.3301e-03 eta 2:06:28
epoch [6/30] batch [120/796] time 0.372 (0.381) data 0.000 (0.008) loss 0.3237 (0.9222) lr 9.3301e-03 eta 2:05:27
epoch [6/30] batch [140/796] time 0.394 (0.379) data 0.000 (0.007) loss 1.7588 (0.9828) lr 9.3301e-03 eta 2:04:50
epoch [6/30] batch [160/796] time 0.381 (0.379) data 0.000 (0.006) loss 1.5234 (1.0052) lr 9.3301e-03 eta 2:04:31
epoch [6/30] batch [180/796] time 0.343 (0.378) data 0.000 (0.006) loss 0.9424 (0.9895) lr 9.3301e-03 eta 2:04:04
epoch [6/30] batch [200/796] time 0.387 (0.377) data 0.000 (0.005) loss 0.5811 (0.9812) lr 9.3301e-03 eta 2:03:54
epoch [6/30] batch [220/796] time 0.389 (0.377) data 0.000 (0.005) loss 0.1769 (0.9436) lr 9.3301e-03 eta 2:03:48
epoch [6/30] batch [240/796] time 0.381 (0.377) data 0.000 (0.004) loss 0.5459 (0.9397) lr 9.3301e-03 eta 2:03:23
epoch [6/30] batch [260/796] time 0.353 (0.376) data 0.000 (0.004) loss 0.5356 (0.9393) lr 9.3301e-03 eta 2:02:58
epoch [6/30] batch [280/796] time 0.380 (0.375) data 0.000 (0.004) loss 0.2749 (0.9250) lr 9.3301e-03 eta 2:02:41
epoch [6/30] batch [300/796] time 0.378 (0.375) data 0.000 (0.003) loss 0.8159 (0.9173) lr 9.3301e-03 eta 2:02:32
epoch [6/30] batch [320/796] time 0.393 (0.375) data 0.000 (0.003) loss 0.8159 (0.9204) lr 9.3301e-03 eta 2:02:23
epoch [6/30] batch [340/796] time 0.380 (0.375) data 0.000 (0.003) loss 0.7485 (0.9223) lr 9.3301e-03 eta 2:02:07
epoch [6/30] batch [360/796] time 0.384 (0.375) data 0.000 (0.003) loss 0.6621 (0.9297) lr 9.3301e-03 eta 2:02:00
epoch [6/30] batch [380/796] time 0.380 (0.375) data 0.000 (0.003) loss 0.3303 (0.9225) lr 9.3301e-03 eta 2:01:52
epoch [6/30] batch [400/796] time 0.339 (0.374) data 0.001 (0.003) loss 1.1562 (0.9466) lr 9.3301e-03 eta 2:01:38
epoch [6/30] batch [420/796] time 0.342 (0.375) data 0.000 (0.003) loss 0.1298 (0.9481) lr 9.3301e-03 eta 2:01:35
epoch [6/30] batch [440/796] time 0.390 (0.374) data 0.000 (0.002) loss 1.3945 (0.9574) lr 9.3301e-03 eta 2:01:24
epoch [6/30] batch [460/796] time 0.388 (0.374) data 0.000 (0.002) loss 0.5698 (0.9552) lr 9.3301e-03 eta 2:01:16
epoch [6/30] batch [480/796] time 0.385 (0.374) data 0.000 (0.002) loss 2.8789 (0.9580) lr 9.3301e-03 eta 2:01:11
epoch [6/30] batch [500/796] time 0.377 (0.375) data 0.000 (0.002) loss 0.2048 (0.9553) lr 9.3301e-03 eta 2:01:05
epoch [6/30] batch [520/796] time 0.387 (0.374) data 0.000 (0.002) loss 1.2012 (0.9489) lr 9.3301e-03 eta 2:00:55
epoch [6/30] batch [540/796] time 0.344 (0.374) data 0.000 (0.002) loss 1.3281 (0.9458) lr 9.3301e-03 eta 2:00:48
epoch [6/30] batch [560/796] time 0.395 (0.374) data 0.000 (0.002) loss 0.4016 (0.9509) lr 9.3301e-03 eta 2:00:40
epoch [6/30] batch [580/796] time 0.383 (0.375) data 0.000 (0.002) loss 0.7866 (0.9511) lr 9.3301e-03 eta 2:00:36
epoch [6/30] batch [600/796] time 0.396 (0.374) data 0.000 (0.002) loss 1.2754 (0.9463) lr 9.3301e-03 eta 2:00:23
epoch [6/30] batch [620/796] time 0.389 (0.374) data 0.000 (0.002) loss 0.4880 (0.9506) lr 9.3301e-03 eta 2:00:19
epoch [6/30] batch [640/796] time 0.397 (0.375) data 0.000 (0.002) loss 1.4404 (0.9514) lr 9.3301e-03 eta 2:00:13
epoch [6/30] batch [660/796] time 0.385 (0.375) data 0.000 (0.002) loss 0.7319 (0.9612) lr 9.3301e-03 eta 2:00:05
epoch [6/30] batch [680/796] time 0.384 (0.374) data 0.000 (0.002) loss 0.8291 (0.9622) lr 9.3301e-03 eta 1:59:57
epoch [6/30] batch [700/796] time 0.387 (0.374) data 0.000 (0.002) loss 3.1641 (0.9642) lr 9.3301e-03 eta 1:59:47
epoch [6/30] batch [720/796] time 0.389 (0.374) data 0.000 (0.002) loss 2.8867 (0.9664) lr 9.3301e-03 eta 1:59:33
epoch [6/30] batch [740/796] time 0.387 (0.374) data 0.000 (0.002) loss 1.8730 (0.9668) lr 9.3301e-03 eta 1:59:26
epoch [6/30] batch [760/796] time 0.380 (0.374) data 0.000 (0.002) loss 0.9355 (0.9673) lr 9.3301e-03 eta 1:59:13
epoch [6/30] batch [780/796] time 0.328 (0.373) data 0.000 (0.001) loss 1.7031 (0.9799) lr 9.3301e-03 eta 1:58:55
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:51,  5.89s/it] 10%|█         | 2/20 [00:06<00:55,  3.08s/it] 15%|█▌        | 3/20 [00:07<00:30,  1.80s/it] 20%|██        | 4/20 [00:07<00:19,  1.20s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.16it/s] 30%|███       | 6/20 [00:08<00:09,  1.50it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.86it/s] 40%|████      | 8/20 [00:08<00:05,  2.22it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.53it/s] 50%|█████     | 10/20 [00:09<00:03,  2.79it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.02it/s] 60%|██████    | 12/20 [00:09<00:02,  3.28it/s] 65%|██████▌   | 13/20 [00:09<00:01,  3.55it/s] 70%|███████   | 14/20 [00:10<00:01,  3.72it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.82it/s] 80%|████████  | 16/20 [00:10<00:01,  3.88it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.89it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.48it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.91it/s]100%|██████████| 20/20 [00:11<00:00,  4.34it/s]100%|██████████| 20/20 [00:11<00:00,  1.70it/s]=> result
* total: 1,990
* correct: 1,529
* accuracy: 76.8%
* error: 23.2%
* macro_f1: 75.8%

epoch [7/30] batch [20/796] time 0.382 (0.423) data 0.000 (0.042) loss 2.1621 (1.0100) lr 9.0451e-03 eta 2:14:24
epoch [7/30] batch [40/796] time 0.352 (0.395) data 0.000 (0.021) loss 0.3430 (0.9530) lr 9.0451e-03 eta 2:05:37
epoch [7/30] batch [60/796] time 0.336 (0.387) data 0.000 (0.014) loss 0.6621 (0.9492) lr 9.0451e-03 eta 2:02:49
epoch [7/30] batch [80/796] time 0.336 (0.384) data 0.000 (0.011) loss 0.8911 (0.9259) lr 9.0451e-03 eta 2:01:48
epoch [7/30] batch [100/796] time 0.386 (0.382) data 0.000 (0.009) loss 1.9639 (0.9274) lr 9.0451e-03 eta 2:01:04
epoch [7/30] batch [120/796] time 0.380 (0.380) data 0.000 (0.007) loss 0.3608 (0.9052) lr 9.0451e-03 eta 2:00:19
epoch [7/30] batch [140/796] time 0.356 (0.380) data 0.000 (0.006) loss 1.2939 (0.9352) lr 9.0451e-03 eta 2:00:03
epoch [7/30] batch [160/796] time 0.368 (0.378) data 0.000 (0.005) loss 0.8047 (0.9413) lr 9.0451e-03 eta 1:59:15
epoch [7/30] batch [180/796] time 0.378 (0.378) data 0.001 (0.005) loss 0.6533 (0.9297) lr 9.0451e-03 eta 1:59:14
epoch [7/30] batch [200/796] time 0.379 (0.378) data 0.000 (0.004) loss 0.4788 (0.9412) lr 9.0451e-03 eta 1:58:58
epoch [7/30] batch [220/796] time 0.393 (0.377) data 0.000 (0.004) loss 1.3633 (0.9414) lr 9.0451e-03 eta 1:58:37
epoch [7/30] batch [240/796] time 0.391 (0.377) data 0.000 (0.004) loss 0.5718 (0.9583) lr 9.0451e-03 eta 1:58:24
epoch [7/30] batch [260/796] time 0.334 (0.376) data 0.000 (0.003) loss 2.1152 (0.9598) lr 9.0451e-03 eta 1:58:08
epoch [7/30] batch [280/796] time 0.338 (0.376) data 0.000 (0.003) loss 0.6582 (0.9442) lr 9.0451e-03 eta 1:57:58
epoch [7/30] batch [300/796] time 0.351 (0.376) data 0.000 (0.003) loss 0.1079 (0.9546) lr 9.0451e-03 eta 1:57:43
epoch [7/30] batch [320/796] time 0.382 (0.376) data 0.000 (0.003) loss 1.3984 (0.9650) lr 9.0451e-03 eta 1:57:34
epoch [7/30] batch [340/796] time 0.380 (0.375) data 0.000 (0.003) loss 0.5205 (0.9458) lr 9.0451e-03 eta 1:57:21
epoch [7/30] batch [360/796] time 0.378 (0.376) data 0.000 (0.003) loss 1.5117 (0.9480) lr 9.0451e-03 eta 1:57:27
epoch [7/30] batch [380/796] time 0.356 (0.376) data 0.000 (0.002) loss 0.5161 (0.9607) lr 9.0451e-03 eta 1:57:27
epoch [7/30] batch [400/796] time 0.338 (0.376) data 0.000 (0.002) loss 0.3115 (0.9514) lr 9.0451e-03 eta 1:57:11
epoch [7/30] batch [420/796] time 0.334 (0.375) data 0.000 (0.002) loss 1.3057 (0.9503) lr 9.0451e-03 eta 1:56:54
epoch [7/30] batch [440/796] time 0.338 (0.375) data 0.000 (0.002) loss 0.7275 (0.9494) lr 9.0451e-03 eta 1:56:41
epoch [7/30] batch [460/796] time 0.374 (0.375) data 0.000 (0.002) loss 1.1777 (0.9412) lr 9.0451e-03 eta 1:56:27
epoch [7/30] batch [480/796] time 0.347 (0.374) data 0.000 (0.002) loss 1.7441 (0.9315) lr 9.0451e-03 eta 1:56:11
epoch [7/30] batch [500/796] time 0.371 (0.374) data 0.000 (0.002) loss 0.9609 (0.9283) lr 9.0451e-03 eta 1:56:03
epoch [7/30] batch [520/796] time 0.381 (0.375) data 0.000 (0.002) loss 1.0576 (0.9255) lr 9.0451e-03 eta 1:56:03
epoch [7/30] batch [540/796] time 0.335 (0.375) data 0.000 (0.002) loss 0.8057 (0.9286) lr 9.0451e-03 eta 1:55:53
epoch [7/30] batch [560/796] time 0.385 (0.374) data 0.000 (0.002) loss 1.4131 (0.9280) lr 9.0451e-03 eta 1:55:42
epoch [7/30] batch [580/796] time 0.373 (0.374) data 0.000 (0.002) loss 0.8325 (0.9293) lr 9.0451e-03 eta 1:55:32
epoch [7/30] batch [600/796] time 0.382 (0.374) data 0.000 (0.002) loss 1.1055 (0.9348) lr 9.0451e-03 eta 1:55:16
epoch [7/30] batch [620/796] time 0.350 (0.374) data 0.000 (0.002) loss 0.5112 (0.9368) lr 9.0451e-03 eta 1:55:10
epoch [7/30] batch [640/796] time 0.391 (0.374) data 0.000 (0.002) loss 0.9165 (0.9407) lr 9.0451e-03 eta 1:55:03
epoch [7/30] batch [660/796] time 0.379 (0.374) data 0.000 (0.002) loss 1.0547 (0.9397) lr 9.0451e-03 eta 1:54:55
epoch [7/30] batch [680/796] time 0.393 (0.374) data 0.000 (0.001) loss 0.2168 (0.9438) lr 9.0451e-03 eta 1:54:48
epoch [7/30] batch [700/796] time 0.378 (0.374) data 0.001 (0.001) loss 0.3545 (0.9402) lr 9.0451e-03 eta 1:54:35
epoch [7/30] batch [720/796] time 0.370 (0.373) data 0.000 (0.001) loss 1.6689 (0.9445) lr 9.0451e-03 eta 1:54:25
epoch [7/30] batch [740/796] time 0.356 (0.373) data 0.000 (0.001) loss 0.0505 (0.9414) lr 9.0451e-03 eta 1:54:14
epoch [7/30] batch [760/796] time 0.364 (0.373) data 0.000 (0.001) loss 1.4863 (0.9435) lr 9.0451e-03 eta 1:54:05
epoch [7/30] batch [780/796] time 0.329 (0.372) data 0.000 (0.001) loss 2.0605 (0.9431) lr 9.0451e-03 eta 1:53:43
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:50,  5.80s/it] 10%|█         | 2/20 [00:06<00:54,  3.01s/it] 15%|█▌        | 3/20 [00:07<00:30,  1.77s/it] 20%|██        | 4/20 [00:07<00:19,  1.19s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.16it/s] 30%|███       | 6/20 [00:08<00:09,  1.51it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.84it/s] 40%|████      | 8/20 [00:08<00:05,  2.17it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.44it/s] 50%|█████     | 10/20 [00:09<00:03,  2.77it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.05it/s] 60%|██████    | 12/20 [00:09<00:02,  3.25it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.39it/s] 70%|███████   | 14/20 [00:10<00:01,  3.69it/s] 75%|███████▌  | 15/20 [00:10<00:01,  4.02it/s] 80%|████████  | 16/20 [00:10<00:01,  3.98it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.00it/s] 90%|█████████ | 18/20 [00:11<00:00,  4.35it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.65it/s]100%|██████████| 20/20 [00:11<00:00,  4.95it/s]100%|██████████| 20/20 [00:11<00:00,  1.74it/s]=> result
* total: 1,990
* correct: 1,540
* accuracy: 77.4%
* error: 22.6%
* macro_f1: 76.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar

epoch [8/30] batch [20/796] time 0.376 (0.420) data 0.000 (0.045) loss 0.2390 (1.0107) lr 8.7157e-03 eta 2:07:53
epoch [8/30] batch [40/796] time 0.351 (0.394) data 0.000 (0.023) loss 2.3320 (0.9374) lr 8.7157e-03 eta 1:59:50
epoch [8/30] batch [60/796] time 0.387 (0.387) data 0.000 (0.015) loss 1.2441 (0.9752) lr 8.7157e-03 eta 1:57:48
epoch [8/30] batch [80/796] time 0.374 (0.385) data 0.000 (0.012) loss 1.7178 (0.9708) lr 8.7157e-03 eta 1:56:56
epoch [8/30] batch [100/796] time 0.348 (0.382) data 0.000 (0.009) loss 1.4688 (0.9938) lr 8.7157e-03 eta 1:55:52
epoch [8/30] batch [120/796] time 0.350 (0.380) data 0.000 (0.008) loss 0.1814 (0.9581) lr 8.7157e-03 eta 1:55:12
epoch [8/30] batch [140/796] time 0.363 (0.379) data 0.000 (0.007) loss 2.7539 (0.9542) lr 8.7157e-03 eta 1:54:39
epoch [8/30] batch [160/796] time 0.396 (0.379) data 0.000 (0.006) loss 1.6113 (0.9554) lr 8.7157e-03 eta 1:54:35
epoch [8/30] batch [180/796] time 0.350 (0.378) data 0.000 (0.005) loss 0.1726 (0.9605) lr 8.7157e-03 eta 1:54:20
epoch [8/30] batch [200/796] time 0.366 (0.378) data 0.000 (0.005) loss 1.0322 (0.9542) lr 8.7157e-03 eta 1:54:08
epoch [8/30] batch [220/796] time 0.387 (0.377) data 0.000 (0.004) loss 1.1914 (0.9664) lr 8.7157e-03 eta 1:53:43
epoch [8/30] batch [240/796] time 0.339 (0.377) data 0.000 (0.004) loss 0.2028 (0.9623) lr 8.7157e-03 eta 1:53:36
epoch [8/30] batch [260/796] time 0.415 (0.377) data 0.000 (0.004) loss 1.0918 (0.9572) lr 8.7157e-03 eta 1:53:24
epoch [8/30] batch [280/796] time 0.356 (0.377) data 0.000 (0.003) loss 0.8438 (0.9703) lr 8.7157e-03 eta 1:53:14
epoch [8/30] batch [300/796] time 0.385 (0.377) data 0.000 (0.003) loss 0.8389 (0.9711) lr 8.7157e-03 eta 1:53:05
epoch [8/30] batch [320/796] time 0.369 (0.376) data 0.000 (0.003) loss 2.2246 (0.9667) lr 8.7157e-03 eta 1:52:49
epoch [8/30] batch [340/796] time 0.349 (0.376) data 0.000 (0.003) loss 1.5215 (0.9688) lr 8.7157e-03 eta 1:52:37
epoch [8/30] batch [360/796] time 0.385 (0.375) data 0.000 (0.003) loss 3.4004 (0.9795) lr 8.7157e-03 eta 1:52:17
epoch [8/30] batch [380/796] time 0.378 (0.375) data 0.000 (0.003) loss 1.0830 (0.9792) lr 8.7157e-03 eta 1:52:03
epoch [8/30] batch [400/796] time 0.381 (0.375) data 0.000 (0.003) loss 1.0098 (0.9757) lr 8.7157e-03 eta 1:51:55
epoch [8/30] batch [420/796] time 0.375 (0.375) data 0.000 (0.002) loss 2.6191 (0.9762) lr 8.7157e-03 eta 1:51:49
epoch [8/30] batch [440/796] time 0.374 (0.375) data 0.000 (0.002) loss 1.4189 (0.9685) lr 8.7157e-03 eta 1:51:36
epoch [8/30] batch [460/796] time 0.337 (0.375) data 0.000 (0.002) loss 0.2690 (0.9648) lr 8.7157e-03 eta 1:51:29
epoch [8/30] batch [480/796] time 0.368 (0.375) data 0.000 (0.002) loss 0.2725 (0.9668) lr 8.7157e-03 eta 1:51:25
epoch [8/30] batch [500/796] time 0.379 (0.375) data 0.000 (0.002) loss 0.7046 (0.9593) lr 8.7157e-03 eta 1:51:22
epoch [8/30] batch [520/796] time 0.339 (0.375) data 0.000 (0.002) loss 2.8848 (0.9607) lr 8.7157e-03 eta 1:51:15
epoch [8/30] batch [540/796] time 0.351 (0.375) data 0.000 (0.002) loss 0.8716 (0.9584) lr 8.7157e-03 eta 1:51:06
epoch [8/30] batch [560/796] time 0.334 (0.375) data 0.000 (0.002) loss 0.3484 (0.9490) lr 8.7157e-03 eta 1:50:53
epoch [8/30] batch [580/796] time 0.347 (0.375) data 0.000 (0.002) loss 3.1465 (0.9514) lr 8.7157e-03 eta 1:50:45
epoch [8/30] batch [600/796] time 0.339 (0.375) data 0.000 (0.002) loss 0.2622 (0.9508) lr 8.7157e-03 eta 1:50:37
epoch [8/30] batch [620/796] time 0.384 (0.375) data 0.000 (0.002) loss 1.1768 (0.9445) lr 8.7157e-03 eta 1:50:29
epoch [8/30] batch [640/796] time 0.364 (0.375) data 0.000 (0.002) loss 0.6221 (0.9454) lr 8.7157e-03 eta 1:50:20
epoch [8/30] batch [660/796] time 0.377 (0.375) data 0.000 (0.002) loss 0.1758 (0.9451) lr 8.7157e-03 eta 1:50:14
epoch [8/30] batch [680/796] time 0.453 (0.375) data 0.000 (0.002) loss 0.6797 (0.9429) lr 8.7157e-03 eta 1:50:11
epoch [8/30] batch [700/796] time 0.396 (0.375) data 0.000 (0.002) loss 0.7856 (0.9404) lr 8.7157e-03 eta 1:50:05
epoch [8/30] batch [720/796] time 0.372 (0.375) data 0.000 (0.002) loss 1.0547 (0.9336) lr 8.7157e-03 eta 1:49:54
epoch [8/30] batch [740/796] time 0.357 (0.375) data 0.000 (0.001) loss 0.9844 (0.9290) lr 8.7157e-03 eta 1:49:44
epoch [8/30] batch [760/796] time 0.411 (0.375) data 0.000 (0.001) loss 2.4961 (0.9346) lr 8.7157e-03 eta 1:49:37
epoch [8/30] batch [780/796] time 0.329 (0.374) data 0.000 (0.001) loss 0.9502 (0.9420) lr 8.7157e-03 eta 1:49:13
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:48,  5.70s/it] 10%|█         | 2/20 [00:06<00:53,  2.98s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.74s/it] 20%|██        | 4/20 [00:07<00:18,  1.16s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.19it/s] 30%|███       | 6/20 [00:07<00:09,  1.54it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.90it/s] 40%|████      | 8/20 [00:08<00:05,  2.24it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.53it/s] 50%|█████     | 10/20 [00:08<00:03,  2.79it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.01it/s] 60%|██████    | 12/20 [00:09<00:02,  3.25it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.49it/s] 70%|███████   | 14/20 [00:09<00:01,  3.61it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.91it/s] 80%|████████  | 16/20 [00:10<00:00,  4.07it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.22it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.10it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.57it/s]100%|██████████| 20/20 [00:11<00:00,  4.04it/s]100%|██████████| 20/20 [00:11<00:00,  1.72it/s]=> result
* total: 1,990
* correct: 1,535
* accuracy: 77.1%
* error: 22.9%
* macro_f1: 76.1%

epoch [9/30] batch [20/796] time 0.336 (0.428) data 0.000 (0.059) loss 1.5215 (1.0780) lr 8.3457e-03 eta 2:04:40
epoch [9/30] batch [40/796] time 0.377 (0.398) data 0.000 (0.029) loss 1.2139 (1.0451) lr 8.3457e-03 eta 1:55:56
epoch [9/30] batch [60/796] time 0.359 (0.389) data 0.000 (0.020) loss 0.2430 (1.0022) lr 8.3457e-03 eta 1:53:06
epoch [9/30] batch [80/796] time 0.335 (0.384) data 0.000 (0.015) loss 0.5781 (0.9607) lr 8.3457e-03 eta 1:51:28
epoch [9/30] batch [100/796] time 0.353 (0.380) data 0.000 (0.012) loss 1.6768 (0.9154) lr 8.3457e-03 eta 1:50:19
epoch [9/30] batch [120/796] time 0.385 (0.379) data 0.000 (0.010) loss 0.2981 (0.9234) lr 8.3457e-03 eta 1:49:43
epoch [9/30] batch [140/796] time 0.392 (0.378) data 0.000 (0.009) loss 0.6816 (0.9198) lr 8.3457e-03 eta 1:49:21
epoch [9/30] batch [160/796] time 0.405 (0.377) data 0.000 (0.008) loss 0.1523 (0.9256) lr 8.3457e-03 eta 1:49:07
epoch [9/30] batch [180/796] time 0.387 (0.377) data 0.000 (0.007) loss 0.7720 (0.9385) lr 8.3457e-03 eta 1:48:50
epoch [9/30] batch [200/796] time 0.371 (0.377) data 0.000 (0.006) loss 1.7197 (0.9521) lr 8.3457e-03 eta 1:48:42
epoch [9/30] batch [220/796] time 0.374 (0.376) data 0.001 (0.006) loss 1.2588 (0.9594) lr 8.3457e-03 eta 1:48:22
epoch [9/30] batch [240/796] time 0.353 (0.376) data 0.000 (0.005) loss 0.3923 (0.9547) lr 8.3457e-03 eta 1:48:20
epoch [9/30] batch [260/796] time 0.342 (0.376) data 0.000 (0.005) loss 0.3540 (0.9413) lr 8.3457e-03 eta 1:47:59
epoch [9/30] batch [280/796] time 0.367 (0.375) data 0.000 (0.004) loss 1.1514 (0.9534) lr 8.3457e-03 eta 1:47:46
epoch [9/30] batch [300/796] time 0.383 (0.375) data 0.000 (0.004) loss 1.7861 (0.9589) lr 8.3457e-03 eta 1:47:37
epoch [9/30] batch [320/796] time 0.399 (0.375) data 0.000 (0.004) loss 1.8418 (0.9416) lr 8.3457e-03 eta 1:47:29
epoch [9/30] batch [340/796] time 0.379 (0.375) data 0.000 (0.004) loss 1.4229 (0.9381) lr 8.3457e-03 eta 1:47:15
epoch [9/30] batch [360/796] time 0.341 (0.374) data 0.000 (0.003) loss 1.4697 (0.9461) lr 8.3457e-03 eta 1:46:57
epoch [9/30] batch [380/796] time 0.375 (0.374) data 0.000 (0.003) loss 0.6885 (0.9505) lr 8.3457e-03 eta 1:46:53
epoch [9/30] batch [400/796] time 0.373 (0.374) data 0.000 (0.003) loss 0.3862 (0.9488) lr 8.3457e-03 eta 1:46:40
epoch [9/30] batch [420/796] time 0.371 (0.374) data 0.000 (0.003) loss 0.7183 (0.9492) lr 8.3457e-03 eta 1:46:38
epoch [9/30] batch [440/796] time 0.360 (0.374) data 0.000 (0.003) loss 1.0664 (0.9462) lr 8.3457e-03 eta 1:46:23
epoch [9/30] batch [460/796] time 0.364 (0.374) data 0.000 (0.003) loss 0.6138 (0.9479) lr 8.3457e-03 eta 1:46:11
epoch [9/30] batch [480/796] time 0.375 (0.374) data 0.000 (0.003) loss 0.6592 (0.9408) lr 8.3457e-03 eta 1:46:02
epoch [9/30] batch [500/796] time 0.350 (0.373) data 0.000 (0.003) loss 0.8936 (0.9495) lr 8.3457e-03 eta 1:45:46
epoch [9/30] batch [520/796] time 0.378 (0.373) data 0.000 (0.003) loss 2.1602 (0.9577) lr 8.3457e-03 eta 1:45:39
epoch [9/30] batch [540/796] time 0.356 (0.373) data 0.000 (0.002) loss 0.5605 (0.9567) lr 8.3457e-03 eta 1:45:35
epoch [9/30] batch [560/796] time 0.365 (0.373) data 0.000 (0.002) loss 0.2854 (0.9514) lr 8.3457e-03 eta 1:45:29
epoch [9/30] batch [580/796] time 0.342 (0.374) data 0.000 (0.002) loss 1.0459 (0.9586) lr 8.3457e-03 eta 1:45:24
epoch [9/30] batch [600/796] time 0.358 (0.374) data 0.000 (0.002) loss 0.6484 (0.9547) lr 8.3457e-03 eta 1:45:16
epoch [9/30] batch [620/796] time 0.364 (0.374) data 0.000 (0.002) loss 1.5703 (0.9560) lr 8.3457e-03 eta 1:45:09
epoch [9/30] batch [640/796] time 0.381 (0.373) data 0.000 (0.002) loss 0.7881 (0.9560) lr 8.3457e-03 eta 1:45:00
epoch [9/30] batch [660/796] time 0.358 (0.373) data 0.000 (0.002) loss 1.9531 (0.9692) lr 8.3457e-03 eta 1:44:51
epoch [9/30] batch [680/796] time 0.339 (0.373) data 0.000 (0.002) loss 1.3564 (0.9734) lr 8.3457e-03 eta 1:44:40
epoch [9/30] batch [700/796] time 0.377 (0.373) data 0.000 (0.002) loss 0.9785 (0.9727) lr 8.3457e-03 eta 1:44:31
epoch [9/30] batch [720/796] time 0.420 (0.373) data 0.000 (0.002) loss 1.7354 (0.9742) lr 8.3457e-03 eta 1:44:23
epoch [9/30] batch [740/796] time 0.348 (0.373) data 0.000 (0.002) loss 0.4438 (0.9780) lr 8.3457e-03 eta 1:44:14
epoch [9/30] batch [760/796] time 0.363 (0.373) data 0.000 (0.002) loss 0.4324 (0.9752) lr 8.3457e-03 eta 1:44:09
epoch [9/30] batch [780/796] time 0.327 (0.372) data 0.000 (0.002) loss 0.6064 (0.9695) lr 8.3457e-03 eta 1:43:46
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:48,  5.69s/it] 10%|█         | 2/20 [00:06<00:51,  2.86s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.68s/it] 20%|██        | 4/20 [00:07<00:18,  1.13s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.22it/s] 30%|███       | 6/20 [00:07<00:08,  1.57it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.92it/s] 40%|████      | 8/20 [00:08<00:05,  2.25it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.55it/s] 50%|█████     | 10/20 [00:08<00:03,  2.80it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.99it/s] 60%|██████    | 12/20 [00:09<00:02,  3.20it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.36it/s] 70%|███████   | 14/20 [00:09<00:01,  3.50it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.63it/s] 80%|████████  | 16/20 [00:10<00:01,  3.74it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.80it/s] 90%|█████████ | 18/20 [00:10<00:00,  3.69it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.00it/s]100%|██████████| 20/20 [00:11<00:00,  4.43it/s]100%|██████████| 20/20 [00:11<00:00,  1.75it/s]=> result
* total: 1,990
* correct: 1,531
* accuracy: 76.9%
* error: 23.1%
* macro_f1: 76.1%

epoch [10/30] batch [20/796] time 0.356 (0.417) data 0.000 (0.041) loss 0.2512 (1.0485) lr 7.9389e-03 eta 1:56:03
epoch [10/30] batch [40/796] time 0.332 (0.391) data 0.000 (0.021) loss 0.2888 (0.9622) lr 7.9389e-03 eta 1:48:45
epoch [10/30] batch [60/796] time 0.456 (0.383) data 0.000 (0.014) loss 0.7236 (0.9147) lr 7.9389e-03 eta 1:46:17
epoch [10/30] batch [80/796] time 0.372 (0.380) data 0.000 (0.011) loss 0.4094 (0.9029) lr 7.9389e-03 eta 1:45:28
epoch [10/30] batch [100/796] time 0.383 (0.377) data 0.000 (0.008) loss 2.2559 (0.8532) lr 7.9389e-03 eta 1:44:26
epoch [10/30] batch [120/796] time 0.363 (0.375) data 0.000 (0.007) loss 1.0020 (0.8932) lr 7.9389e-03 eta 1:43:45
epoch [10/30] batch [140/796] time 0.379 (0.375) data 0.000 (0.006) loss 0.8203 (0.9426) lr 7.9389e-03 eta 1:43:29
epoch [10/30] batch [160/796] time 0.351 (0.374) data 0.000 (0.005) loss 0.6074 (0.9334) lr 7.9389e-03 eta 1:43:08
epoch [10/30] batch [180/796] time 0.361 (0.374) data 0.000 (0.005) loss 0.3997 (0.9498) lr 7.9389e-03 eta 1:43:11
epoch [10/30] batch [200/796] time 0.378 (0.374) data 0.000 (0.004) loss 0.5942 (0.9581) lr 7.9389e-03 eta 1:42:51
epoch [10/30] batch [220/796] time 0.336 (0.373) data 0.000 (0.004) loss 1.2100 (0.9610) lr 7.9389e-03 eta 1:42:38
epoch [10/30] batch [240/796] time 0.362 (0.373) data 0.000 (0.004) loss 1.9658 (0.9503) lr 7.9389e-03 eta 1:42:27
epoch [10/30] batch [260/796] time 0.360 (0.373) data 0.000 (0.003) loss 0.4551 (0.9654) lr 7.9389e-03 eta 1:42:13
epoch [10/30] batch [280/796] time 0.398 (0.373) data 0.000 (0.003) loss 0.6182 (0.9442) lr 7.9389e-03 eta 1:42:10
epoch [10/30] batch [300/796] time 0.388 (0.372) data 0.000 (0.003) loss 0.3564 (0.9426) lr 7.9389e-03 eta 1:41:53
epoch [10/30] batch [320/796] time 0.363 (0.372) data 0.000 (0.003) loss 0.2253 (0.9394) lr 7.9389e-03 eta 1:41:38
epoch [10/30] batch [340/796] time 0.394 (0.373) data 0.000 (0.003) loss 0.9399 (0.9533) lr 7.9389e-03 eta 1:41:40
epoch [10/30] batch [360/796] time 0.342 (0.373) data 0.000 (0.003) loss 1.7402 (0.9548) lr 7.9389e-03 eta 1:41:37
epoch [10/30] batch [380/796] time 0.387 (0.373) data 0.000 (0.002) loss 1.4980 (0.9594) lr 7.9389e-03 eta 1:41:32
epoch [10/30] batch [400/796] time 0.337 (0.373) data 0.000 (0.002) loss 0.4441 (0.9506) lr 7.9389e-03 eta 1:41:21
epoch [10/30] batch [420/796] time 0.345 (0.372) data 0.001 (0.002) loss 0.5864 (0.9552) lr 7.9389e-03 eta 1:41:04
epoch [10/30] batch [440/796] time 0.355 (0.372) data 0.000 (0.002) loss 1.3447 (0.9496) lr 7.9389e-03 eta 1:40:52
epoch [10/30] batch [460/796] time 0.407 (0.372) data 0.000 (0.002) loss 0.2615 (0.9560) lr 7.9389e-03 eta 1:40:50
epoch [10/30] batch [480/796] time 0.360 (0.372) data 0.000 (0.002) loss 0.7383 (0.9529) lr 7.9389e-03 eta 1:40:45
epoch [10/30] batch [500/796] time 0.340 (0.372) data 0.000 (0.002) loss 1.4053 (0.9495) lr 7.9389e-03 eta 1:40:40
epoch [10/30] batch [520/796] time 0.359 (0.372) data 0.000 (0.002) loss 0.7998 (0.9494) lr 7.9389e-03 eta 1:40:30
epoch [10/30] batch [540/796] time 0.378 (0.372) data 0.000 (0.002) loss 2.1953 (0.9455) lr 7.9389e-03 eta 1:40:17
epoch [10/30] batch [560/796] time 0.353 (0.372) data 0.000 (0.002) loss 1.4277 (0.9516) lr 7.9389e-03 eta 1:40:05
epoch [10/30] batch [580/796] time 0.331 (0.372) data 0.000 (0.002) loss 0.4949 (0.9541) lr 7.9389e-03 eta 1:40:02
epoch [10/30] batch [600/796] time 0.351 (0.372) data 0.000 (0.002) loss 0.3950 (0.9576) lr 7.9389e-03 eta 1:39:51
epoch [10/30] batch [620/796] time 0.400 (0.372) data 0.000 (0.002) loss 0.8296 (0.9671) lr 7.9389e-03 eta 1:39:40
epoch [10/30] batch [640/796] time 0.367 (0.372) data 0.000 (0.002) loss 1.5254 (0.9698) lr 7.9389e-03 eta 1:39:33
epoch [10/30] batch [660/796] time 0.348 (0.372) data 0.000 (0.002) loss 1.2363 (0.9677) lr 7.9389e-03 eta 1:39:27
epoch [10/30] batch [680/796] time 0.346 (0.372) data 0.000 (0.001) loss 1.1357 (0.9668) lr 7.9389e-03 eta 1:39:18
epoch [10/30] batch [700/796] time 0.369 (0.371) data 0.000 (0.001) loss 1.6533 (0.9760) lr 7.9389e-03 eta 1:39:08
epoch [10/30] batch [720/796] time 0.372 (0.371) data 0.001 (0.001) loss 0.2898 (0.9766) lr 7.9389e-03 eta 1:38:59
epoch [10/30] batch [740/796] time 0.394 (0.372) data 0.000 (0.001) loss 0.7954 (0.9785) lr 7.9389e-03 eta 1:38:59
epoch [10/30] batch [760/796] time 0.345 (0.372) data 0.000 (0.001) loss 0.4927 (0.9739) lr 7.9389e-03 eta 1:38:52
epoch [10/30] batch [780/796] time 0.325 (0.371) data 0.000 (0.001) loss 1.3945 (0.9807) lr 7.9389e-03 eta 1:38:30
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:50,  5.80s/it] 10%|█         | 2/20 [00:07<00:57,  3.20s/it] 15%|█▌        | 3/20 [00:07<00:31,  1.86s/it] 20%|██        | 4/20 [00:07<00:19,  1.24s/it] 25%|██▌       | 5/20 [00:08<00:13,  1.12it/s] 30%|███       | 6/20 [00:08<00:09,  1.47it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.83it/s] 40%|████      | 8/20 [00:08<00:05,  2.19it/s] 45%|████▌     | 9/20 [00:09<00:04,  2.51it/s] 50%|█████     | 10/20 [00:09<00:03,  2.80it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.06it/s] 60%|██████    | 12/20 [00:09<00:02,  3.34it/s] 65%|██████▌   | 13/20 [00:10<00:01,  3.53it/s] 70%|███████   | 14/20 [00:10<00:01,  3.66it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.89it/s] 80%|████████  | 16/20 [00:10<00:00,  4.20it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.29it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.07it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.54it/s]100%|██████████| 20/20 [00:11<00:00,  4.02it/s]100%|██████████| 20/20 [00:12<00:00,  1.66it/s]=> result
* total: 1,990
* correct: 1,546
* accuracy: 77.7%
* error: 22.3%
* macro_f1: 76.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model.pth.tar-10

epoch [11/30] batch [20/796] time 0.367 (0.419) data 0.000 (0.046) loss 1.0176 (1.2646) lr 7.5000e-03 eta 1:50:55
epoch [11/30] batch [40/796] time 0.414 (0.399) data 0.000 (0.023) loss 1.5918 (1.2481) lr 7.5000e-03 eta 1:45:31
epoch [11/30] batch [60/796] time 0.370 (0.391) data 0.000 (0.015) loss 0.4321 (1.1496) lr 7.5000e-03 eta 1:43:25
epoch [11/30] batch [80/796] time 0.368 (0.385) data 0.000 (0.012) loss 0.6421 (1.0810) lr 7.5000e-03 eta 1:41:32
epoch [11/30] batch [100/796] time 0.380 (0.383) data 0.000 (0.009) loss 1.5332 (1.0943) lr 7.5000e-03 eta 1:40:53
epoch [11/30] batch [120/796] time 0.383 (0.381) data 0.000 (0.008) loss 1.1367 (1.0666) lr 7.5000e-03 eta 1:40:17
epoch [11/30] batch [140/796] time 0.393 (0.379) data 0.000 (0.007) loss 0.2241 (1.0420) lr 7.5000e-03 eta 1:39:48
epoch [11/30] batch [160/796] time 0.363 (0.379) data 0.000 (0.006) loss 0.4404 (1.0149) lr 7.5000e-03 eta 1:39:34
epoch [11/30] batch [180/796] time 0.374 (0.379) data 0.000 (0.005) loss 0.9194 (1.0154) lr 7.5000e-03 eta 1:39:18
epoch [11/30] batch [200/796] time 0.378 (0.378) data 0.000 (0.005) loss 0.1161 (1.0206) lr 7.5000e-03 eta 1:39:01
epoch [11/30] batch [220/796] time 0.386 (0.377) data 0.000 (0.004) loss 1.0410 (1.0115) lr 7.5000e-03 eta 1:38:43
epoch [11/30] batch [240/796] time 0.377 (0.376) data 0.001 (0.004) loss 0.7290 (1.0069) lr 7.5000e-03 eta 1:38:20
epoch [11/30] batch [260/796] time 0.375 (0.375) data 0.000 (0.004) loss 0.1803 (0.9999) lr 7.5000e-03 eta 1:37:50
epoch [11/30] batch [280/796] time 0.379 (0.374) data 0.000 (0.004) loss 0.5308 (1.0201) lr 7.5000e-03 eta 1:37:35
epoch [11/30] batch [300/796] time 0.379 (0.374) data 0.000 (0.003) loss 0.5874 (1.0228) lr 7.5000e-03 eta 1:37:26
epoch [11/30] batch [320/796] time 0.383 (0.374) data 0.000 (0.003) loss 0.7383 (1.0167) lr 7.5000e-03 eta 1:37:18
epoch [11/30] batch [340/796] time 0.386 (0.374) data 0.000 (0.003) loss 0.7139 (1.0125) lr 7.5000e-03 eta 1:37:13
epoch [11/30] batch [360/796] time 0.371 (0.374) data 0.000 (0.003) loss 3.2246 (1.0154) lr 7.5000e-03 eta 1:37:02
epoch [11/30] batch [380/796] time 0.347 (0.374) data 0.000 (0.003) loss 0.7871 (1.0177) lr 7.5000e-03 eta 1:36:50
epoch [11/30] batch [400/796] time 0.384 (0.374) data 0.000 (0.003) loss 0.7480 (1.0095) lr 7.5000e-03 eta 1:36:40
epoch [11/30] batch [420/796] time 0.351 (0.374) data 0.000 (0.002) loss 0.5557 (0.9951) lr 7.5000e-03 eta 1:36:30
epoch [11/30] batch [440/796] time 0.371 (0.373) data 0.000 (0.002) loss 0.3845 (0.9934) lr 7.5000e-03 eta 1:36:21
epoch [11/30] batch [460/796] time 0.376 (0.373) data 0.000 (0.002) loss 1.8740 (0.9951) lr 7.5000e-03 eta 1:36:13
epoch [11/30] batch [480/796] time 0.349 (0.373) data 0.000 (0.002) loss 1.0410 (0.9887) lr 7.5000e-03 eta 1:36:01
epoch [11/30] batch [500/796] time 0.369 (0.373) data 0.000 (0.002) loss 0.2297 (0.9895) lr 7.5000e-03 eta 1:35:47
epoch [11/30] batch [520/796] time 0.339 (0.373) data 0.000 (0.002) loss 0.4944 (0.9812) lr 7.5000e-03 eta 1:35:38
epoch [11/30] batch [540/796] time 0.382 (0.373) data 0.000 (0.002) loss 0.5469 (0.9805) lr 7.5000e-03 eta 1:35:35
epoch [11/30] batch [560/796] time 0.384 (0.373) data 0.000 (0.002) loss 0.6626 (0.9770) lr 7.5000e-03 eta 1:35:27
epoch [11/30] batch [580/796] time 0.353 (0.373) data 0.000 (0.002) loss 0.2646 (0.9757) lr 7.5000e-03 eta 1:35:21
epoch [11/30] batch [600/796] time 0.381 (0.373) data 0.000 (0.002) loss 1.2021 (0.9783) lr 7.5000e-03 eta 1:35:12
epoch [11/30] batch [620/796] time 0.359 (0.373) data 0.000 (0.002) loss 0.1388 (0.9741) lr 7.5000e-03 eta 1:35:02
epoch [11/30] batch [640/796] time 0.352 (0.373) data 0.000 (0.002) loss 0.3550 (0.9730) lr 7.5000e-03 eta 1:34:53
epoch [11/30] batch [660/796] time 0.400 (0.373) data 0.000 (0.002) loss 1.4609 (0.9763) lr 7.5000e-03 eta 1:34:49
epoch [11/30] batch [680/796] time 0.363 (0.373) data 0.000 (0.002) loss 0.8618 (0.9757) lr 7.5000e-03 eta 1:34:41
epoch [11/30] batch [700/796] time 0.337 (0.373) data 0.000 (0.002) loss 1.2910 (0.9710) lr 7.5000e-03 eta 1:34:34
epoch [11/30] batch [720/796] time 0.410 (0.373) data 0.000 (0.002) loss 0.7534 (0.9771) lr 7.5000e-03 eta 1:34:27
epoch [11/30] batch [740/796] time 0.407 (0.373) data 0.000 (0.001) loss 0.7168 (0.9787) lr 7.5000e-03 eta 1:34:14
epoch [11/30] batch [760/796] time 0.375 (0.372) data 0.000 (0.001) loss 2.5586 (0.9802) lr 7.5000e-03 eta 1:34:05
epoch [11/30] batch [780/796] time 0.326 (0.371) data 0.000 (0.001) loss 1.9385 (0.9853) lr 7.5000e-03 eta 1:33:43
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<01:54,  6.02s/it] 10%|█         | 2/20 [00:06<00:49,  2.73s/it] 15%|█▌        | 3/20 [00:06<00:27,  1.62s/it] 20%|██        | 4/20 [00:07<00:17,  1.09s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.25it/s] 30%|███       | 6/20 [00:07<00:08,  1.59it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.92it/s] 40%|████      | 8/20 [00:08<00:05,  2.25it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.56it/s] 50%|█████     | 10/20 [00:08<00:03,  2.83it/s] 55%|█████▌    | 11/20 [00:08<00:02,  3.07it/s] 60%|██████    | 12/20 [00:09<00:02,  3.30it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.46it/s] 70%|███████   | 14/20 [00:09<00:01,  3.62it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.74it/s] 80%|████████  | 16/20 [00:10<00:01,  3.82it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.88it/s] 90%|█████████ | 18/20 [00:10<00:00,  3.90it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.88it/s]100%|██████████| 20/20 [00:11<00:00,  4.05it/s]100%|██████████| 20/20 [00:11<00:00,  1.76it/s]=> result
* total: 1,990
* correct: 1,553
* accuracy: 78.0%
* error: 22.0%
* macro_f1: 77.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar

epoch [12/30] batch [20/796] time 0.374 (0.435) data 0.000 (0.056) loss 0.3489 (1.0369) lr 7.0337e-03 eta 1:49:32
epoch [12/30] batch [40/796] time 0.350 (0.403) data 0.000 (0.028) loss 0.7437 (1.0280) lr 7.0337e-03 eta 1:41:25
epoch [12/30] batch [60/796] time 0.404 (0.393) data 0.000 (0.019) loss 3.0000 (1.0857) lr 7.0337e-03 eta 1:38:37
epoch [12/30] batch [80/796] time 0.371 (0.387) data 0.000 (0.014) loss 1.5547 (1.0496) lr 7.0337e-03 eta 1:36:55
epoch [12/30] batch [100/796] time 0.341 (0.381) data 0.000 (0.011) loss 1.2588 (1.0311) lr 7.0337e-03 eta 1:35:26
epoch [12/30] batch [120/796] time 0.374 (0.380) data 0.000 (0.009) loss 0.8843 (1.0143) lr 7.0337e-03 eta 1:35:01
epoch [12/30] batch [140/796] time 0.390 (0.378) data 0.000 (0.008) loss 1.3965 (1.0011) lr 7.0337e-03 eta 1:34:21
epoch [12/30] batch [160/796] time 0.357 (0.377) data 0.000 (0.007) loss 1.6445 (1.0153) lr 7.0337e-03 eta 1:33:56
epoch [12/30] batch [180/796] time 0.384 (0.376) data 0.000 (0.006) loss 1.8320 (0.9904) lr 7.0337e-03 eta 1:33:39
epoch [12/30] batch [200/796] time 0.389 (0.376) data 0.001 (0.006) loss 1.2031 (1.0062) lr 7.0337e-03 eta 1:33:24
epoch [12/30] batch [220/796] time 0.375 (0.375) data 0.000 (0.005) loss 0.8149 (1.0033) lr 7.0337e-03 eta 1:33:14
epoch [12/30] batch [240/796] time 0.377 (0.374) data 0.000 (0.005) loss 0.2349 (0.9995) lr 7.0337e-03 eta 1:32:50
epoch [12/30] batch [260/796] time 0.389 (0.375) data 0.000 (0.005) loss 1.2949 (1.0015) lr 7.0337e-03 eta 1:32:52
epoch [12/30] batch [280/796] time 0.333 (0.374) data 0.000 (0.004) loss 1.7383 (0.9870) lr 7.0337e-03 eta 1:32:30
epoch [12/30] batch [300/796] time 0.372 (0.374) data 0.000 (0.004) loss 0.1686 (0.9884) lr 7.0337e-03 eta 1:32:21
epoch [12/30] batch [320/796] time 0.345 (0.374) data 0.000 (0.004) loss 1.6602 (0.9939) lr 7.0337e-03 eta 1:32:14
epoch [12/30] batch [340/796] time 0.386 (0.374) data 0.000 (0.004) loss 3.5273 (0.9922) lr 7.0337e-03 eta 1:32:02
epoch [12/30] batch [360/796] time 0.340 (0.373) data 0.000 (0.003) loss 0.4741 (0.9830) lr 7.0337e-03 eta 1:31:47
epoch [12/30] batch [380/796] time 0.353 (0.373) data 0.000 (0.003) loss 2.2969 (0.9938) lr 7.0337e-03 eta 1:31:33
epoch [12/30] batch [400/796] time 0.398 (0.373) data 0.000 (0.003) loss 0.9834 (1.0023) lr 7.0337e-03 eta 1:31:27
epoch [12/30] batch [420/796] time 0.371 (0.373) data 0.000 (0.003) loss 0.4529 (0.9866) lr 7.0337e-03 eta 1:31:19
epoch [12/30] batch [440/796] time 0.385 (0.372) data 0.000 (0.003) loss 1.1777 (0.9823) lr 7.0337e-03 eta 1:31:07
epoch [12/30] batch [460/796] time 0.380 (0.372) data 0.000 (0.003) loss 0.8374 (0.9795) lr 7.0337e-03 eta 1:31:01
epoch [12/30] batch [480/796] time 0.369 (0.372) data 0.000 (0.003) loss 1.3271 (0.9777) lr 7.0337e-03 eta 1:30:54
epoch [12/30] batch [500/796] time 0.373 (0.372) data 0.000 (0.002) loss 0.2915 (0.9739) lr 7.0337e-03 eta 1:30:46
epoch [12/30] batch [520/796] time 0.388 (0.373) data 0.001 (0.002) loss 0.8159 (0.9730) lr 7.0337e-03 eta 1:30:41
epoch [12/30] batch [540/796] time 0.380 (0.373) data 0.000 (0.002) loss 1.5508 (0.9814) lr 7.0337e-03 eta 1:30:33
epoch [12/30] batch [560/796] time 0.372 (0.372) data 0.000 (0.002) loss 0.8140 (0.9729) lr 7.0337e-03 eta 1:30:21
epoch [12/30] batch [580/796] time 0.401 (0.372) data 0.000 (0.002) loss 0.6938 (0.9712) lr 7.0337e-03 eta 1:30:10
epoch [12/30] batch [600/796] time 0.336 (0.372) data 0.000 (0.002) loss 1.2520 (0.9623) lr 7.0337e-03 eta 1:30:03
epoch [12/30] batch [620/796] time 0.358 (0.372) data 0.000 (0.002) loss 1.2598 (0.9579) lr 7.0337e-03 eta 1:29:51
epoch [12/30] batch [640/796] time 0.347 (0.371) data 0.000 (0.002) loss 0.6187 (0.9552) lr 7.0337e-03 eta 1:29:40
epoch [12/30] batch [660/796] time 0.384 (0.371) data 0.000 (0.002) loss 2.0762 (0.9597) lr 7.0337e-03 eta 1:29:29
epoch [12/30] batch [680/796] time 0.345 (0.371) data 0.000 (0.002) loss 0.3669 (0.9615) lr 7.0337e-03 eta 1:29:21
epoch [12/30] batch [700/796] time 0.385 (0.371) data 0.000 (0.002) loss 2.0078 (0.9685) lr 7.0337e-03 eta 1:29:11
epoch [12/30] batch [720/796] time 0.387 (0.371) data 0.000 (0.002) loss 1.0469 (0.9675) lr 7.0337e-03 eta 1:29:01
epoch [12/30] batch [740/796] time 0.346 (0.371) data 0.000 (0.002) loss 0.7949 (0.9660) lr 7.0337e-03 eta 1:28:52
epoch [12/30] batch [760/796] time 0.349 (0.371) data 0.000 (0.002) loss 0.3159 (0.9596) lr 7.0337e-03 eta 1:28:46
epoch [12/30] batch [780/796] time 0.328 (0.370) data 0.000 (0.002) loss 1.1426 (0.9560) lr 7.0337e-03 eta 1:28:25
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:43,  5.45s/it] 10%|█         | 2/20 [00:06<00:54,  3.00s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.76s/it] 20%|██        | 4/20 [00:07<00:18,  1.17s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.18it/s] 30%|███       | 6/20 [00:07<00:09,  1.54it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.89it/s] 40%|████      | 8/20 [00:08<00:05,  2.22it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.54it/s] 50%|█████     | 10/20 [00:08<00:03,  2.79it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.06it/s] 60%|██████    | 12/20 [00:09<00:02,  3.27it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.41it/s] 70%|███████   | 14/20 [00:09<00:01,  3.53it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.66it/s] 80%|████████  | 16/20 [00:10<00:01,  3.71it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.77it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.79it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.86it/s]100%|██████████| 20/20 [00:11<00:00,  3.96it/s]100%|██████████| 20/20 [00:11<00:00,  1.72it/s]=> result
* total: 1,990
* correct: 1,552
* accuracy: 78.0%
* error: 22.0%
* macro_f1: 77.1%

epoch [13/30] batch [20/796] time 0.377 (0.422) data 0.000 (0.040) loss 0.2200 (1.0157) lr 6.5451e-03 eta 1:40:41
epoch [13/30] batch [40/796] time 0.377 (0.394) data 0.000 (0.020) loss 0.8481 (0.9019) lr 6.5451e-03 eta 1:33:45
epoch [13/30] batch [60/796] time 0.385 (0.385) data 0.000 (0.014) loss 1.1426 (0.9470) lr 6.5451e-03 eta 1:31:38
epoch [13/30] batch [80/796] time 0.357 (0.382) data 0.000 (0.010) loss 0.4980 (0.8869) lr 6.5451e-03 eta 1:30:44
epoch [13/30] batch [100/796] time 0.364 (0.381) data 0.000 (0.008) loss 0.8208 (0.8626) lr 6.5451e-03 eta 1:30:18
epoch [13/30] batch [120/796] time 0.344 (0.379) data 0.000 (0.007) loss 2.2168 (0.8972) lr 6.5451e-03 eta 1:29:47
epoch [13/30] batch [140/796] time 0.373 (0.380) data 0.000 (0.006) loss 0.1869 (0.9082) lr 6.5451e-03 eta 1:29:48
epoch [13/30] batch [160/796] time 0.370 (0.379) data 0.000 (0.005) loss 1.4795 (0.9071) lr 6.5451e-03 eta 1:29:24
epoch [13/30] batch [180/796] time 0.374 (0.378) data 0.000 (0.005) loss 0.4700 (0.8936) lr 6.5451e-03 eta 1:29:07
epoch [13/30] batch [200/796] time 0.358 (0.378) data 0.000 (0.004) loss 0.8467 (0.9095) lr 6.5451e-03 eta 1:28:57
epoch [13/30] batch [220/796] time 0.355 (0.377) data 0.000 (0.004) loss 0.6274 (0.8944) lr 6.5451e-03 eta 1:28:38
epoch [13/30] batch [240/796] time 0.380 (0.377) data 0.000 (0.004) loss 1.8623 (0.8995) lr 6.5451e-03 eta 1:28:28
epoch [13/30] batch [260/796] time 0.383 (0.376) data 0.000 (0.003) loss 0.9990 (0.9148) lr 6.5451e-03 eta 1:28:15
epoch [13/30] batch [280/796] time 0.346 (0.376) data 0.000 (0.003) loss 0.8755 (0.9165) lr 6.5451e-03 eta 1:28:01
epoch [13/30] batch [300/796] time 0.377 (0.375) data 0.000 (0.003) loss 0.2466 (0.9104) lr 6.5451e-03 eta 1:27:46
epoch [13/30] batch [320/796] time 0.353 (0.375) data 0.000 (0.003) loss 0.6450 (0.9060) lr 6.5451e-03 eta 1:27:38
epoch [13/30] batch [340/796] time 0.376 (0.375) data 0.000 (0.003) loss 1.2227 (0.8988) lr 6.5451e-03 eta 1:27:27
epoch [13/30] batch [360/796] time 0.392 (0.375) data 0.000 (0.002) loss 0.8247 (0.9099) lr 6.5451e-03 eta 1:27:18
epoch [13/30] batch [380/796] time 0.369 (0.375) data 0.000 (0.002) loss 0.6753 (0.9200) lr 6.5451e-03 eta 1:27:11
epoch [13/30] batch [400/796] time 0.368 (0.375) data 0.000 (0.002) loss 0.5122 (0.9163) lr 6.5451e-03 eta 1:26:59
epoch [13/30] batch [420/796] time 0.352 (0.374) data 0.000 (0.002) loss 0.8174 (0.9079) lr 6.5451e-03 eta 1:26:42
epoch [13/30] batch [440/796] time 0.357 (0.374) data 0.000 (0.002) loss 1.2510 (0.9063) lr 6.5451e-03 eta 1:26:34
epoch [13/30] batch [460/796] time 0.374 (0.374) data 0.000 (0.002) loss 0.2727 (0.8954) lr 6.5451e-03 eta 1:26:22
epoch [13/30] batch [480/796] time 0.366 (0.374) data 0.000 (0.002) loss 0.3347 (0.8875) lr 6.5451e-03 eta 1:26:14
epoch [13/30] batch [500/796] time 0.389 (0.373) data 0.000 (0.002) loss 0.6841 (0.8911) lr 6.5451e-03 eta 1:26:04
epoch [13/30] batch [520/796] time 0.332 (0.373) data 0.000 (0.002) loss 0.3105 (0.8962) lr 6.5451e-03 eta 1:25:51
epoch [13/30] batch [540/796] time 0.352 (0.373) data 0.000 (0.002) loss 0.4893 (0.8979) lr 6.5451e-03 eta 1:25:41
epoch [13/30] batch [560/796] time 0.400 (0.373) data 0.000 (0.002) loss 1.1523 (0.8967) lr 6.5451e-03 eta 1:25:34
epoch [13/30] batch [580/796] time 0.387 (0.373) data 0.000 (0.002) loss 0.7700 (0.9062) lr 6.5451e-03 eta 1:25:30
epoch [13/30] batch [600/796] time 0.388 (0.373) data 0.000 (0.002) loss 1.2529 (0.9046) lr 6.5451e-03 eta 1:25:22
epoch [13/30] batch [620/796] time 0.370 (0.373) data 0.000 (0.002) loss 0.4009 (0.9022) lr 6.5451e-03 eta 1:25:11
epoch [13/30] batch [640/796] time 0.364 (0.373) data 0.000 (0.002) loss 1.0547 (0.8985) lr 6.5451e-03 eta 1:25:03
epoch [13/30] batch [660/796] time 0.353 (0.373) data 0.000 (0.001) loss 0.2646 (0.8989) lr 6.5451e-03 eta 1:24:54
epoch [13/30] batch [680/796] time 0.340 (0.373) data 0.000 (0.001) loss 0.6270 (0.8943) lr 6.5451e-03 eta 1:24:47
epoch [13/30] batch [700/796] time 0.389 (0.373) data 0.000 (0.001) loss 0.3447 (0.9004) lr 6.5451e-03 eta 1:24:43
epoch [13/30] batch [720/796] time 0.355 (0.373) data 0.000 (0.001) loss 0.4141 (0.8971) lr 6.5451e-03 eta 1:24:37
epoch [13/30] batch [740/796] time 0.336 (0.373) data 0.001 (0.001) loss 1.1924 (0.8984) lr 6.5451e-03 eta 1:24:27
epoch [13/30] batch [760/796] time 0.378 (0.373) data 0.000 (0.001) loss 1.5371 (0.8940) lr 6.5451e-03 eta 1:24:21
epoch [13/30] batch [780/796] time 0.332 (0.372) data 0.000 (0.001) loss 1.8750 (0.8955) lr 6.5451e-03 eta 1:24:03
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:44,  5.48s/it] 10%|█         | 2/20 [00:06<00:53,  2.96s/it] 15%|█▌        | 3/20 [00:06<00:29,  1.73s/it] 20%|██        | 4/20 [00:07<00:18,  1.15s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.19it/s] 30%|███       | 6/20 [00:07<00:09,  1.54it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.89it/s] 40%|████      | 8/20 [00:08<00:05,  2.23it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.54it/s] 50%|█████     | 10/20 [00:08<00:03,  2.80it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.01it/s] 60%|██████    | 12/20 [00:09<00:02,  3.22it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.41it/s] 70%|███████   | 14/20 [00:09<00:01,  3.60it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.83it/s] 80%|████████  | 16/20 [00:10<00:01,  3.88it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.90it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.24it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.56it/s]100%|██████████| 20/20 [00:11<00:00,  4.88it/s]100%|██████████| 20/20 [00:11<00:00,  1.77it/s]=> result
* total: 1,990
* correct: 1,562
* accuracy: 78.5%
* error: 21.5%
* macro_f1: 77.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar

epoch [14/30] batch [20/796] time 0.357 (0.427) data 0.000 (0.040) loss 0.3066 (0.8165) lr 6.0396e-03 eta 1:36:05
epoch [14/30] batch [40/796] time 0.384 (0.399) data 0.000 (0.020) loss 0.5903 (0.9332) lr 6.0396e-03 eta 1:29:49
epoch [14/30] batch [60/796] time 0.359 (0.386) data 0.000 (0.013) loss 1.3984 (0.9510) lr 6.0396e-03 eta 1:26:39
epoch [14/30] batch [80/796] time 0.344 (0.379) data 0.000 (0.010) loss 1.0518 (0.9591) lr 6.0396e-03 eta 1:25:00
epoch [14/30] batch [100/796] time 0.352 (0.378) data 0.000 (0.008) loss 2.2578 (1.0023) lr 6.0396e-03 eta 1:24:40
epoch [14/30] batch [120/796] time 0.340 (0.376) data 0.000 (0.007) loss 2.9453 (1.0011) lr 6.0396e-03 eta 1:24:06
epoch [14/30] batch [140/796] time 0.404 (0.376) data 0.000 (0.006) loss 0.3591 (0.9398) lr 6.0396e-03 eta 1:24:01
epoch [14/30] batch [160/796] time 0.337 (0.376) data 0.000 (0.005) loss 1.5723 (0.9380) lr 6.0396e-03 eta 1:23:48
epoch [14/30] batch [180/796] time 0.371 (0.375) data 0.000 (0.005) loss 0.2966 (0.9455) lr 6.0396e-03 eta 1:23:32
epoch [14/30] batch [200/796] time 0.349 (0.375) data 0.000 (0.004) loss 0.4424 (0.9482) lr 6.0396e-03 eta 1:23:21
epoch [14/30] batch [220/796] time 0.351 (0.375) data 0.000 (0.004) loss 0.4580 (0.9428) lr 6.0396e-03 eta 1:23:06
epoch [14/30] batch [240/796] time 0.350 (0.375) data 0.000 (0.004) loss 0.2708 (0.9378) lr 6.0396e-03 eta 1:23:00
epoch [14/30] batch [260/796] time 0.401 (0.375) data 0.000 (0.003) loss 1.8047 (0.9267) lr 6.0396e-03 eta 1:23:00
epoch [14/30] batch [280/796] time 0.377 (0.375) data 0.000 (0.003) loss 0.7876 (0.9358) lr 6.0396e-03 eta 1:22:45
epoch [14/30] batch [300/796] time 0.376 (0.375) data 0.000 (0.003) loss 0.5796 (0.9398) lr 6.0396e-03 eta 1:22:38
epoch [14/30] batch [320/796] time 0.354 (0.375) data 0.000 (0.003) loss 1.5713 (0.9340) lr 6.0396e-03 eta 1:22:29
epoch [14/30] batch [340/796] time 0.360 (0.374) data 0.000 (0.003) loss 2.2363 (0.9414) lr 6.0396e-03 eta 1:22:15
epoch [14/30] batch [360/796] time 0.397 (0.374) data 0.000 (0.002) loss 1.1738 (0.9387) lr 6.0396e-03 eta 1:22:09
epoch [14/30] batch [380/796] time 0.364 (0.375) data 0.000 (0.002) loss 0.7686 (0.9258) lr 6.0396e-03 eta 1:22:06
epoch [14/30] batch [400/796] time 0.366 (0.374) data 0.000 (0.002) loss 0.4939 (0.9209) lr 6.0396e-03 eta 1:21:54
epoch [14/30] batch [420/796] time 0.379 (0.375) data 0.000 (0.002) loss 1.3320 (0.9195) lr 6.0396e-03 eta 1:21:53
epoch [14/30] batch [440/796] time 0.347 (0.375) data 0.000 (0.002) loss 1.6855 (0.9298) lr 6.0396e-03 eta 1:21:43
epoch [14/30] batch [460/796] time 0.378 (0.374) data 0.000 (0.002) loss 1.0576 (0.9210) lr 6.0396e-03 eta 1:21:32
epoch [14/30] batch [480/796] time 0.367 (0.374) data 0.000 (0.002) loss 0.2776 (0.9239) lr 6.0396e-03 eta 1:21:25
epoch [14/30] batch [500/796] time 0.382 (0.374) data 0.000 (0.002) loss 1.6631 (0.9250) lr 6.0396e-03 eta 1:21:15
epoch [14/30] batch [520/796] time 0.387 (0.374) data 0.000 (0.002) loss 0.6631 (0.9269) lr 6.0396e-03 eta 1:21:11
epoch [14/30] batch [540/796] time 0.375 (0.374) data 0.000 (0.002) loss 1.1621 (0.9262) lr 6.0396e-03 eta 1:21:00
epoch [14/30] batch [560/796] time 0.375 (0.374) data 0.000 (0.002) loss 0.4695 (0.9210) lr 6.0396e-03 eta 1:20:48
epoch [14/30] batch [580/796] time 0.375 (0.374) data 0.000 (0.002) loss 0.8867 (0.9087) lr 6.0396e-03 eta 1:20:39
epoch [14/30] batch [600/796] time 0.363 (0.374) data 0.000 (0.002) loss 0.8877 (0.9122) lr 6.0396e-03 eta 1:20:31
epoch [14/30] batch [620/796] time 0.377 (0.374) data 0.000 (0.002) loss 0.4885 (0.9148) lr 6.0396e-03 eta 1:20:25
epoch [14/30] batch [640/796] time 0.379 (0.373) data 0.000 (0.002) loss 0.9805 (0.9194) lr 6.0396e-03 eta 1:20:13
epoch [14/30] batch [660/796] time 0.371 (0.373) data 0.000 (0.001) loss 0.9087 (0.9204) lr 6.0396e-03 eta 1:20:03
epoch [14/30] batch [680/796] time 0.383 (0.373) data 0.000 (0.001) loss 0.8145 (0.9175) lr 6.0396e-03 eta 1:19:53
epoch [14/30] batch [700/796] time 0.370 (0.373) data 0.000 (0.001) loss 0.3628 (0.9144) lr 6.0396e-03 eta 1:19:43
epoch [14/30] batch [720/796] time 0.355 (0.373) data 0.000 (0.001) loss 0.4797 (0.9088) lr 6.0396e-03 eta 1:19:34
epoch [14/30] batch [740/796] time 0.354 (0.373) data 0.000 (0.001) loss 1.5635 (0.9166) lr 6.0396e-03 eta 1:19:28
epoch [14/30] batch [760/796] time 0.397 (0.373) data 0.000 (0.001) loss 0.5981 (0.9147) lr 6.0396e-03 eta 1:19:22
epoch [14/30] batch [780/796] time 0.326 (0.372) data 0.000 (0.001) loss 1.0566 (0.9186) lr 6.0396e-03 eta 1:19:02
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:42,  5.37s/it] 10%|█         | 2/20 [00:06<00:53,  2.99s/it] 15%|█▌        | 3/20 [00:06<00:29,  1.75s/it] 20%|██        | 4/20 [00:07<00:18,  1.17s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.18it/s] 30%|███       | 6/20 [00:07<00:09,  1.53it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.88it/s] 40%|████      | 8/20 [00:08<00:05,  2.21it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.51it/s] 50%|█████     | 10/20 [00:08<00:03,  2.79it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.06it/s] 60%|██████    | 12/20 [00:09<00:02,  3.27it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.46it/s] 70%|███████   | 14/20 [00:09<00:01,  3.61it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.77it/s] 80%|████████  | 16/20 [00:10<00:01,  3.85it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.05it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.38it/s] 95%|█████████▌| 19/20 [00:10<00:00,  4.67it/s]100%|██████████| 20/20 [00:11<00:00,  4.97it/s]100%|██████████| 20/20 [00:11<00:00,  1.77it/s]=> result
* total: 1,990
* correct: 1,552
* accuracy: 78.0%
* error: 22.0%
* macro_f1: 77.2%

epoch [15/30] batch [20/796] time 0.356 (0.433) data 0.000 (0.044) loss 0.6943 (0.8793) lr 5.5226e-03 eta 1:31:41
epoch [15/30] batch [40/796] time 0.343 (0.403) data 0.000 (0.022) loss 0.3188 (0.8443) lr 5.5226e-03 eta 1:25:22
epoch [15/30] batch [60/796] time 0.373 (0.396) data 0.000 (0.015) loss 0.5586 (0.8883) lr 5.5226e-03 eta 1:23:33
epoch [15/30] batch [80/796] time 0.362 (0.391) data 0.000 (0.011) loss 0.5034 (0.8865) lr 5.5226e-03 eta 1:22:26
epoch [15/30] batch [100/796] time 0.340 (0.389) data 0.000 (0.009) loss 2.1328 (0.9257) lr 5.5226e-03 eta 1:21:51
epoch [15/30] batch [120/796] time 0.358 (0.387) data 0.000 (0.008) loss 0.3777 (0.9060) lr 5.5226e-03 eta 1:21:20
epoch [15/30] batch [140/796] time 0.389 (0.384) data 0.000 (0.006) loss 0.5947 (0.9122) lr 5.5226e-03 eta 1:20:38
epoch [15/30] batch [160/796] time 0.334 (0.383) data 0.000 (0.006) loss 0.9062 (0.9090) lr 5.5226e-03 eta 1:20:17
epoch [15/30] batch [180/796] time 0.382 (0.382) data 0.000 (0.005) loss 0.7544 (0.9058) lr 5.5226e-03 eta 1:19:53
epoch [15/30] batch [200/796] time 0.412 (0.381) data 0.000 (0.005) loss 1.2920 (0.8893) lr 5.5226e-03 eta 1:19:42
epoch [15/30] batch [220/796] time 0.391 (0.382) data 0.000 (0.004) loss 0.2379 (0.8808) lr 5.5226e-03 eta 1:19:41
epoch [15/30] batch [240/796] time 0.342 (0.381) data 0.000 (0.004) loss 0.9673 (0.8810) lr 5.5226e-03 eta 1:19:27
epoch [15/30] batch [260/796] time 0.385 (0.381) data 0.000 (0.004) loss 1.5752 (0.8780) lr 5.5226e-03 eta 1:19:08
epoch [15/30] batch [280/796] time 0.407 (0.380) data 0.000 (0.003) loss 0.8032 (0.8791) lr 5.5226e-03 eta 1:18:55
epoch [15/30] batch [300/796] time 0.386 (0.379) data 0.000 (0.003) loss 0.7446 (0.8734) lr 5.5226e-03 eta 1:18:34
epoch [15/30] batch [320/796] time 0.348 (0.378) data 0.000 (0.003) loss 0.8628 (0.8705) lr 5.5226e-03 eta 1:18:18
epoch [15/30] batch [340/796] time 0.365 (0.378) data 0.000 (0.003) loss 0.9424 (0.8728) lr 5.5226e-03 eta 1:18:03
epoch [15/30] batch [360/796] time 0.383 (0.378) data 0.000 (0.003) loss 0.8750 (0.8760) lr 5.5226e-03 eta 1:17:52
epoch [15/30] batch [380/796] time 0.385 (0.377) data 0.000 (0.003) loss 1.6836 (0.8921) lr 5.5226e-03 eta 1:17:40
epoch [15/30] batch [400/796] time 0.336 (0.377) data 0.000 (0.002) loss 0.4150 (0.8836) lr 5.5226e-03 eta 1:17:33
epoch [15/30] batch [420/796] time 0.359 (0.377) data 0.000 (0.002) loss 0.4939 (0.8717) lr 5.5226e-03 eta 1:17:19
epoch [15/30] batch [440/796] time 0.481 (0.377) data 0.000 (0.002) loss 0.3174 (0.8624) lr 5.5226e-03 eta 1:17:09
epoch [15/30] batch [460/796] time 0.369 (0.376) data 0.000 (0.002) loss 0.9321 (0.8667) lr 5.5226e-03 eta 1:16:58
epoch [15/30] batch [480/796] time 0.388 (0.376) data 0.000 (0.002) loss 1.1250 (0.8753) lr 5.5226e-03 eta 1:16:47
epoch [15/30] batch [500/796] time 0.350 (0.376) data 0.000 (0.002) loss 0.6670 (0.8783) lr 5.5226e-03 eta 1:16:39
epoch [15/30] batch [520/796] time 0.369 (0.376) data 0.000 (0.002) loss 0.8892 (0.8825) lr 5.5226e-03 eta 1:16:30
epoch [15/30] batch [540/796] time 0.344 (0.376) data 0.000 (0.002) loss 0.5596 (0.8825) lr 5.5226e-03 eta 1:16:20
epoch [15/30] batch [560/796] time 0.340 (0.375) data 0.000 (0.002) loss 0.7393 (0.8894) lr 5.5226e-03 eta 1:16:11
epoch [15/30] batch [580/796] time 0.337 (0.375) data 0.000 (0.002) loss 0.9541 (0.8866) lr 5.5226e-03 eta 1:16:02
epoch [15/30] batch [600/796] time 0.388 (0.375) data 0.000 (0.002) loss 0.6440 (0.8931) lr 5.5226e-03 eta 1:15:50
epoch [15/30] batch [620/796] time 0.390 (0.375) data 0.000 (0.002) loss 0.7783 (0.8885) lr 5.5226e-03 eta 1:15:41
epoch [15/30] batch [640/796] time 0.366 (0.374) data 0.000 (0.002) loss 0.3850 (0.8928) lr 5.5226e-03 eta 1:15:29
epoch [15/30] batch [660/796] time 0.388 (0.375) data 0.000 (0.002) loss 0.9829 (0.8874) lr 5.5226e-03 eta 1:15:22
epoch [15/30] batch [680/796] time 0.354 (0.375) data 0.000 (0.002) loss 0.7329 (0.8854) lr 5.5226e-03 eta 1:15:15
epoch [15/30] batch [700/796] time 0.376 (0.374) data 0.000 (0.002) loss 0.3472 (0.8858) lr 5.5226e-03 eta 1:15:06
epoch [15/30] batch [720/796] time 0.340 (0.374) data 0.000 (0.001) loss 1.0068 (0.8881) lr 5.5226e-03 eta 1:14:59
epoch [15/30] batch [740/796] time 0.353 (0.374) data 0.000 (0.001) loss 0.3948 (0.8944) lr 5.5226e-03 eta 1:14:51
epoch [15/30] batch [760/796] time 0.391 (0.374) data 0.000 (0.001) loss 0.9814 (0.8969) lr 5.5226e-03 eta 1:14:41
epoch [15/30] batch [780/796] time 0.328 (0.373) data 0.000 (0.001) loss 0.5928 (0.8911) lr 5.5226e-03 eta 1:14:22
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:48,  5.72s/it] 10%|█         | 2/20 [00:06<00:53,  2.96s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.74s/it] 20%|██        | 4/20 [00:07<00:18,  1.17s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.18it/s] 30%|███       | 6/20 [00:07<00:09,  1.53it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.88it/s] 40%|████      | 8/20 [00:08<00:05,  2.21it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.46it/s] 50%|█████     | 10/20 [00:09<00:03,  2.77it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.05it/s] 60%|██████    | 12/20 [00:09<00:02,  3.27it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.45it/s] 70%|███████   | 14/20 [00:10<00:01,  3.56it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.85it/s] 80%|████████  | 16/20 [00:10<00:00,  4.16it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.08it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.29it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.59it/s]100%|██████████| 20/20 [00:11<00:00,  4.91it/s]100%|██████████| 20/20 [00:11<00:00,  1.75it/s]=> result
* total: 1,990
* correct: 1,562
* accuracy: 78.5%
* error: 21.5%
* macro_f1: 77.7%

epoch [16/30] batch [20/796] time 0.365 (0.440) data 0.000 (0.050) loss 0.4763 (1.0977) lr 5.0000e-03 eta 1:27:28
epoch [16/30] batch [40/796] time 0.366 (0.404) data 0.000 (0.025) loss 0.3210 (0.9658) lr 5.0000e-03 eta 1:20:10
epoch [16/30] batch [60/796] time 0.363 (0.393) data 0.000 (0.017) loss 3.1035 (1.0352) lr 5.0000e-03 eta 1:17:46
epoch [16/30] batch [80/796] time 0.344 (0.387) data 0.000 (0.013) loss 0.5845 (0.9915) lr 5.0000e-03 eta 1:16:26
epoch [16/30] batch [100/796] time 0.390 (0.383) data 0.000 (0.010) loss 0.6699 (0.9504) lr 5.0000e-03 eta 1:15:30
epoch [16/30] batch [120/796] time 0.373 (0.381) data 0.001 (0.009) loss 0.7427 (0.9502) lr 5.0000e-03 eta 1:15:05
epoch [16/30] batch [140/796] time 0.385 (0.380) data 0.000 (0.007) loss 0.8740 (0.9347) lr 5.0000e-03 eta 1:14:42
epoch [16/30] batch [160/796] time 0.364 (0.379) data 0.000 (0.006) loss 0.4709 (0.9170) lr 5.0000e-03 eta 1:14:24
epoch [16/30] batch [180/796] time 0.348 (0.378) data 0.000 (0.006) loss 1.8584 (0.8998) lr 5.0000e-03 eta 1:14:01
epoch [16/30] batch [200/796] time 0.358 (0.377) data 0.000 (0.005) loss 0.7847 (0.9010) lr 5.0000e-03 eta 1:13:45
epoch [16/30] batch [220/796] time 0.340 (0.376) data 0.000 (0.005) loss 1.0215 (0.8856) lr 5.0000e-03 eta 1:13:22
epoch [16/30] batch [240/796] time 0.393 (0.375) data 0.001 (0.004) loss 0.6221 (0.8969) lr 5.0000e-03 eta 1:13:12
epoch [16/30] batch [260/796] time 0.382 (0.375) data 0.000 (0.004) loss 1.2031 (0.8994) lr 5.0000e-03 eta 1:13:00
epoch [16/30] batch [280/796] time 0.370 (0.375) data 0.000 (0.004) loss 0.4712 (0.8891) lr 5.0000e-03 eta 1:12:47
epoch [16/30] batch [300/796] time 0.349 (0.375) data 0.000 (0.004) loss 0.2214 (0.8833) lr 5.0000e-03 eta 1:12:41
epoch [16/30] batch [320/796] time 0.366 (0.374) data 0.000 (0.003) loss 2.4844 (0.8870) lr 5.0000e-03 eta 1:12:29
epoch [16/30] batch [340/796] time 0.368 (0.375) data 0.000 (0.003) loss 1.3232 (0.8841) lr 5.0000e-03 eta 1:12:26
epoch [16/30] batch [360/796] time 0.339 (0.374) data 0.000 (0.003) loss 0.7280 (0.9039) lr 5.0000e-03 eta 1:12:11
epoch [16/30] batch [380/796] time 0.382 (0.374) data 0.000 (0.003) loss 0.2751 (0.9001) lr 5.0000e-03 eta 1:12:04
epoch [16/30] batch [400/796] time 0.341 (0.374) data 0.000 (0.003) loss 0.7925 (0.9078) lr 5.0000e-03 eta 1:11:53
epoch [16/30] batch [420/796] time 0.369 (0.374) data 0.000 (0.003) loss 0.8374 (0.9130) lr 5.0000e-03 eta 1:11:46
epoch [16/30] batch [440/796] time 0.370 (0.374) data 0.000 (0.003) loss 0.4324 (0.9122) lr 5.0000e-03 eta 1:11:38
epoch [16/30] batch [460/796] time 0.385 (0.373) data 0.000 (0.002) loss 0.9810 (0.9123) lr 5.0000e-03 eta 1:11:27
epoch [16/30] batch [480/796] time 0.406 (0.374) data 0.001 (0.002) loss 0.1218 (0.9176) lr 5.0000e-03 eta 1:11:23
epoch [16/30] batch [500/796] time 0.371 (0.373) data 0.000 (0.002) loss 0.6284 (0.9201) lr 5.0000e-03 eta 1:11:11
epoch [16/30] batch [520/796] time 0.378 (0.373) data 0.000 (0.002) loss 0.7920 (0.9295) lr 5.0000e-03 eta 1:11:04
epoch [16/30] batch [540/796] time 0.395 (0.374) data 0.000 (0.002) loss 0.2622 (0.9182) lr 5.0000e-03 eta 1:11:02
epoch [16/30] batch [560/796] time 0.395 (0.374) data 0.000 (0.002) loss 1.5781 (0.9221) lr 5.0000e-03 eta 1:10:55
epoch [16/30] batch [580/796] time 0.387 (0.374) data 0.000 (0.002) loss 0.6636 (0.9155) lr 5.0000e-03 eta 1:10:48
epoch [16/30] batch [600/796] time 0.388 (0.374) data 0.000 (0.002) loss 0.3198 (0.9167) lr 5.0000e-03 eta 1:10:42
epoch [16/30] batch [620/796] time 0.386 (0.374) data 0.001 (0.002) loss 1.6221 (0.9242) lr 5.0000e-03 eta 1:10:33
epoch [16/30] batch [640/796] time 0.352 (0.374) data 0.000 (0.002) loss 0.6230 (0.9205) lr 5.0000e-03 eta 1:10:26
epoch [16/30] batch [660/796] time 0.374 (0.374) data 0.000 (0.002) loss 1.4844 (0.9202) lr 5.0000e-03 eta 1:10:22
epoch [16/30] batch [680/796] time 0.332 (0.374) data 0.000 (0.002) loss 0.6348 (0.9173) lr 5.0000e-03 eta 1:10:13
epoch [16/30] batch [700/796] time 0.371 (0.374) data 0.000 (0.002) loss 0.2306 (0.9147) lr 5.0000e-03 eta 1:10:07
epoch [16/30] batch [720/796] time 0.338 (0.374) data 0.000 (0.002) loss 0.4458 (0.9145) lr 5.0000e-03 eta 1:09:58
epoch [16/30] batch [740/796] time 0.392 (0.374) data 0.000 (0.002) loss 2.0566 (0.9143) lr 5.0000e-03 eta 1:09:51
epoch [16/30] batch [760/796] time 0.345 (0.374) data 0.000 (0.002) loss 0.9189 (0.9073) lr 5.0000e-03 eta 1:09:41
epoch [16/30] batch [780/796] time 0.328 (0.373) data 0.000 (0.002) loss 0.1575 (0.9100) lr 5.0000e-03 eta 1:09:23
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:46,  5.62s/it] 10%|█         | 2/20 [00:06<00:52,  2.94s/it] 15%|█▌        | 3/20 [00:06<00:29,  1.72s/it] 20%|██        | 4/20 [00:07<00:18,  1.16s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.19it/s] 30%|███       | 6/20 [00:07<00:09,  1.54it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.88it/s] 40%|████      | 8/20 [00:08<00:05,  2.20it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.48it/s] 50%|█████     | 10/20 [00:08<00:03,  2.76it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.98it/s] 60%|██████    | 12/20 [00:09<00:02,  3.16it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.30it/s] 70%|███████   | 14/20 [00:10<00:01,  3.46it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.57it/s] 80%|████████  | 16/20 [00:10<00:01,  3.66it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.69it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.09it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.43it/s]100%|██████████| 20/20 [00:11<00:00,  4.78it/s]100%|██████████| 20/20 [00:11<00:00,  1.74it/s]=> result
* total: 1,990
* correct: 1,568
* accuracy: 78.8%
* error: 21.2%
* macro_f1: 78.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar

epoch [17/30] batch [20/796] time 0.387 (0.427) data 0.000 (0.045) loss 0.2036 (0.7758) lr 4.4774e-03 eta 1:19:07
epoch [17/30] batch [40/796] time 0.379 (0.405) data 0.000 (0.022) loss 0.7573 (0.8415) lr 4.4774e-03 eta 1:14:55
epoch [17/30] batch [60/796] time 0.385 (0.388) data 0.000 (0.015) loss 0.3738 (0.8334) lr 4.4774e-03 eta 1:11:45
epoch [17/30] batch [80/796] time 0.374 (0.383) data 0.000 (0.011) loss 1.0518 (0.8098) lr 4.4774e-03 eta 1:10:40
epoch [17/30] batch [100/796] time 0.383 (0.380) data 0.000 (0.009) loss 0.3188 (0.8404) lr 4.4774e-03 eta 1:10:01
epoch [17/30] batch [120/796] time 0.333 (0.378) data 0.000 (0.008) loss 0.9253 (0.8939) lr 4.4774e-03 eta 1:09:27
epoch [17/30] batch [140/796] time 0.378 (0.376) data 0.000 (0.007) loss 0.5156 (0.9069) lr 4.4774e-03 eta 1:08:55
epoch [17/30] batch [160/796] time 0.336 (0.375) data 0.000 (0.006) loss 0.7334 (0.9119) lr 4.4774e-03 eta 1:08:40
epoch [17/30] batch [180/796] time 0.375 (0.375) data 0.000 (0.005) loss 0.9277 (0.9061) lr 4.4774e-03 eta 1:08:34
epoch [17/30] batch [200/796] time 0.370 (0.374) data 0.000 (0.005) loss 0.3257 (0.9151) lr 4.4774e-03 eta 1:08:13
epoch [17/30] batch [220/796] time 0.340 (0.374) data 0.000 (0.004) loss 0.8174 (0.9023) lr 4.4774e-03 eta 1:08:02
epoch [17/30] batch [240/796] time 0.391 (0.374) data 0.000 (0.004) loss 0.2266 (0.9115) lr 4.4774e-03 eta 1:07:53
epoch [17/30] batch [260/796] time 0.335 (0.372) data 0.000 (0.004) loss 0.4583 (0.9085) lr 4.4774e-03 eta 1:07:30
epoch [17/30] batch [280/796] time 0.397 (0.372) data 0.000 (0.003) loss 0.2125 (0.9153) lr 4.4774e-03 eta 1:07:21
epoch [17/30] batch [300/796] time 0.405 (0.372) data 0.000 (0.003) loss 0.3640 (0.9255) lr 4.4774e-03 eta 1:07:13
epoch [17/30] batch [320/796] time 0.346 (0.372) data 0.001 (0.003) loss 0.3499 (0.9252) lr 4.4774e-03 eta 1:07:04
epoch [17/30] batch [340/796] time 0.382 (0.372) data 0.000 (0.003) loss 0.4236 (0.9283) lr 4.4774e-03 eta 1:06:54
epoch [17/30] batch [360/796] time 0.382 (0.371) data 0.000 (0.003) loss 0.5586 (0.9242) lr 4.4774e-03 eta 1:06:45
epoch [17/30] batch [380/796] time 0.380 (0.372) data 0.000 (0.003) loss 0.9067 (0.9162) lr 4.4774e-03 eta 1:06:38
epoch [17/30] batch [400/796] time 0.394 (0.371) data 0.000 (0.002) loss 0.5396 (0.9162) lr 4.4774e-03 eta 1:06:31
epoch [17/30] batch [420/796] time 0.349 (0.372) data 0.000 (0.002) loss 0.4744 (0.9051) lr 4.4774e-03 eta 1:06:24
epoch [17/30] batch [440/796] time 0.364 (0.371) data 0.000 (0.002) loss 1.1113 (0.9017) lr 4.4774e-03 eta 1:06:14
epoch [17/30] batch [460/796] time 0.361 (0.371) data 0.000 (0.002) loss 0.9707 (0.9036) lr 4.4774e-03 eta 1:06:03
epoch [17/30] batch [480/796] time 0.384 (0.371) data 0.000 (0.002) loss 0.7964 (0.9000) lr 4.4774e-03 eta 1:05:57
epoch [17/30] batch [500/796] time 0.384 (0.371) data 0.001 (0.002) loss 0.6421 (0.9026) lr 4.4774e-03 eta 1:05:50
epoch [17/30] batch [520/796] time 0.359 (0.371) data 0.000 (0.002) loss 0.7271 (0.9088) lr 4.4774e-03 eta 1:05:40
epoch [17/30] batch [540/796] time 0.391 (0.371) data 0.000 (0.002) loss 2.9375 (0.9196) lr 4.4774e-03 eta 1:05:33
epoch [17/30] batch [560/796] time 0.386 (0.371) data 0.000 (0.002) loss 0.8013 (0.9202) lr 4.4774e-03 eta 1:05:25
epoch [17/30] batch [580/796] time 0.339 (0.371) data 0.000 (0.002) loss 1.3877 (0.9153) lr 4.4774e-03 eta 1:05:16
epoch [17/30] batch [600/796] time 0.401 (0.371) data 0.000 (0.002) loss 1.0615 (0.9147) lr 4.4774e-03 eta 1:05:10
epoch [17/30] batch [620/796] time 0.386 (0.371) data 0.000 (0.002) loss 0.1421 (0.9154) lr 4.4774e-03 eta 1:05:02
epoch [17/30] batch [640/796] time 0.391 (0.371) data 0.000 (0.002) loss 0.5210 (0.9115) lr 4.4774e-03 eta 1:04:56
epoch [17/30] batch [660/796] time 0.394 (0.371) data 0.000 (0.002) loss 0.1164 (0.9063) lr 4.4774e-03 eta 1:04:50
epoch [17/30] batch [680/796] time 0.381 (0.371) data 0.000 (0.002) loss 1.2109 (0.9101) lr 4.4774e-03 eta 1:04:43
epoch [17/30] batch [700/796] time 0.386 (0.371) data 0.000 (0.002) loss 0.2313 (0.9067) lr 4.4774e-03 eta 1:04:36
epoch [17/30] batch [720/796] time 0.375 (0.371) data 0.000 (0.002) loss 0.3281 (0.9057) lr 4.4774e-03 eta 1:04:27
epoch [17/30] batch [740/796] time 0.364 (0.371) data 0.000 (0.001) loss 0.4399 (0.8982) lr 4.4774e-03 eta 1:04:19
epoch [17/30] batch [760/796] time 0.358 (0.371) data 0.000 (0.001) loss 0.9995 (0.8931) lr 4.4774e-03 eta 1:04:09
epoch [17/30] batch [780/796] time 0.328 (0.370) data 0.000 (0.001) loss 0.6719 (0.8924) lr 4.4774e-03 eta 1:03:53
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:39,  5.25s/it] 10%|█         | 2/20 [00:06<00:55,  3.07s/it] 15%|█▌        | 3/20 [00:07<00:30,  1.80s/it] 20%|██        | 4/20 [00:07<00:19,  1.21s/it] 25%|██▌       | 5/20 [00:07<00:13,  1.15it/s] 30%|███       | 6/20 [00:07<00:09,  1.50it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.85it/s] 40%|████      | 8/20 [00:08<00:05,  2.15it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.43it/s] 50%|█████     | 10/20 [00:09<00:03,  2.72it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.98it/s] 60%|██████    | 12/20 [00:09<00:02,  3.24it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.50it/s] 70%|███████   | 14/20 [00:10<00:01,  3.83it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.85it/s] 80%|████████  | 16/20 [00:10<00:01,  3.87it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.88it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.07it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.54it/s]100%|██████████| 20/20 [00:11<00:00,  4.02it/s]100%|██████████| 20/20 [00:11<00:00,  1.70it/s]=> result
* total: 1,990
* correct: 1,572
* accuracy: 79.0%
* error: 21.0%
* macro_f1: 78.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar

epoch [18/30] batch [20/796] time 0.380 (0.424) data 0.000 (0.040) loss 0.5791 (0.7607) lr 3.9604e-03 eta 1:12:55
epoch [18/30] batch [40/796] time 0.337 (0.399) data 0.000 (0.020) loss 0.8330 (0.8079) lr 3.9604e-03 eta 1:08:34
epoch [18/30] batch [60/796] time 0.379 (0.388) data 0.000 (0.014) loss 0.3384 (0.8622) lr 3.9604e-03 eta 1:06:30
epoch [18/30] batch [80/796] time 0.338 (0.380) data 0.000 (0.010) loss 1.1875 (0.8561) lr 3.9604e-03 eta 1:05:01
epoch [18/30] batch [100/796] time 0.386 (0.379) data 0.000 (0.008) loss 1.0479 (0.8324) lr 3.9604e-03 eta 1:04:42
epoch [18/30] batch [120/796] time 0.372 (0.377) data 0.000 (0.007) loss 0.3369 (0.8349) lr 3.9604e-03 eta 1:04:15
epoch [18/30] batch [140/796] time 0.346 (0.376) data 0.000 (0.006) loss 0.7632 (0.8474) lr 3.9604e-03 eta 1:03:56
epoch [18/30] batch [160/796] time 0.342 (0.375) data 0.000 (0.005) loss 0.4221 (0.8429) lr 3.9604e-03 eta 1:03:36
epoch [18/30] batch [180/796] time 0.382 (0.375) data 0.000 (0.005) loss 0.6758 (0.8304) lr 3.9604e-03 eta 1:03:27
epoch [18/30] batch [200/796] time 0.373 (0.375) data 0.000 (0.004) loss 0.7178 (0.8354) lr 3.9604e-03 eta 1:03:21
epoch [18/30] batch [220/796] time 0.379 (0.375) data 0.000 (0.004) loss 0.4636 (0.8264) lr 3.9604e-03 eta 1:03:15
epoch [18/30] batch [240/796] time 0.372 (0.374) data 0.000 (0.004) loss 0.1807 (0.8304) lr 3.9604e-03 eta 1:02:56
epoch [18/30] batch [260/796] time 0.349 (0.373) data 0.000 (0.003) loss 0.4761 (0.8343) lr 3.9604e-03 eta 1:02:45
epoch [18/30] batch [280/796] time 0.388 (0.373) data 0.000 (0.003) loss 0.4939 (0.8474) lr 3.9604e-03 eta 1:02:37
epoch [18/30] batch [300/796] time 0.379 (0.373) data 0.000 (0.003) loss 0.4590 (0.8431) lr 3.9604e-03 eta 1:02:26
epoch [18/30] batch [320/796] time 0.385 (0.373) data 0.000 (0.003) loss 0.6382 (0.8541) lr 3.9604e-03 eta 1:02:17
epoch [18/30] batch [340/796] time 0.343 (0.373) data 0.000 (0.003) loss 1.2197 (0.8585) lr 3.9604e-03 eta 1:02:11
epoch [18/30] batch [360/796] time 0.385 (0.373) data 0.000 (0.002) loss 0.3293 (0.8557) lr 3.9604e-03 eta 1:02:05
epoch [18/30] batch [380/796] time 0.362 (0.373) data 0.000 (0.002) loss 1.0518 (0.8499) lr 3.9604e-03 eta 1:01:53
epoch [18/30] batch [400/796] time 0.337 (0.372) data 0.000 (0.002) loss 1.5186 (0.8513) lr 3.9604e-03 eta 1:01:43
epoch [18/30] batch [420/796] time 0.394 (0.372) data 0.000 (0.002) loss 0.5068 (0.8533) lr 3.9604e-03 eta 1:01:37
epoch [18/30] batch [440/796] time 0.360 (0.372) data 0.000 (0.002) loss 1.3857 (0.8658) lr 3.9604e-03 eta 1:01:28
epoch [18/30] batch [460/796] time 0.340 (0.372) data 0.000 (0.002) loss 1.0488 (0.8648) lr 3.9604e-03 eta 1:01:19
epoch [18/30] batch [480/796] time 0.378 (0.372) data 0.000 (0.002) loss 0.1830 (0.8709) lr 3.9604e-03 eta 1:01:09
epoch [18/30] batch [500/796] time 0.361 (0.372) data 0.000 (0.002) loss 0.3066 (0.8750) lr 3.9604e-03 eta 1:01:07
epoch [18/30] batch [520/796] time 0.335 (0.373) data 0.000 (0.002) loss 2.2402 (0.8766) lr 3.9604e-03 eta 1:01:02
epoch [18/30] batch [540/796] time 0.371 (0.373) data 0.000 (0.002) loss 0.1237 (0.8760) lr 3.9604e-03 eta 1:00:56
epoch [18/30] batch [560/796] time 0.354 (0.373) data 0.000 (0.002) loss 0.8467 (0.8862) lr 3.9604e-03 eta 1:00:49
epoch [18/30] batch [580/796] time 0.380 (0.373) data 0.000 (0.002) loss 1.3018 (0.8857) lr 3.9604e-03 eta 1:00:38
epoch [18/30] batch [600/796] time 0.380 (0.372) data 0.000 (0.002) loss 2.7988 (0.8887) lr 3.9604e-03 eta 1:00:30
epoch [18/30] batch [620/796] time 0.362 (0.373) data 0.000 (0.002) loss 0.8745 (0.8945) lr 3.9604e-03 eta 1:00:23
epoch [18/30] batch [640/796] time 0.359 (0.373) data 0.000 (0.001) loss 1.1777 (0.8994) lr 3.9604e-03 eta 1:00:16
epoch [18/30] batch [660/796] time 0.395 (0.372) data 0.000 (0.001) loss 0.2163 (0.9012) lr 3.9604e-03 eta 1:00:07
epoch [18/30] batch [680/796] time 0.360 (0.372) data 0.000 (0.001) loss 1.6104 (0.9016) lr 3.9604e-03 eta 0:59:59
epoch [18/30] batch [700/796] time 0.371 (0.372) data 0.000 (0.001) loss 1.9697 (0.9051) lr 3.9604e-03 eta 0:59:52
epoch [18/30] batch [720/796] time 0.381 (0.372) data 0.000 (0.001) loss 0.2888 (0.8987) lr 3.9604e-03 eta 0:59:44
epoch [18/30] batch [740/796] time 0.381 (0.372) data 0.000 (0.001) loss 0.7041 (0.8949) lr 3.9604e-03 eta 0:59:34
epoch [18/30] batch [760/796] time 0.350 (0.372) data 0.000 (0.001) loss 0.7793 (0.9021) lr 3.9604e-03 eta 0:59:23
epoch [18/30] batch [780/796] time 0.336 (0.371) data 0.000 (0.001) loss 1.8867 (0.8991) lr 3.9604e-03 eta 0:59:08
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:51,  5.88s/it] 10%|█         | 2/20 [00:06<00:50,  2.82s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.66s/it] 20%|██        | 4/20 [00:07<00:17,  1.11s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.24it/s] 30%|███       | 6/20 [00:07<00:08,  1.60it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.96it/s] 40%|████      | 8/20 [00:08<00:05,  2.29it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.60it/s] 50%|█████     | 10/20 [00:08<00:03,  2.89it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.07it/s] 60%|██████    | 12/20 [00:09<00:02,  3.23it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.43it/s] 70%|███████   | 14/20 [00:09<00:01,  3.59it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.71it/s] 80%|████████  | 16/20 [00:10<00:01,  3.74it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.82it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.13it/s] 95%|█████████▌| 19/20 [00:10<00:00,  4.47it/s]100%|██████████| 20/20 [00:11<00:00,  4.81it/s]100%|██████████| 20/20 [00:11<00:00,  1.78it/s]=> result
* total: 1,990
* correct: 1,576
* accuracy: 79.2%
* error: 20.8%
* macro_f1: 78.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar

epoch [19/30] batch [20/796] time 0.377 (0.425) data 0.000 (0.050) loss 0.4211 (0.9558) lr 3.4549e-03 eta 1:07:30
epoch [19/30] batch [40/796] time 0.396 (0.400) data 0.000 (0.025) loss 2.0176 (0.9130) lr 3.4549e-03 eta 1:03:28
epoch [19/30] batch [60/796] time 0.357 (0.392) data 0.000 (0.017) loss 0.6118 (0.9546) lr 3.4549e-03 eta 1:01:58
epoch [19/30] batch [80/796] time 0.393 (0.385) data 0.000 (0.013) loss 0.1747 (0.9472) lr 3.4549e-03 eta 1:00:47
epoch [19/30] batch [100/796] time 0.339 (0.381) data 0.000 (0.010) loss 2.1953 (0.9336) lr 3.4549e-03 eta 1:00:01
epoch [19/30] batch [120/796] time 0.398 (0.380) data 0.000 (0.008) loss 0.5405 (0.9606) lr 3.4549e-03 eta 0:59:39
epoch [19/30] batch [140/796] time 0.360 (0.378) data 0.000 (0.007) loss 2.0508 (0.9303) lr 3.4549e-03 eta 0:59:14
epoch [19/30] batch [160/796] time 0.380 (0.377) data 0.000 (0.006) loss 1.2422 (0.9128) lr 3.4549e-03 eta 0:58:57
epoch [19/30] batch [180/796] time 0.377 (0.376) data 0.000 (0.006) loss 0.1833 (0.9083) lr 3.4549e-03 eta 0:58:47
epoch [19/30] batch [200/796] time 0.334 (0.377) data 0.000 (0.005) loss 0.3750 (0.9097) lr 3.4549e-03 eta 0:58:44
epoch [19/30] batch [220/796] time 0.346 (0.375) data 0.000 (0.005) loss 0.6797 (0.9093) lr 3.4549e-03 eta 0:58:22
epoch [19/30] batch [240/796] time 0.413 (0.375) data 0.000 (0.004) loss 2.6367 (0.9132) lr 3.4549e-03 eta 0:58:15
epoch [19/30] batch [260/796] time 0.369 (0.375) data 0.000 (0.004) loss 0.2532 (0.9195) lr 3.4549e-03 eta 0:58:06
epoch [19/30] batch [280/796] time 0.368 (0.375) data 0.000 (0.004) loss 0.6699 (0.9045) lr 3.4549e-03 eta 0:57:53
epoch [19/30] batch [300/796] time 0.341 (0.374) data 0.000 (0.004) loss 0.5054 (0.9023) lr 3.4549e-03 eta 0:57:43
epoch [19/30] batch [320/796] time 0.367 (0.375) data 0.000 (0.003) loss 1.3779 (0.8937) lr 3.4549e-03 eta 0:57:37
epoch [19/30] batch [340/796] time 0.378 (0.375) data 0.000 (0.003) loss 0.1588 (0.8896) lr 3.4549e-03 eta 0:57:30
epoch [19/30] batch [360/796] time 0.356 (0.374) data 0.000 (0.003) loss 0.8335 (0.8826) lr 3.4549e-03 eta 0:57:18
epoch [19/30] batch [380/796] time 0.365 (0.374) data 0.000 (0.003) loss 0.6294 (0.8970) lr 3.4549e-03 eta 0:57:11
epoch [19/30] batch [400/796] time 0.382 (0.374) data 0.000 (0.003) loss 1.5430 (0.9025) lr 3.4549e-03 eta 0:57:02
epoch [19/30] batch [420/796] time 0.343 (0.374) data 0.000 (0.003) loss 0.9346 (0.9073) lr 3.4549e-03 eta 0:56:55
epoch [19/30] batch [440/796] time 0.405 (0.373) data 0.000 (0.002) loss 1.4648 (0.9084) lr 3.4549e-03 eta 0:56:43
epoch [19/30] batch [460/796] time 0.364 (0.374) data 0.000 (0.002) loss 1.0869 (0.8956) lr 3.4549e-03 eta 0:56:36
epoch [19/30] batch [480/796] time 0.378 (0.373) data 0.000 (0.002) loss 1.2324 (0.8963) lr 3.4549e-03 eta 0:56:26
epoch [19/30] batch [500/796] time 0.370 (0.373) data 0.000 (0.002) loss 1.1816 (0.8890) lr 3.4549e-03 eta 0:56:18
epoch [19/30] batch [520/796] time 0.399 (0.373) data 0.000 (0.002) loss 0.8589 (0.8804) lr 3.4549e-03 eta 0:56:10
epoch [19/30] batch [540/796] time 0.345 (0.373) data 0.000 (0.002) loss 2.5117 (0.8916) lr 3.4549e-03 eta 0:56:02
epoch [19/30] batch [560/796] time 0.342 (0.373) data 0.000 (0.002) loss 0.1711 (0.8818) lr 3.4549e-03 eta 0:55:53
epoch [19/30] batch [580/796] time 0.402 (0.373) data 0.000 (0.002) loss 0.3188 (0.8829) lr 3.4549e-03 eta 0:55:45
epoch [19/30] batch [600/796] time 0.370 (0.373) data 0.000 (0.002) loss 0.5845 (0.8861) lr 3.4549e-03 eta 0:55:37
epoch [19/30] batch [620/796] time 0.350 (0.373) data 0.000 (0.002) loss 1.9551 (0.8785) lr 3.4549e-03 eta 0:55:29
epoch [19/30] batch [640/796] time 0.349 (0.373) data 0.000 (0.002) loss 0.2170 (0.8747) lr 3.4549e-03 eta 0:55:21
epoch [19/30] batch [660/796] time 0.372 (0.373) data 0.000 (0.002) loss 0.4028 (0.8803) lr 3.4549e-03 eta 0:55:14
epoch [19/30] batch [680/796] time 0.394 (0.373) data 0.000 (0.002) loss 0.6084 (0.8735) lr 3.4549e-03 eta 0:55:06
epoch [19/30] batch [700/796] time 0.372 (0.373) data 0.000 (0.002) loss 2.0762 (0.8740) lr 3.4549e-03 eta 0:54:58
epoch [19/30] batch [720/796] time 0.399 (0.373) data 0.000 (0.002) loss 0.8936 (0.8756) lr 3.4549e-03 eta 0:54:53
epoch [19/30] batch [740/796] time 0.348 (0.373) data 0.000 (0.002) loss 0.8525 (0.8731) lr 3.4549e-03 eta 0:54:46
epoch [19/30] batch [760/796] time 0.382 (0.373) data 0.000 (0.002) loss 0.6592 (0.8704) lr 3.4549e-03 eta 0:54:40
epoch [19/30] batch [780/796] time 0.340 (0.372) data 0.000 (0.002) loss 1.9014 (0.8656) lr 3.4549e-03 eta 0:54:25
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:04<01:28,  4.66s/it] 10%|█         | 2/20 [00:06<00:55,  3.08s/it] 15%|█▌        | 3/20 [00:06<00:30,  1.81s/it] 20%|██        | 4/20 [00:07<00:19,  1.21s/it] 25%|██▌       | 5/20 [00:07<00:13,  1.13it/s] 30%|███       | 6/20 [00:07<00:09,  1.47it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.82it/s] 40%|████      | 8/20 [00:08<00:05,  2.16it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.44it/s] 50%|█████     | 10/20 [00:08<00:03,  2.77it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.03it/s] 60%|██████    | 12/20 [00:09<00:02,  3.28it/s] 65%|██████▌   | 13/20 [00:09<00:01,  3.54it/s] 70%|███████   | 14/20 [00:09<00:01,  3.66it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.69it/s] 80%|████████  | 16/20 [00:10<00:01,  3.80it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.95it/s] 90%|█████████ | 18/20 [00:10<00:00,  3.74it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.13it/s]100%|██████████| 20/20 [00:11<00:00,  4.53it/s]100%|██████████| 20/20 [00:11<00:00,  1.75it/s]=> result
* total: 1,990
* correct: 1,570
* accuracy: 78.9%
* error: 21.1%
* macro_f1: 78.2%

epoch [20/30] batch [20/796] time 0.364 (0.422) data 0.000 (0.042) loss 0.7744 (0.6744) lr 2.9663e-03 eta 1:01:30
epoch [20/30] batch [40/796] time 0.397 (0.394) data 0.000 (0.021) loss 0.1068 (0.7266) lr 2.9663e-03 eta 0:57:11
epoch [20/30] batch [60/796] time 0.343 (0.388) data 0.000 (0.014) loss 0.3181 (0.7750) lr 2.9663e-03 eta 0:56:18
epoch [20/30] batch [80/796] time 0.408 (0.386) data 0.000 (0.011) loss 1.5098 (0.7863) lr 2.9663e-03 eta 0:55:48
epoch [20/30] batch [100/796] time 0.373 (0.382) data 0.000 (0.009) loss 0.3293 (0.8017) lr 2.9663e-03 eta 0:55:08
epoch [20/30] batch [120/796] time 0.350 (0.380) data 0.000 (0.007) loss 1.0088 (0.8281) lr 2.9663e-03 eta 0:54:38
epoch [20/30] batch [140/796] time 0.342 (0.377) data 0.000 (0.006) loss 0.7168 (0.8179) lr 2.9663e-03 eta 0:54:09
epoch [20/30] batch [160/796] time 0.376 (0.376) data 0.000 (0.005) loss 0.6514 (0.8082) lr 2.9663e-03 eta 0:53:51
epoch [20/30] batch [180/796] time 0.375 (0.376) data 0.000 (0.005) loss 0.7935 (0.8170) lr 2.9663e-03 eta 0:53:42
epoch [20/30] batch [200/796] time 0.364 (0.375) data 0.000 (0.004) loss 2.3457 (0.8215) lr 2.9663e-03 eta 0:53:30
epoch [20/30] batch [220/796] time 0.370 (0.375) data 0.000 (0.004) loss 0.3728 (0.8370) lr 2.9663e-03 eta 0:53:17
epoch [20/30] batch [240/796] time 0.386 (0.374) data 0.000 (0.004) loss 0.3906 (0.8374) lr 2.9663e-03 eta 0:53:08
epoch [20/30] batch [260/796] time 0.351 (0.374) data 0.000 (0.003) loss 0.2590 (0.8445) lr 2.9663e-03 eta 0:52:54
epoch [20/30] batch [280/796] time 0.348 (0.373) data 0.000 (0.003) loss 0.8564 (0.8410) lr 2.9663e-03 eta 0:52:44
epoch [20/30] batch [300/796] time 0.361 (0.372) data 0.000 (0.003) loss 0.1659 (0.8418) lr 2.9663e-03 eta 0:52:29
epoch [20/30] batch [320/796] time 0.386 (0.373) data 0.000 (0.003) loss 0.9888 (0.8517) lr 2.9663e-03 eta 0:52:27
epoch [20/30] batch [340/796] time 0.384 (0.373) data 0.000 (0.003) loss 0.2015 (0.8535) lr 2.9663e-03 eta 0:52:23
epoch [20/30] batch [360/796] time 0.363 (0.373) data 0.001 (0.003) loss 0.2578 (0.8635) lr 2.9663e-03 eta 0:52:15
epoch [20/30] batch [380/796] time 0.345 (0.373) data 0.000 (0.002) loss 1.2930 (0.8609) lr 2.9663e-03 eta 0:52:05
epoch [20/30] batch [400/796] time 0.375 (0.373) data 0.000 (0.002) loss 1.4902 (0.8620) lr 2.9663e-03 eta 0:51:57
epoch [20/30] batch [420/796] time 0.337 (0.373) data 0.000 (0.002) loss 0.2510 (0.8615) lr 2.9663e-03 eta 0:51:48
epoch [20/30] batch [440/796] time 0.404 (0.373) data 0.000 (0.002) loss 0.5078 (0.8677) lr 2.9663e-03 eta 0:51:41
epoch [20/30] batch [460/796] time 0.367 (0.373) data 0.000 (0.002) loss 4.1250 (0.8704) lr 2.9663e-03 eta 0:51:31
epoch [20/30] batch [480/796] time 0.388 (0.373) data 0.000 (0.002) loss 0.7480 (0.8735) lr 2.9663e-03 eta 0:51:24
epoch [20/30] batch [500/796] time 0.354 (0.373) data 0.000 (0.002) loss 0.2157 (0.8664) lr 2.9663e-03 eta 0:51:19
epoch [20/30] batch [520/796] time 0.358 (0.373) data 0.000 (0.002) loss 0.1177 (0.8634) lr 2.9663e-03 eta 0:51:10
epoch [20/30] batch [540/796] time 0.387 (0.373) data 0.000 (0.002) loss 1.3867 (0.8591) lr 2.9663e-03 eta 0:51:03
epoch [20/30] batch [560/796] time 0.366 (0.373) data 0.000 (0.002) loss 2.5332 (0.8665) lr 2.9663e-03 eta 0:50:55
epoch [20/30] batch [580/796] time 0.389 (0.373) data 0.000 (0.002) loss 1.0781 (0.8606) lr 2.9663e-03 eta 0:50:51
epoch [20/30] batch [600/796] time 0.379 (0.373) data 0.000 (0.002) loss 0.8828 (0.8562) lr 2.9663e-03 eta 0:50:45
epoch [20/30] batch [620/796] time 0.393 (0.373) data 0.000 (0.002) loss 0.7383 (0.8618) lr 2.9663e-03 eta 0:50:37
epoch [20/30] batch [640/796] time 0.394 (0.373) data 0.000 (0.002) loss 0.2822 (0.8614) lr 2.9663e-03 eta 0:50:28
epoch [20/30] batch [660/796] time 0.351 (0.373) data 0.000 (0.002) loss 0.8862 (0.8665) lr 2.9663e-03 eta 0:50:19
epoch [20/30] batch [680/796] time 0.422 (0.373) data 0.000 (0.001) loss 1.7236 (0.8680) lr 2.9663e-03 eta 0:50:11
epoch [20/30] batch [700/796] time 0.359 (0.373) data 0.000 (0.001) loss 0.5659 (0.8690) lr 2.9663e-03 eta 0:50:03
epoch [20/30] batch [720/796] time 0.376 (0.373) data 0.000 (0.001) loss 1.5557 (0.8704) lr 2.9663e-03 eta 0:49:54
epoch [20/30] batch [740/796] time 0.369 (0.373) data 0.000 (0.001) loss 1.8203 (0.8726) lr 2.9663e-03 eta 0:49:46
epoch [20/30] batch [760/796] time 0.350 (0.372) data 0.000 (0.001) loss 0.1718 (0.8685) lr 2.9663e-03 eta 0:49:37
epoch [20/30] batch [780/796] time 0.328 (0.372) data 0.000 (0.001) loss 0.9219 (0.8654) lr 2.9663e-03 eta 0:49:24
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:47,  5.67s/it] 10%|█         | 2/20 [00:06<00:51,  2.86s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.69s/it] 20%|██        | 4/20 [00:07<00:18,  1.13s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.22it/s] 30%|███       | 6/20 [00:07<00:08,  1.57it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.92it/s] 40%|████      | 8/20 [00:08<00:05,  2.27it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.57it/s] 50%|█████     | 10/20 [00:08<00:03,  2.83it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.07it/s] 60%|██████    | 12/20 [00:09<00:02,  3.21it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.40it/s] 70%|███████   | 14/20 [00:09<00:01,  3.56it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.59it/s] 80%|████████  | 16/20 [00:10<00:01,  3.66it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.72it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.01it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.36it/s]100%|██████████| 20/20 [00:11<00:00,  4.72it/s]100%|██████████| 20/20 [00:11<00:00,  1.77it/s]=> result
* total: 1,990
* correct: 1,570
* accuracy: 78.9%
* error: 21.1%
* macro_f1: 78.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model.pth.tar-20

epoch [21/30] batch [20/796] time 0.383 (0.421) data 0.000 (0.041) loss 0.4253 (0.8695) lr 2.5000e-03 eta 0:55:41
epoch [21/30] batch [40/796] time 0.362 (0.400) data 0.000 (0.021) loss 0.2087 (0.7380) lr 2.5000e-03 eta 0:52:46
epoch [21/30] batch [60/796] time 0.365 (0.390) data 0.000 (0.014) loss 1.0312 (0.7250) lr 2.5000e-03 eta 0:51:17
epoch [21/30] batch [80/796] time 0.377 (0.384) data 0.000 (0.011) loss 1.7529 (0.7754) lr 2.5000e-03 eta 0:50:27
epoch [21/30] batch [100/796] time 0.385 (0.382) data 0.000 (0.008) loss 0.9692 (0.7692) lr 2.5000e-03 eta 0:50:00
epoch [21/30] batch [120/796] time 0.382 (0.380) data 0.000 (0.007) loss 0.8364 (0.7624) lr 2.5000e-03 eta 0:49:35
epoch [21/30] batch [140/796] time 0.336 (0.379) data 0.000 (0.006) loss 0.1571 (0.7808) lr 2.5000e-03 eta 0:49:23
epoch [21/30] batch [160/796] time 0.349 (0.378) data 0.000 (0.005) loss 0.2593 (0.7800) lr 2.5000e-03 eta 0:49:10
epoch [21/30] batch [180/796] time 0.357 (0.378) data 0.000 (0.005) loss 0.1981 (0.7827) lr 2.5000e-03 eta 0:49:00
epoch [21/30] batch [200/796] time 0.363 (0.378) data 0.000 (0.004) loss 1.7598 (0.8102) lr 2.5000e-03 eta 0:48:49
epoch [21/30] batch [220/796] time 0.394 (0.376) data 0.000 (0.004) loss 0.5786 (0.8268) lr 2.5000e-03 eta 0:48:32
epoch [21/30] batch [240/796] time 0.335 (0.375) data 0.000 (0.004) loss 0.8804 (0.8440) lr 2.5000e-03 eta 0:48:14
epoch [21/30] batch [260/796] time 0.339 (0.375) data 0.000 (0.003) loss 1.1191 (0.8520) lr 2.5000e-03 eta 0:48:03
epoch [21/30] batch [280/796] time 0.378 (0.374) data 0.000 (0.003) loss 0.1805 (0.8457) lr 2.5000e-03 eta 0:47:51
epoch [21/30] batch [300/796] time 0.398 (0.373) data 0.000 (0.003) loss 2.0879 (0.8387) lr 2.5000e-03 eta 0:47:40
epoch [21/30] batch [320/796] time 0.364 (0.373) data 0.000 (0.003) loss 0.7397 (0.8318) lr 2.5000e-03 eta 0:47:32
epoch [21/30] batch [340/796] time 0.335 (0.373) data 0.000 (0.003) loss 0.7139 (0.8509) lr 2.5000e-03 eta 0:47:23
epoch [21/30] batch [360/796] time 0.377 (0.373) data 0.000 (0.003) loss 1.6494 (0.8475) lr 2.5000e-03 eta 0:47:12
epoch [21/30] batch [380/796] time 0.392 (0.373) data 0.000 (0.002) loss 1.3525 (0.8531) lr 2.5000e-03 eta 0:47:07
epoch [21/30] batch [400/796] time 0.383 (0.373) data 0.000 (0.002) loss 0.5825 (0.8605) lr 2.5000e-03 eta 0:46:58
epoch [21/30] batch [420/796] time 0.337 (0.373) data 0.000 (0.002) loss 1.4424 (0.8520) lr 2.5000e-03 eta 0:46:50
epoch [21/30] batch [440/796] time 0.387 (0.373) data 0.000 (0.002) loss 1.0107 (0.8498) lr 2.5000e-03 eta 0:46:42
epoch [21/30] batch [460/796] time 0.370 (0.373) data 0.000 (0.002) loss 0.1676 (0.8640) lr 2.5000e-03 eta 0:46:35
epoch [21/30] batch [480/796] time 0.363 (0.373) data 0.000 (0.002) loss 0.2996 (0.8595) lr 2.5000e-03 eta 0:46:29
epoch [21/30] batch [500/796] time 0.340 (0.373) data 0.000 (0.002) loss 0.7529 (0.8533) lr 2.5000e-03 eta 0:46:24
epoch [21/30] batch [520/796] time 0.363 (0.373) data 0.000 (0.002) loss 0.3584 (0.8507) lr 2.5000e-03 eta 0:46:16
epoch [21/30] batch [540/796] time 0.346 (0.373) data 0.000 (0.002) loss 0.6152 (0.8441) lr 2.5000e-03 eta 0:46:08
epoch [21/30] batch [560/796] time 0.391 (0.373) data 0.000 (0.002) loss 0.1897 (0.8501) lr 2.5000e-03 eta 0:46:00
epoch [21/30] batch [580/796] time 0.371 (0.373) data 0.000 (0.002) loss 0.5068 (0.8537) lr 2.5000e-03 eta 0:45:52
epoch [21/30] batch [600/796] time 0.388 (0.373) data 0.000 (0.002) loss 0.3313 (0.8541) lr 2.5000e-03 eta 0:45:45
epoch [21/30] batch [620/796] time 0.354 (0.373) data 0.000 (0.002) loss 0.3630 (0.8499) lr 2.5000e-03 eta 0:45:37
epoch [21/30] batch [640/796] time 0.374 (0.373) data 0.000 (0.002) loss 0.5107 (0.8565) lr 2.5000e-03 eta 0:45:28
epoch [21/30] batch [660/796] time 0.343 (0.373) data 0.000 (0.002) loss 1.3672 (0.8549) lr 2.5000e-03 eta 0:45:20
epoch [21/30] batch [680/796] time 0.372 (0.373) data 0.000 (0.001) loss 0.5112 (0.8615) lr 2.5000e-03 eta 0:45:12
epoch [21/30] batch [700/796] time 0.439 (0.373) data 0.000 (0.001) loss 0.9883 (0.8639) lr 2.5000e-03 eta 0:45:04
epoch [21/30] batch [720/796] time 0.381 (0.373) data 0.000 (0.001) loss 1.4688 (0.8639) lr 2.5000e-03 eta 0:44:59
epoch [21/30] batch [740/796] time 0.356 (0.373) data 0.000 (0.001) loss 0.6841 (0.8681) lr 2.5000e-03 eta 0:44:52
epoch [21/30] batch [760/796] time 0.378 (0.373) data 0.000 (0.001) loss 0.4702 (0.8702) lr 2.5000e-03 eta 0:44:44
epoch [21/30] batch [780/796] time 0.327 (0.372) data 0.000 (0.001) loss 0.2969 (0.8694) lr 2.5000e-03 eta 0:44:30
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:48,  5.70s/it] 10%|█         | 2/20 [00:06<00:52,  2.89s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.70s/it] 20%|██        | 4/20 [00:07<00:18,  1.14s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.21it/s] 30%|███       | 6/20 [00:07<00:09,  1.55it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.92it/s] 40%|████      | 8/20 [00:08<00:05,  2.26it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.55it/s] 50%|█████     | 10/20 [00:08<00:03,  2.81it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.08it/s] 60%|██████    | 12/20 [00:09<00:02,  3.30it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.46it/s] 70%|███████   | 14/20 [00:09<00:01,  3.61it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.74it/s] 80%|████████  | 16/20 [00:10<00:01,  3.80it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.84it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.23it/s] 95%|█████████▌| 19/20 [00:10<00:00,  4.55it/s]100%|██████████| 20/20 [00:11<00:00,  4.87it/s]100%|██████████| 20/20 [00:11<00:00,  1.77it/s]=> result
* total: 1,990
* correct: 1,586
* accuracy: 79.7%
* error: 20.3%
* macro_f1: 79.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar

epoch [22/30] batch [20/796] time 0.357 (0.430) data 0.000 (0.045) loss 0.2727 (0.9456) lr 2.0611e-03 eta 0:51:09
epoch [22/30] batch [40/796] time 0.379 (0.396) data 0.000 (0.023) loss 1.0156 (0.9985) lr 2.0611e-03 eta 0:47:01
epoch [22/30] batch [60/796] time 0.355 (0.389) data 0.000 (0.015) loss 0.6094 (0.9682) lr 2.0611e-03 eta 0:46:04
epoch [22/30] batch [80/796] time 0.375 (0.385) data 0.000 (0.011) loss 0.6333 (0.9186) lr 2.0611e-03 eta 0:45:29
epoch [22/30] batch [100/796] time 0.345 (0.383) data 0.000 (0.009) loss 0.8501 (0.8889) lr 2.0611e-03 eta 0:45:07
epoch [22/30] batch [120/796] time 0.340 (0.380) data 0.000 (0.008) loss 2.1094 (0.8965) lr 2.0611e-03 eta 0:44:36
epoch [22/30] batch [140/796] time 0.368 (0.379) data 0.000 (0.007) loss 0.8120 (0.8688) lr 2.0611e-03 eta 0:44:21
epoch [22/30] batch [160/796] time 0.384 (0.378) data 0.000 (0.006) loss 1.7051 (0.8625) lr 2.0611e-03 eta 0:44:07
epoch [22/30] batch [180/796] time 0.353 (0.378) data 0.001 (0.005) loss 0.8174 (0.8856) lr 2.0611e-03 eta 0:43:58
epoch [22/30] batch [200/796] time 0.456 (0.378) data 0.000 (0.005) loss 0.8120 (0.9009) lr 2.0611e-03 eta 0:43:54
epoch [22/30] batch [220/796] time 0.372 (0.378) data 0.000 (0.004) loss 0.3665 (0.8825) lr 2.0611e-03 eta 0:43:44
epoch [22/30] batch [240/796] time 0.351 (0.378) data 0.000 (0.004) loss 0.5039 (0.8867) lr 2.0611e-03 eta 0:43:34
epoch [22/30] batch [260/796] time 0.357 (0.378) data 0.000 (0.004) loss 0.7461 (0.8863) lr 2.0611e-03 eta 0:43:26
epoch [22/30] batch [280/796] time 0.405 (0.377) data 0.000 (0.003) loss 1.9346 (0.8821) lr 2.0611e-03 eta 0:43:17
epoch [22/30] batch [300/796] time 0.359 (0.377) data 0.000 (0.003) loss 2.1992 (0.8775) lr 2.0611e-03 eta 0:43:07
epoch [22/30] batch [320/796] time 0.375 (0.377) data 0.000 (0.003) loss 0.4561 (0.8758) lr 2.0611e-03 eta 0:42:59
epoch [22/30] batch [340/796] time 0.338 (0.377) data 0.000 (0.003) loss 0.6938 (0.8747) lr 2.0611e-03 eta 0:42:49
epoch [22/30] batch [360/796] time 0.370 (0.376) data 0.000 (0.003) loss 0.7354 (0.8604) lr 2.0611e-03 eta 0:42:40
epoch [22/30] batch [380/796] time 0.335 (0.376) data 0.000 (0.003) loss 0.8618 (0.8602) lr 2.0611e-03 eta 0:42:27
epoch [22/30] batch [400/796] time 0.394 (0.375) data 0.000 (0.003) loss 0.5015 (0.8541) lr 2.0611e-03 eta 0:42:17
epoch [22/30] batch [420/796] time 0.375 (0.375) data 0.000 (0.002) loss 0.3806 (0.8564) lr 2.0611e-03 eta 0:42:08
epoch [22/30] batch [440/796] time 0.380 (0.375) data 0.000 (0.002) loss 1.2588 (0.8626) lr 2.0611e-03 eta 0:41:58
epoch [22/30] batch [460/796] time 0.360 (0.374) data 0.000 (0.002) loss 0.9043 (0.8706) lr 2.0611e-03 eta 0:41:49
epoch [22/30] batch [480/796] time 0.378 (0.374) data 0.000 (0.002) loss 0.0856 (0.8695) lr 2.0611e-03 eta 0:41:40
epoch [22/30] batch [500/796] time 0.350 (0.374) data 0.000 (0.002) loss 1.3779 (0.8759) lr 2.0611e-03 eta 0:41:35
epoch [22/30] batch [520/796] time 0.385 (0.374) data 0.000 (0.002) loss 0.4734 (0.8746) lr 2.0611e-03 eta 0:41:24
epoch [22/30] batch [540/796] time 0.385 (0.374) data 0.000 (0.002) loss 1.8076 (0.8789) lr 2.0611e-03 eta 0:41:15
epoch [22/30] batch [560/796] time 0.364 (0.374) data 0.000 (0.002) loss 0.5195 (0.8777) lr 2.0611e-03 eta 0:41:06
epoch [22/30] batch [580/796] time 0.343 (0.373) data 0.000 (0.002) loss 0.8105 (0.8785) lr 2.0611e-03 eta 0:40:58
epoch [22/30] batch [600/796] time 0.386 (0.373) data 0.000 (0.002) loss 0.5586 (0.8772) lr 2.0611e-03 eta 0:40:51
epoch [22/30] batch [620/796] time 0.346 (0.373) data 0.000 (0.002) loss 1.3594 (0.8736) lr 2.0611e-03 eta 0:40:41
epoch [22/30] batch [640/796] time 0.386 (0.373) data 0.000 (0.002) loss 0.7964 (0.8789) lr 2.0611e-03 eta 0:40:33
epoch [22/30] batch [660/796] time 0.384 (0.373) data 0.000 (0.002) loss 0.1951 (0.8727) lr 2.0611e-03 eta 0:40:25
epoch [22/30] batch [680/796] time 0.376 (0.373) data 0.000 (0.002) loss 0.3337 (0.8698) lr 2.0611e-03 eta 0:40:17
epoch [22/30] batch [700/796] time 0.357 (0.373) data 0.000 (0.002) loss 0.1418 (0.8639) lr 2.0611e-03 eta 0:40:09
epoch [22/30] batch [720/796] time 0.352 (0.372) data 0.000 (0.002) loss 0.5977 (0.8634) lr 2.0611e-03 eta 0:39:59
epoch [22/30] batch [740/796] time 0.350 (0.372) data 0.000 (0.001) loss 0.1398 (0.8589) lr 2.0611e-03 eta 0:39:50
epoch [22/30] batch [760/796] time 0.381 (0.372) data 0.000 (0.001) loss 0.8198 (0.8644) lr 2.0611e-03 eta 0:39:42
epoch [22/30] batch [780/796] time 0.327 (0.371) data 0.000 (0.001) loss 0.7656 (0.8647) lr 2.0611e-03 eta 0:39:29
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:52,  5.93s/it] 10%|█         | 2/20 [00:06<00:52,  2.94s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.73s/it] 20%|██        | 4/20 [00:07<00:18,  1.16s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.20it/s] 30%|███       | 6/20 [00:07<00:09,  1.55it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.91it/s] 40%|████      | 8/20 [00:08<00:05,  2.25it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.54it/s] 50%|█████     | 10/20 [00:08<00:03,  2.82it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.07it/s] 60%|██████    | 12/20 [00:09<00:02,  3.26it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.48it/s] 70%|███████   | 14/20 [00:09<00:01,  3.63it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.81it/s] 80%|████████  | 16/20 [00:10<00:00,  4.08it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.29it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.01it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.49it/s]100%|██████████| 20/20 [00:11<00:00,  3.97it/s]100%|██████████| 20/20 [00:11<00:00,  1.71it/s]=> result
* total: 1,990
* correct: 1,581
* accuracy: 79.4%
* error: 20.6%
* macro_f1: 78.8%

epoch [23/30] batch [20/796] time 0.386 (0.419) data 0.000 (0.046) loss 2.7637 (0.9498) lr 1.6543e-03 eta 0:44:20
epoch [23/30] batch [40/796] time 0.344 (0.395) data 0.000 (0.023) loss 0.8267 (0.9104) lr 1.6543e-03 eta 0:41:37
epoch [23/30] batch [60/796] time 0.338 (0.384) data 0.000 (0.016) loss 0.2949 (0.8571) lr 1.6543e-03 eta 0:40:19
epoch [23/30] batch [80/796] time 0.351 (0.378) data 0.000 (0.012) loss 1.9736 (0.8512) lr 1.6543e-03 eta 0:39:39
epoch [23/30] batch [100/796] time 0.338 (0.376) data 0.000 (0.009) loss 0.3118 (0.8707) lr 1.6543e-03 eta 0:39:16
epoch [23/30] batch [120/796] time 0.349 (0.374) data 0.000 (0.008) loss 0.3076 (0.8390) lr 1.6543e-03 eta 0:38:58
epoch [23/30] batch [140/796] time 0.357 (0.374) data 0.000 (0.007) loss 0.4622 (0.8790) lr 1.6543e-03 eta 0:38:49
epoch [23/30] batch [160/796] time 0.388 (0.374) data 0.000 (0.006) loss 0.8296 (0.8929) lr 1.6543e-03 eta 0:38:39
epoch [23/30] batch [180/796] time 0.397 (0.374) data 0.000 (0.005) loss 0.9199 (0.8962) lr 1.6543e-03 eta 0:38:31
epoch [23/30] batch [200/796] time 0.387 (0.373) data 0.000 (0.005) loss 0.4324 (0.8913) lr 1.6543e-03 eta 0:38:20
epoch [23/30] batch [220/796] time 0.394 (0.373) data 0.000 (0.004) loss 0.5532 (0.8662) lr 1.6543e-03 eta 0:38:11
epoch [23/30] batch [240/796] time 0.354 (0.373) data 0.000 (0.004) loss 1.0527 (0.8726) lr 1.6543e-03 eta 0:38:05
epoch [23/30] batch [260/796] time 0.396 (0.373) data 0.000 (0.004) loss 0.8638 (0.8648) lr 1.6543e-03 eta 0:37:58
epoch [23/30] batch [280/796] time 0.384 (0.373) data 0.000 (0.004) loss 0.2040 (0.8656) lr 1.6543e-03 eta 0:37:50
epoch [23/30] batch [300/796] time 0.400 (0.373) data 0.000 (0.003) loss 2.1328 (0.8554) lr 1.6543e-03 eta 0:37:42
epoch [23/30] batch [320/796] time 0.370 (0.373) data 0.000 (0.003) loss 0.7153 (0.8464) lr 1.6543e-03 eta 0:37:33
epoch [23/30] batch [340/796] time 0.380 (0.373) data 0.000 (0.003) loss 1.6025 (0.8416) lr 1.6543e-03 eta 0:37:25
epoch [23/30] batch [360/796] time 0.384 (0.372) data 0.000 (0.003) loss 0.3718 (0.8381) lr 1.6543e-03 eta 0:37:16
epoch [23/30] batch [380/796] time 0.380 (0.372) data 0.000 (0.003) loss 1.1279 (0.8361) lr 1.6543e-03 eta 0:37:07
epoch [23/30] batch [400/796] time 0.382 (0.372) data 0.000 (0.003) loss 0.4192 (0.8431) lr 1.6543e-03 eta 0:37:00
epoch [23/30] batch [420/796] time 0.336 (0.372) data 0.000 (0.002) loss 0.8105 (0.8466) lr 1.6543e-03 eta 0:36:53
epoch [23/30] batch [440/796] time 0.383 (0.372) data 0.000 (0.002) loss 1.4854 (0.8503) lr 1.6543e-03 eta 0:36:45
epoch [23/30] batch [460/796] time 0.378 (0.372) data 0.000 (0.002) loss 1.0195 (0.8535) lr 1.6543e-03 eta 0:36:38
epoch [23/30] batch [480/796] time 0.339 (0.372) data 0.000 (0.002) loss 0.3857 (0.8594) lr 1.6543e-03 eta 0:36:30
epoch [23/30] batch [500/796] time 0.376 (0.372) data 0.000 (0.002) loss 0.5337 (0.8525) lr 1.6543e-03 eta 0:36:24
epoch [23/30] batch [520/796] time 0.362 (0.372) data 0.000 (0.002) loss 1.2725 (0.8575) lr 1.6543e-03 eta 0:36:14
epoch [23/30] batch [540/796] time 0.365 (0.372) data 0.000 (0.002) loss 1.0303 (0.8525) lr 1.6543e-03 eta 0:36:08
epoch [23/30] batch [560/796] time 0.391 (0.372) data 0.000 (0.002) loss 0.9419 (0.8501) lr 1.6543e-03 eta 0:36:02
epoch [23/30] batch [580/796] time 0.347 (0.372) data 0.000 (0.002) loss 0.2115 (0.8507) lr 1.6543e-03 eta 0:35:54
epoch [23/30] batch [600/796] time 0.353 (0.372) data 0.000 (0.002) loss 0.2207 (0.8471) lr 1.6543e-03 eta 0:35:47
epoch [23/30] batch [620/796] time 0.336 (0.372) data 0.000 (0.002) loss 0.0804 (0.8493) lr 1.6543e-03 eta 0:35:38
epoch [23/30] batch [640/796] time 0.365 (0.372) data 0.000 (0.002) loss 0.9316 (0.8562) lr 1.6543e-03 eta 0:35:31
epoch [23/30] batch [660/796] time 0.388 (0.372) data 0.000 (0.002) loss 0.2573 (0.8528) lr 1.6543e-03 eta 0:35:25
epoch [23/30] batch [680/796] time 0.397 (0.372) data 0.000 (0.002) loss 2.4004 (0.8573) lr 1.6543e-03 eta 0:35:17
epoch [23/30] batch [700/796] time 0.337 (0.372) data 0.000 (0.002) loss 0.5786 (0.8547) lr 1.6543e-03 eta 0:35:09
epoch [23/30] batch [720/796] time 0.375 (0.372) data 0.000 (0.002) loss 0.9102 (0.8548) lr 1.6543e-03 eta 0:35:02
epoch [23/30] batch [740/796] time 0.339 (0.372) data 0.000 (0.002) loss 0.8540 (0.8537) lr 1.6543e-03 eta 0:34:54
epoch [23/30] batch [760/796] time 0.378 (0.372) data 0.000 (0.001) loss 1.5195 (0.8528) lr 1.6543e-03 eta 0:34:45
epoch [23/30] batch [780/796] time 0.330 (0.371) data 0.000 (0.001) loss 0.3782 (0.8566) lr 1.6543e-03 eta 0:34:34
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:46,  5.61s/it] 10%|█         | 2/20 [00:06<00:53,  2.98s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.75s/it] 20%|██        | 4/20 [00:07<00:18,  1.17s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.18it/s] 30%|███       | 6/20 [00:07<00:09,  1.53it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.89it/s] 40%|████      | 8/20 [00:08<00:05,  2.21it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.52it/s] 50%|█████     | 10/20 [00:08<00:03,  2.84it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.13it/s] 60%|██████    | 12/20 [00:09<00:02,  3.31it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.46it/s] 70%|███████   | 14/20 [00:09<00:01,  3.72it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.96it/s] 80%|████████  | 16/20 [00:10<00:00,  4.07it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.99it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.09it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.43it/s]100%|██████████| 20/20 [00:11<00:00,  4.78it/s]100%|██████████| 20/20 [00:11<00:00,  1.76it/s]=> result
* total: 1,990
* correct: 1,578
* accuracy: 79.3%
* error: 20.7%
* macro_f1: 78.7%

epoch [24/30] batch [20/796] time 0.350 (0.418) data 0.000 (0.041) loss 0.9785 (0.8134) lr 1.2843e-03 eta 0:38:42
epoch [24/30] batch [40/796] time 0.341 (0.392) data 0.000 (0.021) loss 0.2532 (0.8524) lr 1.2843e-03 eta 0:36:11
epoch [24/30] batch [60/796] time 0.328 (0.382) data 0.000 (0.014) loss 0.7593 (0.8100) lr 1.2843e-03 eta 0:35:07
epoch [24/30] batch [80/796] time 0.397 (0.382) data 0.000 (0.010) loss 0.3608 (0.8081) lr 1.2843e-03 eta 0:34:57
epoch [24/30] batch [100/796] time 0.393 (0.381) data 0.000 (0.008) loss 0.0728 (0.8313) lr 1.2843e-03 eta 0:34:43
epoch [24/30] batch [120/796] time 0.344 (0.377) data 0.000 (0.007) loss 0.9819 (0.8250) lr 1.2843e-03 eta 0:34:16
epoch [24/30] batch [140/796] time 0.364 (0.376) data 0.000 (0.006) loss 1.0957 (0.7965) lr 1.2843e-03 eta 0:34:02
epoch [24/30] batch [160/796] time 0.387 (0.375) data 0.000 (0.005) loss 0.1902 (0.8138) lr 1.2843e-03 eta 0:33:50
epoch [24/30] batch [180/796] time 0.377 (0.375) data 0.000 (0.005) loss 0.5122 (0.8062) lr 1.2843e-03 eta 0:33:44
epoch [24/30] batch [200/796] time 0.359 (0.375) data 0.000 (0.004) loss 0.4348 (0.7876) lr 1.2843e-03 eta 0:33:35
epoch [24/30] batch [220/796] time 0.382 (0.375) data 0.000 (0.004) loss 1.4346 (0.7963) lr 1.2843e-03 eta 0:33:25
epoch [24/30] batch [240/796] time 0.347 (0.374) data 0.000 (0.004) loss 0.6221 (0.7847) lr 1.2843e-03 eta 0:33:13
epoch [24/30] batch [260/796] time 0.363 (0.373) data 0.000 (0.003) loss 0.1166 (0.7821) lr 1.2843e-03 eta 0:33:02
epoch [24/30] batch [280/796] time 0.353 (0.373) data 0.000 (0.003) loss 2.7988 (0.7981) lr 1.2843e-03 eta 0:32:55
epoch [24/30] batch [300/796] time 0.345 (0.373) data 0.000 (0.003) loss 1.3193 (0.8015) lr 1.2843e-03 eta 0:32:46
epoch [24/30] batch [320/796] time 0.345 (0.373) data 0.000 (0.003) loss 0.0850 (0.7885) lr 1.2843e-03 eta 0:32:38
epoch [24/30] batch [340/796] time 0.383 (0.373) data 0.000 (0.003) loss 0.9697 (0.7945) lr 1.2843e-03 eta 0:32:31
epoch [24/30] batch [360/796] time 0.360 (0.373) data 0.000 (0.003) loss 0.7041 (0.7984) lr 1.2843e-03 eta 0:32:24
epoch [24/30] batch [380/796] time 0.401 (0.373) data 0.000 (0.002) loss 1.4199 (0.8010) lr 1.2843e-03 eta 0:32:18
epoch [24/30] batch [400/796] time 0.367 (0.373) data 0.000 (0.002) loss 1.3350 (0.7972) lr 1.2843e-03 eta 0:32:08
epoch [24/30] batch [420/796] time 0.394 (0.373) data 0.000 (0.002) loss 0.1372 (0.8015) lr 1.2843e-03 eta 0:32:00
epoch [24/30] batch [440/796] time 0.341 (0.372) data 0.000 (0.002) loss 1.4912 (0.8074) lr 1.2843e-03 eta 0:31:50
epoch [24/30] batch [460/796] time 0.368 (0.372) data 0.000 (0.002) loss 0.3865 (0.8106) lr 1.2843e-03 eta 0:31:43
epoch [24/30] batch [480/796] time 0.369 (0.372) data 0.000 (0.002) loss 1.1631 (0.8120) lr 1.2843e-03 eta 0:31:35
epoch [24/30] batch [500/796] time 0.363 (0.372) data 0.000 (0.002) loss 0.3391 (0.8069) lr 1.2843e-03 eta 0:31:26
epoch [24/30] batch [520/796] time 0.398 (0.372) data 0.000 (0.002) loss 1.1377 (0.8066) lr 1.2843e-03 eta 0:31:19
epoch [24/30] batch [540/796] time 0.394 (0.372) data 0.000 (0.002) loss 0.3821 (0.8179) lr 1.2843e-03 eta 0:31:13
epoch [24/30] batch [560/796] time 0.346 (0.372) data 0.000 (0.002) loss 0.9775 (0.8277) lr 1.2843e-03 eta 0:31:05
epoch [24/30] batch [580/796] time 0.392 (0.372) data 0.000 (0.002) loss 0.7686 (0.8230) lr 1.2843e-03 eta 0:30:57
epoch [24/30] batch [600/796] time 0.372 (0.372) data 0.000 (0.002) loss 1.1348 (0.8231) lr 1.2843e-03 eta 0:30:48
epoch [24/30] batch [620/796] time 0.380 (0.372) data 0.000 (0.002) loss 2.0254 (0.8227) lr 1.2843e-03 eta 0:30:41
epoch [24/30] batch [640/796] time 0.398 (0.372) data 0.000 (0.002) loss 1.6846 (0.8230) lr 1.2843e-03 eta 0:30:34
epoch [24/30] batch [660/796] time 0.348 (0.372) data 0.000 (0.002) loss 0.8081 (0.8261) lr 1.2843e-03 eta 0:30:26
epoch [24/30] batch [680/796] time 0.387 (0.372) data 0.000 (0.001) loss 0.8433 (0.8220) lr 1.2843e-03 eta 0:30:19
epoch [24/30] batch [700/796] time 0.344 (0.372) data 0.000 (0.001) loss 0.9521 (0.8205) lr 1.2843e-03 eta 0:30:10
epoch [24/30] batch [720/796] time 0.337 (0.371) data 0.000 (0.001) loss 2.3242 (0.8191) lr 1.2843e-03 eta 0:30:02
epoch [24/30] batch [740/796] time 0.366 (0.371) data 0.000 (0.001) loss 0.5986 (0.8171) lr 1.2843e-03 eta 0:29:53
epoch [24/30] batch [760/796] time 0.377 (0.371) data 0.000 (0.001) loss 0.9639 (0.8169) lr 1.2843e-03 eta 0:29:45
epoch [24/30] batch [780/796] time 0.327 (0.370) data 0.000 (0.001) loss 1.1719 (0.8210) lr 1.2843e-03 eta 0:29:33
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:45,  5.54s/it] 10%|█         | 2/20 [00:06<00:50,  2.83s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.67s/it] 20%|██        | 4/20 [00:07<00:17,  1.12s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.23it/s] 30%|███       | 6/20 [00:07<00:08,  1.59it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.94it/s] 40%|████      | 8/20 [00:08<00:05,  2.28it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.56it/s] 50%|█████     | 10/20 [00:08<00:03,  2.82it/s] 55%|█████▌    | 11/20 [00:08<00:02,  3.03it/s] 60%|██████    | 12/20 [00:09<00:02,  3.22it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.39it/s] 70%|███████   | 14/20 [00:09<00:01,  3.56it/s] 75%|███████▌  | 15/20 [00:09<00:01,  3.80it/s] 80%|████████  | 16/20 [00:10<00:01,  3.89it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.89it/s] 90%|█████████ | 18/20 [00:10<00:00,  3.54it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.80it/s]100%|██████████| 20/20 [00:11<00:00,  4.25it/s]100%|██████████| 20/20 [00:11<00:00,  1.76it/s]=> result
* total: 1,990
* correct: 1,583
* accuracy: 79.5%
* error: 20.5%
* macro_f1: 78.9%

epoch [25/30] batch [20/796] time 0.361 (0.428) data 0.000 (0.044) loss 0.8096 (0.6438) lr 9.5492e-04 eta 0:33:55
epoch [25/30] batch [40/796] time 0.392 (0.396) data 0.000 (0.022) loss 0.9722 (0.7115) lr 9.5492e-04 eta 0:31:13
epoch [25/30] batch [60/796] time 0.390 (0.388) data 0.000 (0.015) loss 1.7461 (0.7656) lr 9.5492e-04 eta 0:30:31
epoch [25/30] batch [80/796] time 0.382 (0.384) data 0.000 (0.011) loss 0.7109 (0.8109) lr 9.5492e-04 eta 0:30:02
epoch [25/30] batch [100/796] time 0.351 (0.381) data 0.000 (0.009) loss 0.1788 (0.7898) lr 9.5492e-04 eta 0:29:40
epoch [25/30] batch [120/796] time 0.381 (0.377) data 0.000 (0.008) loss 0.7686 (0.8166) lr 9.5492e-04 eta 0:29:16
epoch [25/30] batch [140/796] time 0.357 (0.377) data 0.000 (0.006) loss 0.3892 (0.8526) lr 9.5492e-04 eta 0:29:07
epoch [25/30] batch [160/796] time 0.366 (0.377) data 0.000 (0.006) loss 0.3503 (0.8334) lr 9.5492e-04 eta 0:29:01
epoch [25/30] batch [180/796] time 0.349 (0.377) data 0.000 (0.005) loss 0.6968 (0.8441) lr 9.5492e-04 eta 0:28:51
epoch [25/30] batch [200/796] time 0.348 (0.376) data 0.000 (0.005) loss 0.1428 (0.8208) lr 9.5492e-04 eta 0:28:40
epoch [25/30] batch [220/796] time 0.354 (0.376) data 0.001 (0.004) loss 0.8403 (0.8088) lr 9.5492e-04 eta 0:28:34
epoch [25/30] batch [240/796] time 0.338 (0.375) data 0.000 (0.004) loss 0.3086 (0.8158) lr 9.5492e-04 eta 0:28:19
epoch [25/30] batch [260/796] time 0.349 (0.374) data 0.000 (0.004) loss 0.7061 (0.8217) lr 9.5492e-04 eta 0:28:09
epoch [25/30] batch [280/796] time 0.366 (0.374) data 0.000 (0.003) loss 0.1229 (0.8211) lr 9.5492e-04 eta 0:28:00
epoch [25/30] batch [300/796] time 0.335 (0.373) data 0.000 (0.003) loss 1.0146 (0.8159) lr 9.5492e-04 eta 0:27:48
epoch [25/30] batch [320/796] time 0.380 (0.372) data 0.000 (0.003) loss 0.3604 (0.8085) lr 9.5492e-04 eta 0:27:39
epoch [25/30] batch [340/796] time 0.377 (0.373) data 0.000 (0.003) loss 0.4365 (0.8047) lr 9.5492e-04 eta 0:27:32
epoch [25/30] batch [360/796] time 0.388 (0.372) data 0.000 (0.003) loss 1.4043 (0.8064) lr 9.5492e-04 eta 0:27:23
epoch [25/30] batch [380/796] time 0.338 (0.372) data 0.000 (0.003) loss 0.4482 (0.8109) lr 9.5492e-04 eta 0:27:14
epoch [25/30] batch [400/796] time 0.345 (0.372) data 0.000 (0.002) loss 0.5957 (0.8106) lr 9.5492e-04 eta 0:27:07
epoch [25/30] batch [420/796] time 0.368 (0.372) data 0.000 (0.002) loss 0.1926 (0.8065) lr 9.5492e-04 eta 0:27:00
epoch [25/30] batch [440/796] time 0.386 (0.372) data 0.000 (0.002) loss 2.0840 (0.8122) lr 9.5492e-04 eta 0:26:53
epoch [25/30] batch [460/796] time 0.385 (0.372) data 0.000 (0.002) loss 1.1348 (0.8135) lr 9.5492e-04 eta 0:26:43
epoch [25/30] batch [480/796] time 0.361 (0.372) data 0.000 (0.002) loss 0.7095 (0.8148) lr 9.5492e-04 eta 0:26:36
epoch [25/30] batch [500/796] time 0.365 (0.372) data 0.000 (0.002) loss 0.9932 (0.8161) lr 9.5492e-04 eta 0:26:28
epoch [25/30] batch [520/796] time 0.355 (0.371) data 0.000 (0.002) loss 0.3232 (0.8113) lr 9.5492e-04 eta 0:26:19
epoch [25/30] batch [540/796] time 0.347 (0.371) data 0.000 (0.002) loss 0.2820 (0.8119) lr 9.5492e-04 eta 0:26:10
epoch [25/30] batch [560/796] time 0.339 (0.371) data 0.000 (0.002) loss 0.5381 (0.8065) lr 9.5492e-04 eta 0:26:02
epoch [25/30] batch [580/796] time 0.403 (0.371) data 0.000 (0.002) loss 0.3201 (0.8095) lr 9.5492e-04 eta 0:25:56
epoch [25/30] batch [600/796] time 0.371 (0.371) data 0.000 (0.002) loss 0.2162 (0.8113) lr 9.5492e-04 eta 0:25:48
epoch [25/30] batch [620/796] time 0.353 (0.371) data 0.000 (0.002) loss 1.8486 (0.8195) lr 9.5492e-04 eta 0:25:40
epoch [25/30] batch [640/796] time 0.386 (0.370) data 0.000 (0.002) loss 0.4980 (0.8185) lr 9.5492e-04 eta 0:25:31
epoch [25/30] batch [660/796] time 0.368 (0.370) data 0.000 (0.002) loss 0.5161 (0.8167) lr 9.5492e-04 eta 0:25:23
epoch [25/30] batch [680/796] time 0.345 (0.370) data 0.000 (0.002) loss 0.5288 (0.8229) lr 9.5492e-04 eta 0:25:16
epoch [25/30] batch [700/796] time 0.374 (0.370) data 0.000 (0.001) loss 2.2598 (0.8188) lr 9.5492e-04 eta 0:25:09
epoch [25/30] batch [720/796] time 0.386 (0.370) data 0.000 (0.001) loss 1.2793 (0.8161) lr 9.5492e-04 eta 0:25:01
epoch [25/30] batch [740/796] time 0.331 (0.370) data 0.000 (0.001) loss 0.8271 (0.8144) lr 9.5492e-04 eta 0:24:53
epoch [25/30] batch [760/796] time 0.340 (0.370) data 0.000 (0.001) loss 0.8828 (0.8188) lr 9.5492e-04 eta 0:24:44
epoch [25/30] batch [780/796] time 0.332 (0.369) data 0.000 (0.001) loss 1.4658 (0.8217) lr 9.5492e-04 eta 0:24:33
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:48,  5.72s/it] 10%|█         | 2/20 [00:06<00:55,  3.07s/it] 15%|█▌        | 3/20 [00:07<00:30,  1.80s/it] 20%|██        | 4/20 [00:07<00:19,  1.21s/it] 25%|██▌       | 5/20 [00:07<00:13,  1.15it/s] 30%|███       | 6/20 [00:08<00:09,  1.49it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.83it/s] 40%|████      | 8/20 [00:08<00:05,  2.14it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.46it/s] 50%|█████     | 10/20 [00:09<00:03,  2.87it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.20it/s] 60%|██████    | 12/20 [00:09<00:02,  3.50it/s] 65%|██████▌   | 13/20 [00:09<00:01,  3.86it/s] 70%|███████   | 14/20 [00:10<00:01,  3.96it/s] 75%|███████▌  | 15/20 [00:10<00:01,  4.25it/s] 80%|████████  | 16/20 [00:10<00:00,  4.28it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.23it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.61it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.03it/s]100%|██████████| 20/20 [00:11<00:00,  4.39it/s]100%|██████████| 20/20 [00:11<00:00,  1.72it/s]=> result
* total: 1,990
* correct: 1,589
* accuracy: 79.8%
* error: 20.2%
* macro_f1: 79.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar

epoch [26/30] batch [20/796] time 0.346 (0.424) data 0.000 (0.041) loss 0.2781 (0.7353) lr 6.6987e-04 eta 0:28:00
epoch [26/30] batch [40/796] time 0.401 (0.401) data 0.000 (0.021) loss 0.3789 (0.8979) lr 6.6987e-04 eta 0:26:18
epoch [26/30] batch [60/796] time 0.349 (0.390) data 0.001 (0.014) loss 0.1929 (0.8386) lr 6.6987e-04 eta 0:25:29
epoch [26/30] batch [80/796] time 0.387 (0.390) data 0.000 (0.010) loss 0.6924 (0.8186) lr 6.6987e-04 eta 0:25:21
epoch [26/30] batch [100/796] time 0.354 (0.387) data 0.000 (0.008) loss 0.5527 (0.8635) lr 6.6987e-04 eta 0:24:59
epoch [26/30] batch [120/796] time 0.376 (0.383) data 0.000 (0.007) loss 0.5376 (0.8866) lr 6.6987e-04 eta 0:24:37
epoch [26/30] batch [140/796] time 0.334 (0.381) data 0.000 (0.006) loss 0.3398 (0.8706) lr 6.6987e-04 eta 0:24:21
epoch [26/30] batch [160/796] time 0.350 (0.379) data 0.000 (0.005) loss 0.5005 (0.8829) lr 6.6987e-04 eta 0:24:07
epoch [26/30] batch [180/796] time 0.345 (0.378) data 0.000 (0.005) loss 1.5439 (0.8872) lr 6.6987e-04 eta 0:23:56
epoch [26/30] batch [200/796] time 0.352 (0.377) data 0.000 (0.004) loss 0.6768 (0.8842) lr 6.6987e-04 eta 0:23:44
epoch [26/30] batch [220/796] time 0.340 (0.376) data 0.000 (0.004) loss 0.5205 (0.8573) lr 6.6987e-04 eta 0:23:33
epoch [26/30] batch [240/796] time 0.364 (0.375) data 0.000 (0.004) loss 1.0312 (0.8629) lr 6.6987e-04 eta 0:23:22
epoch [26/30] batch [260/796] time 0.371 (0.374) data 0.000 (0.003) loss 0.9990 (0.8818) lr 6.6987e-04 eta 0:23:13
epoch [26/30] batch [280/796] time 0.379 (0.374) data 0.000 (0.003) loss 0.9482 (0.8726) lr 6.6987e-04 eta 0:23:05
epoch [26/30] batch [300/796] time 0.325 (0.374) data 0.000 (0.003) loss 0.3740 (0.8714) lr 6.6987e-04 eta 0:22:54
epoch [26/30] batch [320/796] time 0.343 (0.373) data 0.000 (0.003) loss 1.2920 (0.8621) lr 6.6987e-04 eta 0:22:46
epoch [26/30] batch [340/796] time 0.392 (0.373) data 0.000 (0.003) loss 0.2905 (0.8600) lr 6.6987e-04 eta 0:22:38
epoch [26/30] batch [360/796] time 0.402 (0.374) data 0.000 (0.003) loss 0.5293 (0.8466) lr 6.6987e-04 eta 0:22:32
epoch [26/30] batch [380/796] time 0.395 (0.374) data 0.000 (0.002) loss 0.3638 (0.8360) lr 6.6987e-04 eta 0:22:24
epoch [26/30] batch [400/796] time 0.347 (0.373) data 0.000 (0.002) loss 2.2852 (0.8422) lr 6.6987e-04 eta 0:22:16
epoch [26/30] batch [420/796] time 0.359 (0.374) data 0.000 (0.002) loss 0.6221 (0.8360) lr 6.6987e-04 eta 0:22:11
epoch [26/30] batch [440/796] time 0.394 (0.374) data 0.000 (0.002) loss 0.7300 (0.8377) lr 6.6987e-04 eta 0:22:04
epoch [26/30] batch [460/796] time 0.347 (0.374) data 0.000 (0.002) loss 0.9648 (0.8389) lr 6.6987e-04 eta 0:21:58
epoch [26/30] batch [480/796] time 0.376 (0.375) data 0.000 (0.002) loss 1.0098 (0.8392) lr 6.6987e-04 eta 0:21:50
epoch [26/30] batch [500/796] time 0.352 (0.374) data 0.000 (0.002) loss 0.5229 (0.8355) lr 6.6987e-04 eta 0:21:42
epoch [26/30] batch [520/796] time 0.370 (0.374) data 0.000 (0.002) loss 0.9429 (0.8299) lr 6.6987e-04 eta 0:21:34
epoch [26/30] batch [540/796] time 0.399 (0.374) data 0.000 (0.002) loss 1.6484 (0.8304) lr 6.6987e-04 eta 0:21:27
epoch [26/30] batch [560/796] time 0.356 (0.374) data 0.000 (0.002) loss 1.3916 (0.8321) lr 6.6987e-04 eta 0:21:19
epoch [26/30] batch [580/796] time 0.383 (0.374) data 0.000 (0.002) loss 0.2473 (0.8321) lr 6.6987e-04 eta 0:21:12
epoch [26/30] batch [600/796] time 0.384 (0.374) data 0.000 (0.002) loss 0.1176 (0.8291) lr 6.6987e-04 eta 0:21:04
epoch [26/30] batch [620/796] time 0.352 (0.374) data 0.000 (0.002) loss 0.6597 (0.8266) lr 6.6987e-04 eta 0:20:56
epoch [26/30] batch [640/796] time 0.362 (0.374) data 0.000 (0.002) loss 0.2203 (0.8313) lr 6.6987e-04 eta 0:20:47
epoch [26/30] batch [660/796] time 0.336 (0.373) data 0.000 (0.002) loss 1.4424 (0.8284) lr 6.6987e-04 eta 0:20:39
epoch [26/30] batch [680/796] time 0.392 (0.373) data 0.000 (0.001) loss 1.0840 (0.8355) lr 6.6987e-04 eta 0:20:31
epoch [26/30] batch [700/796] time 0.371 (0.373) data 0.000 (0.001) loss 1.0127 (0.8345) lr 6.6987e-04 eta 0:20:23
epoch [26/30] batch [720/796] time 0.380 (0.373) data 0.000 (0.001) loss 0.9951 (0.8386) lr 6.6987e-04 eta 0:20:16
epoch [26/30] batch [740/796] time 0.382 (0.373) data 0.000 (0.001) loss 0.5439 (0.8384) lr 6.6987e-04 eta 0:20:09
epoch [26/30] batch [760/796] time 0.375 (0.373) data 0.000 (0.001) loss 0.3806 (0.8377) lr 6.6987e-04 eta 0:20:01
epoch [26/30] batch [780/796] time 0.325 (0.372) data 0.000 (0.001) loss 0.5239 (0.8393) lr 6.6987e-04 eta 0:19:51
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:46,  5.62s/it] 10%|█         | 2/20 [00:06<00:53,  2.96s/it] 15%|█▌        | 3/20 [00:06<00:29,  1.73s/it] 20%|██        | 4/20 [00:07<00:18,  1.16s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.19it/s] 30%|███       | 6/20 [00:07<00:09,  1.54it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.89it/s] 40%|████      | 8/20 [00:08<00:05,  2.22it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.53it/s] 50%|█████     | 10/20 [00:08<00:03,  2.80it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.05it/s] 60%|██████    | 12/20 [00:09<00:02,  3.33it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.50it/s] 70%|███████   | 14/20 [00:09<00:01,  3.64it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.77it/s] 80%|████████  | 16/20 [00:10<00:01,  3.80it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.87it/s] 90%|█████████ | 18/20 [00:10<00:00,  3.95it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.32it/s]100%|██████████| 20/20 [00:11<00:00,  4.70it/s]100%|██████████| 20/20 [00:11<00:00,  1.75it/s]=> result
* total: 1,990
* correct: 1,588
* accuracy: 79.8%
* error: 20.2%
* macro_f1: 79.2%

epoch [27/30] batch [20/796] time 0.379 (0.420) data 0.000 (0.049) loss 0.4812 (0.9143) lr 4.3227e-04 eta 0:22:09
epoch [27/30] batch [40/796] time 0.382 (0.396) data 0.000 (0.024) loss 0.4819 (0.9217) lr 4.3227e-04 eta 0:20:45
epoch [27/30] batch [60/796] time 0.344 (0.385) data 0.000 (0.016) loss 0.5923 (0.8841) lr 4.3227e-04 eta 0:20:01
epoch [27/30] batch [80/796] time 0.375 (0.380) data 0.000 (0.012) loss 0.7310 (0.8372) lr 4.3227e-04 eta 0:19:40
epoch [27/30] batch [100/796] time 0.372 (0.377) data 0.000 (0.010) loss 0.4043 (0.8424) lr 4.3227e-04 eta 0:19:23
epoch [27/30] batch [120/796] time 0.354 (0.375) data 0.000 (0.008) loss 0.6460 (0.8819) lr 4.3227e-04 eta 0:19:08
epoch [27/30] batch [140/796] time 0.347 (0.373) data 0.000 (0.007) loss 0.7129 (0.8599) lr 4.3227e-04 eta 0:18:55
epoch [27/30] batch [160/796] time 0.340 (0.373) data 0.000 (0.006) loss 0.7285 (0.8289) lr 4.3227e-04 eta 0:18:47
epoch [27/30] batch [180/796] time 0.374 (0.372) data 0.000 (0.006) loss 0.7441 (0.8303) lr 4.3227e-04 eta 0:18:37
epoch [27/30] batch [200/796] time 0.411 (0.373) data 0.000 (0.005) loss 1.3799 (0.8242) lr 4.3227e-04 eta 0:18:31
epoch [27/30] batch [220/796] time 0.355 (0.372) data 0.000 (0.005) loss 0.5791 (0.8397) lr 4.3227e-04 eta 0:18:22
epoch [27/30] batch [240/796] time 0.387 (0.372) data 0.000 (0.004) loss 0.5220 (0.8460) lr 4.3227e-04 eta 0:18:15
epoch [27/30] batch [260/796] time 0.343 (0.372) data 0.000 (0.004) loss 0.6143 (0.8431) lr 4.3227e-04 eta 0:18:08
epoch [27/30] batch [280/796] time 0.362 (0.372) data 0.000 (0.004) loss 0.7168 (0.8356) lr 4.3227e-04 eta 0:18:00
epoch [27/30] batch [300/796] time 0.378 (0.372) data 0.000 (0.003) loss 1.5059 (0.8345) lr 4.3227e-04 eta 0:17:51
epoch [27/30] batch [320/796] time 0.372 (0.371) data 0.000 (0.003) loss 0.4556 (0.8484) lr 4.3227e-04 eta 0:17:42
epoch [27/30] batch [340/796] time 0.344 (0.371) data 0.000 (0.003) loss 0.8052 (0.8544) lr 4.3227e-04 eta 0:17:33
epoch [27/30] batch [360/796] time 0.417 (0.370) data 0.000 (0.003) loss 0.6802 (0.8522) lr 4.3227e-04 eta 0:17:25
epoch [27/30] batch [380/796] time 0.374 (0.370) data 0.000 (0.003) loss 2.2090 (0.8504) lr 4.3227e-04 eta 0:17:17
epoch [27/30] batch [400/796] time 0.352 (0.370) data 0.000 (0.003) loss 0.2756 (0.8558) lr 4.3227e-04 eta 0:17:09
epoch [27/30] batch [420/796] time 0.366 (0.370) data 0.001 (0.003) loss 0.4656 (0.8546) lr 4.3227e-04 eta 0:17:03
epoch [27/30] batch [440/796] time 0.364 (0.371) data 0.000 (0.002) loss 0.6777 (0.8478) lr 4.3227e-04 eta 0:16:57
epoch [27/30] batch [460/796] time 0.382 (0.371) data 0.000 (0.002) loss 1.4385 (0.8429) lr 4.3227e-04 eta 0:16:49
epoch [27/30] batch [480/796] time 0.378 (0.370) data 0.000 (0.002) loss 1.8848 (0.8540) lr 4.3227e-04 eta 0:16:41
epoch [27/30] batch [500/796] time 0.384 (0.371) data 0.000 (0.002) loss 0.6826 (0.8483) lr 4.3227e-04 eta 0:16:34
epoch [27/30] batch [520/796] time 0.352 (0.371) data 0.000 (0.002) loss 1.0068 (0.8568) lr 4.3227e-04 eta 0:16:27
epoch [27/30] batch [540/796] time 0.339 (0.370) data 0.000 (0.002) loss 0.3418 (0.8557) lr 4.3227e-04 eta 0:16:19
epoch [27/30] batch [560/796] time 0.381 (0.370) data 0.000 (0.002) loss 0.8276 (0.8589) lr 4.3227e-04 eta 0:16:11
epoch [27/30] batch [580/796] time 0.451 (0.370) data 0.000 (0.002) loss 1.0596 (0.8658) lr 4.3227e-04 eta 0:16:04
epoch [27/30] batch [600/796] time 0.399 (0.370) data 0.000 (0.002) loss 0.9873 (0.8584) lr 4.3227e-04 eta 0:15:56
epoch [27/30] batch [620/796] time 0.385 (0.370) data 0.000 (0.002) loss 0.7300 (0.8546) lr 4.3227e-04 eta 0:15:48
epoch [27/30] batch [640/796] time 0.400 (0.370) data 0.000 (0.002) loss 0.2224 (0.8538) lr 4.3227e-04 eta 0:15:40
epoch [27/30] batch [660/796] time 0.374 (0.370) data 0.000 (0.002) loss 1.2207 (0.8514) lr 4.3227e-04 eta 0:15:33
epoch [27/30] batch [680/796] time 0.344 (0.370) data 0.000 (0.002) loss 0.1576 (0.8483) lr 4.3227e-04 eta 0:15:26
epoch [27/30] batch [700/796] time 0.340 (0.370) data 0.000 (0.002) loss 1.3613 (0.8516) lr 4.3227e-04 eta 0:15:18
epoch [27/30] batch [720/796] time 0.382 (0.370) data 0.000 (0.002) loss 0.6909 (0.8515) lr 4.3227e-04 eta 0:15:11
epoch [27/30] batch [740/796] time 0.349 (0.370) data 0.000 (0.002) loss 0.6787 (0.8455) lr 4.3227e-04 eta 0:15:04
epoch [27/30] batch [760/796] time 0.382 (0.370) data 0.000 (0.002) loss 0.7905 (0.8456) lr 4.3227e-04 eta 0:14:56
epoch [27/30] batch [780/796] time 0.333 (0.369) data 0.000 (0.002) loss 0.1494 (0.8428) lr 4.3227e-04 eta 0:14:48
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<01:56,  6.14s/it] 10%|█         | 2/20 [00:07<00:56,  3.16s/it] 15%|█▌        | 3/20 [00:07<00:31,  1.84s/it] 20%|██        | 4/20 [00:07<00:19,  1.23s/it] 25%|██▌       | 5/20 [00:08<00:13,  1.13it/s] 30%|███       | 6/20 [00:08<00:09,  1.45it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.79it/s] 40%|████      | 8/20 [00:08<00:05,  2.13it/s] 45%|████▌     | 9/20 [00:09<00:04,  2.41it/s] 50%|█████     | 10/20 [00:09<00:03,  2.68it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.89it/s] 60%|██████    | 12/20 [00:10<00:02,  3.09it/s] 65%|██████▌   | 13/20 [00:10<00:02,  3.25it/s] 70%|███████   | 14/20 [00:10<00:01,  3.48it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.64it/s] 80%|████████  | 16/20 [00:11<00:01,  3.86it/s] 85%|████████▌ | 17/20 [00:11<00:00,  4.01it/s] 90%|█████████ | 18/20 [00:11<00:00,  2.53it/s] 95%|█████████▌| 19/20 [00:12<00:00,  3.02it/s]100%|██████████| 20/20 [00:12<00:00,  3.54it/s]100%|██████████| 20/20 [00:12<00:00,  1.60it/s]=> result
* total: 1,990
* correct: 1,592
* accuracy: 80.0%
* error: 20.0%
* macro_f1: 79.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar

epoch [28/30] batch [20/796] time 0.386 (0.434) data 0.000 (0.039) loss 1.3047 (0.8966) lr 2.4472e-04 eta 0:17:07
epoch [28/30] batch [40/796] time 0.342 (0.403) data 0.000 (0.020) loss 1.3418 (0.8286) lr 2.4472e-04 eta 0:15:47
epoch [28/30] batch [60/796] time 0.364 (0.391) data 0.000 (0.013) loss 1.4531 (0.8641) lr 2.4472e-04 eta 0:15:10
epoch [28/30] batch [80/796] time 0.381 (0.385) data 0.000 (0.010) loss 0.9902 (0.8892) lr 2.4472e-04 eta 0:14:49
epoch [28/30] batch [100/796] time 0.356 (0.382) data 0.000 (0.008) loss 1.2090 (0.9157) lr 2.4472e-04 eta 0:14:34
epoch [28/30] batch [120/796] time 0.460 (0.381) data 0.000 (0.007) loss 0.2195 (0.9157) lr 2.4472e-04 eta 0:14:24
epoch [28/30] batch [140/796] time 0.333 (0.379) data 0.000 (0.006) loss 1.0869 (0.9040) lr 2.4472e-04 eta 0:14:11
epoch [28/30] batch [160/796] time 0.362 (0.379) data 0.000 (0.005) loss 0.3882 (0.8869) lr 2.4472e-04 eta 0:14:04
epoch [28/30] batch [180/796] time 0.374 (0.378) data 0.000 (0.005) loss 0.9131 (0.8941) lr 2.4472e-04 eta 0:13:55
epoch [28/30] batch [200/796] time 0.345 (0.377) data 0.000 (0.004) loss 0.7036 (0.8929) lr 2.4472e-04 eta 0:13:45
epoch [28/30] batch [220/796] time 0.381 (0.376) data 0.000 (0.004) loss 1.8525 (0.8942) lr 2.4472e-04 eta 0:13:36
epoch [28/30] batch [240/796] time 0.358 (0.376) data 0.000 (0.004) loss 0.5723 (0.8752) lr 2.4472e-04 eta 0:13:28
epoch [28/30] batch [260/796] time 0.392 (0.376) data 0.000 (0.003) loss 0.2581 (0.8570) lr 2.4472e-04 eta 0:13:19
epoch [28/30] batch [280/796] time 0.364 (0.376) data 0.000 (0.003) loss 1.7451 (0.8487) lr 2.4472e-04 eta 0:13:11
epoch [28/30] batch [300/796] time 0.370 (0.375) data 0.000 (0.003) loss 0.2073 (0.8472) lr 2.4472e-04 eta 0:13:03
epoch [28/30] batch [320/796] time 0.381 (0.375) data 0.000 (0.003) loss 0.7090 (0.8440) lr 2.4472e-04 eta 0:12:55
epoch [28/30] batch [340/796] time 0.391 (0.374) data 0.000 (0.003) loss 2.0840 (0.8388) lr 2.4472e-04 eta 0:12:46
epoch [28/30] batch [360/796] time 0.371 (0.374) data 0.000 (0.002) loss 0.7354 (0.8358) lr 2.4472e-04 eta 0:12:38
epoch [28/30] batch [380/796] time 0.371 (0.374) data 0.000 (0.002) loss 2.6270 (0.8321) lr 2.4472e-04 eta 0:12:30
epoch [28/30] batch [400/796] time 0.398 (0.373) data 0.000 (0.002) loss 1.2500 (0.8235) lr 2.4472e-04 eta 0:12:22
epoch [28/30] batch [420/796] time 0.370 (0.373) data 0.000 (0.002) loss 0.4177 (0.8184) lr 2.4472e-04 eta 0:12:14
epoch [28/30] batch [440/796] time 0.375 (0.373) data 0.000 (0.002) loss 1.7822 (0.8202) lr 2.4472e-04 eta 0:12:06
epoch [28/30] batch [460/796] time 0.348 (0.373) data 0.000 (0.002) loss 1.9727 (0.8290) lr 2.4472e-04 eta 0:11:58
epoch [28/30] batch [480/796] time 0.345 (0.372) data 0.000 (0.002) loss 0.7441 (0.8269) lr 2.4472e-04 eta 0:11:50
epoch [28/30] batch [500/796] time 0.364 (0.372) data 0.000 (0.002) loss 0.5601 (0.8247) lr 2.4472e-04 eta 0:11:41
epoch [28/30] batch [520/796] time 0.377 (0.371) data 0.000 (0.002) loss 0.8452 (0.8241) lr 2.4472e-04 eta 0:11:33
epoch [28/30] batch [540/796] time 0.357 (0.372) data 0.000 (0.002) loss 0.4783 (0.8269) lr 2.4472e-04 eta 0:11:26
epoch [28/30] batch [560/796] time 0.390 (0.372) data 0.000 (0.002) loss 2.4570 (0.8385) lr 2.4472e-04 eta 0:11:19
epoch [28/30] batch [580/796] time 0.376 (0.372) data 0.000 (0.002) loss 1.5508 (0.8357) lr 2.4472e-04 eta 0:11:11
epoch [28/30] batch [600/796] time 0.402 (0.372) data 0.000 (0.002) loss 0.4082 (0.8363) lr 2.4472e-04 eta 0:11:04
epoch [28/30] batch [620/796] time 0.341 (0.372) data 0.000 (0.002) loss 0.7485 (0.8338) lr 2.4472e-04 eta 0:10:57
epoch [28/30] batch [640/796] time 0.340 (0.372) data 0.000 (0.002) loss 0.6934 (0.8353) lr 2.4472e-04 eta 0:10:50
epoch [28/30] batch [660/796] time 0.397 (0.372) data 0.000 (0.001) loss 0.7173 (0.8308) lr 2.4472e-04 eta 0:10:43
epoch [28/30] batch [680/796] time 0.391 (0.372) data 0.000 (0.001) loss 0.5576 (0.8320) lr 2.4472e-04 eta 0:10:35
epoch [28/30] batch [700/796] time 0.336 (0.372) data 0.000 (0.001) loss 0.5039 (0.8347) lr 2.4472e-04 eta 0:10:28
epoch [28/30] batch [720/796] time 0.400 (0.372) data 0.000 (0.001) loss 0.3647 (0.8341) lr 2.4472e-04 eta 0:10:20
epoch [28/30] batch [740/796] time 0.358 (0.372) data 0.000 (0.001) loss 0.5156 (0.8402) lr 2.4472e-04 eta 0:10:13
epoch [28/30] batch [760/796] time 0.379 (0.372) data 0.000 (0.001) loss 1.1729 (0.8441) lr 2.4472e-04 eta 0:10:05
epoch [28/30] batch [780/796] time 0.331 (0.371) data 0.000 (0.001) loss 2.6934 (0.8455) lr 2.4472e-04 eta 0:09:57
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:51,  5.87s/it] 10%|█         | 2/20 [00:07<00:55,  3.11s/it] 15%|█▌        | 3/20 [00:07<00:30,  1.82s/it] 20%|██        | 4/20 [00:07<00:19,  1.22s/it] 25%|██▌       | 5/20 [00:07<00:13,  1.13it/s] 30%|███       | 6/20 [00:08<00:09,  1.47it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.81it/s] 40%|████      | 8/20 [00:08<00:05,  2.15it/s] 45%|████▌     | 9/20 [00:09<00:04,  2.46it/s] 50%|█████     | 10/20 [00:09<00:03,  2.73it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.98it/s] 60%|██████    | 12/20 [00:09<00:02,  3.19it/s] 65%|██████▌   | 13/20 [00:10<00:02,  3.46it/s] 70%|███████   | 14/20 [00:10<00:01,  3.72it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.78it/s] 80%|████████  | 16/20 [00:10<00:01,  3.91it/s] 85%|████████▌ | 17/20 [00:11<00:00,  4.00it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.21it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.67it/s]100%|██████████| 20/20 [00:11<00:00,  4.14it/s]100%|██████████| 20/20 [00:11<00:00,  1.67it/s]=> result
* total: 1,990
* correct: 1,592
* accuracy: 80.0%
* error: 20.0%
* macro_f1: 79.4%

epoch [29/30] batch [20/796] time 0.404 (0.427) data 0.000 (0.040) loss 0.4026 (0.6473) lr 1.0926e-04 eta 0:11:11
epoch [29/30] batch [40/796] time 0.368 (0.400) data 0.000 (0.020) loss 0.2632 (0.7206) lr 1.0926e-04 eta 0:10:21
epoch [29/30] batch [60/796] time 0.375 (0.387) data 0.000 (0.014) loss 0.9414 (0.7560) lr 1.0926e-04 eta 0:09:53
epoch [29/30] batch [80/796] time 0.344 (0.381) data 0.000 (0.010) loss 1.1143 (0.7736) lr 1.0926e-04 eta 0:09:36
epoch [29/30] batch [100/796] time 0.351 (0.379) data 0.000 (0.008) loss 0.4966 (0.7734) lr 1.0926e-04 eta 0:09:25
epoch [29/30] batch [120/796] time 0.370 (0.377) data 0.000 (0.007) loss 0.5000 (0.7801) lr 1.0926e-04 eta 0:09:14
epoch [29/30] batch [140/796] time 0.375 (0.375) data 0.000 (0.006) loss 0.0792 (0.7938) lr 1.0926e-04 eta 0:09:04
epoch [29/30] batch [160/796] time 0.387 (0.374) data 0.000 (0.005) loss 0.2512 (0.7929) lr 1.0926e-04 eta 0:08:56
epoch [29/30] batch [180/796] time 0.344 (0.373) data 0.000 (0.005) loss 1.2129 (0.7908) lr 1.0926e-04 eta 0:08:46
epoch [29/30] batch [200/796] time 0.386 (0.372) data 0.000 (0.004) loss 1.2139 (0.7958) lr 1.0926e-04 eta 0:08:38
epoch [29/30] batch [220/796] time 0.381 (0.372) data 0.000 (0.004) loss 1.4707 (0.8038) lr 1.0926e-04 eta 0:08:30
epoch [29/30] batch [240/796] time 0.349 (0.373) data 0.000 (0.004) loss 0.7271 (0.8116) lr 1.0926e-04 eta 0:08:23
epoch [29/30] batch [260/796] time 0.352 (0.372) data 0.000 (0.003) loss 1.4648 (0.8109) lr 1.0926e-04 eta 0:08:16
epoch [29/30] batch [280/796] time 0.334 (0.372) data 0.000 (0.003) loss 1.1582 (0.8033) lr 1.0926e-04 eta 0:08:08
epoch [29/30] batch [300/796] time 0.342 (0.372) data 0.000 (0.003) loss 0.6470 (0.8032) lr 1.0926e-04 eta 0:08:01
epoch [29/30] batch [320/796] time 0.423 (0.373) data 0.000 (0.003) loss 0.4500 (0.8049) lr 1.0926e-04 eta 0:07:54
epoch [29/30] batch [340/796] time 0.379 (0.373) data 0.000 (0.003) loss 0.2122 (0.8114) lr 1.0926e-04 eta 0:07:46
epoch [29/30] batch [360/796] time 0.382 (0.372) data 0.000 (0.002) loss 0.7817 (0.8084) lr 1.0926e-04 eta 0:07:38
epoch [29/30] batch [380/796] time 0.364 (0.372) data 0.000 (0.002) loss 0.6436 (0.8037) lr 1.0926e-04 eta 0:07:31
epoch [29/30] batch [400/796] time 0.347 (0.372) data 0.000 (0.002) loss 0.1244 (0.8071) lr 1.0926e-04 eta 0:07:23
epoch [29/30] batch [420/796] time 0.355 (0.372) data 0.000 (0.002) loss 0.6133 (0.8039) lr 1.0926e-04 eta 0:07:15
epoch [29/30] batch [440/796] time 0.376 (0.371) data 0.000 (0.002) loss 0.3633 (0.7965) lr 1.0926e-04 eta 0:07:07
epoch [29/30] batch [460/796] time 0.378 (0.371) data 0.000 (0.002) loss 0.1919 (0.7915) lr 1.0926e-04 eta 0:07:00
epoch [29/30] batch [480/796] time 0.349 (0.371) data 0.000 (0.002) loss 1.0244 (0.8003) lr 1.0926e-04 eta 0:06:52
epoch [29/30] batch [500/796] time 0.390 (0.371) data 0.000 (0.002) loss 1.8232 (0.8040) lr 1.0926e-04 eta 0:06:45
epoch [29/30] batch [520/796] time 0.354 (0.371) data 0.000 (0.002) loss 0.4990 (0.8120) lr 1.0926e-04 eta 0:06:38
epoch [29/30] batch [540/796] time 0.367 (0.372) data 0.000 (0.002) loss 1.1768 (0.8204) lr 1.0926e-04 eta 0:06:30
epoch [29/30] batch [560/796] time 0.390 (0.371) data 0.000 (0.002) loss 1.3359 (0.8252) lr 1.0926e-04 eta 0:06:23
epoch [29/30] batch [580/796] time 0.368 (0.371) data 0.000 (0.002) loss 0.6133 (0.8274) lr 1.0926e-04 eta 0:06:15
epoch [29/30] batch [600/796] time 0.360 (0.371) data 0.000 (0.002) loss 0.3682 (0.8257) lr 1.0926e-04 eta 0:06:08
epoch [29/30] batch [620/796] time 0.406 (0.371) data 0.000 (0.002) loss 0.2330 (0.8266) lr 1.0926e-04 eta 0:06:00
epoch [29/30] batch [640/796] time 0.393 (0.371) data 0.000 (0.002) loss 0.3511 (0.8256) lr 1.0926e-04 eta 0:05:53
epoch [29/30] batch [660/796] time 0.382 (0.371) data 0.000 (0.001) loss 0.9722 (0.8276) lr 1.0926e-04 eta 0:05:45
epoch [29/30] batch [680/796] time 0.342 (0.371) data 0.000 (0.001) loss 0.2091 (0.8251) lr 1.0926e-04 eta 0:05:38
epoch [29/30] batch [700/796] time 0.400 (0.371) data 0.000 (0.001) loss 0.6201 (0.8247) lr 1.0926e-04 eta 0:05:30
epoch [29/30] batch [720/796] time 0.374 (0.371) data 0.000 (0.001) loss 1.0508 (0.8294) lr 1.0926e-04 eta 0:05:23
epoch [29/30] batch [740/796] time 0.382 (0.371) data 0.000 (0.001) loss 1.8242 (0.8301) lr 1.0926e-04 eta 0:05:15
epoch [29/30] batch [760/796] time 0.347 (0.371) data 0.000 (0.001) loss 0.1425 (0.8221) lr 1.0926e-04 eta 0:05:08
epoch [29/30] batch [780/796] time 0.324 (0.370) data 0.000 (0.001) loss 0.8652 (0.8205) lr 1.0926e-04 eta 0:05:00
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:50,  5.84s/it] 10%|█         | 2/20 [00:06<00:50,  2.83s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.67s/it] 20%|██        | 4/20 [00:07<00:18,  1.13s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.22it/s] 30%|███       | 6/20 [00:07<00:08,  1.58it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.93it/s] 40%|████      | 8/20 [00:08<00:05,  2.25it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.55it/s] 50%|█████     | 10/20 [00:08<00:03,  2.78it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.97it/s] 60%|██████    | 12/20 [00:09<00:02,  3.18it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.39it/s] 70%|███████   | 14/20 [00:09<00:01,  3.55it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.64it/s] 80%|████████  | 16/20 [00:10<00:01,  3.88it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.86it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.05it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.40it/s]100%|██████████| 20/20 [00:11<00:00,  4.76it/s]100%|██████████| 20/20 [00:11<00:00,  1.77it/s]=> result
* total: 1,990
* correct: 1,592
* accuracy: 80.0%
* error: 20.0%
* macro_f1: 79.4%

epoch [30/30] batch [20/796] time 0.381 (0.425) data 0.000 (0.049) loss 1.8594 (1.1202) lr 2.7391e-05 eta 0:05:30
epoch [30/30] batch [40/796] time 0.374 (0.400) data 0.000 (0.025) loss 0.6284 (1.0343) lr 2.7391e-05 eta 0:05:02
epoch [30/30] batch [60/796] time 0.403 (0.390) data 0.000 (0.016) loss 0.5093 (0.9830) lr 2.7391e-05 eta 0:04:46
epoch [30/30] batch [80/796] time 0.382 (0.386) data 0.000 (0.012) loss 0.7402 (0.9015) lr 2.7391e-05 eta 0:04:36
epoch [30/30] batch [100/796] time 0.373 (0.384) data 0.000 (0.010) loss 0.7886 (0.8838) lr 2.7391e-05 eta 0:04:27
epoch [30/30] batch [120/796] time 0.345 (0.381) data 0.000 (0.008) loss 1.6406 (0.8803) lr 2.7391e-05 eta 0:04:17
epoch [30/30] batch [140/796] time 0.398 (0.381) data 0.000 (0.007) loss 0.2427 (0.8539) lr 2.7391e-05 eta 0:04:10
epoch [30/30] batch [160/796] time 0.339 (0.379) data 0.000 (0.006) loss 1.4834 (0.8413) lr 2.7391e-05 eta 0:04:01
epoch [30/30] batch [180/796] time 0.341 (0.377) data 0.000 (0.006) loss 0.2450 (0.8355) lr 2.7391e-05 eta 0:03:52
epoch [30/30] batch [200/796] time 0.361 (0.376) data 0.001 (0.005) loss 0.5918 (0.8365) lr 2.7391e-05 eta 0:03:44
epoch [30/30] batch [220/796] time 0.332 (0.374) data 0.000 (0.005) loss 0.5957 (0.8331) lr 2.7391e-05 eta 0:03:35
epoch [30/30] batch [240/796] time 0.410 (0.375) data 0.000 (0.004) loss 0.7368 (0.8232) lr 2.7391e-05 eta 0:03:28
epoch [30/30] batch [260/796] time 0.388 (0.374) data 0.000 (0.004) loss 1.0264 (0.8247) lr 2.7391e-05 eta 0:03:20
epoch [30/30] batch [280/796] time 0.355 (0.373) data 0.000 (0.004) loss 0.9961 (0.8329) lr 2.7391e-05 eta 0:03:12
epoch [30/30] batch [300/796] time 0.347 (0.373) data 0.000 (0.003) loss 1.4023 (0.8284) lr 2.7391e-05 eta 0:03:05
epoch [30/30] batch [320/796] time 0.337 (0.373) data 0.000 (0.003) loss 0.8486 (0.8337) lr 2.7391e-05 eta 0:02:57
epoch [30/30] batch [340/796] time 0.363 (0.373) data 0.000 (0.003) loss 0.6489 (0.8319) lr 2.7391e-05 eta 0:02:50
epoch [30/30] batch [360/796] time 0.372 (0.373) data 0.000 (0.003) loss 1.5752 (0.8206) lr 2.7391e-05 eta 0:02:42
epoch [30/30] batch [380/796] time 0.368 (0.373) data 0.000 (0.003) loss 0.8667 (0.8221) lr 2.7391e-05 eta 0:02:35
epoch [30/30] batch [400/796] time 0.358 (0.372) data 0.000 (0.003) loss 1.5508 (0.8227) lr 2.7391e-05 eta 0:02:27
epoch [30/30] batch [420/796] time 0.371 (0.373) data 0.000 (0.003) loss 0.8887 (0.8142) lr 2.7391e-05 eta 0:02:20
epoch [30/30] batch [440/796] time 0.351 (0.373) data 0.000 (0.002) loss 0.3521 (0.8130) lr 2.7391e-05 eta 0:02:12
epoch [30/30] batch [460/796] time 0.356 (0.372) data 0.000 (0.002) loss 1.6768 (0.8271) lr 2.7391e-05 eta 0:02:04
epoch [30/30] batch [480/796] time 0.349 (0.372) data 0.000 (0.002) loss 1.1045 (0.8264) lr 2.7391e-05 eta 0:01:57
epoch [30/30] batch [500/796] time 0.371 (0.372) data 0.000 (0.002) loss 0.5029 (0.8282) lr 2.7391e-05 eta 0:01:50
epoch [30/30] batch [520/796] time 0.352 (0.372) data 0.000 (0.002) loss 0.5972 (0.8296) lr 2.7391e-05 eta 0:01:42
epoch [30/30] batch [540/796] time 0.357 (0.372) data 0.000 (0.002) loss 0.4121 (0.8250) lr 2.7391e-05 eta 0:01:35
epoch [30/30] batch [560/796] time 0.364 (0.372) data 0.000 (0.002) loss 0.7681 (0.8241) lr 2.7391e-05 eta 0:01:27
epoch [30/30] batch [580/796] time 0.372 (0.371) data 0.000 (0.002) loss 1.4590 (0.8235) lr 2.7391e-05 eta 0:01:20
epoch [30/30] batch [600/796] time 0.384 (0.371) data 0.001 (0.002) loss 0.5913 (0.8232) lr 2.7391e-05 eta 0:01:12
epoch [30/30] batch [620/796] time 0.385 (0.371) data 0.000 (0.002) loss 0.6245 (0.8226) lr 2.7391e-05 eta 0:01:05
epoch [30/30] batch [640/796] time 0.361 (0.371) data 0.000 (0.002) loss 1.0586 (0.8263) lr 2.7391e-05 eta 0:00:57
epoch [30/30] batch [660/796] time 0.349 (0.372) data 0.000 (0.002) loss 1.0791 (0.8298) lr 2.7391e-05 eta 0:00:50
epoch [30/30] batch [680/796] time 0.347 (0.372) data 0.000 (0.002) loss 0.2908 (0.8301) lr 2.7391e-05 eta 0:00:43
epoch [30/30] batch [700/796] time 0.394 (0.372) data 0.000 (0.002) loss 1.3613 (0.8322) lr 2.7391e-05 eta 0:00:35
epoch [30/30] batch [720/796] time 0.408 (0.372) data 0.000 (0.002) loss 0.2386 (0.8271) lr 2.7391e-05 eta 0:00:28
epoch [30/30] batch [740/796] time 0.375 (0.372) data 0.000 (0.002) loss 0.4761 (0.8252) lr 2.7391e-05 eta 0:00:20
epoch [30/30] batch [760/796] time 0.383 (0.372) data 0.000 (0.002) loss 1.1699 (0.8250) lr 2.7391e-05 eta 0:00:13
epoch [30/30] batch [780/796] time 0.330 (0.371) data 0.000 (0.001) loss 0.9102 (0.8235) lr 2.7391e-05 eta 0:00:05
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:49,  5.78s/it] 10%|█         | 2/20 [00:06<00:52,  2.89s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.69s/it] 20%|██        | 4/20 [00:07<00:18,  1.14s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.20it/s] 30%|███       | 6/20 [00:07<00:08,  1.56it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.92it/s] 40%|████      | 8/20 [00:08<00:05,  2.24it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.55it/s] 50%|█████     | 10/20 [00:08<00:03,  2.75it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.96it/s] 60%|██████    | 12/20 [00:09<00:02,  3.14it/s] 65%|██████▌   | 13/20 [00:09<00:01,  3.56it/s] 70%|███████   | 14/20 [00:09<00:01,  3.66it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.71it/s] 80%|████████  | 16/20 [00:10<00:01,  3.88it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.01it/s] 90%|█████████ | 18/20 [00:10<00:00,  3.57it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.99it/s]100%|██████████| 20/20 [00:11<00:00,  4.41it/s]100%|██████████| 20/20 [00:11<00:00,  1.74it/s]
=> result
* total: 1,990
* correct: 1,593
* accuracy: 80.1%
* error: 19.9%
* macro_f1: 79.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:05<09:35,  5.82s/it]  2%|▏         | 2/100 [00:06<04:19,  2.65s/it]  3%|▎         | 3/100 [00:07<03:36,  2.23s/it]  4%|▍         | 4/100 [00:08<02:25,  1.51s/it]  5%|▌         | 5/100 [00:08<01:46,  1.12s/it]  6%|▌         | 6/100 [00:09<01:25,  1.10it/s]  7%|▋         | 7/100 [00:09<01:10,  1.33it/s]  8%|▊         | 8/100 [00:10<00:59,  1.56it/s]  9%|▉         | 9/100 [00:10<00:50,  1.79it/s] 10%|█         | 10/100 [00:10<00:46,  1.92it/s] 11%|█         | 11/100 [00:11<00:43,  2.04it/s] 12%|█▏        | 12/100 [00:11<00:41,  2.13it/s] 13%|█▎        | 13/100 [00:12<00:39,  2.22it/s] 14%|█▍        | 14/100 [00:12<00:39,  2.19it/s] 15%|█▌        | 15/100 [00:13<00:37,  2.28it/s] 16%|█▌        | 16/100 [00:13<00:37,  2.26it/s] 17%|█▋        | 17/100 [00:13<00:36,  2.26it/s] 18%|█▊        | 18/100 [00:14<00:36,  2.27it/s] 19%|█▉        | 19/100 [00:14<00:35,  2.26it/s] 20%|██        | 20/100 [00:15<00:36,  2.22it/s] 21%|██        | 21/100 [00:15<00:35,  2.25it/s] 22%|██▏       | 22/100 [00:16<00:33,  2.30it/s] 23%|██▎       | 23/100 [00:16<00:34,  2.25it/s] 24%|██▍       | 24/100 [00:17<00:34,  2.20it/s] 25%|██▌       | 25/100 [00:17<00:34,  2.18it/s] 26%|██▌       | 26/100 [00:18<00:35,  2.11it/s] 27%|██▋       | 27/100 [00:18<00:34,  2.11it/s] 28%|██▊       | 28/100 [00:19<00:34,  2.08it/s] 29%|██▉       | 29/100 [00:19<00:34,  2.05it/s] 30%|███       | 30/100 [00:20<00:33,  2.11it/s] 31%|███       | 31/100 [00:20<00:32,  2.15it/s] 32%|███▏      | 32/100 [00:20<00:32,  2.07it/s] 33%|███▎      | 33/100 [00:21<00:32,  2.05it/s] 34%|███▍      | 34/100 [00:21<00:31,  2.09it/s] 35%|███▌      | 35/100 [00:22<00:30,  2.11it/s] 36%|███▌      | 36/100 [00:22<00:29,  2.13it/s] 37%|███▋      | 37/100 [00:23<00:30,  2.09it/s] 38%|███▊      | 38/100 [00:23<00:30,  2.04it/s] 39%|███▉      | 39/100 [00:24<00:29,  2.05it/s] 40%|████      | 40/100 [00:24<00:29,  2.05it/s] 41%|████      | 41/100 [00:25<00:27,  2.12it/s] 42%|████▏     | 42/100 [00:25<00:24,  2.33it/s] 43%|████▎     | 43/100 [00:25<00:23,  2.39it/s] 44%|████▍     | 44/100 [00:26<00:22,  2.50it/s] 45%|████▌     | 45/100 [00:26<00:22,  2.48it/s] 46%|████▌     | 46/100 [00:27<00:21,  2.51it/s] 47%|████▋     | 47/100 [00:27<00:20,  2.54it/s] 48%|████▊     | 48/100 [00:27<00:20,  2.52it/s] 49%|████▉     | 49/100 [00:28<00:20,  2.47it/s] 50%|█████     | 50/100 [00:28<00:21,  2.35it/s] 51%|█████     | 51/100 [00:29<00:21,  2.26it/s] 52%|█████▏    | 52/100 [00:29<00:20,  2.34it/s] 53%|█████▎    | 53/100 [00:30<00:19,  2.39it/s] 54%|█████▍    | 54/100 [00:30<00:18,  2.51it/s] 55%|█████▌    | 55/100 [00:30<00:18,  2.46it/s] 56%|█████▌    | 56/100 [00:31<00:18,  2.44it/s] 57%|█████▋    | 57/100 [00:31<00:18,  2.36it/s] 58%|█████▊    | 58/100 [00:32<00:17,  2.35it/s] 59%|█████▉    | 59/100 [00:32<00:16,  2.50it/s] 60%|██████    | 60/100 [00:32<00:15,  2.55it/s] 61%|██████    | 61/100 [00:33<00:15,  2.59it/s] 62%|██████▏   | 62/100 [00:33<00:14,  2.61it/s] 63%|██████▎   | 63/100 [00:34<00:14,  2.62it/s] 64%|██████▍   | 64/100 [00:34<00:14,  2.48it/s] 65%|██████▌   | 65/100 [00:34<00:14,  2.44it/s] 66%|██████▌   | 66/100 [00:35<00:14,  2.35it/s] 67%|██████▋   | 67/100 [00:35<00:14,  2.26it/s] 68%|██████▊   | 68/100 [00:36<00:14,  2.21it/s] 69%|██████▉   | 69/100 [00:36<00:14,  2.20it/s] 70%|███████   | 70/100 [00:37<00:13,  2.28it/s] 71%|███████   | 71/100 [00:37<00:11,  2.42it/s] 72%|███████▏  | 72/100 [00:37<00:11,  2.52it/s] 73%|███████▎  | 73/100 [00:38<00:10,  2.66it/s] 74%|███████▍  | 74/100 [00:38<00:09,  2.72it/s] 75%|███████▌  | 75/100 [00:38<00:08,  2.80it/s] 76%|███████▌  | 76/100 [00:39<00:08,  2.86it/s] 77%|███████▋  | 77/100 [00:39<00:07,  2.95it/s] 78%|███████▊  | 78/100 [00:39<00:07,  3.08it/s] 79%|███████▉  | 79/100 [00:40<00:06,  3.16it/s] 80%|████████  | 80/100 [00:40<00:06,  3.31it/s] 81%|████████  | 81/100 [00:40<00:05,  3.55it/s] 82%|████████▏ | 82/100 [00:40<00:04,  3.97it/s] 83%|████████▎ | 83/100 [00:41<00:03,  4.33it/s] 84%|████████▍ | 84/100 [00:41<00:03,  4.63it/s] 85%|████████▌ | 85/100 [00:41<00:03,  4.86it/s] 86%|████████▌ | 86/100 [00:41<00:02,  5.04it/s] 87%|████████▋ | 87/100 [00:41<00:02,  5.17it/s] 88%|████████▊ | 88/100 [00:41<00:02,  5.27it/s] 89%|████████▉ | 89/100 [00:42<00:02,  5.34it/s] 90%|█████████ | 90/100 [00:42<00:01,  5.40it/s] 91%|█████████ | 91/100 [00:42<00:01,  5.43it/s] 92%|█████████▏| 92/100 [00:42<00:01,  5.46it/s] 93%|█████████▎| 93/100 [00:42<00:01,  5.48it/s] 94%|█████████▍| 94/100 [00:43<00:01,  5.49it/s] 95%|█████████▌| 95/100 [00:43<00:00,  5.50it/s] 96%|█████████▌| 96/100 [00:43<00:00,  5.51it/s] 97%|█████████▋| 97/100 [00:43<00:00,  5.51it/s] 98%|█████████▊| 98/100 [00:43<00:00,  5.52it/s] 99%|█████████▉| 99/100 [00:43<00:00,  5.52it/s]100%|██████████| 100/100 [00:44<00:00,  6.06it/s]100%|██████████| 100/100 [00:44<00:00,  2.26it/s]
=> result
* total: 9,950
* correct: 7,999
* accuracy: 80.4%
* error: 19.6%
* macro_f1: 80.1%
Elapsed: 2:34:20
+ sh scripts/rpo_prime/base2new_test.sh sun397 2 0 main_tmp 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/sun397/shots_16/RPO_prime/main_tmp/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/sun397/shots_16/RPO_prime/main_tmp/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:SUN397
Loading dataset: SUN397
Reading split from /shared/s2/lab01/dataset/clip/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/sun397/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
3168 1980 9900
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  198
# train_x  3,168
# val      1,980
# test     9,900
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed2/prompt_learner/model-best.pth.tar" (epoch = 30)
Evaluate on the *test* set
  0%|          | 0/99 [00:00<?, ?it/s]  1%|          | 1/99 [00:09<16:02,  9.82s/it]  2%|▏         | 2/99 [00:10<06:55,  4.28s/it]  3%|▎         | 3/99 [00:10<04:01,  2.51s/it]  4%|▍         | 4/99 [00:11<02:39,  1.67s/it]  5%|▌         | 5/99 [00:11<01:54,  1.22s/it]  6%|▌         | 6/99 [00:11<01:25,  1.09it/s]  7%|▋         | 7/99 [00:12<01:06,  1.39it/s]  8%|▊         | 8/99 [00:12<00:54,  1.68it/s]  9%|▉         | 9/99 [00:12<00:46,  1.94it/s] 10%|█         | 10/99 [00:13<00:41,  2.15it/s] 11%|█         | 11/99 [00:13<00:38,  2.29it/s] 12%|█▏        | 12/99 [00:13<00:36,  2.40it/s] 13%|█▎        | 13/99 [00:14<00:34,  2.48it/s] 14%|█▍        | 14/99 [00:14<00:35,  2.37it/s] 15%|█▌        | 15/99 [00:15<00:35,  2.36it/s] 16%|█▌        | 16/99 [00:15<00:35,  2.36it/s] 17%|█▋        | 17/99 [00:15<00:34,  2.40it/s] 18%|█▊        | 18/99 [00:16<00:34,  2.32it/s] 19%|█▉        | 19/99 [00:16<00:35,  2.27it/s] 20%|██        | 20/99 [00:17<00:35,  2.25it/s] 21%|██        | 21/99 [00:17<00:34,  2.25it/s] 22%|██▏       | 22/99 [00:18<00:34,  2.26it/s] 23%|██▎       | 23/99 [00:18<00:32,  2.35it/s] 24%|██▍       | 24/99 [00:18<00:31,  2.40it/s] 25%|██▌       | 25/99 [00:19<00:31,  2.38it/s] 26%|██▋       | 26/99 [00:19<00:29,  2.48it/s] 27%|██▋       | 27/99 [00:20<00:28,  2.53it/s] 28%|██▊       | 28/99 [00:20<00:26,  2.65it/s] 29%|██▉       | 29/99 [00:20<00:26,  2.62it/s] 30%|███       | 30/99 [00:21<00:25,  2.72it/s] 31%|███▏      | 31/99 [00:21<00:24,  2.74it/s] 32%|███▏      | 32/99 [00:21<00:24,  2.74it/s] 33%|███▎      | 33/99 [00:22<00:24,  2.69it/s] 34%|███▍      | 34/99 [00:22<00:24,  2.64it/s] 35%|███▌      | 35/99 [00:23<00:26,  2.44it/s] 36%|███▋      | 36/99 [00:23<00:26,  2.39it/s] 37%|███▋      | 37/99 [00:24<00:26,  2.33it/s] 38%|███▊      | 38/99 [00:24<00:26,  2.26it/s] 39%|███▉      | 39/99 [00:24<00:26,  2.28it/s] 40%|████      | 40/99 [00:25<00:24,  2.38it/s] 41%|████▏     | 41/99 [00:25<00:24,  2.40it/s] 42%|████▏     | 42/99 [00:26<00:22,  2.53it/s] 43%|████▎     | 43/99 [00:26<00:21,  2.58it/s] 44%|████▍     | 44/99 [00:26<00:20,  2.67it/s] 45%|████▌     | 45/99 [00:27<00:20,  2.68it/s] 46%|████▋     | 46/99 [00:27<00:19,  2.67it/s] 47%|████▋     | 47/99 [00:27<00:19,  2.62it/s] 48%|████▊     | 48/99 [00:28<00:19,  2.58it/s] 49%|████▉     | 49/99 [00:28<00:19,  2.51it/s] 51%|█████     | 50/99 [00:29<00:19,  2.53it/s] 52%|█████▏    | 51/99 [00:29<00:19,  2.52it/s] 53%|█████▎    | 52/99 [00:30<00:19,  2.42it/s] 54%|█████▎    | 53/99 [00:30<00:19,  2.37it/s] 55%|█████▍    | 54/99 [00:30<00:18,  2.39it/s] 56%|█████▌    | 55/99 [00:31<00:19,  2.24it/s] 57%|█████▋    | 56/99 [00:31<00:19,  2.20it/s] 58%|█████▊    | 57/99 [00:32<00:18,  2.23it/s] 59%|█████▊    | 58/99 [00:32<00:17,  2.39it/s] 60%|█████▉    | 59/99 [00:33<00:16,  2.47it/s] 61%|██████    | 60/99 [00:33<00:15,  2.55it/s] 62%|██████▏   | 61/99 [00:33<00:14,  2.54it/s] 63%|██████▎   | 62/99 [00:34<00:14,  2.52it/s] 64%|██████▎   | 63/99 [00:34<00:14,  2.52it/s] 65%|██████▍   | 64/99 [00:35<00:14,  2.49it/s] 66%|██████▌   | 65/99 [00:35<00:13,  2.47it/s] 67%|██████▋   | 66/99 [00:35<00:13,  2.48it/s] 68%|██████▊   | 67/99 [00:36<00:12,  2.59it/s] 69%|██████▊   | 68/99 [00:36<00:11,  2.62it/s] 70%|██████▉   | 69/99 [00:36<00:11,  2.68it/s] 71%|███████   | 70/99 [00:37<00:10,  2.71it/s] 72%|███████▏  | 71/99 [00:37<00:10,  2.77it/s] 73%|███████▎  | 72/99 [00:37<00:09,  2.79it/s] 74%|███████▎  | 73/99 [00:38<00:09,  2.83it/s] 75%|███████▍  | 74/99 [00:38<00:08,  2.95it/s] 76%|███████▌  | 75/99 [00:38<00:07,  3.01it/s] 77%|███████▋  | 76/99 [00:39<00:07,  3.11it/s] 78%|███████▊  | 77/99 [00:39<00:06,  3.27it/s] 79%|███████▉  | 78/99 [00:39<00:06,  3.37it/s] 80%|███████▉  | 79/99 [00:40<00:05,  3.45it/s] 81%|████████  | 80/99 [00:40<00:05,  3.60it/s] 82%|████████▏ | 81/99 [00:40<00:04,  3.85it/s] 83%|████████▎ | 82/99 [00:40<00:04,  3.92it/s] 84%|████████▍ | 83/99 [00:40<00:03,  4.04it/s] 85%|████████▍ | 84/99 [00:41<00:03,  4.40it/s] 86%|████████▌ | 85/99 [00:41<00:02,  4.69it/s] 87%|████████▋ | 86/99 [00:41<00:02,  4.73it/s] 88%|████████▊ | 87/99 [00:41<00:02,  4.93it/s] 89%|████████▉ | 88/99 [00:41<00:02,  5.09it/s] 90%|████████▉ | 89/99 [00:42<00:01,  5.21it/s] 91%|█████████ | 90/99 [00:42<00:01,  5.15it/s] 92%|█████████▏| 91/99 [00:42<00:01,  5.23it/s] 93%|█████████▎| 92/99 [00:42<00:01,  5.29it/s] 94%|█████████▍| 93/99 [00:42<00:01,  5.34it/s] 95%|█████████▍| 94/99 [00:43<00:00,  5.37it/s] 96%|█████████▌| 95/99 [00:43<00:00,  5.39it/s] 97%|█████████▋| 96/99 [00:43<00:00,  5.41it/s] 98%|█████████▊| 97/99 [00:43<00:00,  5.42it/s] 99%|█████████▉| 98/99 [00:43<00:00,  5.45it/s]100%|██████████| 99/99 [00:43<00:00,  5.48it/s]100%|██████████| 99/99 [00:44<00:00,  2.25it/s]
=> result
* total: 9,900
* correct: 7,813
* accuracy: 78.9%
* error: 21.1%
* macro_f1: 78.1%
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train.sh sun397 3 0 main_tmp 16
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:SUN397
Loading dataset: SUN397
Reading split from /shared/s2/lab01/dataset/clip/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/sun397/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
3184 1990 9950
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  199
# train_x  3,184
# val      1,990
# test     9,950
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
epoch [1/30] batch [20/796] time 0.405 (0.508) data 0.000 (0.058) loss 1.5645 (1.3578) lr 1.0000e-02 eta 3:22:12
epoch [1/30] batch [40/796] time 0.367 (0.437) data 0.000 (0.029) loss 2.4473 (1.4241) lr 1.0000e-02 eta 2:53:34
epoch [1/30] batch [60/796] time 0.413 (0.415) data 0.000 (0.019) loss 1.9609 (1.4085) lr 1.0000e-02 eta 2:44:34
epoch [1/30] batch [80/796] time 0.385 (0.404) data 0.000 (0.015) loss 1.6709 (1.3659) lr 1.0000e-02 eta 2:40:25
epoch [1/30] batch [100/796] time 0.394 (0.397) data 0.000 (0.012) loss 1.0342 (1.3456) lr 1.0000e-02 eta 2:37:19
epoch [1/30] batch [120/796] time 0.399 (0.394) data 0.000 (0.010) loss 0.5239 (1.2775) lr 1.0000e-02 eta 2:35:49
epoch [1/30] batch [140/796] time 0.336 (0.390) data 0.000 (0.008) loss 2.3809 (1.2873) lr 1.0000e-02 eta 2:34:09
epoch [1/30] batch [160/796] time 0.382 (0.386) data 0.000 (0.007) loss 0.8740 (1.2833) lr 1.0000e-02 eta 2:32:37
epoch [1/30] batch [180/796] time 0.385 (0.385) data 0.000 (0.007) loss 0.3438 (1.2450) lr 1.0000e-02 eta 2:31:58
epoch [1/30] batch [200/796] time 0.362 (0.383) data 0.000 (0.006) loss 2.3320 (1.2564) lr 1.0000e-02 eta 2:31:00
epoch [1/30] batch [220/796] time 0.375 (0.381) data 0.000 (0.005) loss 0.5552 (1.2394) lr 1.0000e-02 eta 2:30:13
epoch [1/30] batch [240/796] time 0.348 (0.380) data 0.000 (0.005) loss 0.9502 (1.2104) lr 1.0000e-02 eta 2:29:44
epoch [1/30] batch [260/796] time 0.401 (0.380) data 0.000 (0.005) loss 1.0615 (1.1943) lr 1.0000e-02 eta 2:29:28
epoch [1/30] batch [280/796] time 0.415 (0.379) data 0.000 (0.004) loss 0.6772 (1.1690) lr 1.0000e-02 eta 2:28:55
epoch [1/30] batch [300/796] time 0.386 (0.378) data 0.000 (0.004) loss 1.9180 (1.1650) lr 1.0000e-02 eta 2:28:38
epoch [1/30] batch [320/796] time 0.393 (0.378) data 0.000 (0.004) loss 1.4668 (1.1537) lr 1.0000e-02 eta 2:28:21
epoch [1/30] batch [340/796] time 0.387 (0.377) data 0.000 (0.004) loss 0.9229 (1.1437) lr 1.0000e-02 eta 2:28:05
epoch [1/30] batch [360/796] time 0.353 (0.377) data 0.000 (0.003) loss 0.4370 (1.1224) lr 1.0000e-02 eta 2:27:46
epoch [1/30] batch [380/796] time 0.383 (0.377) data 0.000 (0.003) loss 0.2324 (1.1312) lr 1.0000e-02 eta 2:27:29
epoch [1/30] batch [400/796] time 0.380 (0.376) data 0.000 (0.003) loss 0.2058 (1.1178) lr 1.0000e-02 eta 2:27:13
epoch [1/30] batch [420/796] time 0.387 (0.376) data 0.000 (0.003) loss 1.9248 (1.1346) lr 1.0000e-02 eta 2:27:04
epoch [1/30] batch [440/796] time 0.349 (0.376) data 0.000 (0.003) loss 1.5889 (1.1319) lr 1.0000e-02 eta 2:26:51
epoch [1/30] batch [460/796] time 0.380 (0.375) data 0.000 (0.003) loss 0.5938 (1.1320) lr 1.0000e-02 eta 2:26:29
epoch [1/30] batch [480/796] time 0.351 (0.375) data 0.000 (0.003) loss 1.2129 (1.1379) lr 1.0000e-02 eta 2:26:16
epoch [1/30] batch [500/796] time 0.379 (0.375) data 0.000 (0.003) loss 0.7070 (1.1250) lr 1.0000e-02 eta 2:25:57
epoch [1/30] batch [520/796] time 0.395 (0.374) data 0.000 (0.002) loss 0.6982 (1.1131) lr 1.0000e-02 eta 2:25:35
epoch [1/30] batch [540/796] time 0.388 (0.374) data 0.000 (0.002) loss 1.2842 (1.1153) lr 1.0000e-02 eta 2:25:31
epoch [1/30] batch [560/796] time 0.375 (0.374) data 0.000 (0.002) loss 1.2188 (1.1140) lr 1.0000e-02 eta 2:25:23
epoch [1/30] batch [580/796] time 0.379 (0.374) data 0.000 (0.002) loss 0.9058 (1.1011) lr 1.0000e-02 eta 2:25:09
epoch [1/30] batch [600/796] time 0.380 (0.374) data 0.000 (0.002) loss 0.9082 (1.1003) lr 1.0000e-02 eta 2:24:56
epoch [1/30] batch [620/796] time 0.375 (0.373) data 0.000 (0.002) loss 1.3076 (1.0953) lr 1.0000e-02 eta 2:24:43
epoch [1/30] batch [640/796] time 0.365 (0.373) data 0.000 (0.002) loss 0.6362 (1.0885) lr 1.0000e-02 eta 2:24:31
epoch [1/30] batch [660/796] time 0.354 (0.373) data 0.000 (0.002) loss 0.2920 (1.0862) lr 1.0000e-02 eta 2:24:23
epoch [1/30] batch [680/796] time 0.350 (0.373) data 0.000 (0.002) loss 3.2207 (1.0874) lr 1.0000e-02 eta 2:24:11
epoch [1/30] batch [700/796] time 0.353 (0.373) data 0.000 (0.002) loss 1.9688 (1.0891) lr 1.0000e-02 eta 2:24:03
epoch [1/30] batch [720/796] time 0.376 (0.373) data 0.000 (0.002) loss 0.7202 (1.0910) lr 1.0000e-02 eta 2:23:51
epoch [1/30] batch [740/796] time 0.374 (0.372) data 0.000 (0.002) loss 2.3027 (1.0826) lr 1.0000e-02 eta 2:23:36
epoch [1/30] batch [760/796] time 0.353 (0.372) data 0.000 (0.002) loss 0.3096 (1.0755) lr 1.0000e-02 eta 2:23:30
epoch [1/30] batch [780/796] time 0.330 (0.372) data 0.000 (0.002) loss 3.6465 (1.0773) lr 1.0000e-02 eta 2:23:04
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:53,  5.96s/it] 10%|█         | 2/20 [00:06<00:52,  2.92s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.71s/it] 20%|██        | 4/20 [00:07<00:18,  1.15s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.20it/s] 30%|███       | 6/20 [00:07<00:09,  1.55it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.90it/s] 40%|████      | 8/20 [00:08<00:05,  2.25it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.53it/s] 50%|█████     | 10/20 [00:08<00:03,  2.79it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.94it/s] 60%|██████    | 12/20 [00:09<00:02,  3.11it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.45it/s] 70%|███████   | 14/20 [00:10<00:01,  3.67it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.76it/s] 80%|████████  | 16/20 [00:10<00:01,  3.99it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.07it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.67it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.08it/s]100%|██████████| 20/20 [00:11<00:00,  4.49it/s]100%|██████████| 20/20 [00:11<00:00,  1.73it/s]=> result
* total: 1,990
* correct: 1,499
* accuracy: 75.3%
* error: 24.7%
* macro_f1: 74.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar

epoch [2/30] batch [20/796] time 0.386 (0.416) data 0.000 (0.038) loss 0.5845 (1.0406) lr 9.9726e-03 eta 2:39:59
epoch [2/30] batch [40/796] time 0.383 (0.396) data 0.000 (0.019) loss 1.7061 (1.0796) lr 9.9726e-03 eta 2:32:07
epoch [2/30] batch [60/796] time 0.407 (0.391) data 0.000 (0.013) loss 0.3586 (1.1115) lr 9.9726e-03 eta 2:29:57
epoch [2/30] batch [80/796] time 0.337 (0.387) data 0.000 (0.010) loss 1.3037 (1.1001) lr 9.9726e-03 eta 2:28:14
epoch [2/30] batch [100/796] time 0.383 (0.383) data 0.000 (0.008) loss 2.0410 (1.0703) lr 9.9726e-03 eta 2:26:49
epoch [2/30] batch [120/796] time 0.380 (0.382) data 0.000 (0.007) loss 0.1263 (1.0504) lr 9.9726e-03 eta 2:26:11
epoch [2/30] batch [140/796] time 0.389 (0.380) data 0.000 (0.006) loss 0.3083 (1.0374) lr 9.9726e-03 eta 2:25:10
epoch [2/30] batch [160/796] time 0.352 (0.378) data 0.000 (0.005) loss 0.6514 (1.0091) lr 9.9726e-03 eta 2:24:19
epoch [2/30] batch [180/796] time 0.380 (0.376) data 0.000 (0.005) loss 1.1152 (1.0186) lr 9.9726e-03 eta 2:23:22
epoch [2/30] batch [200/796] time 0.371 (0.375) data 0.000 (0.004) loss 0.2163 (1.0037) lr 9.9726e-03 eta 2:22:55
epoch [2/30] batch [220/796] time 0.450 (0.374) data 0.000 (0.004) loss 0.2551 (1.0097) lr 9.9726e-03 eta 2:22:25
epoch [2/30] batch [240/796] time 0.400 (0.373) data 0.000 (0.003) loss 1.2031 (1.0162) lr 9.9726e-03 eta 2:21:54
epoch [2/30] batch [260/796] time 0.368 (0.373) data 0.000 (0.003) loss 1.0225 (1.0349) lr 9.9726e-03 eta 2:21:43
epoch [2/30] batch [280/796] time 0.400 (0.373) data 0.000 (0.003) loss 0.5264 (1.0348) lr 9.9726e-03 eta 2:21:37
epoch [2/30] batch [300/796] time 0.367 (0.372) data 0.000 (0.003) loss 0.5635 (1.0396) lr 9.9726e-03 eta 2:21:22
epoch [2/30] batch [320/796] time 0.364 (0.372) data 0.000 (0.003) loss 0.9829 (1.0418) lr 9.9726e-03 eta 2:21:06
epoch [2/30] batch [340/796] time 0.340 (0.372) data 0.000 (0.003) loss 0.7358 (1.0451) lr 9.9726e-03 eta 2:20:49
epoch [2/30] batch [360/796] time 0.371 (0.372) data 0.000 (0.002) loss 0.4841 (1.0243) lr 9.9726e-03 eta 2:20:46
epoch [2/30] batch [380/796] time 0.379 (0.372) data 0.000 (0.002) loss 0.3918 (1.0206) lr 9.9726e-03 eta 2:20:51
epoch [2/30] batch [400/796] time 0.340 (0.372) data 0.000 (0.002) loss 1.1992 (1.0156) lr 9.9726e-03 eta 2:20:38
epoch [2/30] batch [420/796] time 0.372 (0.372) data 0.000 (0.002) loss 1.0244 (1.0107) lr 9.9726e-03 eta 2:20:21
epoch [2/30] batch [440/796] time 0.371 (0.372) data 0.000 (0.002) loss 0.8848 (1.0049) lr 9.9726e-03 eta 2:20:17
epoch [2/30] batch [460/796] time 0.423 (0.372) data 0.000 (0.002) loss 0.2942 (0.9994) lr 9.9726e-03 eta 2:20:10
epoch [2/30] batch [480/796] time 0.336 (0.371) data 0.000 (0.002) loss 0.6709 (0.9909) lr 9.9726e-03 eta 2:19:50
epoch [2/30] batch [500/796] time 0.384 (0.371) data 0.000 (0.002) loss 2.5117 (1.0000) lr 9.9726e-03 eta 2:19:37
epoch [2/30] batch [520/796] time 0.347 (0.371) data 0.000 (0.002) loss 0.6001 (1.0031) lr 9.9726e-03 eta 2:19:28
epoch [2/30] batch [540/796] time 0.388 (0.371) data 0.000 (0.002) loss 0.4827 (0.9921) lr 9.9726e-03 eta 2:19:23
epoch [2/30] batch [560/796] time 0.380 (0.371) data 0.000 (0.002) loss 2.2051 (0.9983) lr 9.9726e-03 eta 2:19:08
epoch [2/30] batch [580/796] time 0.352 (0.371) data 0.000 (0.002) loss 1.8848 (0.9969) lr 9.9726e-03 eta 2:19:05
epoch [2/30] batch [600/796] time 0.386 (0.371) data 0.000 (0.002) loss 1.0127 (1.0026) lr 9.9726e-03 eta 2:19:02
epoch [2/30] batch [620/796] time 0.391 (0.371) data 0.000 (0.002) loss 0.3176 (0.9998) lr 9.9726e-03 eta 2:18:52
epoch [2/30] batch [640/796] time 0.355 (0.371) data 0.000 (0.001) loss 1.4375 (0.9984) lr 9.9726e-03 eta 2:18:48
epoch [2/30] batch [660/796] time 0.401 (0.371) data 0.000 (0.001) loss 0.5659 (0.9962) lr 9.9726e-03 eta 2:18:44
epoch [2/30] batch [680/796] time 0.354 (0.371) data 0.000 (0.001) loss 0.7881 (0.9905) lr 9.9726e-03 eta 2:18:37
epoch [2/30] batch [700/796] time 0.365 (0.371) data 0.000 (0.001) loss 0.9834 (0.9899) lr 9.9726e-03 eta 2:18:30
epoch [2/30] batch [720/796] time 0.364 (0.371) data 0.000 (0.001) loss 0.6875 (0.9877) lr 9.9726e-03 eta 2:18:20
epoch [2/30] batch [740/796] time 0.380 (0.371) data 0.000 (0.001) loss 0.5728 (0.9862) lr 9.9726e-03 eta 2:18:09
epoch [2/30] batch [760/796] time 0.377 (0.371) data 0.001 (0.001) loss 2.6230 (0.9908) lr 9.9726e-03 eta 2:18:00
epoch [2/30] batch [780/796] time 0.333 (0.370) data 0.000 (0.001) loss 0.2135 (0.9933) lr 9.9726e-03 eta 2:17:43
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:53,  5.97s/it] 10%|█         | 2/20 [00:06<00:53,  2.99s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.76s/it] 20%|██        | 4/20 [00:07<00:18,  1.18s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.17it/s] 30%|███       | 6/20 [00:08<00:09,  1.51it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.87it/s] 40%|████      | 8/20 [00:08<00:05,  2.19it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.46it/s] 50%|█████     | 10/20 [00:09<00:03,  2.69it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.02it/s] 60%|██████    | 12/20 [00:09<00:02,  3.31it/s] 65%|██████▌   | 13/20 [00:09<00:01,  3.52it/s] 70%|███████   | 14/20 [00:10<00:01,  3.70it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.87it/s] 80%|████████  | 16/20 [00:10<00:01,  3.99it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.17it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.27it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.73it/s]100%|██████████| 20/20 [00:11<00:00,  4.19it/s]100%|██████████| 20/20 [00:11<00:00,  1.70it/s]=> result
* total: 1,990
* correct: 1,521
* accuracy: 76.4%
* error: 23.6%
* macro_f1: 75.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar

epoch [3/30] batch [20/796] time 0.354 (0.416) data 0.000 (0.039) loss 2.0820 (0.9210) lr 9.8907e-03 eta 2:34:28
epoch [3/30] batch [40/796] time 0.346 (0.393) data 0.000 (0.019) loss 0.7925 (0.9485) lr 9.8907e-03 eta 2:25:34
epoch [3/30] batch [60/796] time 0.344 (0.384) data 0.000 (0.013) loss 0.7217 (0.9986) lr 9.8907e-03 eta 2:22:08
epoch [3/30] batch [80/796] time 0.376 (0.381) data 0.000 (0.010) loss 0.9580 (1.0035) lr 9.8907e-03 eta 2:21:10
epoch [3/30] batch [100/796] time 0.360 (0.379) data 0.000 (0.008) loss 1.4297 (1.0086) lr 9.8907e-03 eta 2:20:01
epoch [3/30] batch [120/796] time 0.380 (0.378) data 0.000 (0.007) loss 0.2949 (1.0277) lr 9.8907e-03 eta 2:19:44
epoch [3/30] batch [140/796] time 0.401 (0.379) data 0.000 (0.006) loss 1.6025 (1.0101) lr 9.8907e-03 eta 2:19:46
epoch [3/30] batch [160/796] time 0.387 (0.378) data 0.000 (0.005) loss 0.5552 (1.0272) lr 9.8907e-03 eta 2:19:20
epoch [3/30] batch [180/796] time 0.350 (0.377) data 0.000 (0.005) loss 0.7754 (1.0188) lr 9.8907e-03 eta 2:18:57
epoch [3/30] batch [200/796] time 0.346 (0.376) data 0.000 (0.004) loss 0.9648 (1.0068) lr 9.8907e-03 eta 2:18:22
epoch [3/30] batch [220/796] time 0.374 (0.375) data 0.000 (0.004) loss 1.9336 (1.0213) lr 9.8907e-03 eta 2:17:59
epoch [3/30] batch [240/796] time 0.361 (0.373) data 0.000 (0.003) loss 1.0371 (1.0104) lr 9.8907e-03 eta 2:17:13
epoch [3/30] batch [260/796] time 0.332 (0.373) data 0.000 (0.003) loss 2.4609 (1.0070) lr 9.8907e-03 eta 2:17:01
epoch [3/30] batch [280/796] time 0.374 (0.373) data 0.000 (0.003) loss 0.7095 (1.0279) lr 9.8907e-03 eta 2:16:51
epoch [3/30] batch [300/796] time 0.358 (0.373) data 0.000 (0.003) loss 0.2328 (1.0227) lr 9.8907e-03 eta 2:16:45
epoch [3/30] batch [320/796] time 0.379 (0.373) data 0.000 (0.003) loss 2.2969 (1.0208) lr 9.8907e-03 eta 2:16:37
epoch [3/30] batch [340/796] time 0.380 (0.373) data 0.000 (0.003) loss 0.1815 (1.0054) lr 9.8907e-03 eta 2:16:20
epoch [3/30] batch [360/796] time 0.393 (0.373) data 0.000 (0.002) loss 1.6016 (0.9988) lr 9.8907e-03 eta 2:16:18
epoch [3/30] batch [380/796] time 0.380 (0.373) data 0.000 (0.002) loss 0.6616 (0.9900) lr 9.8907e-03 eta 2:16:08
epoch [3/30] batch [400/796] time 0.407 (0.373) data 0.000 (0.002) loss 1.4883 (0.9937) lr 9.8907e-03 eta 2:16:13
epoch [3/30] batch [420/796] time 0.373 (0.373) data 0.000 (0.002) loss 0.3735 (0.9962) lr 9.8907e-03 eta 2:16:02
epoch [3/30] batch [440/796] time 0.360 (0.373) data 0.000 (0.002) loss 0.2712 (0.9868) lr 9.8907e-03 eta 2:15:50
epoch [3/30] batch [460/796] time 0.377 (0.373) data 0.000 (0.002) loss 0.4873 (0.9871) lr 9.8907e-03 eta 2:15:47
epoch [3/30] batch [480/796] time 0.384 (0.373) data 0.000 (0.002) loss 1.6777 (0.9889) lr 9.8907e-03 eta 2:15:31
epoch [3/30] batch [500/796] time 0.344 (0.372) data 0.000 (0.002) loss 1.7832 (0.9922) lr 9.8907e-03 eta 2:15:15
epoch [3/30] batch [520/796] time 0.338 (0.372) data 0.000 (0.002) loss 0.7241 (0.9866) lr 9.8907e-03 eta 2:15:03
epoch [3/30] batch [540/796] time 0.350 (0.372) data 0.000 (0.002) loss 1.7773 (0.9869) lr 9.8907e-03 eta 2:14:51
epoch [3/30] batch [560/796] time 0.390 (0.372) data 0.000 (0.002) loss 0.3286 (0.9982) lr 9.8907e-03 eta 2:14:43
epoch [3/30] batch [580/796] time 0.371 (0.372) data 0.000 (0.002) loss 2.5391 (1.0097) lr 9.8907e-03 eta 2:14:38
epoch [3/30] batch [600/796] time 0.337 (0.372) data 0.000 (0.002) loss 1.7305 (1.0058) lr 9.8907e-03 eta 2:14:28
epoch [3/30] batch [620/796] time 0.391 (0.372) data 0.000 (0.002) loss 1.0361 (1.0110) lr 9.8907e-03 eta 2:14:17
epoch [3/30] batch [640/796] time 0.385 (0.372) data 0.000 (0.001) loss 0.5127 (1.0138) lr 9.8907e-03 eta 2:14:08
epoch [3/30] batch [660/796] time 0.384 (0.372) data 0.000 (0.001) loss 0.7090 (1.0037) lr 9.8907e-03 eta 2:14:00
epoch [3/30] batch [680/796] time 0.381 (0.372) data 0.000 (0.001) loss 1.6689 (1.0033) lr 9.8907e-03 eta 2:13:48
epoch [3/30] batch [700/796] time 0.407 (0.372) data 0.000 (0.001) loss 1.0049 (1.0077) lr 9.8907e-03 eta 2:13:43
epoch [3/30] batch [720/796] time 0.393 (0.372) data 0.000 (0.001) loss 1.8418 (1.0117) lr 9.8907e-03 eta 2:13:39
epoch [3/30] batch [740/796] time 0.425 (0.372) data 0.000 (0.001) loss 0.6074 (1.0117) lr 9.8907e-03 eta 2:13:34
epoch [3/30] batch [760/796] time 0.362 (0.372) data 0.000 (0.001) loss 0.6470 (1.0116) lr 9.8907e-03 eta 2:13:23
epoch [3/30] batch [780/796] time 0.327 (0.371) data 0.000 (0.001) loss 1.3184 (1.0193) lr 9.8907e-03 eta 2:13:02
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<02:03,  6.48s/it] 10%|█         | 2/20 [00:06<00:52,  2.94s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.73s/it] 20%|██        | 4/20 [00:07<00:18,  1.16s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.18it/s] 30%|███       | 6/20 [00:08<00:09,  1.53it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.88it/s] 40%|████      | 8/20 [00:08<00:05,  2.23it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.53it/s] 50%|█████     | 10/20 [00:09<00:03,  2.78it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.01it/s] 60%|██████    | 12/20 [00:09<00:02,  3.27it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.42it/s] 70%|███████   | 14/20 [00:10<00:01,  3.60it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.70it/s] 80%|████████  | 16/20 [00:10<00:01,  3.77it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.81it/s] 90%|█████████ | 18/20 [00:11<00:00,  4.01it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.37it/s]100%|██████████| 20/20 [00:11<00:00,  4.73it/s]100%|██████████| 20/20 [00:11<00:00,  1.71it/s]=> result
* total: 1,990
* correct: 1,536
* accuracy: 77.2%
* error: 22.8%
* macro_f1: 76.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar

epoch [4/30] batch [20/796] time 0.371 (0.420) data 0.000 (0.042) loss 0.4524 (1.2069) lr 9.7553e-03 eta 2:30:22
epoch [4/30] batch [40/796] time 0.379 (0.400) data 0.000 (0.021) loss 0.8696 (1.2052) lr 9.7553e-03 eta 2:22:50
epoch [4/30] batch [60/796] time 0.353 (0.389) data 0.000 (0.014) loss 1.5713 (1.1095) lr 9.7553e-03 eta 2:18:55
epoch [4/30] batch [80/796] time 0.390 (0.383) data 0.000 (0.011) loss 2.3066 (1.1302) lr 9.7553e-03 eta 2:16:30
epoch [4/30] batch [100/796] time 0.359 (0.382) data 0.000 (0.009) loss 0.7930 (1.0931) lr 9.7553e-03 eta 2:16:14
epoch [4/30] batch [120/796] time 0.337 (0.379) data 0.000 (0.007) loss 1.8936 (1.1016) lr 9.7553e-03 eta 2:14:52
epoch [4/30] batch [140/796] time 0.335 (0.378) data 0.000 (0.006) loss 0.6274 (1.0855) lr 9.7553e-03 eta 2:14:22
epoch [4/30] batch [160/796] time 0.395 (0.376) data 0.000 (0.006) loss 1.5361 (1.0740) lr 9.7553e-03 eta 2:13:42
epoch [4/30] batch [180/796] time 0.358 (0.375) data 0.000 (0.005) loss 0.6211 (1.0848) lr 9.7553e-03 eta 2:13:17
epoch [4/30] batch [200/796] time 0.356 (0.375) data 0.000 (0.004) loss 0.6670 (1.0807) lr 9.7553e-03 eta 2:12:55
epoch [4/30] batch [220/796] time 0.400 (0.375) data 0.000 (0.004) loss 0.3823 (1.0860) lr 9.7553e-03 eta 2:12:49
epoch [4/30] batch [240/796] time 0.375 (0.374) data 0.000 (0.004) loss 1.2490 (1.1003) lr 9.7553e-03 eta 2:12:27
epoch [4/30] batch [260/796] time 0.357 (0.373) data 0.000 (0.004) loss 0.2170 (1.0813) lr 9.7553e-03 eta 2:12:09
epoch [4/30] batch [280/796] time 0.362 (0.373) data 0.000 (0.003) loss 0.7510 (1.0649) lr 9.7553e-03 eta 2:11:44
epoch [4/30] batch [300/796] time 0.379 (0.373) data 0.000 (0.003) loss 0.2079 (1.0565) lr 9.7553e-03 eta 2:11:38
epoch [4/30] batch [320/796] time 0.349 (0.373) data 0.000 (0.003) loss 0.5776 (1.0578) lr 9.7553e-03 eta 2:11:40
epoch [4/30] batch [340/796] time 0.371 (0.373) data 0.000 (0.003) loss 0.6982 (1.0391) lr 9.7553e-03 eta 2:11:25
epoch [4/30] batch [360/796] time 0.363 (0.373) data 0.000 (0.003) loss 2.3555 (1.0303) lr 9.7553e-03 eta 2:11:21
epoch [4/30] batch [380/796] time 0.380 (0.373) data 0.000 (0.002) loss 2.2500 (1.0454) lr 9.7553e-03 eta 2:11:13
epoch [4/30] batch [400/796] time 0.362 (0.373) data 0.000 (0.002) loss 0.7993 (1.0327) lr 9.7553e-03 eta 2:11:02
epoch [4/30] batch [420/796] time 0.352 (0.373) data 0.000 (0.002) loss 0.2585 (1.0248) lr 9.7553e-03 eta 2:10:50
epoch [4/30] batch [440/796] time 0.376 (0.373) data 0.000 (0.002) loss 0.7930 (1.0201) lr 9.7553e-03 eta 2:10:48
epoch [4/30] batch [460/796] time 0.397 (0.373) data 0.000 (0.002) loss 0.5552 (1.0217) lr 9.7553e-03 eta 2:10:47
epoch [4/30] batch [480/796] time 0.368 (0.373) data 0.000 (0.002) loss 1.5635 (1.0189) lr 9.7553e-03 eta 2:10:42
epoch [4/30] batch [500/796] time 0.381 (0.373) data 0.000 (0.002) loss 0.9580 (1.0197) lr 9.7553e-03 eta 2:10:24
epoch [4/30] batch [520/796] time 0.370 (0.373) data 0.000 (0.002) loss 0.3149 (1.0117) lr 9.7553e-03 eta 2:10:22
epoch [4/30] batch [540/796] time 0.385 (0.373) data 0.000 (0.002) loss 1.7236 (1.0087) lr 9.7553e-03 eta 2:10:14
epoch [4/30] batch [560/796] time 0.377 (0.373) data 0.000 (0.002) loss 0.9526 (1.0150) lr 9.7553e-03 eta 2:10:09
epoch [4/30] batch [580/796] time 0.363 (0.373) data 0.000 (0.002) loss 0.9858 (1.0095) lr 9.7553e-03 eta 2:09:59
epoch [4/30] batch [600/796] time 0.416 (0.373) data 0.000 (0.002) loss 1.1309 (1.0096) lr 9.7553e-03 eta 2:09:53
epoch [4/30] batch [620/796] time 0.340 (0.373) data 0.000 (0.002) loss 0.8516 (1.0069) lr 9.7553e-03 eta 2:09:44
epoch [4/30] batch [640/796] time 0.377 (0.373) data 0.000 (0.002) loss 0.6309 (1.0045) lr 9.7553e-03 eta 2:09:36
epoch [4/30] batch [660/796] time 0.364 (0.373) data 0.000 (0.002) loss 0.6567 (0.9970) lr 9.7553e-03 eta 2:09:27
epoch [4/30] batch [680/796] time 0.382 (0.373) data 0.000 (0.002) loss 1.7549 (0.9990) lr 9.7553e-03 eta 2:09:15
epoch [4/30] batch [700/796] time 0.384 (0.373) data 0.000 (0.001) loss 0.4485 (0.9922) lr 9.7553e-03 eta 2:09:07
epoch [4/30] batch [720/796] time 0.344 (0.372) data 0.000 (0.001) loss 2.3555 (0.9895) lr 9.7553e-03 eta 2:08:51
epoch [4/30] batch [740/796] time 0.372 (0.372) data 0.000 (0.001) loss 0.4324 (0.9887) lr 9.7553e-03 eta 2:08:40
epoch [4/30] batch [760/796] time 0.383 (0.372) data 0.000 (0.001) loss 0.3174 (0.9796) lr 9.7553e-03 eta 2:08:31
epoch [4/30] batch [780/796] time 0.323 (0.371) data 0.000 (0.001) loss 1.1396 (0.9795) lr 9.7553e-03 eta 2:08:03
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:50,  5.80s/it] 10%|█         | 2/20 [00:06<00:50,  2.83s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.67s/it] 20%|██        | 4/20 [00:07<00:17,  1.12s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.22it/s] 30%|███       | 6/20 [00:07<00:08,  1.57it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.93it/s] 40%|████      | 8/20 [00:08<00:05,  2.24it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.52it/s] 50%|█████     | 10/20 [00:08<00:03,  2.76it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.92it/s] 60%|██████    | 12/20 [00:09<00:02,  3.10it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.38it/s] 70%|███████   | 14/20 [00:09<00:01,  3.63it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.80it/s] 80%|████████  | 16/20 [00:10<00:01,  3.98it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.03it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.18it/s] 95%|█████████▌| 19/20 [00:10<00:00,  4.50it/s]100%|██████████| 20/20 [00:11<00:00,  4.84it/s]100%|██████████| 20/20 [00:11<00:00,  1.77it/s]=> result
* total: 1,990
* correct: 1,535
* accuracy: 77.1%
* error: 22.9%
* macro_f1: 76.1%

epoch [5/30] batch [20/796] time 0.352 (0.432) data 0.000 (0.043) loss 0.8779 (0.9832) lr 9.5677e-03 eta 2:28:50
epoch [5/30] batch [40/796] time 0.387 (0.399) data 0.000 (0.022) loss 1.0713 (0.9976) lr 9.5677e-03 eta 2:17:12
epoch [5/30] batch [60/796] time 0.388 (0.390) data 0.000 (0.015) loss 0.9419 (1.0727) lr 9.5677e-03 eta 2:14:14
epoch [5/30] batch [80/796] time 0.344 (0.383) data 0.000 (0.011) loss 1.0820 (1.0416) lr 9.5677e-03 eta 2:11:45
epoch [5/30] batch [100/796] time 0.371 (0.381) data 0.000 (0.009) loss 1.2168 (0.9670) lr 9.5677e-03 eta 2:10:41
epoch [5/30] batch [120/796] time 0.385 (0.377) data 0.000 (0.007) loss 1.5195 (0.9732) lr 9.5677e-03 eta 2:09:24
epoch [5/30] batch [140/796] time 0.352 (0.375) data 0.000 (0.006) loss 0.3171 (0.9558) lr 9.5677e-03 eta 2:08:32
epoch [5/30] batch [160/796] time 0.394 (0.375) data 0.000 (0.006) loss 0.6582 (0.9691) lr 9.5677e-03 eta 2:08:30
epoch [5/30] batch [180/796] time 0.388 (0.374) data 0.000 (0.005) loss 1.0566 (0.9682) lr 9.5677e-03 eta 2:08:03
epoch [5/30] batch [200/796] time 0.392 (0.375) data 0.000 (0.005) loss 0.3547 (0.9656) lr 9.5677e-03 eta 2:07:55
epoch [5/30] batch [220/796] time 0.367 (0.374) data 0.000 (0.004) loss 0.2954 (0.9482) lr 9.5677e-03 eta 2:07:31
epoch [5/30] batch [240/796] time 0.362 (0.373) data 0.000 (0.004) loss 0.9990 (0.9350) lr 9.5677e-03 eta 2:07:16
epoch [5/30] batch [260/796] time 0.380 (0.373) data 0.000 (0.004) loss 1.3350 (0.9307) lr 9.5677e-03 eta 2:06:58
epoch [5/30] batch [280/796] time 0.379 (0.373) data 0.000 (0.003) loss 1.8867 (0.9429) lr 9.5677e-03 eta 2:06:49
epoch [5/30] batch [300/796] time 0.354 (0.373) data 0.001 (0.003) loss 1.3770 (0.9575) lr 9.5677e-03 eta 2:06:42
epoch [5/30] batch [320/796] time 0.384 (0.372) data 0.000 (0.003) loss 1.3193 (0.9645) lr 9.5677e-03 eta 2:06:15
epoch [5/30] batch [340/796] time 0.335 (0.371) data 0.000 (0.003) loss 0.4875 (0.9748) lr 9.5677e-03 eta 2:05:52
epoch [5/30] batch [360/796] time 0.341 (0.371) data 0.000 (0.003) loss 0.7651 (0.9621) lr 9.5677e-03 eta 2:05:40
epoch [5/30] batch [380/796] time 0.397 (0.371) data 0.000 (0.003) loss 1.5244 (0.9547) lr 9.5677e-03 eta 2:05:40
epoch [5/30] batch [400/796] time 0.379 (0.371) data 0.000 (0.002) loss 0.4668 (0.9423) lr 9.5677e-03 eta 2:05:20
epoch [5/30] batch [420/796] time 0.383 (0.370) data 0.000 (0.002) loss 1.1406 (0.9421) lr 9.5677e-03 eta 2:05:06
epoch [5/30] batch [440/796] time 0.391 (0.370) data 0.000 (0.002) loss 0.5571 (0.9394) lr 9.5677e-03 eta 2:05:00
epoch [5/30] batch [460/796] time 0.362 (0.370) data 0.000 (0.002) loss 0.8281 (0.9333) lr 9.5677e-03 eta 2:04:51
epoch [5/30] batch [480/796] time 0.382 (0.370) data 0.000 (0.002) loss 0.0350 (0.9236) lr 9.5677e-03 eta 2:04:38
epoch [5/30] batch [500/796] time 0.347 (0.370) data 0.000 (0.002) loss 1.6777 (0.9254) lr 9.5677e-03 eta 2:04:37
epoch [5/30] batch [520/796] time 0.375 (0.370) data 0.000 (0.002) loss 0.7759 (0.9348) lr 9.5677e-03 eta 2:04:25
epoch [5/30] batch [540/796] time 0.353 (0.370) data 0.000 (0.002) loss 1.4277 (0.9410) lr 9.5677e-03 eta 2:04:21
epoch [5/30] batch [560/796] time 0.380 (0.370) data 0.000 (0.002) loss 0.0536 (0.9346) lr 9.5677e-03 eta 2:04:11
epoch [5/30] batch [580/796] time 0.340 (0.370) data 0.000 (0.002) loss 0.9946 (0.9314) lr 9.5677e-03 eta 2:04:01
epoch [5/30] batch [600/796] time 0.365 (0.370) data 0.000 (0.002) loss 1.5791 (0.9354) lr 9.5677e-03 eta 2:03:54
epoch [5/30] batch [620/796] time 0.361 (0.370) data 0.000 (0.002) loss 0.9175 (0.9351) lr 9.5677e-03 eta 2:03:48
epoch [5/30] batch [640/796] time 0.394 (0.370) data 0.000 (0.002) loss 0.2219 (0.9357) lr 9.5677e-03 eta 2:03:43
epoch [5/30] batch [660/796] time 0.359 (0.370) data 0.000 (0.002) loss 0.4341 (0.9284) lr 9.5677e-03 eta 2:03:37
epoch [5/30] batch [680/796] time 0.385 (0.370) data 0.000 (0.002) loss 2.6328 (0.9267) lr 9.5677e-03 eta 2:03:29
epoch [5/30] batch [700/796] time 0.369 (0.370) data 0.000 (0.002) loss 1.5371 (0.9293) lr 9.5677e-03 eta 2:03:19
epoch [5/30] batch [720/796] time 0.338 (0.370) data 0.000 (0.001) loss 0.7456 (0.9297) lr 9.5677e-03 eta 2:03:10
epoch [5/30] batch [740/796] time 0.366 (0.370) data 0.000 (0.001) loss 0.8960 (0.9243) lr 9.5677e-03 eta 2:03:07
epoch [5/30] batch [760/796] time 0.334 (0.370) data 0.000 (0.001) loss 1.5830 (0.9271) lr 9.5677e-03 eta 2:02:55
epoch [5/30] batch [780/796] time 0.327 (0.369) data 0.000 (0.001) loss 0.6187 (0.9285) lr 9.5677e-03 eta 2:02:31
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<01:56,  6.11s/it] 10%|█         | 2/20 [00:06<00:54,  3.00s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.76s/it] 20%|██        | 4/20 [00:07<00:18,  1.17s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.17it/s] 30%|███       | 6/20 [00:08<00:09,  1.52it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.86it/s] 40%|████      | 8/20 [00:08<00:05,  2.16it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.43it/s] 50%|█████     | 10/20 [00:09<00:03,  2.66it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.85it/s] 60%|██████    | 12/20 [00:09<00:02,  3.04it/s] 65%|██████▌   | 13/20 [00:10<00:02,  3.29it/s] 70%|███████   | 14/20 [00:10<00:01,  3.44it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.65it/s] 80%|████████  | 16/20 [00:10<00:01,  3.86it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.15it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.31it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.67it/s]100%|██████████| 20/20 [00:11<00:00,  4.14it/s]100%|██████████| 20/20 [00:11<00:00,  1.67it/s]=> result
* total: 1,990
* correct: 1,546
* accuracy: 77.7%
* error: 22.3%
* macro_f1: 76.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar

epoch [6/30] batch [20/796] time 0.371 (0.413) data 0.000 (0.047) loss 2.5020 (1.0814) lr 9.3301e-03 eta 2:16:43
epoch [6/30] batch [40/796] time 0.382 (0.392) data 0.000 (0.024) loss 0.7979 (1.0331) lr 9.3301e-03 eta 2:09:43
epoch [6/30] batch [60/796] time 0.345 (0.382) data 0.000 (0.016) loss 0.9009 (0.9706) lr 9.3301e-03 eta 2:06:22
epoch [6/30] batch [80/796] time 0.379 (0.378) data 0.000 (0.012) loss 2.3965 (0.9401) lr 9.3301e-03 eta 2:04:59
epoch [6/30] batch [100/796] time 0.373 (0.375) data 0.000 (0.010) loss 0.8794 (0.9422) lr 9.3301e-03 eta 2:03:46
epoch [6/30] batch [120/796] time 0.343 (0.372) data 0.000 (0.008) loss 2.3672 (0.9013) lr 9.3301e-03 eta 2:02:40
epoch [6/30] batch [140/796] time 0.383 (0.372) data 0.000 (0.007) loss 0.6284 (0.8945) lr 9.3301e-03 eta 2:02:27
epoch [6/30] batch [160/796] time 0.370 (0.371) data 0.000 (0.006) loss 1.3516 (0.8930) lr 9.3301e-03 eta 2:01:59
epoch [6/30] batch [180/796] time 0.343 (0.372) data 0.000 (0.005) loss 0.4653 (0.8773) lr 9.3301e-03 eta 2:02:07
epoch [6/30] batch [200/796] time 0.332 (0.370) data 0.000 (0.005) loss 0.5264 (0.8695) lr 9.3301e-03 eta 2:01:38
epoch [6/30] batch [220/796] time 0.361 (0.370) data 0.000 (0.005) loss 2.3398 (0.8878) lr 9.3301e-03 eta 2:01:24
epoch [6/30] batch [240/796] time 0.355 (0.371) data 0.000 (0.004) loss 1.1377 (0.8890) lr 9.3301e-03 eta 2:01:25
epoch [6/30] batch [260/796] time 0.365 (0.370) data 0.000 (0.004) loss 0.1100 (0.8874) lr 9.3301e-03 eta 2:01:15
epoch [6/30] batch [280/796] time 0.353 (0.370) data 0.000 (0.004) loss 0.7607 (0.8799) lr 9.3301e-03 eta 2:01:01
epoch [6/30] batch [300/796] time 0.400 (0.370) data 0.000 (0.003) loss 0.6484 (0.8857) lr 9.3301e-03 eta 2:00:59
epoch [6/30] batch [320/796] time 0.365 (0.370) data 0.000 (0.003) loss 0.8979 (0.8749) lr 9.3301e-03 eta 2:00:45
epoch [6/30] batch [340/796] time 0.367 (0.370) data 0.000 (0.003) loss 0.0831 (0.8761) lr 9.3301e-03 eta 2:00:34
epoch [6/30] batch [360/796] time 0.368 (0.370) data 0.000 (0.003) loss 1.4736 (0.8798) lr 9.3301e-03 eta 2:00:36
epoch [6/30] batch [380/796] time 0.365 (0.370) data 0.000 (0.003) loss 0.3896 (0.8916) lr 9.3301e-03 eta 2:00:24
epoch [6/30] batch [400/796] time 0.366 (0.370) data 0.000 (0.003) loss 2.5664 (0.8865) lr 9.3301e-03 eta 2:00:21
epoch [6/30] batch [420/796] time 0.378 (0.371) data 0.000 (0.002) loss 1.2041 (0.8851) lr 9.3301e-03 eta 2:00:19
epoch [6/30] batch [440/796] time 0.359 (0.371) data 0.000 (0.002) loss 0.7925 (0.8915) lr 9.3301e-03 eta 2:00:10
epoch [6/30] batch [460/796] time 0.348 (0.371) data 0.000 (0.002) loss 1.6230 (0.8974) lr 9.3301e-03 eta 2:00:02
epoch [6/30] batch [480/796] time 0.389 (0.370) data 0.000 (0.002) loss 0.7705 (0.8930) lr 9.3301e-03 eta 1:59:51
epoch [6/30] batch [500/796] time 0.384 (0.370) data 0.000 (0.002) loss 0.6152 (0.8970) lr 9.3301e-03 eta 1:59:40
epoch [6/30] batch [520/796] time 0.342 (0.370) data 0.000 (0.002) loss 0.2981 (0.9013) lr 9.3301e-03 eta 1:59:34
epoch [6/30] batch [540/796] time 0.378 (0.370) data 0.000 (0.002) loss 0.2056 (0.8960) lr 9.3301e-03 eta 1:59:28
epoch [6/30] batch [560/796] time 0.428 (0.370) data 0.000 (0.002) loss 0.7266 (0.8992) lr 9.3301e-03 eta 1:59:19
epoch [6/30] batch [580/796] time 0.348 (0.370) data 0.000 (0.002) loss 0.2324 (0.8988) lr 9.3301e-03 eta 1:59:15
epoch [6/30] batch [600/796] time 0.375 (0.370) data 0.000 (0.002) loss 0.0739 (0.8961) lr 9.3301e-03 eta 1:59:04
epoch [6/30] batch [620/796] time 0.399 (0.370) data 0.000 (0.002) loss 1.2725 (0.9059) lr 9.3301e-03 eta 1:59:01
epoch [6/30] batch [640/796] time 0.372 (0.370) data 0.000 (0.002) loss 0.4465 (0.8982) lr 9.3301e-03 eta 1:58:54
epoch [6/30] batch [660/796] time 0.389 (0.370) data 0.000 (0.002) loss 1.0742 (0.9003) lr 9.3301e-03 eta 1:58:45
epoch [6/30] batch [680/796] time 0.347 (0.370) data 0.000 (0.002) loss 1.5186 (0.9002) lr 9.3301e-03 eta 1:58:37
epoch [6/30] batch [700/796] time 0.379 (0.371) data 0.000 (0.002) loss 0.2020 (0.9022) lr 9.3301e-03 eta 1:58:33
epoch [6/30] batch [720/796] time 0.352 (0.371) data 0.000 (0.002) loss 0.1530 (0.9054) lr 9.3301e-03 eta 1:58:29
epoch [6/30] batch [740/796] time 0.382 (0.371) data 0.000 (0.002) loss 0.7378 (0.9160) lr 9.3301e-03 eta 1:58:20
epoch [6/30] batch [760/796] time 0.395 (0.371) data 0.000 (0.002) loss 2.1211 (0.9170) lr 9.3301e-03 eta 1:58:12
epoch [6/30] batch [780/796] time 0.329 (0.370) data 0.000 (0.001) loss 0.3276 (0.9141) lr 9.3301e-03 eta 1:57:49
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:52,  5.91s/it] 10%|█         | 2/20 [00:07<00:57,  3.22s/it] 15%|█▌        | 3/20 [00:07<00:31,  1.88s/it] 20%|██        | 4/20 [00:07<00:19,  1.25s/it] 25%|██▌       | 5/20 [00:08<00:13,  1.11it/s] 30%|███       | 6/20 [00:08<00:09,  1.44it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.79it/s] 40%|████      | 8/20 [00:08<00:05,  2.13it/s] 45%|████▌     | 9/20 [00:09<00:04,  2.46it/s] 50%|█████     | 10/20 [00:09<00:03,  2.76it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.07it/s] 60%|██████    | 12/20 [00:09<00:02,  3.30it/s] 65%|██████▌   | 13/20 [00:10<00:02,  3.40it/s] 70%|███████   | 14/20 [00:10<00:01,  3.64it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.77it/s] 80%|████████  | 16/20 [00:10<00:01,  3.86it/s] 85%|████████▌ | 17/20 [00:11<00:00,  3.87it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.20it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.55it/s]100%|██████████| 20/20 [00:12<00:00,  4.02it/s]100%|██████████| 20/20 [00:12<00:00,  1.64it/s]=> result
* total: 1,990
* correct: 1,561
* accuracy: 78.4%
* error: 21.6%
* macro_f1: 77.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar

epoch [7/30] batch [20/796] time 0.364 (0.431) data 0.000 (0.040) loss 0.5483 (0.8979) lr 9.0451e-03 eta 2:16:58
epoch [7/30] batch [40/796] time 0.337 (0.401) data 0.000 (0.020) loss 1.4346 (0.8690) lr 9.0451e-03 eta 2:07:18
epoch [7/30] batch [60/796] time 0.404 (0.389) data 0.000 (0.014) loss 1.2334 (0.8801) lr 9.0451e-03 eta 2:03:36
epoch [7/30] batch [80/796] time 0.349 (0.384) data 0.000 (0.010) loss 0.2096 (0.8747) lr 9.0451e-03 eta 2:01:48
epoch [7/30] batch [100/796] time 0.352 (0.379) data 0.000 (0.008) loss 1.0166 (0.9228) lr 9.0451e-03 eta 2:00:02
epoch [7/30] batch [120/796] time 0.338 (0.378) data 0.000 (0.007) loss 1.0381 (0.8975) lr 9.0451e-03 eta 1:59:37
epoch [7/30] batch [140/796] time 0.370 (0.376) data 0.000 (0.006) loss 0.8394 (0.9025) lr 9.0451e-03 eta 1:58:58
epoch [7/30] batch [160/796] time 0.408 (0.376) data 0.000 (0.005) loss 1.5527 (0.8902) lr 9.0451e-03 eta 1:58:52
epoch [7/30] batch [180/796] time 0.361 (0.376) data 0.000 (0.005) loss 1.6914 (0.9038) lr 9.0451e-03 eta 1:58:29
epoch [7/30] batch [200/796] time 0.336 (0.375) data 0.000 (0.004) loss 0.2979 (0.8830) lr 9.0451e-03 eta 1:58:08
epoch [7/30] batch [220/796] time 0.391 (0.375) data 0.000 (0.004) loss 1.2373 (0.9057) lr 9.0451e-03 eta 1:57:52
epoch [7/30] batch [240/796] time 0.387 (0.373) data 0.000 (0.004) loss 0.7349 (0.8933) lr 9.0451e-03 eta 1:57:24
epoch [7/30] batch [260/796] time 0.374 (0.373) data 0.000 (0.003) loss 0.2296 (0.8864) lr 9.0451e-03 eta 1:57:02
epoch [7/30] batch [280/796] time 0.373 (0.373) data 0.000 (0.003) loss 0.3811 (0.8957) lr 9.0451e-03 eta 1:56:53
epoch [7/30] batch [300/796] time 0.367 (0.372) data 0.000 (0.003) loss 0.9165 (0.8833) lr 9.0451e-03 eta 1:56:41
epoch [7/30] batch [320/796] time 0.389 (0.372) data 0.000 (0.003) loss 0.2515 (0.9029) lr 9.0451e-03 eta 1:56:23
epoch [7/30] batch [340/796] time 0.381 (0.371) data 0.000 (0.003) loss 1.2109 (0.9063) lr 9.0451e-03 eta 1:56:01
epoch [7/30] batch [360/796] time 0.397 (0.372) data 0.000 (0.003) loss 1.2520 (0.8972) lr 9.0451e-03 eta 1:56:05
epoch [7/30] batch [380/796] time 0.388 (0.372) data 0.000 (0.002) loss 1.5254 (0.8953) lr 9.0451e-03 eta 1:55:58
epoch [7/30] batch [400/796] time 0.352 (0.372) data 0.000 (0.002) loss 0.5200 (0.9004) lr 9.0451e-03 eta 1:55:54
epoch [7/30] batch [420/796] time 0.366 (0.371) data 0.000 (0.002) loss 1.5576 (0.9145) lr 9.0451e-03 eta 1:55:38
epoch [7/30] batch [440/796] time 0.333 (0.371) data 0.000 (0.002) loss 0.6460 (0.9157) lr 9.0451e-03 eta 1:55:23
epoch [7/30] batch [460/796] time 0.381 (0.371) data 0.000 (0.002) loss 0.5342 (0.9137) lr 9.0451e-03 eta 1:55:10
epoch [7/30] batch [480/796] time 0.396 (0.371) data 0.000 (0.002) loss 0.5034 (0.9095) lr 9.0451e-03 eta 1:55:08
epoch [7/30] batch [500/796] time 0.362 (0.371) data 0.000 (0.002) loss 2.1895 (0.9184) lr 9.0451e-03 eta 1:55:04
epoch [7/30] batch [520/796] time 0.362 (0.371) data 0.000 (0.002) loss 1.4697 (0.9189) lr 9.0451e-03 eta 1:54:48
epoch [7/30] batch [540/796] time 0.333 (0.370) data 0.000 (0.002) loss 0.3169 (0.9082) lr 9.0451e-03 eta 1:54:35
epoch [7/30] batch [560/796] time 0.389 (0.370) data 0.000 (0.002) loss 0.1714 (0.9115) lr 9.0451e-03 eta 1:54:26
epoch [7/30] batch [580/796] time 0.392 (0.370) data 0.000 (0.002) loss 1.5410 (0.9169) lr 9.0451e-03 eta 1:54:14
epoch [7/30] batch [600/796] time 0.378 (0.370) data 0.000 (0.002) loss 0.8511 (0.9265) lr 9.0451e-03 eta 1:54:09
epoch [7/30] batch [620/796] time 0.394 (0.370) data 0.001 (0.002) loss 0.6406 (0.9281) lr 9.0451e-03 eta 1:54:05
epoch [7/30] batch [640/796] time 0.395 (0.371) data 0.000 (0.002) loss 0.3281 (0.9273) lr 9.0451e-03 eta 1:54:03
epoch [7/30] batch [660/796] time 0.372 (0.370) data 0.000 (0.001) loss 1.7988 (0.9255) lr 9.0451e-03 eta 1:53:53
epoch [7/30] batch [680/796] time 0.395 (0.370) data 0.000 (0.001) loss 0.7759 (0.9280) lr 9.0451e-03 eta 1:53:44
epoch [7/30] batch [700/796] time 0.380 (0.370) data 0.000 (0.001) loss 0.1768 (0.9236) lr 9.0451e-03 eta 1:53:29
epoch [7/30] batch [720/796] time 0.345 (0.370) data 0.000 (0.001) loss 2.2656 (0.9239) lr 9.0451e-03 eta 1:53:23
epoch [7/30] batch [740/796] time 0.342 (0.370) data 0.000 (0.001) loss 1.4668 (0.9377) lr 9.0451e-03 eta 1:53:11
epoch [7/30] batch [760/796] time 0.373 (0.370) data 0.000 (0.001) loss 0.1810 (0.9369) lr 9.0451e-03 eta 1:53:05
epoch [7/30] batch [780/796] time 0.324 (0.369) data 0.000 (0.001) loss 0.5918 (0.9354) lr 9.0451e-03 eta 1:52:40
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:51,  5.86s/it] 10%|█         | 2/20 [00:06<00:50,  2.82s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.66s/it] 20%|██        | 4/20 [00:07<00:17,  1.12s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.23it/s] 30%|███       | 6/20 [00:07<00:08,  1.58it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.94it/s] 40%|████      | 8/20 [00:08<00:05,  2.29it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.57it/s] 50%|█████     | 10/20 [00:08<00:03,  2.82it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.05it/s] 60%|██████    | 12/20 [00:09<00:02,  3.26it/s] 65%|██████▌   | 13/20 [00:09<00:01,  3.56it/s] 70%|███████   | 14/20 [00:09<00:01,  3.67it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.89it/s] 80%|████████  | 16/20 [00:10<00:00,  4.12it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.07it/s] 90%|█████████ | 18/20 [00:10<00:00,  3.63it/s] 95%|█████████▌| 19/20 [00:10<00:00,  4.05it/s]100%|██████████| 20/20 [00:11<00:00,  4.47it/s]100%|██████████| 20/20 [00:11<00:00,  1.77it/s]=> result
* total: 1,990
* correct: 1,561
* accuracy: 78.4%
* error: 21.6%
* macro_f1: 77.6%

epoch [8/30] batch [20/796] time 0.386 (0.424) data 0.000 (0.042) loss 1.5166 (0.7805) lr 8.7157e-03 eta 2:09:19
epoch [8/30] batch [40/796] time 0.359 (0.403) data 0.000 (0.021) loss 0.5996 (0.8696) lr 8.7157e-03 eta 2:02:48
epoch [8/30] batch [60/796] time 0.362 (0.397) data 0.000 (0.014) loss 1.8223 (0.9477) lr 8.7157e-03 eta 2:00:41
epoch [8/30] batch [80/796] time 0.371 (0.391) data 0.000 (0.011) loss 1.0459 (0.8952) lr 8.7157e-03 eta 1:58:43
epoch [8/30] batch [100/796] time 0.366 (0.384) data 0.000 (0.009) loss 0.7202 (0.9206) lr 8.7157e-03 eta 1:56:32
epoch [8/30] batch [120/796] time 0.377 (0.380) data 0.000 (0.007) loss 0.8179 (0.8692) lr 8.7157e-03 eta 1:55:20
epoch [8/30] batch [140/796] time 0.372 (0.379) data 0.000 (0.006) loss 1.9580 (0.8929) lr 8.7157e-03 eta 1:54:44
epoch [8/30] batch [160/796] time 0.383 (0.378) data 0.000 (0.006) loss 0.6011 (0.9030) lr 8.7157e-03 eta 1:54:12
epoch [8/30] batch [180/796] time 0.387 (0.377) data 0.000 (0.005) loss 0.9287 (0.9301) lr 8.7157e-03 eta 1:53:50
epoch [8/30] batch [200/796] time 0.363 (0.376) data 0.000 (0.004) loss 1.0479 (0.9101) lr 8.7157e-03 eta 1:53:30
epoch [8/30] batch [220/796] time 0.365 (0.375) data 0.000 (0.004) loss 0.4792 (0.9172) lr 8.7157e-03 eta 1:53:06
epoch [8/30] batch [240/796] time 0.385 (0.374) data 0.000 (0.004) loss 2.0391 (0.9111) lr 8.7157e-03 eta 1:52:43
epoch [8/30] batch [260/796] time 0.371 (0.373) data 0.000 (0.003) loss 2.0664 (0.9035) lr 8.7157e-03 eta 1:52:20
epoch [8/30] batch [280/796] time 0.386 (0.373) data 0.000 (0.003) loss 0.7153 (0.9102) lr 8.7157e-03 eta 1:52:04
epoch [8/30] batch [300/796] time 0.355 (0.373) data 0.000 (0.003) loss 0.1326 (0.9068) lr 8.7157e-03 eta 1:51:49
epoch [8/30] batch [320/796] time 0.388 (0.372) data 0.000 (0.003) loss 1.1982 (0.9167) lr 8.7157e-03 eta 1:51:36
epoch [8/30] batch [340/796] time 0.374 (0.372) data 0.000 (0.003) loss 0.9561 (0.9235) lr 8.7157e-03 eta 1:51:29
epoch [8/30] batch [360/796] time 0.400 (0.373) data 0.000 (0.003) loss 1.1934 (0.9224) lr 8.7157e-03 eta 1:51:31
epoch [8/30] batch [380/796] time 0.381 (0.373) data 0.000 (0.002) loss 1.1270 (0.9264) lr 8.7157e-03 eta 1:51:24
epoch [8/30] batch [400/796] time 0.364 (0.372) data 0.000 (0.002) loss 0.4578 (0.9151) lr 8.7157e-03 eta 1:51:08
epoch [8/30] batch [420/796] time 0.399 (0.372) data 0.000 (0.002) loss 1.4570 (0.9185) lr 8.7157e-03 eta 1:50:56
epoch [8/30] batch [440/796] time 0.386 (0.372) data 0.000 (0.002) loss 0.5659 (0.9149) lr 8.7157e-03 eta 1:50:43
epoch [8/30] batch [460/796] time 0.388 (0.372) data 0.000 (0.002) loss 2.1992 (0.9273) lr 8.7157e-03 eta 1:50:34
epoch [8/30] batch [480/796] time 0.377 (0.372) data 0.000 (0.002) loss 0.6265 (0.9288) lr 8.7157e-03 eta 1:50:25
epoch [8/30] batch [500/796] time 0.352 (0.371) data 0.000 (0.002) loss 0.0920 (0.9208) lr 8.7157e-03 eta 1:50:15
epoch [8/30] batch [520/796] time 0.375 (0.371) data 0.000 (0.002) loss 1.0352 (0.9161) lr 8.7157e-03 eta 1:50:04
epoch [8/30] batch [540/796] time 0.376 (0.371) data 0.000 (0.002) loss 1.2246 (0.9228) lr 8.7157e-03 eta 1:49:58
epoch [8/30] batch [560/796] time 0.353 (0.371) data 0.000 (0.002) loss 0.8071 (0.9289) lr 8.7157e-03 eta 1:49:45
epoch [8/30] batch [580/796] time 0.377 (0.371) data 0.000 (0.002) loss 0.9390 (0.9204) lr 8.7157e-03 eta 1:49:30
epoch [8/30] batch [600/796] time 0.378 (0.371) data 0.000 (0.002) loss 0.6289 (0.9123) lr 8.7157e-03 eta 1:49:24
epoch [8/30] batch [620/796] time 0.383 (0.371) data 0.000 (0.002) loss 0.2576 (0.9152) lr 8.7157e-03 eta 1:49:18
epoch [8/30] batch [640/796] time 0.329 (0.370) data 0.000 (0.002) loss 0.2759 (0.9095) lr 8.7157e-03 eta 1:49:04
epoch [8/30] batch [660/796] time 0.362 (0.370) data 0.000 (0.002) loss 1.6807 (0.9065) lr 8.7157e-03 eta 1:48:57
epoch [8/30] batch [680/796] time 0.397 (0.370) data 0.000 (0.001) loss 1.0479 (0.9044) lr 8.7157e-03 eta 1:48:48
epoch [8/30] batch [700/796] time 0.348 (0.370) data 0.000 (0.001) loss 0.2820 (0.8932) lr 8.7157e-03 eta 1:48:40
epoch [8/30] batch [720/796] time 0.379 (0.370) data 0.000 (0.001) loss 1.0068 (0.8937) lr 8.7157e-03 eta 1:48:31
epoch [8/30] batch [740/796] time 0.374 (0.370) data 0.000 (0.001) loss 1.3057 (0.8961) lr 8.7157e-03 eta 1:48:23
epoch [8/30] batch [760/796] time 0.344 (0.370) data 0.000 (0.001) loss 1.3535 (0.8932) lr 8.7157e-03 eta 1:48:15
epoch [8/30] batch [780/796] time 0.323 (0.369) data 0.000 (0.001) loss 0.9912 (0.8929) lr 8.7157e-03 eta 1:47:50
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:49,  5.77s/it] 10%|█         | 2/20 [00:06<00:52,  2.93s/it] 15%|█▌        | 3/20 [00:06<00:29,  1.72s/it] 20%|██        | 4/20 [00:07<00:18,  1.15s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.20it/s] 30%|███       | 6/20 [00:07<00:09,  1.55it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.90it/s] 40%|████      | 8/20 [00:08<00:05,  2.23it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.53it/s] 50%|█████     | 10/20 [00:08<00:03,  2.79it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.04it/s] 60%|██████    | 12/20 [00:09<00:02,  3.22it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.50it/s] 70%|███████   | 14/20 [00:09<00:01,  3.61it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.71it/s] 80%|████████  | 16/20 [00:10<00:01,  3.78it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.82it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.17it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.50it/s]100%|██████████| 20/20 [00:11<00:00,  4.84it/s]100%|██████████| 20/20 [00:11<00:00,  1.75it/s]=> result
* total: 1,990
* correct: 1,559
* accuracy: 78.3%
* error: 21.7%
* macro_f1: 77.5%

epoch [9/30] batch [20/796] time 0.399 (0.428) data 0.000 (0.053) loss 0.5225 (0.7374) lr 8.3457e-03 eta 2:04:53
epoch [9/30] batch [40/796] time 0.426 (0.401) data 0.000 (0.027) loss 0.4963 (0.8775) lr 8.3457e-03 eta 1:56:50
epoch [9/30] batch [60/796] time 0.346 (0.394) data 0.000 (0.018) loss 0.1774 (0.8751) lr 8.3457e-03 eta 1:54:34
epoch [9/30] batch [80/796] time 0.373 (0.387) data 0.000 (0.013) loss 2.4023 (0.8971) lr 8.3457e-03 eta 1:52:21
epoch [9/30] batch [100/796] time 0.343 (0.383) data 0.000 (0.011) loss 0.7944 (0.8882) lr 8.3457e-03 eta 1:51:02
epoch [9/30] batch [120/796] time 0.375 (0.380) data 0.000 (0.009) loss 0.2944 (0.9112) lr 8.3457e-03 eta 1:50:13
epoch [9/30] batch [140/796] time 0.360 (0.379) data 0.000 (0.008) loss 1.0537 (0.9153) lr 8.3457e-03 eta 1:49:38
epoch [9/30] batch [160/796] time 0.370 (0.378) data 0.000 (0.007) loss 1.9365 (0.9062) lr 8.3457e-03 eta 1:49:27
epoch [9/30] batch [180/796] time 0.329 (0.377) data 0.000 (0.006) loss 0.4570 (0.8991) lr 8.3457e-03 eta 1:48:51
epoch [9/30] batch [200/796] time 0.391 (0.377) data 0.000 (0.006) loss 0.8198 (0.8876) lr 8.3457e-03 eta 1:48:38
epoch [9/30] batch [220/796] time 0.328 (0.375) data 0.000 (0.005) loss 0.5483 (0.8676) lr 8.3457e-03 eta 1:48:10
epoch [9/30] batch [240/796] time 0.367 (0.374) data 0.000 (0.005) loss 2.3574 (0.9001) lr 8.3457e-03 eta 1:47:43
epoch [9/30] batch [260/796] time 0.368 (0.374) data 0.000 (0.004) loss 0.8579 (0.8744) lr 8.3457e-03 eta 1:47:31
epoch [9/30] batch [280/796] time 0.369 (0.373) data 0.000 (0.004) loss 0.2112 (0.8738) lr 8.3457e-03 eta 1:47:12
epoch [9/30] batch [300/796] time 0.351 (0.373) data 0.000 (0.004) loss 0.9775 (0.8800) lr 8.3457e-03 eta 1:46:59
epoch [9/30] batch [320/796] time 0.387 (0.373) data 0.000 (0.004) loss 0.8521 (0.8807) lr 8.3457e-03 eta 1:46:50
epoch [9/30] batch [340/796] time 0.440 (0.373) data 0.000 (0.003) loss 0.1111 (0.8796) lr 8.3457e-03 eta 1:46:39
epoch [9/30] batch [360/796] time 0.394 (0.373) data 0.000 (0.003) loss 1.3418 (0.8763) lr 8.3457e-03 eta 1:46:35
epoch [9/30] batch [380/796] time 0.357 (0.373) data 0.000 (0.003) loss 0.2290 (0.8692) lr 8.3457e-03 eta 1:46:29
epoch [9/30] batch [400/796] time 0.362 (0.373) data 0.000 (0.003) loss 0.3999 (0.8639) lr 8.3457e-03 eta 1:46:24
epoch [9/30] batch [420/796] time 0.347 (0.373) data 0.000 (0.003) loss 0.8652 (0.8614) lr 8.3457e-03 eta 1:46:14
epoch [9/30] batch [440/796] time 0.334 (0.373) data 0.000 (0.003) loss 0.8555 (0.8623) lr 8.3457e-03 eta 1:46:01
epoch [9/30] batch [460/796] time 0.335 (0.373) data 0.000 (0.003) loss 1.0410 (0.8610) lr 8.3457e-03 eta 1:45:53
epoch [9/30] batch [480/796] time 0.370 (0.372) data 0.000 (0.002) loss 0.4285 (0.8706) lr 8.3457e-03 eta 1:45:44
epoch [9/30] batch [500/796] time 0.363 (0.373) data 0.000 (0.002) loss 0.2935 (0.8796) lr 8.3457e-03 eta 1:45:38
epoch [9/30] batch [520/796] time 0.376 (0.373) data 0.001 (0.002) loss 0.3171 (0.8761) lr 8.3457e-03 eta 1:45:33
epoch [9/30] batch [540/796] time 0.387 (0.373) data 0.000 (0.002) loss 0.9985 (0.8763) lr 8.3457e-03 eta 1:45:26
epoch [9/30] batch [560/796] time 0.370 (0.373) data 0.001 (0.002) loss 1.1191 (0.8769) lr 8.3457e-03 eta 1:45:17
epoch [9/30] batch [580/796] time 0.379 (0.373) data 0.000 (0.002) loss 0.7368 (0.8873) lr 8.3457e-03 eta 1:45:16
epoch [9/30] batch [600/796] time 0.372 (0.373) data 0.000 (0.002) loss 0.4048 (0.8872) lr 8.3457e-03 eta 1:45:02
epoch [9/30] batch [620/796] time 0.381 (0.373) data 0.000 (0.002) loss 0.7139 (0.8859) lr 8.3457e-03 eta 1:44:54
epoch [9/30] batch [640/796] time 0.338 (0.373) data 0.000 (0.002) loss 0.3879 (0.8895) lr 8.3457e-03 eta 1:44:45
epoch [9/30] batch [660/796] time 0.357 (0.372) data 0.000 (0.002) loss 1.3779 (0.8966) lr 8.3457e-03 eta 1:44:36
epoch [9/30] batch [680/796] time 0.345 (0.372) data 0.000 (0.002) loss 0.3040 (0.8961) lr 8.3457e-03 eta 1:44:27
epoch [9/30] batch [700/796] time 0.392 (0.372) data 0.000 (0.002) loss 0.8047 (0.8941) lr 8.3457e-03 eta 1:44:20
epoch [9/30] batch [720/796] time 0.352 (0.372) data 0.000 (0.002) loss 0.3171 (0.8964) lr 8.3457e-03 eta 1:44:08
epoch [9/30] batch [740/796] time 0.378 (0.372) data 0.000 (0.002) loss 0.2712 (0.9002) lr 8.3457e-03 eta 1:44:00
epoch [9/30] batch [760/796] time 0.339 (0.372) data 0.000 (0.002) loss 1.3428 (0.9018) lr 8.3457e-03 eta 1:43:49
epoch [9/30] batch [780/796] time 0.324 (0.371) data 0.000 (0.002) loss 0.7681 (0.8992) lr 8.3457e-03 eta 1:43:26
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:50,  5.83s/it] 10%|█         | 2/20 [00:07<00:56,  3.14s/it] 15%|█▌        | 3/20 [00:07<00:31,  1.84s/it] 20%|██        | 4/20 [00:07<00:19,  1.23s/it] 25%|██▌       | 5/20 [00:07<00:13,  1.12it/s] 30%|███       | 6/20 [00:08<00:09,  1.46it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.79it/s] 40%|████      | 8/20 [00:08<00:05,  2.12it/s] 45%|████▌     | 9/20 [00:09<00:04,  2.45it/s] 50%|█████     | 10/20 [00:09<00:03,  2.73it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.97it/s] 60%|██████    | 12/20 [00:09<00:02,  3.19it/s] 65%|██████▌   | 13/20 [00:10<00:02,  3.35it/s] 70%|███████   | 14/20 [00:10<00:01,  3.50it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.62it/s] 80%|████████  | 16/20 [00:10<00:01,  3.73it/s] 85%|████████▌ | 17/20 [00:11<00:00,  3.81it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.94it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.31it/s]100%|██████████| 20/20 [00:11<00:00,  4.69it/s]100%|██████████| 20/20 [00:11<00:00,  1.68it/s]=> result
* total: 1,990
* correct: 1,554
* accuracy: 78.1%
* error: 21.9%
* macro_f1: 77.2%

epoch [10/30] batch [20/796] time 0.381 (0.430) data 0.000 (0.046) loss 1.0713 (0.8069) lr 7.9389e-03 eta 1:59:31
epoch [10/30] batch [40/796] time 0.381 (0.402) data 0.000 (0.023) loss 0.7275 (0.8202) lr 7.9389e-03 eta 1:51:51
epoch [10/30] batch [60/796] time 0.356 (0.387) data 0.000 (0.016) loss 0.5708 (0.8013) lr 7.9389e-03 eta 1:47:33
epoch [10/30] batch [80/796] time 0.354 (0.383) data 0.000 (0.012) loss 0.3391 (0.7822) lr 7.9389e-03 eta 1:46:05
epoch [10/30] batch [100/796] time 0.370 (0.382) data 0.001 (0.009) loss 0.2927 (0.7890) lr 7.9389e-03 eta 1:45:39
epoch [10/30] batch [120/796] time 0.362 (0.380) data 0.000 (0.008) loss 1.6943 (0.8027) lr 7.9389e-03 eta 1:45:02
epoch [10/30] batch [140/796] time 0.345 (0.378) data 0.000 (0.007) loss 0.4460 (0.8131) lr 7.9389e-03 eta 1:44:26
epoch [10/30] batch [160/796] time 0.360 (0.376) data 0.000 (0.006) loss 0.3918 (0.7969) lr 7.9389e-03 eta 1:43:52
epoch [10/30] batch [180/796] time 0.348 (0.375) data 0.000 (0.005) loss 0.2330 (0.7903) lr 7.9389e-03 eta 1:43:28
epoch [10/30] batch [200/796] time 0.352 (0.375) data 0.000 (0.005) loss 1.8916 (0.8102) lr 7.9389e-03 eta 1:43:13
epoch [10/30] batch [220/796] time 0.400 (0.375) data 0.000 (0.004) loss 1.6221 (0.8094) lr 7.9389e-03 eta 1:43:00
epoch [10/30] batch [240/796] time 0.362 (0.374) data 0.000 (0.004) loss 1.0439 (0.8149) lr 7.9389e-03 eta 1:42:46
epoch [10/30] batch [260/796] time 0.395 (0.374) data 0.000 (0.004) loss 1.3047 (0.8188) lr 7.9389e-03 eta 1:42:30
epoch [10/30] batch [280/796] time 0.359 (0.373) data 0.000 (0.004) loss 0.5205 (0.8109) lr 7.9389e-03 eta 1:42:16
epoch [10/30] batch [300/796] time 0.368 (0.373) data 0.000 (0.003) loss 0.9282 (0.8228) lr 7.9389e-03 eta 1:42:01
epoch [10/30] batch [320/796] time 0.359 (0.373) data 0.000 (0.003) loss 0.6196 (0.8242) lr 7.9389e-03 eta 1:41:51
epoch [10/30] batch [340/796] time 0.373 (0.372) data 0.000 (0.003) loss 1.2402 (0.8348) lr 7.9389e-03 eta 1:41:35
epoch [10/30] batch [360/796] time 0.387 (0.372) data 0.000 (0.003) loss 1.0693 (0.8451) lr 7.9389e-03 eta 1:41:25
epoch [10/30] batch [380/796] time 0.358 (0.372) data 0.000 (0.003) loss 1.0498 (0.8435) lr 7.9389e-03 eta 1:41:18
epoch [10/30] batch [400/796] time 0.346 (0.372) data 0.000 (0.003) loss 2.4121 (0.8499) lr 7.9389e-03 eta 1:41:11
epoch [10/30] batch [420/796] time 0.376 (0.372) data 0.000 (0.002) loss 1.0625 (0.8532) lr 7.9389e-03 eta 1:41:00
epoch [10/30] batch [440/796] time 0.349 (0.372) data 0.000 (0.002) loss 0.2556 (0.8502) lr 7.9389e-03 eta 1:40:48
epoch [10/30] batch [460/796] time 0.353 (0.371) data 0.000 (0.002) loss 1.0322 (0.8541) lr 7.9389e-03 eta 1:40:34
epoch [10/30] batch [480/796] time 0.376 (0.371) data 0.000 (0.002) loss 1.6230 (0.8449) lr 7.9389e-03 eta 1:40:25
epoch [10/30] batch [500/796] time 0.370 (0.371) data 0.000 (0.002) loss 0.4321 (0.8583) lr 7.9389e-03 eta 1:40:15
epoch [10/30] batch [520/796] time 0.355 (0.371) data 0.000 (0.002) loss 0.7212 (0.8596) lr 7.9389e-03 eta 1:40:05
epoch [10/30] batch [540/796] time 0.349 (0.371) data 0.000 (0.002) loss 0.1621 (0.8616) lr 7.9389e-03 eta 1:39:56
epoch [10/30] batch [560/796] time 0.386 (0.371) data 0.000 (0.002) loss 0.9512 (0.8550) lr 7.9389e-03 eta 1:39:49
epoch [10/30] batch [580/796] time 0.331 (0.370) data 0.000 (0.002) loss 0.2952 (0.8513) lr 7.9389e-03 eta 1:39:36
epoch [10/30] batch [600/796] time 0.408 (0.370) data 0.000 (0.002) loss 0.7905 (0.8572) lr 7.9389e-03 eta 1:39:29
epoch [10/30] batch [620/796] time 0.369 (0.370) data 0.000 (0.002) loss 0.5977 (0.8466) lr 7.9389e-03 eta 1:39:22
epoch [10/30] batch [640/796] time 0.364 (0.370) data 0.000 (0.002) loss 0.5391 (0.8504) lr 7.9389e-03 eta 1:39:13
epoch [10/30] batch [660/796] time 0.363 (0.370) data 0.000 (0.002) loss 1.4434 (0.8509) lr 7.9389e-03 eta 1:39:02
epoch [10/30] batch [680/796] time 0.341 (0.370) data 0.000 (0.002) loss 0.6719 (0.8572) lr 7.9389e-03 eta 1:38:56
epoch [10/30] batch [700/796] time 0.379 (0.370) data 0.000 (0.002) loss 1.7510 (0.8698) lr 7.9389e-03 eta 1:38:47
epoch [10/30] batch [720/796] time 0.331 (0.370) data 0.000 (0.002) loss 1.4209 (0.8672) lr 7.9389e-03 eta 1:38:34
epoch [10/30] batch [740/796] time 0.346 (0.370) data 0.000 (0.001) loss 2.2559 (0.8696) lr 7.9389e-03 eta 1:38:26
epoch [10/30] batch [760/796] time 0.359 (0.370) data 0.000 (0.001) loss 2.0020 (0.8745) lr 7.9389e-03 eta 1:38:15
epoch [10/30] batch [780/796] time 0.324 (0.369) data 0.000 (0.001) loss 2.6816 (0.8812) lr 7.9389e-03 eta 1:37:56
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:44,  5.52s/it] 10%|█         | 2/20 [00:06<00:52,  2.92s/it] 15%|█▌        | 3/20 [00:06<00:29,  1.72s/it] 20%|██        | 4/20 [00:07<00:18,  1.15s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.19it/s] 30%|███       | 6/20 [00:07<00:09,  1.54it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.89it/s] 40%|████      | 8/20 [00:08<00:05,  2.21it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.47it/s] 50%|█████     | 10/20 [00:08<00:03,  2.72it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.03it/s] 60%|██████    | 12/20 [00:09<00:02,  3.26it/s] 65%|██████▌   | 13/20 [00:09<00:01,  3.67it/s] 70%|███████   | 14/20 [00:09<00:01,  3.86it/s] 75%|███████▌  | 15/20 [00:10<00:01,  4.06it/s] 80%|████████  | 16/20 [00:10<00:00,  4.29it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.38it/s] 90%|█████████ | 18/20 [00:10<00:00,  3.14it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.61it/s]100%|██████████| 20/20 [00:11<00:00,  4.08it/s]100%|██████████| 20/20 [00:11<00:00,  1.74it/s]=> result
* total: 1,990
* correct: 1,558
* accuracy: 78.3%
* error: 21.7%
* macro_f1: 77.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model.pth.tar-10

epoch [11/30] batch [20/796] time 0.336 (0.417) data 0.000 (0.040) loss 0.7520 (0.8042) lr 7.5000e-03 eta 1:50:28
epoch [11/30] batch [40/796] time 0.359 (0.393) data 0.000 (0.020) loss 0.1940 (0.8049) lr 7.5000e-03 eta 1:43:53
epoch [11/30] batch [60/796] time 0.375 (0.386) data 0.000 (0.014) loss 0.7461 (0.8210) lr 7.5000e-03 eta 1:42:07
epoch [11/30] batch [80/796] time 0.377 (0.384) data 0.000 (0.010) loss 0.8042 (0.7741) lr 7.5000e-03 eta 1:41:14
epoch [11/30] batch [100/796] time 0.383 (0.379) data 0.000 (0.008) loss 1.8457 (0.8081) lr 7.5000e-03 eta 1:39:57
epoch [11/30] batch [120/796] time 0.379 (0.376) data 0.000 (0.007) loss 0.2461 (0.8187) lr 7.5000e-03 eta 1:39:05
epoch [11/30] batch [140/796] time 0.371 (0.375) data 0.000 (0.006) loss 0.3811 (0.8153) lr 7.5000e-03 eta 1:38:39
epoch [11/30] batch [160/796] time 0.360 (0.375) data 0.000 (0.005) loss 0.5142 (0.8258) lr 7.5000e-03 eta 1:38:28
epoch [11/30] batch [180/796] time 0.375 (0.374) data 0.000 (0.005) loss 1.2139 (0.8327) lr 7.5000e-03 eta 1:38:10
epoch [11/30] batch [200/796] time 0.388 (0.375) data 0.000 (0.004) loss 1.9932 (0.8570) lr 7.5000e-03 eta 1:38:10
epoch [11/30] batch [220/796] time 0.336 (0.374) data 0.000 (0.004) loss 2.5195 (0.8629) lr 7.5000e-03 eta 1:37:45
epoch [11/30] batch [240/796] time 0.374 (0.372) data 0.000 (0.004) loss 0.6875 (0.8690) lr 7.5000e-03 eta 1:37:16
epoch [11/30] batch [260/796] time 0.342 (0.371) data 0.000 (0.003) loss 0.9116 (0.8612) lr 7.5000e-03 eta 1:36:57
epoch [11/30] batch [280/796] time 0.353 (0.371) data 0.000 (0.003) loss 2.1484 (0.8684) lr 7.5000e-03 eta 1:36:41
epoch [11/30] batch [300/796] time 0.331 (0.371) data 0.000 (0.003) loss 0.5381 (0.8649) lr 7.5000e-03 eta 1:36:35
epoch [11/30] batch [320/796] time 0.367 (0.371) data 0.000 (0.003) loss 1.1592 (0.8712) lr 7.5000e-03 eta 1:36:27
epoch [11/30] batch [340/796] time 0.339 (0.371) data 0.000 (0.003) loss 1.2822 (0.8804) lr 7.5000e-03 eta 1:36:13
epoch [11/30] batch [360/796] time 0.343 (0.370) data 0.000 (0.002) loss 0.4739 (0.8738) lr 7.5000e-03 eta 1:36:03
epoch [11/30] batch [380/796] time 0.349 (0.370) data 0.001 (0.002) loss 1.0762 (0.8734) lr 7.5000e-03 eta 1:35:54
epoch [11/30] batch [400/796] time 0.352 (0.370) data 0.001 (0.002) loss 0.4016 (0.8694) lr 7.5000e-03 eta 1:35:40
epoch [11/30] batch [420/796] time 0.364 (0.370) data 0.000 (0.002) loss 0.9541 (0.8586) lr 7.5000e-03 eta 1:35:33
epoch [11/30] batch [440/796] time 0.374 (0.370) data 0.000 (0.002) loss 0.0643 (0.8702) lr 7.5000e-03 eta 1:35:28
epoch [11/30] batch [460/796] time 0.355 (0.370) data 0.000 (0.002) loss 0.8862 (0.8576) lr 7.5000e-03 eta 1:35:22
epoch [11/30] batch [480/796] time 0.339 (0.370) data 0.000 (0.002) loss 0.8066 (0.8590) lr 7.5000e-03 eta 1:35:11
epoch [11/30] batch [500/796] time 0.327 (0.370) data 0.000 (0.002) loss 0.3394 (0.8486) lr 7.5000e-03 eta 1:35:02
epoch [11/30] batch [520/796] time 0.385 (0.370) data 0.000 (0.002) loss 0.6255 (0.8493) lr 7.5000e-03 eta 1:34:56
epoch [11/30] batch [540/796] time 0.379 (0.370) data 0.000 (0.002) loss 2.4570 (0.8448) lr 7.5000e-03 eta 1:34:53
epoch [11/30] batch [560/796] time 0.335 (0.370) data 0.000 (0.002) loss 1.0176 (0.8406) lr 7.5000e-03 eta 1:34:41
epoch [11/30] batch [580/796] time 0.364 (0.370) data 0.000 (0.002) loss 0.3911 (0.8465) lr 7.5000e-03 eta 1:34:31
epoch [11/30] batch [600/796] time 0.380 (0.370) data 0.000 (0.002) loss 1.4883 (0.8530) lr 7.5000e-03 eta 1:34:22
epoch [11/30] batch [620/796] time 0.387 (0.369) data 0.000 (0.002) loss 0.3955 (0.8568) lr 7.5000e-03 eta 1:34:12
epoch [11/30] batch [640/796] time 0.361 (0.369) data 0.000 (0.002) loss 0.7798 (0.8600) lr 7.5000e-03 eta 1:34:03
epoch [11/30] batch [660/796] time 0.377 (0.370) data 0.000 (0.001) loss 1.3750 (0.8648) lr 7.5000e-03 eta 1:33:59
epoch [11/30] batch [680/796] time 0.408 (0.370) data 0.000 (0.001) loss 0.4414 (0.8650) lr 7.5000e-03 eta 1:33:52
epoch [11/30] batch [700/796] time 0.385 (0.370) data 0.000 (0.001) loss 0.4634 (0.8674) lr 7.5000e-03 eta 1:33:47
epoch [11/30] batch [720/796] time 0.332 (0.370) data 0.000 (0.001) loss 0.5073 (0.8654) lr 7.5000e-03 eta 1:33:37
epoch [11/30] batch [740/796] time 0.364 (0.370) data 0.000 (0.001) loss 0.5576 (0.8634) lr 7.5000e-03 eta 1:33:29
epoch [11/30] batch [760/796] time 0.380 (0.369) data 0.000 (0.001) loss 1.3066 (0.8595) lr 7.5000e-03 eta 1:33:20
epoch [11/30] batch [780/796] time 0.325 (0.369) data 0.000 (0.001) loss 0.8354 (0.8623) lr 7.5000e-03 eta 1:33:00
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:45,  5.54s/it] 10%|█         | 2/20 [00:06<00:53,  2.96s/it] 15%|█▌        | 3/20 [00:06<00:29,  1.74s/it] 20%|██        | 4/20 [00:07<00:18,  1.16s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.19it/s] 30%|███       | 6/20 [00:07<00:09,  1.54it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.88it/s] 40%|████      | 8/20 [00:08<00:05,  2.22it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.53it/s] 50%|█████     | 10/20 [00:08<00:03,  2.81it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.07it/s] 60%|██████    | 12/20 [00:09<00:02,  3.27it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.42it/s] 70%|███████   | 14/20 [00:09<00:01,  3.58it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.76it/s] 80%|████████  | 16/20 [00:10<00:00,  4.02it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.97it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.33it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.78it/s]100%|██████████| 20/20 [00:11<00:00,  4.23it/s]100%|██████████| 20/20 [00:11<00:00,  1.73it/s]=> result
* total: 1,990
* correct: 1,564
* accuracy: 78.6%
* error: 21.4%
* macro_f1: 77.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar

epoch [12/30] batch [20/796] time 0.386 (0.435) data 0.000 (0.040) loss 0.4568 (0.8786) lr 7.0337e-03 eta 1:49:26
epoch [12/30] batch [40/796] time 0.388 (0.401) data 0.000 (0.020) loss 1.2705 (0.9448) lr 7.0337e-03 eta 1:40:55
epoch [12/30] batch [60/796] time 0.373 (0.391) data 0.000 (0.013) loss 0.5820 (0.9141) lr 7.0337e-03 eta 1:38:10
epoch [12/30] batch [80/796] time 0.374 (0.386) data 0.000 (0.010) loss 1.8896 (0.8629) lr 7.0337e-03 eta 1:36:45
epoch [12/30] batch [100/796] time 0.385 (0.382) data 0.000 (0.008) loss 0.9233 (0.8940) lr 7.0337e-03 eta 1:35:37
epoch [12/30] batch [120/796] time 0.393 (0.381) data 0.000 (0.007) loss 1.7988 (0.8947) lr 7.0337e-03 eta 1:35:14
epoch [12/30] batch [140/796] time 0.361 (0.378) data 0.000 (0.006) loss 1.6406 (0.9226) lr 7.0337e-03 eta 1:34:24
epoch [12/30] batch [160/796] time 0.347 (0.377) data 0.000 (0.005) loss 1.1396 (0.9330) lr 7.0337e-03 eta 1:34:00
epoch [12/30] batch [180/796] time 0.385 (0.377) data 0.000 (0.005) loss 1.8926 (0.9296) lr 7.0337e-03 eta 1:33:46
epoch [12/30] batch [200/796] time 0.341 (0.376) data 0.000 (0.004) loss 1.0225 (0.9365) lr 7.0337e-03 eta 1:33:31
epoch [12/30] batch [220/796] time 0.383 (0.375) data 0.000 (0.004) loss 0.5864 (0.9253) lr 7.0337e-03 eta 1:33:15
epoch [12/30] batch [240/796] time 0.355 (0.376) data 0.000 (0.004) loss 0.3672 (0.9227) lr 7.0337e-03 eta 1:33:11
epoch [12/30] batch [260/796] time 0.373 (0.375) data 0.000 (0.003) loss 0.5684 (0.9050) lr 7.0337e-03 eta 1:32:55
epoch [12/30] batch [280/796] time 0.336 (0.374) data 0.000 (0.003) loss 0.6831 (0.8861) lr 7.0337e-03 eta 1:32:31
epoch [12/30] batch [300/796] time 0.349 (0.373) data 0.000 (0.003) loss 0.1125 (0.8883) lr 7.0337e-03 eta 1:32:10
epoch [12/30] batch [320/796] time 0.374 (0.373) data 0.000 (0.003) loss 1.6035 (0.8747) lr 7.0337e-03 eta 1:32:02
epoch [12/30] batch [340/796] time 0.364 (0.373) data 0.000 (0.003) loss 1.2002 (0.8762) lr 7.0337e-03 eta 1:31:49
epoch [12/30] batch [360/796] time 0.348 (0.372) data 0.000 (0.002) loss 1.0889 (0.8849) lr 7.0337e-03 eta 1:31:33
epoch [12/30] batch [380/796] time 0.380 (0.372) data 0.000 (0.002) loss 1.0537 (0.8875) lr 7.0337e-03 eta 1:31:26
epoch [12/30] batch [400/796] time 0.360 (0.372) data 0.000 (0.002) loss 0.9077 (0.8821) lr 7.0337e-03 eta 1:31:21
epoch [12/30] batch [420/796] time 0.378 (0.372) data 0.000 (0.002) loss 0.7739 (0.8809) lr 7.0337e-03 eta 1:31:10
epoch [12/30] batch [440/796] time 0.367 (0.372) data 0.000 (0.002) loss 1.8193 (0.8831) lr 7.0337e-03 eta 1:30:58
epoch [12/30] batch [460/796] time 0.379 (0.372) data 0.000 (0.002) loss 0.9233 (0.8829) lr 7.0337e-03 eta 1:30:50
epoch [12/30] batch [480/796] time 0.352 (0.372) data 0.000 (0.002) loss 0.3218 (0.8854) lr 7.0337e-03 eta 1:30:43
epoch [12/30] batch [500/796] time 0.358 (0.372) data 0.001 (0.002) loss 1.4561 (0.8987) lr 7.0337e-03 eta 1:30:32
epoch [12/30] batch [520/796] time 0.356 (0.372) data 0.000 (0.002) loss 0.6597 (0.9040) lr 7.0337e-03 eta 1:30:25
epoch [12/30] batch [540/796] time 0.345 (0.371) data 0.000 (0.002) loss 0.3577 (0.9107) lr 7.0337e-03 eta 1:30:13
epoch [12/30] batch [560/796] time 0.383 (0.371) data 0.000 (0.002) loss 1.1084 (0.9074) lr 7.0337e-03 eta 1:30:01
epoch [12/30] batch [580/796] time 0.371 (0.371) data 0.000 (0.002) loss 1.0742 (0.9061) lr 7.0337e-03 eta 1:29:50
epoch [12/30] batch [600/796] time 0.346 (0.371) data 0.001 (0.002) loss 1.3184 (0.8997) lr 7.0337e-03 eta 1:29:43
epoch [12/30] batch [620/796] time 0.375 (0.370) data 0.000 (0.002) loss 1.4062 (0.8996) lr 7.0337e-03 eta 1:29:32
epoch [12/30] batch [640/796] time 0.366 (0.370) data 0.000 (0.002) loss 1.2861 (0.8999) lr 7.0337e-03 eta 1:29:20
epoch [12/30] batch [660/796] time 0.372 (0.370) data 0.000 (0.001) loss 1.7656 (0.9021) lr 7.0337e-03 eta 1:29:15
epoch [12/30] batch [680/796] time 0.364 (0.370) data 0.000 (0.001) loss 2.2617 (0.8954) lr 7.0337e-03 eta 1:29:10
epoch [12/30] batch [700/796] time 0.351 (0.371) data 0.000 (0.001) loss 2.4277 (0.8985) lr 7.0337e-03 eta 1:29:04
epoch [12/30] batch [720/796] time 0.338 (0.371) data 0.000 (0.001) loss 0.3386 (0.9013) lr 7.0337e-03 eta 1:29:00
epoch [12/30] batch [740/796] time 0.390 (0.371) data 0.000 (0.001) loss 0.5210 (0.9014) lr 7.0337e-03 eta 1:28:56
epoch [12/30] batch [760/796] time 0.345 (0.371) data 0.000 (0.001) loss 1.8125 (0.9035) lr 7.0337e-03 eta 1:28:49
epoch [12/30] batch [780/796] time 0.326 (0.370) data 0.000 (0.001) loss 2.0742 (0.9012) lr 7.0337e-03 eta 1:28:29
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:48,  5.70s/it] 10%|█         | 2/20 [00:06<00:51,  2.84s/it] 15%|█▌        | 3/20 [00:06<00:28,  1.67s/it] 20%|██        | 4/20 [00:07<00:18,  1.13s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.21it/s] 30%|███       | 6/20 [00:07<00:08,  1.56it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.90it/s] 40%|████      | 8/20 [00:08<00:05,  2.24it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.52it/s] 50%|█████     | 10/20 [00:08<00:03,  2.78it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.01it/s] 60%|██████    | 12/20 [00:09<00:02,  3.24it/s] 65%|██████▌   | 13/20 [00:09<00:01,  3.50it/s] 70%|███████   | 14/20 [00:09<00:01,  3.72it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.94it/s] 80%|████████  | 16/20 [00:10<00:01,  3.96it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.99it/s] 90%|█████████ | 18/20 [00:10<00:00,  3.42it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.81it/s]100%|██████████| 20/20 [00:11<00:00,  4.25it/s]100%|██████████| 20/20 [00:11<00:00,  1.75it/s]=> result
* total: 1,990
* correct: 1,570
* accuracy: 78.9%
* error: 21.1%
* macro_f1: 78.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar

epoch [13/30] batch [20/796] time 0.368 (0.424) data 0.000 (0.050) loss 0.4514 (0.9600) lr 6.5451e-03 eta 1:41:04
epoch [13/30] batch [40/796] time 0.390 (0.398) data 0.000 (0.025) loss 0.5039 (0.9440) lr 6.5451e-03 eta 1:34:46
epoch [13/30] batch [60/796] time 0.372 (0.387) data 0.000 (0.017) loss 0.6313 (0.9096) lr 6.5451e-03 eta 1:32:04
epoch [13/30] batch [80/796] time 0.397 (0.384) data 0.000 (0.013) loss 0.1898 (0.8901) lr 6.5451e-03 eta 1:31:07
epoch [13/30] batch [100/796] time 0.343 (0.382) data 0.000 (0.010) loss 0.4294 (0.9128) lr 6.5451e-03 eta 1:30:28
epoch [13/30] batch [120/796] time 0.370 (0.380) data 0.000 (0.008) loss 0.1577 (0.9000) lr 6.5451e-03 eta 1:30:00
epoch [13/30] batch [140/796] time 0.376 (0.378) data 0.000 (0.007) loss 0.6807 (0.9012) lr 6.5451e-03 eta 1:29:25
epoch [13/30] batch [160/796] time 0.387 (0.378) data 0.000 (0.006) loss 0.7231 (0.9324) lr 6.5451e-03 eta 1:29:17
epoch [13/30] batch [180/796] time 0.387 (0.377) data 0.000 (0.006) loss 0.8247 (0.9211) lr 6.5451e-03 eta 1:28:51
epoch [13/30] batch [200/796] time 0.372 (0.375) data 0.000 (0.005) loss 0.1071 (0.9074) lr 6.5451e-03 eta 1:28:21
epoch [13/30] batch [220/796] time 0.368 (0.375) data 0.000 (0.005) loss 0.7119 (0.9217) lr 6.5451e-03 eta 1:28:06
epoch [13/30] batch [240/796] time 0.391 (0.374) data 0.000 (0.004) loss 0.5479 (0.9074) lr 6.5451e-03 eta 1:27:45
epoch [13/30] batch [260/796] time 0.401 (0.374) data 0.000 (0.004) loss 0.4978 (0.9141) lr 6.5451e-03 eta 1:27:35
epoch [13/30] batch [280/796] time 0.382 (0.374) data 0.000 (0.004) loss 0.2656 (0.9141) lr 6.5451e-03 eta 1:27:31
epoch [13/30] batch [300/796] time 0.374 (0.373) data 0.000 (0.004) loss 0.6216 (0.8963) lr 6.5451e-03 eta 1:27:17
epoch [13/30] batch [320/796] time 0.386 (0.373) data 0.000 (0.003) loss 0.6548 (0.9036) lr 6.5451e-03 eta 1:27:11
epoch [13/30] batch [340/796] time 0.374 (0.373) data 0.000 (0.003) loss 0.4319 (0.8976) lr 6.5451e-03 eta 1:27:00
epoch [13/30] batch [360/796] time 0.377 (0.373) data 0.000 (0.003) loss 1.1045 (0.8845) lr 6.5451e-03 eta 1:26:52
epoch [13/30] batch [380/796] time 0.346 (0.373) data 0.000 (0.003) loss 0.4436 (0.8919) lr 6.5451e-03 eta 1:26:40
epoch [13/30] batch [400/796] time 0.359 (0.374) data 0.000 (0.003) loss 2.1289 (0.8956) lr 6.5451e-03 eta 1:26:42
epoch [13/30] batch [420/796] time 0.375 (0.374) data 0.000 (0.003) loss 0.2159 (0.8931) lr 6.5451e-03 eta 1:26:35
epoch [13/30] batch [440/796] time 0.340 (0.373) data 0.000 (0.003) loss 1.1699 (0.9022) lr 6.5451e-03 eta 1:26:24
epoch [13/30] batch [460/796] time 0.345 (0.373) data 0.000 (0.002) loss 1.3242 (0.8990) lr 6.5451e-03 eta 1:26:11
epoch [13/30] batch [480/796] time 0.365 (0.373) data 0.000 (0.002) loss 0.4150 (0.8879) lr 6.5451e-03 eta 1:26:06
epoch [13/30] batch [500/796] time 0.362 (0.373) data 0.000 (0.002) loss 0.4841 (0.8793) lr 6.5451e-03 eta 1:25:57
epoch [13/30] batch [520/796] time 0.341 (0.373) data 0.000 (0.002) loss 1.5908 (0.8726) lr 6.5451e-03 eta 1:25:45
epoch [13/30] batch [540/796] time 0.343 (0.373) data 0.000 (0.002) loss 0.3516 (0.8630) lr 6.5451e-03 eta 1:25:38
epoch [13/30] batch [560/796] time 0.334 (0.372) data 0.000 (0.002) loss 0.5239 (0.8639) lr 6.5451e-03 eta 1:25:27
epoch [13/30] batch [580/796] time 0.391 (0.372) data 0.000 (0.002) loss 0.7319 (0.8709) lr 6.5451e-03 eta 1:25:20
epoch [13/30] batch [600/796] time 0.352 (0.372) data 0.000 (0.002) loss 2.4297 (0.8733) lr 6.5451e-03 eta 1:25:11
epoch [13/30] batch [620/796] time 0.383 (0.372) data 0.000 (0.002) loss 1.1016 (0.8729) lr 6.5451e-03 eta 1:25:04
epoch [13/30] batch [640/796] time 0.347 (0.372) data 0.000 (0.002) loss 2.5996 (0.8791) lr 6.5451e-03 eta 1:24:58
epoch [13/30] batch [660/796] time 0.344 (0.373) data 0.000 (0.002) loss 2.4297 (0.8834) lr 6.5451e-03 eta 1:24:51
epoch [13/30] batch [680/796] time 0.398 (0.372) data 0.000 (0.002) loss 0.3855 (0.8766) lr 6.5451e-03 eta 1:24:40
epoch [13/30] batch [700/796] time 0.375 (0.372) data 0.000 (0.002) loss 1.0361 (0.8829) lr 6.5451e-03 eta 1:24:32
epoch [13/30] batch [720/796] time 0.369 (0.372) data 0.000 (0.002) loss 1.1631 (0.8879) lr 6.5451e-03 eta 1:24:23
epoch [13/30] batch [740/796] time 0.398 (0.372) data 0.000 (0.002) loss 0.8638 (0.8848) lr 6.5451e-03 eta 1:24:17
epoch [13/30] batch [760/796] time 0.375 (0.372) data 0.000 (0.002) loss 1.6426 (0.8860) lr 6.5451e-03 eta 1:24:09
epoch [13/30] batch [780/796] time 0.322 (0.371) data 0.000 (0.002) loss 0.7241 (0.8820) lr 6.5451e-03 eta 1:23:48
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:04<01:32,  4.89s/it] 10%|█         | 2/20 [00:06<00:52,  2.92s/it] 15%|█▌        | 3/20 [00:06<00:29,  1.72s/it] 20%|██        | 4/20 [00:07<00:18,  1.16s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.18it/s] 30%|███       | 6/20 [00:07<00:09,  1.52it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.86it/s] 40%|████      | 8/20 [00:08<00:05,  2.17it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.49it/s] 50%|█████     | 10/20 [00:08<00:03,  2.75it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.98it/s] 60%|██████    | 12/20 [00:09<00:02,  3.10it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.26it/s] 70%|███████   | 14/20 [00:09<00:01,  3.56it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.68it/s] 80%|████████  | 16/20 [00:10<00:01,  3.79it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.86it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.24it/s] 95%|█████████▌| 19/20 [00:10<00:00,  4.55it/s]100%|██████████| 20/20 [00:11<00:00,  4.88it/s]100%|██████████| 20/20 [00:11<00:00,  1.78it/s]=> result
* total: 1,990
* correct: 1,570
* accuracy: 78.9%
* error: 21.1%
* macro_f1: 78.2%

epoch [14/30] batch [20/796] time 0.398 (0.417) data 0.000 (0.038) loss 0.1571 (0.9457) lr 6.0396e-03 eta 1:33:51
epoch [14/30] batch [40/796] time 0.388 (0.392) data 0.000 (0.019) loss 0.3801 (0.9492) lr 6.0396e-03 eta 1:28:09
epoch [14/30] batch [60/796] time 0.350 (0.384) data 0.000 (0.013) loss 0.6343 (1.0427) lr 6.0396e-03 eta 1:26:19
epoch [14/30] batch [80/796] time 0.391 (0.382) data 0.000 (0.010) loss 1.6377 (0.9966) lr 6.0396e-03 eta 1:25:41
epoch [14/30] batch [100/796] time 0.367 (0.379) data 0.000 (0.008) loss 1.4111 (0.9534) lr 6.0396e-03 eta 1:24:49
epoch [14/30] batch [120/796] time 0.344 (0.376) data 0.000 (0.007) loss 2.8926 (0.9805) lr 6.0396e-03 eta 1:24:07
epoch [14/30] batch [140/796] time 0.379 (0.375) data 0.000 (0.006) loss 1.0332 (0.9766) lr 6.0396e-03 eta 1:23:46
epoch [14/30] batch [160/796] time 0.390 (0.375) data 0.000 (0.005) loss 1.7734 (0.9817) lr 6.0396e-03 eta 1:23:28
epoch [14/30] batch [180/796] time 0.393 (0.374) data 0.000 (0.004) loss 1.8926 (0.9790) lr 6.0396e-03 eta 1:23:14
epoch [14/30] batch [200/796] time 0.378 (0.374) data 0.000 (0.004) loss 1.3799 (0.9653) lr 6.0396e-03 eta 1:23:03
epoch [14/30] batch [220/796] time 0.382 (0.375) data 0.000 (0.004) loss 0.2372 (0.9534) lr 6.0396e-03 eta 1:23:06
epoch [14/30] batch [240/796] time 0.348 (0.374) data 0.001 (0.003) loss 0.2886 (0.9495) lr 6.0396e-03 eta 1:22:53
epoch [14/30] batch [260/796] time 0.353 (0.374) data 0.000 (0.003) loss 0.5698 (0.9118) lr 6.0396e-03 eta 1:22:39
epoch [14/30] batch [280/796] time 0.379 (0.374) data 0.000 (0.003) loss 0.6128 (0.9125) lr 6.0396e-03 eta 1:22:36
epoch [14/30] batch [300/796] time 0.395 (0.374) data 0.000 (0.003) loss 1.0732 (0.9168) lr 6.0396e-03 eta 1:22:23
epoch [14/30] batch [320/796] time 0.430 (0.373) data 0.000 (0.003) loss 0.1700 (0.9186) lr 6.0396e-03 eta 1:22:13
epoch [14/30] batch [340/796] time 0.382 (0.373) data 0.000 (0.002) loss 1.4541 (0.9044) lr 6.0396e-03 eta 1:21:56
epoch [14/30] batch [360/796] time 0.359 (0.372) data 0.000 (0.002) loss 0.3376 (0.8984) lr 6.0396e-03 eta 1:21:45
epoch [14/30] batch [380/796] time 0.346 (0.372) data 0.000 (0.002) loss 0.6533 (0.8929) lr 6.0396e-03 eta 1:21:33
epoch [14/30] batch [400/796] time 0.357 (0.373) data 0.000 (0.002) loss 0.8711 (0.8921) lr 6.0396e-03 eta 1:21:32
epoch [14/30] batch [420/796] time 0.377 (0.372) data 0.000 (0.002) loss 2.1973 (0.8972) lr 6.0396e-03 eta 1:21:22
epoch [14/30] batch [440/796] time 0.389 (0.372) data 0.000 (0.002) loss 0.9922 (0.8902) lr 6.0396e-03 eta 1:21:15
epoch [14/30] batch [460/796] time 0.386 (0.372) data 0.000 (0.002) loss 0.2494 (0.8920) lr 6.0396e-03 eta 1:21:06
epoch [14/30] batch [480/796] time 0.389 (0.372) data 0.000 (0.002) loss 1.4580 (0.8886) lr 6.0396e-03 eta 1:20:54
epoch [14/30] batch [500/796] time 0.372 (0.372) data 0.000 (0.002) loss 0.3264 (0.8873) lr 6.0396e-03 eta 1:20:45
epoch [14/30] batch [520/796] time 0.382 (0.372) data 0.000 (0.002) loss 0.5103 (0.8875) lr 6.0396e-03 eta 1:20:37
epoch [14/30] batch [540/796] time 0.381 (0.372) data 0.000 (0.002) loss 0.5571 (0.8854) lr 6.0396e-03 eta 1:20:32
epoch [14/30] batch [560/796] time 0.350 (0.372) data 0.000 (0.002) loss 0.1338 (0.8850) lr 6.0396e-03 eta 1:20:26
epoch [14/30] batch [580/796] time 0.352 (0.372) data 0.000 (0.002) loss 1.0312 (0.8905) lr 6.0396e-03 eta 1:20:19
epoch [14/30] batch [600/796] time 0.330 (0.372) data 0.000 (0.002) loss 0.5591 (0.8832) lr 6.0396e-03 eta 1:20:13
epoch [14/30] batch [620/796] time 0.389 (0.372) data 0.000 (0.001) loss 1.6143 (0.8768) lr 6.0396e-03 eta 1:20:05
epoch [14/30] batch [640/796] time 0.378 (0.372) data 0.000 (0.001) loss 0.3826 (0.8785) lr 6.0396e-03 eta 1:19:56
epoch [14/30] batch [660/796] time 0.387 (0.372) data 0.000 (0.001) loss 0.2615 (0.8771) lr 6.0396e-03 eta 1:19:48
epoch [14/30] batch [680/796] time 0.341 (0.372) data 0.000 (0.001) loss 0.1121 (0.8817) lr 6.0396e-03 eta 1:19:40
epoch [14/30] batch [700/796] time 0.483 (0.372) data 0.000 (0.001) loss 0.8940 (0.8786) lr 6.0396e-03 eta 1:19:34
epoch [14/30] batch [720/796] time 0.380 (0.372) data 0.000 (0.001) loss 0.3940 (0.8739) lr 6.0396e-03 eta 1:19:26
epoch [14/30] batch [740/796] time 0.368 (0.372) data 0.000 (0.001) loss 0.5576 (0.8758) lr 6.0396e-03 eta 1:19:16
epoch [14/30] batch [760/796] time 0.336 (0.372) data 0.000 (0.001) loss 0.2123 (0.8723) lr 6.0396e-03 eta 1:19:07
epoch [14/30] batch [780/796] time 0.327 (0.371) data 0.000 (0.001) loss 0.2148 (0.8653) lr 6.0396e-03 eta 1:18:50
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:50,  5.81s/it] 10%|█         | 2/20 [00:06<00:50,  2.78s/it] 15%|█▌        | 3/20 [00:06<00:27,  1.64s/it] 20%|██        | 4/20 [00:07<00:17,  1.10s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.24it/s] 30%|███       | 6/20 [00:07<00:08,  1.60it/s] 35%|███▌      | 7/20 [00:07<00:06,  1.95it/s] 40%|████      | 8/20 [00:08<00:05,  2.27it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.57it/s] 50%|█████     | 10/20 [00:08<00:03,  2.80it/s] 55%|█████▌    | 11/20 [00:08<00:02,  3.01it/s] 60%|██████    | 12/20 [00:09<00:02,  3.25it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.45it/s] 70%|███████   | 14/20 [00:09<00:01,  3.59it/s] 75%|███████▌  | 15/20 [00:09<00:01,  3.70it/s] 80%|████████  | 16/20 [00:10<00:01,  3.77it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.84it/s] 90%|█████████ | 18/20 [00:10<00:00,  4.05it/s] 95%|█████████▌| 19/20 [00:10<00:00,  4.40it/s]100%|██████████| 20/20 [00:11<00:00,  4.76it/s]100%|██████████| 20/20 [00:11<00:00,  1.78it/s]=> result
* total: 1,990
* correct: 1,572
* accuracy: 79.0%
* error: 21.0%
* macro_f1: 78.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar

epoch [15/30] batch [20/796] time 0.398 (0.424) data 0.000 (0.043) loss 0.6543 (0.6557) lr 5.5226e-03 eta 1:29:51
epoch [15/30] batch [40/796] time 0.383 (0.397) data 0.000 (0.022) loss 0.4150 (0.8565) lr 5.5226e-03 eta 1:23:55
epoch [15/30] batch [60/796] time 0.346 (0.389) data 0.000 (0.015) loss 0.0477 (0.8619) lr 5.5226e-03 eta 1:22:15
epoch [15/30] batch [80/796] time 0.373 (0.385) data 0.000 (0.011) loss 1.2461 (0.8641) lr 5.5226e-03 eta 1:21:08
epoch [15/30] batch [100/796] time 0.334 (0.379) data 0.000 (0.009) loss 1.5020 (0.8365) lr 5.5226e-03 eta 1:19:52
epoch [15/30] batch [120/796] time 0.365 (0.377) data 0.000 (0.007) loss 1.1855 (0.8773) lr 5.5226e-03 eta 1:19:14
epoch [15/30] batch [140/796] time 0.356 (0.375) data 0.000 (0.006) loss 0.1805 (0.8777) lr 5.5226e-03 eta 1:18:47
epoch [15/30] batch [160/796] time 0.404 (0.375) data 0.000 (0.006) loss 0.3733 (0.9077) lr 5.5226e-03 eta 1:18:32
epoch [15/30] batch [180/796] time 0.367 (0.374) data 0.000 (0.005) loss 0.4844 (0.8898) lr 5.5226e-03 eta 1:18:15
epoch [15/30] batch [200/796] time 0.392 (0.373) data 0.000 (0.005) loss 0.1476 (0.8797) lr 5.5226e-03 eta 1:17:52
epoch [15/30] batch [220/796] time 0.369 (0.372) data 0.000 (0.004) loss 0.3499 (0.8771) lr 5.5226e-03 eta 1:17:36
epoch [15/30] batch [240/796] time 0.332 (0.372) data 0.000 (0.004) loss 1.8418 (0.8819) lr 5.5226e-03 eta 1:17:28
epoch [15/30] batch [260/796] time 0.344 (0.372) data 0.000 (0.004) loss 0.7529 (0.8753) lr 5.5226e-03 eta 1:17:17
epoch [15/30] batch [280/796] time 0.392 (0.372) data 0.001 (0.003) loss 0.4490 (0.8872) lr 5.5226e-03 eta 1:17:11
epoch [15/30] batch [300/796] time 0.374 (0.372) data 0.000 (0.003) loss 0.1158 (0.8708) lr 5.5226e-03 eta 1:17:01
epoch [15/30] batch [320/796] time 0.411 (0.372) data 0.000 (0.003) loss 0.4304 (0.8708) lr 5.5226e-03 eta 1:16:57
epoch [15/30] batch [340/796] time 0.360 (0.372) data 0.000 (0.003) loss 0.8652 (0.8735) lr 5.5226e-03 eta 1:16:48
epoch [15/30] batch [360/796] time 0.385 (0.372) data 0.000 (0.003) loss 0.2561 (0.8747) lr 5.5226e-03 eta 1:16:48
epoch [15/30] batch [380/796] time 0.343 (0.372) data 0.000 (0.003) loss 2.0137 (0.8698) lr 5.5226e-03 eta 1:16:36
epoch [15/30] batch [400/796] time 0.383 (0.372) data 0.000 (0.002) loss 0.8145 (0.8705) lr 5.5226e-03 eta 1:16:27
epoch [15/30] batch [420/796] time 0.362 (0.372) data 0.000 (0.002) loss 0.1194 (0.8634) lr 5.5226e-03 eta 1:16:18
epoch [15/30] batch [440/796] time 0.394 (0.372) data 0.000 (0.002) loss 0.3970 (0.8630) lr 5.5226e-03 eta 1:16:11
epoch [15/30] batch [460/796] time 0.351 (0.372) data 0.000 (0.002) loss 0.7969 (0.8630) lr 5.5226e-03 eta 1:16:02
epoch [15/30] batch [480/796] time 0.388 (0.371) data 0.000 (0.002) loss 0.4419 (0.8548) lr 5.5226e-03 eta 1:15:51
epoch [15/30] batch [500/796] time 0.359 (0.371) data 0.000 (0.002) loss 1.6230 (0.8593) lr 5.5226e-03 eta 1:15:41
epoch [15/30] batch [520/796] time 0.340 (0.371) data 0.000 (0.002) loss 0.9409 (0.8533) lr 5.5226e-03 eta 1:15:30
epoch [15/30] batch [540/796] time 0.360 (0.371) data 0.000 (0.002) loss 0.2939 (0.8491) lr 5.5226e-03 eta 1:15:20
epoch [15/30] batch [560/796] time 0.385 (0.371) data 0.000 (0.002) loss 0.5518 (0.8471) lr 5.5226e-03 eta 1:15:11
epoch [15/30] batch [580/796] time 0.381 (0.371) data 0.000 (0.002) loss 0.5483 (0.8422) lr 5.5226e-03 eta 1:15:04
epoch [15/30] batch [600/796] time 0.352 (0.370) data 0.000 (0.002) loss 2.2617 (0.8458) lr 5.5226e-03 eta 1:14:54
epoch [15/30] batch [620/796] time 0.374 (0.370) data 0.000 (0.002) loss 0.9146 (0.8473) lr 5.5226e-03 eta 1:14:44
epoch [15/30] batch [640/796] time 0.374 (0.370) data 0.000 (0.002) loss 0.7456 (0.8399) lr 5.5226e-03 eta 1:14:35
epoch [15/30] batch [660/796] time 0.391 (0.370) data 0.000 (0.002) loss 0.4739 (0.8442) lr 5.5226e-03 eta 1:14:24
epoch [15/30] batch [680/796] time 0.369 (0.370) data 0.000 (0.002) loss 0.1250 (0.8433) lr 5.5226e-03 eta 1:14:19
epoch [15/30] batch [700/796] time 0.364 (0.370) data 0.000 (0.001) loss 0.1547 (0.8405) lr 5.5226e-03 eta 1:14:09
epoch [15/30] batch [720/796] time 0.383 (0.370) data 0.000 (0.001) loss 0.1987 (0.8386) lr 5.5226e-03 eta 1:14:02
epoch [15/30] batch [740/796] time 0.342 (0.370) data 0.000 (0.001) loss 0.9990 (0.8344) lr 5.5226e-03 eta 1:13:57
epoch [15/30] batch [760/796] time 0.336 (0.370) data 0.000 (0.001) loss 0.6694 (0.8337) lr 5.5226e-03 eta 1:13:48
epoch [15/30] batch [780/796] time 0.324 (0.369) data 0.000 (0.001) loss 0.0915 (0.8296) lr 5.5226e-03 eta 1:13:33
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<01:58,  6.23s/it] 10%|█         | 2/20 [00:07<00:55,  3.09s/it] 15%|█▌        | 3/20 [00:07<00:30,  1.81s/it] 20%|██        | 4/20 [00:07<00:19,  1.20s/it] 25%|██▌       | 5/20 [00:07<00:13,  1.15it/s] 30%|███       | 6/20 [00:08<00:09,  1.49it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.84it/s] 40%|████      | 8/20 [00:08<00:05,  2.19it/s] 45%|████▌     | 9/20 [00:09<00:04,  2.49it/s] 50%|█████     | 10/20 [00:09<00:03,  2.75it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.96it/s] 60%|██████    | 12/20 [00:09<00:02,  3.10it/s] 65%|██████▌   | 13/20 [00:10<00:02,  3.37it/s] 70%|███████   | 14/20 [00:10<00:01,  3.56it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.94it/s] 80%|████████  | 16/20 [00:10<00:00,  4.09it/s] 85%|████████▌ | 17/20 [00:11<00:00,  4.24it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.21it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.68it/s]100%|██████████| 20/20 [00:11<00:00,  4.15it/s]100%|██████████| 20/20 [00:12<00:00,  1.66it/s]=> result
* total: 1,990
* correct: 1,571
* accuracy: 78.9%
* error: 21.1%
* macro_f1: 78.3%

epoch [16/30] batch [20/796] time 0.386 (0.424) data 0.000 (0.050) loss 0.6978 (0.7784) lr 5.0000e-03 eta 1:24:19
epoch [16/30] batch [40/796] time 0.350 (0.393) data 0.000 (0.025) loss 0.1969 (0.8931) lr 5.0000e-03 eta 1:17:58
epoch [16/30] batch [60/796] time 0.333 (0.383) data 0.000 (0.017) loss 0.7314 (0.9462) lr 5.0000e-03 eta 1:15:55
epoch [16/30] batch [80/796] time 0.366 (0.378) data 0.000 (0.013) loss 1.9219 (0.9092) lr 5.0000e-03 eta 1:14:45
epoch [16/30] batch [100/796] time 0.381 (0.375) data 0.000 (0.010) loss 3.1465 (0.9112) lr 5.0000e-03 eta 1:14:05
epoch [16/30] batch [120/796] time 0.366 (0.374) data 0.000 (0.008) loss 2.3789 (0.9034) lr 5.0000e-03 eta 1:13:41
epoch [16/30] batch [140/796] time 0.382 (0.375) data 0.000 (0.007) loss 0.5317 (0.8756) lr 5.0000e-03 eta 1:13:50
epoch [16/30] batch [160/796] time 0.386 (0.375) data 0.000 (0.006) loss 0.6172 (0.8641) lr 5.0000e-03 eta 1:13:40
epoch [16/30] batch [180/796] time 0.342 (0.375) data 0.000 (0.006) loss 1.0576 (0.8599) lr 5.0000e-03 eta 1:13:26
epoch [16/30] batch [200/796] time 0.337 (0.374) data 0.000 (0.005) loss 2.5938 (0.8705) lr 5.0000e-03 eta 1:13:16
epoch [16/30] batch [220/796] time 0.374 (0.374) data 0.000 (0.005) loss 0.5664 (0.8842) lr 5.0000e-03 eta 1:13:08
epoch [16/30] batch [240/796] time 0.386 (0.374) data 0.000 (0.004) loss 0.7319 (0.8965) lr 5.0000e-03 eta 1:12:54
epoch [16/30] batch [260/796] time 0.351 (0.374) data 0.000 (0.004) loss 0.0739 (0.8794) lr 5.0000e-03 eta 1:12:43
epoch [16/30] batch [280/796] time 0.362 (0.373) data 0.000 (0.004) loss 0.2678 (0.8728) lr 5.0000e-03 eta 1:12:27
epoch [16/30] batch [300/796] time 0.381 (0.374) data 0.000 (0.004) loss 1.6475 (0.8914) lr 5.0000e-03 eta 1:12:27
epoch [16/30] batch [320/796] time 0.382 (0.373) data 0.000 (0.003) loss 1.1211 (0.9020) lr 5.0000e-03 eta 1:12:14
epoch [16/30] batch [340/796] time 0.384 (0.373) data 0.000 (0.003) loss 1.5254 (0.9095) lr 5.0000e-03 eta 1:12:08
epoch [16/30] batch [360/796] time 0.367 (0.373) data 0.000 (0.003) loss 1.7881 (0.8884) lr 5.0000e-03 eta 1:11:59
epoch [16/30] batch [380/796] time 0.378 (0.372) data 0.000 (0.003) loss 1.3359 (0.8777) lr 5.0000e-03 eta 1:11:45
epoch [16/30] batch [400/796] time 0.382 (0.372) data 0.000 (0.003) loss 0.6836 (0.8729) lr 5.0000e-03 eta 1:11:31
epoch [16/30] batch [420/796] time 0.391 (0.372) data 0.000 (0.003) loss 0.8638 (0.8712) lr 5.0000e-03 eta 1:11:24
epoch [16/30] batch [440/796] time 0.362 (0.371) data 0.000 (0.002) loss 0.2426 (0.8728) lr 5.0000e-03 eta 1:11:10
epoch [16/30] batch [460/796] time 0.358 (0.371) data 0.000 (0.002) loss 0.4866 (0.8754) lr 5.0000e-03 eta 1:10:58
epoch [16/30] batch [480/796] time 0.378 (0.371) data 0.000 (0.002) loss 0.1871 (0.8736) lr 5.0000e-03 eta 1:10:49
epoch [16/30] batch [500/796] time 0.384 (0.370) data 0.000 (0.002) loss 0.2013 (0.8733) lr 5.0000e-03 eta 1:10:37
epoch [16/30] batch [520/796] time 0.368 (0.370) data 0.000 (0.002) loss 0.3796 (0.8748) lr 5.0000e-03 eta 1:10:24
epoch [16/30] batch [540/796] time 0.368 (0.370) data 0.000 (0.002) loss 1.0840 (0.8788) lr 5.0000e-03 eta 1:10:16
epoch [16/30] batch [560/796] time 0.383 (0.370) data 0.000 (0.002) loss 0.4531 (0.8822) lr 5.0000e-03 eta 1:10:13
epoch [16/30] batch [580/796] time 0.380 (0.370) data 0.000 (0.002) loss 0.2012 (0.8832) lr 5.0000e-03 eta 1:10:03
epoch [16/30] batch [600/796] time 0.370 (0.370) data 0.000 (0.002) loss 1.6934 (0.8797) lr 5.0000e-03 eta 1:09:52
epoch [16/30] batch [620/796] time 0.372 (0.369) data 0.000 (0.002) loss 0.2439 (0.8781) lr 5.0000e-03 eta 1:09:41
epoch [16/30] batch [640/796] time 0.400 (0.369) data 0.000 (0.002) loss 0.9858 (0.8691) lr 5.0000e-03 eta 1:09:34
epoch [16/30] batch [660/796] time 0.368 (0.370) data 0.000 (0.002) loss 1.1094 (0.8608) lr 5.0000e-03 eta 1:09:28
epoch [16/30] batch [680/796] time 0.337 (0.369) data 0.000 (0.002) loss 0.3203 (0.8561) lr 5.0000e-03 eta 1:09:19
epoch [16/30] batch [700/796] time 0.337 (0.370) data 0.000 (0.002) loss 0.9937 (0.8519) lr 5.0000e-03 eta 1:09:13
epoch [16/30] batch [720/796] time 0.369 (0.369) data 0.000 (0.002) loss 1.1553 (0.8514) lr 5.0000e-03 eta 1:09:04
epoch [16/30] batch [740/796] time 0.425 (0.369) data 0.000 (0.002) loss 1.9629 (0.8513) lr 5.0000e-03 eta 1:08:57
epoch [16/30] batch [760/796] time 0.375 (0.370) data 0.000 (0.002) loss 0.3845 (0.8505) lr 5.0000e-03 eta 1:08:51
epoch [16/30] batch [780/796] time 0.323 (0.369) data 0.000 (0.002) loss 1.1572 (0.8522) lr 5.0000e-03 eta 1:08:32
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<01:54,  6.02s/it] 10%|█         | 2/20 [00:06<00:53,  2.97s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.75s/it] 20%|██        | 4/20 [00:07<00:18,  1.17s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.17it/s] 30%|███       | 6/20 [00:08<00:09,  1.50it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.83it/s] 40%|████      | 8/20 [00:08<00:05,  2.13it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.42it/s] 50%|█████     | 10/20 [00:09<00:03,  2.64it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.84it/s] 60%|██████    | 12/20 [00:09<00:02,  3.02it/s] 65%|██████▌   | 13/20 [00:10<00:02,  3.23it/s] 70%|███████   | 14/20 [00:10<00:01,  3.44it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.73it/s] 80%|████████  | 16/20 [00:10<00:01,  3.82it/s] 85%|████████▌ | 17/20 [00:11<00:00,  3.86it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.35it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.80it/s]100%|██████████| 20/20 [00:11<00:00,  4.25it/s]100%|██████████| 20/20 [00:11<00:00,  1.68it/s]=> result
* total: 1,990
* correct: 1,568
* accuracy: 78.8%
* error: 21.2%
* macro_f1: 78.0%

epoch [17/30] batch [20/796] time 0.344 (0.414) data 0.000 (0.046) loss 0.7222 (0.8448) lr 4.4774e-03 eta 1:16:50
epoch [17/30] batch [40/796] time 0.334 (0.388) data 0.000 (0.023) loss 1.4375 (0.8732) lr 4.4774e-03 eta 1:11:44
epoch [17/30] batch [60/796] time 0.361 (0.383) data 0.000 (0.015) loss 0.8667 (0.8353) lr 4.4774e-03 eta 1:10:43
epoch [17/30] batch [80/796] time 0.384 (0.379) data 0.000 (0.012) loss 0.6528 (0.8177) lr 4.4774e-03 eta 1:09:53
epoch [17/30] batch [100/796] time 0.340 (0.376) data 0.000 (0.009) loss 2.9258 (0.8743) lr 4.4774e-03 eta 1:09:16
epoch [17/30] batch [120/796] time 0.376 (0.376) data 0.000 (0.008) loss 0.8701 (0.8945) lr 4.4774e-03 eta 1:09:07
epoch [17/30] batch [140/796] time 0.380 (0.374) data 0.000 (0.007) loss 0.1813 (0.8745) lr 4.4774e-03 eta 1:08:39
epoch [17/30] batch [160/796] time 0.362 (0.374) data 0.000 (0.006) loss 0.8794 (0.8798) lr 4.4774e-03 eta 1:08:25
epoch [17/30] batch [180/796] time 0.380 (0.373) data 0.000 (0.005) loss 0.6108 (0.8701) lr 4.4774e-03 eta 1:08:08
epoch [17/30] batch [200/796] time 0.363 (0.373) data 0.000 (0.005) loss 0.5327 (0.8696) lr 4.4774e-03 eta 1:08:00
epoch [17/30] batch [220/796] time 0.414 (0.373) data 0.000 (0.004) loss 0.2646 (0.8636) lr 4.4774e-03 eta 1:07:52
epoch [17/30] batch [240/796] time 0.374 (0.373) data 0.000 (0.004) loss 0.3164 (0.8610) lr 4.4774e-03 eta 1:07:47
epoch [17/30] batch [260/796] time 0.349 (0.372) data 0.000 (0.004) loss 1.5439 (0.8623) lr 4.4774e-03 eta 1:07:32
epoch [17/30] batch [280/796] time 0.381 (0.372) data 0.000 (0.003) loss 0.6855 (0.8637) lr 4.4774e-03 eta 1:07:21
epoch [17/30] batch [300/796] time 0.377 (0.372) data 0.000 (0.003) loss 0.2018 (0.8590) lr 4.4774e-03 eta 1:07:09
epoch [17/30] batch [320/796] time 0.359 (0.371) data 0.000 (0.003) loss 0.1322 (0.8644) lr 4.4774e-03 eta 1:06:56
epoch [17/30] batch [340/796] time 0.385 (0.371) data 0.000 (0.003) loss 1.2998 (0.8503) lr 4.4774e-03 eta 1:06:52
epoch [17/30] batch [360/796] time 0.340 (0.371) data 0.000 (0.003) loss 2.2129 (0.8587) lr 4.4774e-03 eta 1:06:43
epoch [17/30] batch [380/796] time 0.377 (0.371) data 0.000 (0.003) loss 0.1254 (0.8682) lr 4.4774e-03 eta 1:06:35
epoch [17/30] batch [400/796] time 0.382 (0.371) data 0.000 (0.003) loss 0.3201 (0.8637) lr 4.4774e-03 eta 1:06:28
epoch [17/30] batch [420/796] time 0.359 (0.371) data 0.000 (0.002) loss 0.9736 (0.8500) lr 4.4774e-03 eta 1:06:21
epoch [17/30] batch [440/796] time 0.367 (0.371) data 0.000 (0.002) loss 1.0713 (0.8491) lr 4.4774e-03 eta 1:06:10
epoch [17/30] batch [460/796] time 0.348 (0.371) data 0.000 (0.002) loss 0.1270 (0.8397) lr 4.4774e-03 eta 1:06:01
epoch [17/30] batch [480/796] time 0.384 (0.370) data 0.000 (0.002) loss 1.1172 (0.8395) lr 4.4774e-03 eta 1:05:50
epoch [17/30] batch [500/796] time 0.373 (0.370) data 0.000 (0.002) loss 1.7686 (0.8356) lr 4.4774e-03 eta 1:05:43
epoch [17/30] batch [520/796] time 0.326 (0.370) data 0.000 (0.002) loss 0.7900 (0.8352) lr 4.4774e-03 eta 1:05:33
epoch [17/30] batch [540/796] time 0.350 (0.370) data 0.000 (0.002) loss 0.2866 (0.8311) lr 4.4774e-03 eta 1:05:24
epoch [17/30] batch [560/796] time 0.379 (0.370) data 0.000 (0.002) loss 0.1881 (0.8249) lr 4.4774e-03 eta 1:05:15
epoch [17/30] batch [580/796] time 0.400 (0.370) data 0.000 (0.002) loss 0.4224 (0.8248) lr 4.4774e-03 eta 1:05:10
epoch [17/30] batch [600/796] time 0.388 (0.371) data 0.000 (0.002) loss 1.0840 (0.8249) lr 4.4774e-03 eta 1:05:08
epoch [17/30] batch [620/796] time 0.356 (0.371) data 0.000 (0.002) loss 0.0515 (0.8251) lr 4.4774e-03 eta 1:05:01
epoch [17/30] batch [640/796] time 0.392 (0.371) data 0.000 (0.002) loss 0.9253 (0.8240) lr 4.4774e-03 eta 1:04:54
epoch [17/30] batch [660/796] time 0.377 (0.371) data 0.000 (0.002) loss 0.7964 (0.8229) lr 4.4774e-03 eta 1:04:45
epoch [17/30] batch [680/796] time 0.366 (0.371) data 0.000 (0.002) loss 0.4155 (0.8191) lr 4.4774e-03 eta 1:04:37
epoch [17/30] batch [700/796] time 0.386 (0.371) data 0.000 (0.002) loss 0.5532 (0.8188) lr 4.4774e-03 eta 1:04:29
epoch [17/30] batch [720/796] time 0.406 (0.370) data 0.000 (0.002) loss 0.3005 (0.8214) lr 4.4774e-03 eta 1:04:21
epoch [17/30] batch [740/796] time 0.370 (0.371) data 0.000 (0.001) loss 0.1376 (0.8196) lr 4.4774e-03 eta 1:04:14
epoch [17/30] batch [760/796] time 0.379 (0.370) data 0.000 (0.001) loss 0.2522 (0.8162) lr 4.4774e-03 eta 1:04:05
epoch [17/30] batch [780/796] time 0.325 (0.369) data 0.000 (0.001) loss 0.8311 (0.8185) lr 4.4774e-03 eta 1:03:49
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<02:01,  6.38s/it] 10%|█         | 2/20 [00:06<00:53,  2.97s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.74s/it] 20%|██        | 4/20 [00:07<00:18,  1.16s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.19it/s] 30%|███       | 6/20 [00:08<00:09,  1.54it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.88it/s] 40%|████      | 8/20 [00:08<00:05,  2.23it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.55it/s] 50%|█████     | 10/20 [00:09<00:03,  2.84it/s] 55%|█████▌    | 11/20 [00:09<00:03,  3.00it/s] 60%|██████    | 12/20 [00:09<00:02,  3.20it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.34it/s] 70%|███████   | 14/20 [00:10<00:01,  3.52it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.63it/s] 80%|████████  | 16/20 [00:10<00:01,  3.75it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.90it/s] 90%|█████████ | 18/20 [00:11<00:00,  2.15it/s] 95%|█████████▌| 19/20 [00:12<00:00,  2.63it/s]100%|██████████| 20/20 [00:12<00:00,  3.15it/s]100%|██████████| 20/20 [00:12<00:00,  1.61it/s]=> result
* total: 1,990
* correct: 1,573
* accuracy: 79.0%
* error: 21.0%
* macro_f1: 78.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar

epoch [18/30] batch [20/796] time 0.374 (0.430) data 0.000 (0.053) loss 0.9609 (0.7118) lr 3.9604e-03 eta 1:14:00
epoch [18/30] batch [40/796] time 0.353 (0.400) data 0.000 (0.027) loss 1.3545 (0.7506) lr 3.9604e-03 eta 1:08:47
epoch [18/30] batch [60/796] time 0.364 (0.387) data 0.000 (0.018) loss 0.9209 (0.7189) lr 3.9604e-03 eta 1:06:25
epoch [18/30] batch [80/796] time 0.392 (0.382) data 0.000 (0.013) loss 0.8442 (0.6739) lr 3.9604e-03 eta 1:05:26
epoch [18/30] batch [100/796] time 0.361 (0.381) data 0.000 (0.011) loss 1.3193 (0.6644) lr 3.9604e-03 eta 1:05:02
epoch [18/30] batch [120/796] time 0.373 (0.378) data 0.000 (0.009) loss 2.0449 (0.7010) lr 3.9604e-03 eta 1:04:27
epoch [18/30] batch [140/796] time 0.367 (0.377) data 0.000 (0.008) loss 0.3970 (0.7131) lr 3.9604e-03 eta 1:04:08
epoch [18/30] batch [160/796] time 0.374 (0.376) data 0.000 (0.007) loss 0.6538 (0.7169) lr 3.9604e-03 eta 1:03:49
epoch [18/30] batch [180/796] time 0.350 (0.375) data 0.000 (0.006) loss 1.2158 (0.7321) lr 3.9604e-03 eta 1:03:33
epoch [18/30] batch [200/796] time 0.350 (0.375) data 0.000 (0.006) loss 0.4016 (0.7328) lr 3.9604e-03 eta 1:03:21
epoch [18/30] batch [220/796] time 0.377 (0.374) data 0.000 (0.005) loss 0.2058 (0.7241) lr 3.9604e-03 eta 1:03:09
epoch [18/30] batch [240/796] time 0.404 (0.373) data 0.000 (0.005) loss 1.3916 (0.7383) lr 3.9604e-03 eta 1:02:50
epoch [18/30] batch [260/796] time 0.344 (0.373) data 0.000 (0.004) loss 0.4443 (0.7263) lr 3.9604e-03 eta 1:02:42
epoch [18/30] batch [280/796] time 0.339 (0.373) data 0.000 (0.004) loss 0.4092 (0.7339) lr 3.9604e-03 eta 1:02:32
epoch [18/30] batch [300/796] time 0.368 (0.372) data 0.000 (0.004) loss 1.4238 (0.7400) lr 3.9604e-03 eta 1:02:20
epoch [18/30] batch [320/796] time 0.378 (0.372) data 0.000 (0.004) loss 1.5840 (0.7448) lr 3.9604e-03 eta 1:02:09
epoch [18/30] batch [340/796] time 0.336 (0.371) data 0.000 (0.003) loss 0.5542 (0.7450) lr 3.9604e-03 eta 1:01:56
epoch [18/30] batch [360/796] time 0.362 (0.372) data 0.000 (0.003) loss 1.0078 (0.7407) lr 3.9604e-03 eta 1:01:51
epoch [18/30] batch [380/796] time 0.367 (0.371) data 0.000 (0.003) loss 0.4167 (0.7463) lr 3.9604e-03 eta 1:01:40
epoch [18/30] batch [400/796] time 0.372 (0.371) data 0.000 (0.003) loss 0.8369 (0.7471) lr 3.9604e-03 eta 1:01:26
epoch [18/30] batch [420/796] time 0.364 (0.371) data 0.000 (0.003) loss 0.6177 (0.7519) lr 3.9604e-03 eta 1:01:23
epoch [18/30] batch [440/796] time 0.374 (0.371) data 0.000 (0.003) loss 1.1621 (0.7526) lr 3.9604e-03 eta 1:01:18
epoch [18/30] batch [460/796] time 0.370 (0.371) data 0.000 (0.003) loss 1.6113 (0.7585) lr 3.9604e-03 eta 1:01:11
epoch [18/30] batch [480/796] time 0.398 (0.371) data 0.000 (0.002) loss 1.2861 (0.7571) lr 3.9604e-03 eta 1:01:00
epoch [18/30] batch [500/796] time 0.348 (0.371) data 0.000 (0.002) loss 1.1543 (0.7563) lr 3.9604e-03 eta 1:00:52
epoch [18/30] batch [520/796] time 0.368 (0.371) data 0.000 (0.002) loss 0.4353 (0.7611) lr 3.9604e-03 eta 1:00:46
epoch [18/30] batch [540/796] time 0.396 (0.371) data 0.000 (0.002) loss 1.4678 (0.7692) lr 3.9604e-03 eta 1:00:38
epoch [18/30] batch [560/796] time 0.405 (0.371) data 0.000 (0.002) loss 0.3469 (0.7682) lr 3.9604e-03 eta 1:00:27
epoch [18/30] batch [580/796] time 0.397 (0.371) data 0.000 (0.002) loss 0.3323 (0.7698) lr 3.9604e-03 eta 1:00:23
epoch [18/30] batch [600/796] time 0.362 (0.371) data 0.000 (0.002) loss 0.8921 (0.7726) lr 3.9604e-03 eta 1:00:17
epoch [18/30] batch [620/796] time 0.342 (0.371) data 0.000 (0.002) loss 1.4922 (0.7806) lr 3.9604e-03 eta 1:00:06
epoch [18/30] batch [640/796] time 0.370 (0.371) data 0.000 (0.002) loss 0.4900 (0.7851) lr 3.9604e-03 eta 0:59:58
epoch [18/30] batch [660/796] time 0.359 (0.371) data 0.000 (0.002) loss 0.1615 (0.7821) lr 3.9604e-03 eta 0:59:51
epoch [18/30] batch [680/796] time 0.376 (0.370) data 0.000 (0.002) loss 1.4736 (0.7830) lr 3.9604e-03 eta 0:59:41
epoch [18/30] batch [700/796] time 0.381 (0.370) data 0.000 (0.002) loss 0.8945 (0.7795) lr 3.9604e-03 eta 0:59:33
epoch [18/30] batch [720/796] time 0.351 (0.370) data 0.000 (0.002) loss 0.0460 (0.7798) lr 3.9604e-03 eta 0:59:26
epoch [18/30] batch [740/796] time 0.332 (0.370) data 0.000 (0.002) loss 0.2573 (0.7824) lr 3.9604e-03 eta 0:59:17
epoch [18/30] batch [760/796] time 0.343 (0.370) data 0.000 (0.002) loss 0.3279 (0.7793) lr 3.9604e-03 eta 0:59:08
epoch [18/30] batch [780/796] time 0.324 (0.370) data 0.000 (0.002) loss 1.2197 (0.7779) lr 3.9604e-03 eta 0:58:55
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<02:12,  6.97s/it] 10%|█         | 2/20 [00:07<01:00,  3.37s/it] 15%|█▌        | 3/20 [00:08<00:33,  1.96s/it] 20%|██        | 4/20 [00:08<00:20,  1.30s/it] 25%|██▌       | 5/20 [00:08<00:13,  1.07it/s] 30%|███       | 6/20 [00:08<00:10,  1.40it/s] 35%|███▌      | 7/20 [00:09<00:07,  1.75it/s] 40%|████      | 8/20 [00:09<00:05,  2.07it/s] 45%|████▌     | 9/20 [00:09<00:04,  2.39it/s] 50%|█████     | 10/20 [00:10<00:03,  2.59it/s] 55%|█████▌    | 11/20 [00:10<00:03,  2.76it/s] 60%|██████    | 12/20 [00:10<00:02,  2.95it/s] 65%|██████▌   | 13/20 [00:10<00:02,  3.15it/s] 70%|███████   | 14/20 [00:11<00:01,  3.32it/s] 75%|███████▌  | 15/20 [00:11<00:01,  3.45it/s] 80%|████████  | 16/20 [00:11<00:01,  3.54it/s] 85%|████████▌ | 17/20 [00:12<00:00,  3.63it/s] 90%|█████████ | 18/20 [00:12<00:00,  3.42it/s] 95%|█████████▌| 19/20 [00:12<00:00,  3.54it/s]100%|██████████| 20/20 [00:12<00:00,  3.71it/s]100%|██████████| 20/20 [00:13<00:00,  1.53it/s]=> result
* total: 1,990
* correct: 1,576
* accuracy: 79.2%
* error: 20.8%
* macro_f1: 78.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar

epoch [19/30] batch [20/796] time 0.331 (0.427) data 0.000 (0.041) loss 0.4585 (0.7090) lr 3.4549e-03 eta 1:07:54
epoch [19/30] batch [40/796] time 0.375 (0.403) data 0.000 (0.021) loss 0.9375 (0.7058) lr 3.4549e-03 eta 1:03:57
epoch [19/30] batch [60/796] time 0.380 (0.399) data 0.000 (0.014) loss 0.1923 (0.7323) lr 3.4549e-03 eta 1:03:11
epoch [19/30] batch [80/796] time 0.393 (0.397) data 0.000 (0.010) loss 0.5420 (0.7884) lr 3.4549e-03 eta 1:02:35
epoch [19/30] batch [100/796] time 0.395 (0.394) data 0.000 (0.008) loss 0.3621 (0.7946) lr 3.4549e-03 eta 1:02:05
epoch [19/30] batch [120/796] time 0.342 (0.392) data 0.000 (0.007) loss 1.5010 (0.8074) lr 3.4549e-03 eta 1:01:34
epoch [19/30] batch [140/796] time 0.391 (0.392) data 0.000 (0.006) loss 0.5459 (0.8106) lr 3.4549e-03 eta 1:01:33
epoch [19/30] batch [160/796] time 0.398 (0.390) data 0.000 (0.005) loss 2.0645 (0.8504) lr 3.4549e-03 eta 1:00:58
epoch [19/30] batch [180/796] time 0.387 (0.387) data 0.000 (0.005) loss 0.5239 (0.8307) lr 3.4549e-03 eta 1:00:28
epoch [19/30] batch [200/796] time 0.404 (0.387) data 0.000 (0.004) loss 0.5811 (0.8354) lr 3.4549e-03 eta 1:00:19
epoch [19/30] batch [220/796] time 0.367 (0.388) data 0.000 (0.004) loss 0.4731 (0.8313) lr 3.4549e-03 eta 1:00:19
epoch [19/30] batch [240/796] time 0.375 (0.386) data 0.000 (0.004) loss 0.3989 (0.8215) lr 3.4549e-03 eta 0:59:55
epoch [19/30] batch [260/796] time 0.367 (0.384) data 0.000 (0.003) loss 0.5293 (0.8115) lr 3.4549e-03 eta 0:59:31
epoch [19/30] batch [280/796] time 0.385 (0.383) data 0.000 (0.003) loss 0.7021 (0.8214) lr 3.4549e-03 eta 0:59:15
epoch [19/30] batch [300/796] time 0.391 (0.382) data 0.000 (0.003) loss 2.1504 (0.8098) lr 3.4549e-03 eta 0:58:55
epoch [19/30] batch [320/796] time 0.385 (0.382) data 0.000 (0.003) loss 0.4980 (0.8096) lr 3.4549e-03 eta 0:58:42
epoch [19/30] batch [340/796] time 0.362 (0.381) data 0.000 (0.003) loss 3.4355 (0.8192) lr 3.4549e-03 eta 0:58:30
epoch [19/30] batch [360/796] time 0.389 (0.381) data 0.000 (0.003) loss 3.4395 (0.8229) lr 3.4549e-03 eta 0:58:20
epoch [19/30] batch [380/796] time 0.355 (0.381) data 0.000 (0.002) loss 0.8613 (0.8309) lr 3.4549e-03 eta 0:58:11
epoch [19/30] batch [400/796] time 0.378 (0.379) data 0.000 (0.002) loss 0.5361 (0.8207) lr 3.4549e-03 eta 0:57:52
epoch [19/30] batch [420/796] time 0.388 (0.380) data 0.000 (0.002) loss 0.6421 (0.8173) lr 3.4549e-03 eta 0:57:46
epoch [19/30] batch [440/796] time 0.367 (0.379) data 0.000 (0.002) loss 0.8789 (0.8230) lr 3.4549e-03 eta 0:57:35
epoch [19/30] batch [460/796] time 0.394 (0.380) data 0.000 (0.002) loss 0.1564 (0.8144) lr 3.4549e-03 eta 0:57:31
epoch [19/30] batch [480/796] time 0.379 (0.380) data 0.000 (0.002) loss 1.6377 (0.8123) lr 3.4549e-03 eta 0:57:27
epoch [19/30] batch [500/796] time 0.353 (0.379) data 0.000 (0.002) loss 0.9971 (0.8133) lr 3.4549e-03 eta 0:57:13
epoch [19/30] batch [520/796] time 0.363 (0.379) data 0.000 (0.002) loss 1.1709 (0.8152) lr 3.4549e-03 eta 0:57:00
epoch [19/30] batch [540/796] time 0.367 (0.378) data 0.000 (0.002) loss 0.1810 (0.8096) lr 3.4549e-03 eta 0:56:48
epoch [19/30] batch [560/796] time 0.370 (0.378) data 0.000 (0.002) loss 0.3237 (0.8040) lr 3.4549e-03 eta 0:56:38
epoch [19/30] batch [580/796] time 0.355 (0.378) data 0.000 (0.002) loss 0.3127 (0.8041) lr 3.4549e-03 eta 0:56:28
epoch [19/30] batch [600/796] time 0.376 (0.378) data 0.000 (0.002) loss 0.9194 (0.7989) lr 3.4549e-03 eta 0:56:21
epoch [19/30] batch [620/796] time 0.393 (0.378) data 0.000 (0.002) loss 0.4387 (0.8002) lr 3.4549e-03 eta 0:56:11
epoch [19/30] batch [640/796] time 0.363 (0.377) data 0.000 (0.002) loss 0.6323 (0.7964) lr 3.4549e-03 eta 0:56:02
epoch [19/30] batch [660/796] time 0.330 (0.377) data 0.000 (0.002) loss 0.3669 (0.8027) lr 3.4549e-03 eta 0:55:52
epoch [19/30] batch [680/796] time 0.339 (0.377) data 0.000 (0.001) loss 1.1934 (0.8029) lr 3.4549e-03 eta 0:55:41
epoch [19/30] batch [700/796] time 0.381 (0.377) data 0.000 (0.001) loss 0.4182 (0.7991) lr 3.4549e-03 eta 0:55:33
epoch [19/30] batch [720/796] time 0.361 (0.376) data 0.000 (0.001) loss 1.1318 (0.7967) lr 3.4549e-03 eta 0:55:24
epoch [19/30] batch [740/796] time 0.365 (0.376) data 0.000 (0.001) loss 0.8228 (0.7938) lr 3.4549e-03 eta 0:55:16
epoch [19/30] batch [760/796] time 0.383 (0.376) data 0.000 (0.001) loss 0.2318 (0.7983) lr 3.4549e-03 eta 0:55:06
epoch [19/30] batch [780/796] time 0.323 (0.375) data 0.000 (0.001) loss 1.3398 (0.7997) lr 3.4549e-03 eta 0:54:48
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<01:54,  6.03s/it] 10%|█         | 2/20 [00:06<00:54,  3.03s/it] 15%|█▌        | 3/20 [00:07<00:30,  1.77s/it] 20%|██        | 4/20 [00:07<00:18,  1.18s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.16it/s] 30%|███       | 6/20 [00:08<00:09,  1.50it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.85it/s] 40%|████      | 8/20 [00:08<00:05,  2.21it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.52it/s] 50%|█████     | 10/20 [00:09<00:03,  2.81it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.04it/s] 60%|██████    | 12/20 [00:09<00:02,  3.20it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.44it/s] 70%|███████   | 14/20 [00:10<00:01,  3.56it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.68it/s] 80%|████████  | 16/20 [00:10<00:01,  3.78it/s] 85%|████████▌ | 17/20 [00:10<00:00,  3.89it/s] 90%|█████████ | 18/20 [00:11<00:00,  4.27it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.58it/s]100%|██████████| 20/20 [00:11<00:00,  4.90it/s]100%|██████████| 20/20 [00:11<00:00,  1.72it/s]=> result
* total: 1,990
* correct: 1,574
* accuracy: 79.1%
* error: 20.9%
* macro_f1: 78.4%

epoch [20/30] batch [20/796] time 0.377 (0.421) data 0.000 (0.045) loss 1.0742 (1.0434) lr 2.9663e-03 eta 1:01:16
epoch [20/30] batch [40/796] time 0.351 (0.397) data 0.000 (0.023) loss 0.7900 (1.0416) lr 2.9663e-03 eta 0:57:36
epoch [20/30] batch [60/796] time 0.380 (0.388) data 0.000 (0.015) loss 0.1437 (0.9580) lr 2.9663e-03 eta 0:56:12
epoch [20/30] batch [80/796] time 0.383 (0.383) data 0.000 (0.011) loss 1.0049 (0.8938) lr 2.9663e-03 eta 0:55:21
epoch [20/30] batch [100/796] time 0.382 (0.379) data 0.000 (0.009) loss 0.6870 (0.8569) lr 2.9663e-03 eta 0:54:43
epoch [20/30] batch [120/796] time 0.332 (0.377) data 0.000 (0.008) loss 0.0893 (0.8299) lr 2.9663e-03 eta 0:54:16
epoch [20/30] batch [140/796] time 0.392 (0.377) data 0.000 (0.007) loss 0.6895 (0.8363) lr 2.9663e-03 eta 0:54:10
epoch [20/30] batch [160/796] time 0.335 (0.376) data 0.000 (0.006) loss 0.6025 (0.8250) lr 2.9663e-03 eta 0:53:48
epoch [20/30] batch [180/796] time 0.379 (0.375) data 0.000 (0.005) loss 1.8945 (0.8259) lr 2.9663e-03 eta 0:53:33
epoch [20/30] batch [200/796] time 0.349 (0.374) data 0.000 (0.005) loss 0.5562 (0.8079) lr 2.9663e-03 eta 0:53:15
epoch [20/30] batch [220/796] time 0.334 (0.372) data 0.000 (0.004) loss 0.7490 (0.8089) lr 2.9663e-03 eta 0:52:58
epoch [20/30] batch [240/796] time 0.362 (0.372) data 0.000 (0.004) loss 1.1006 (0.8206) lr 2.9663e-03 eta 0:52:48
epoch [20/30] batch [260/796] time 0.393 (0.373) data 0.000 (0.004) loss 0.5879 (0.8271) lr 2.9663e-03 eta 0:52:48
epoch [20/30] batch [280/796] time 0.381 (0.373) data 0.000 (0.003) loss 2.2012 (0.8247) lr 2.9663e-03 eta 0:52:42
epoch [20/30] batch [300/796] time 0.368 (0.373) data 0.000 (0.003) loss 0.6211 (0.8320) lr 2.9663e-03 eta 0:52:36
epoch [20/30] batch [320/796] time 0.376 (0.372) data 0.000 (0.003) loss 0.5024 (0.8295) lr 2.9663e-03 eta 0:52:21
epoch [20/30] batch [340/796] time 0.389 (0.372) data 0.000 (0.003) loss 0.6992 (0.8209) lr 2.9663e-03 eta 0:52:13
epoch [20/30] batch [360/796] time 0.410 (0.372) data 0.000 (0.003) loss 1.4727 (0.8219) lr 2.9663e-03 eta 0:52:04
epoch [20/30] batch [380/796] time 0.360 (0.372) data 0.000 (0.003) loss 0.2939 (0.8172) lr 2.9663e-03 eta 0:51:55
epoch [20/30] batch [400/796] time 0.358 (0.372) data 0.000 (0.002) loss 0.2720 (0.8186) lr 2.9663e-03 eta 0:51:48
epoch [20/30] batch [420/796] time 0.345 (0.372) data 0.000 (0.002) loss 0.3635 (0.8173) lr 2.9663e-03 eta 0:51:37
epoch [20/30] batch [440/796] time 0.333 (0.371) data 0.000 (0.002) loss 0.5469 (0.8168) lr 2.9663e-03 eta 0:51:26
epoch [20/30] batch [460/796] time 0.380 (0.371) data 0.000 (0.002) loss 0.4592 (0.8135) lr 2.9663e-03 eta 0:51:17
epoch [20/30] batch [480/796] time 0.391 (0.371) data 0.000 (0.002) loss 0.0487 (0.8177) lr 2.9663e-03 eta 0:51:10
epoch [20/30] batch [500/796] time 0.349 (0.371) data 0.000 (0.002) loss 0.4705 (0.8106) lr 2.9663e-03 eta 0:51:00
epoch [20/30] batch [520/796] time 0.346 (0.371) data 0.000 (0.002) loss 0.9873 (0.8087) lr 2.9663e-03 eta 0:50:54
epoch [20/30] batch [540/796] time 0.380 (0.371) data 0.000 (0.002) loss 0.9370 (0.8080) lr 2.9663e-03 eta 0:50:48
epoch [20/30] batch [560/796] time 0.378 (0.371) data 0.000 (0.002) loss 0.3584 (0.8062) lr 2.9663e-03 eta 0:50:41
epoch [20/30] batch [580/796] time 0.363 (0.371) data 0.000 (0.002) loss 1.9756 (0.8022) lr 2.9663e-03 eta 0:50:33
epoch [20/30] batch [600/796] time 0.359 (0.371) data 0.000 (0.002) loss 0.2559 (0.8041) lr 2.9663e-03 eta 0:50:26
epoch [20/30] batch [620/796] time 0.402 (0.371) data 0.000 (0.002) loss 0.2725 (0.7996) lr 2.9663e-03 eta 0:50:19
epoch [20/30] batch [640/796] time 0.343 (0.371) data 0.000 (0.002) loss 0.4189 (0.7972) lr 2.9663e-03 eta 0:50:11
epoch [20/30] batch [660/796] time 0.342 (0.371) data 0.000 (0.002) loss 0.0777 (0.7916) lr 2.9663e-03 eta 0:50:00
epoch [20/30] batch [680/796] time 0.343 (0.371) data 0.000 (0.002) loss 0.1429 (0.7930) lr 2.9663e-03 eta 0:49:53
epoch [20/30] batch [700/796] time 0.407 (0.371) data 0.000 (0.002) loss 0.0727 (0.7929) lr 2.9663e-03 eta 0:49:44
epoch [20/30] batch [720/796] time 0.357 (0.371) data 0.000 (0.002) loss 1.1387 (0.7927) lr 2.9663e-03 eta 0:49:39
epoch [20/30] batch [740/796] time 0.374 (0.371) data 0.000 (0.001) loss 0.6504 (0.7920) lr 2.9663e-03 eta 0:49:33
epoch [20/30] batch [760/796] time 0.406 (0.371) data 0.000 (0.001) loss 0.9175 (0.7922) lr 2.9663e-03 eta 0:49:26
epoch [20/30] batch [780/796] time 0.329 (0.370) data 0.000 (0.001) loss 0.1760 (0.7967) lr 2.9663e-03 eta 0:49:12
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<01:57,  6.16s/it] 10%|█         | 2/20 [00:06<00:51,  2.86s/it] 15%|█▌        | 3/20 [00:07<00:28,  1.69s/it] 20%|██        | 4/20 [00:07<00:18,  1.13s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.22it/s] 30%|███       | 6/20 [00:07<00:08,  1.57it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.91it/s] 40%|████      | 8/20 [00:08<00:05,  2.24it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.54it/s] 50%|█████     | 10/20 [00:08<00:03,  2.81it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.01it/s] 60%|██████    | 12/20 [00:09<00:02,  3.19it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.39it/s] 70%|███████   | 14/20 [00:10<00:01,  3.57it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.71it/s] 80%|████████  | 16/20 [00:10<00:01,  3.86it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.12it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.64it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.06it/s]100%|██████████| 20/20 [00:11<00:00,  4.48it/s]100%|██████████| 20/20 [00:11<00:00,  1.73it/s]=> result
* total: 1,990
* correct: 1,578
* accuracy: 79.3%
* error: 20.7%
* macro_f1: 78.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model.pth.tar-20

epoch [21/30] batch [20/796] time 0.344 (0.419) data 0.000 (0.042) loss 1.5762 (0.6987) lr 2.5000e-03 eta 0:55:27
epoch [21/30] batch [40/796] time 0.387 (0.393) data 0.000 (0.021) loss 0.7534 (0.7175) lr 2.5000e-03 eta 0:51:51
epoch [21/30] batch [60/796] time 0.366 (0.386) data 0.000 (0.014) loss 0.4507 (0.8313) lr 2.5000e-03 eta 0:50:51
epoch [21/30] batch [80/796] time 0.359 (0.379) data 0.000 (0.011) loss 1.8145 (0.7940) lr 2.5000e-03 eta 0:49:43
epoch [21/30] batch [100/796] time 0.378 (0.376) data 0.000 (0.009) loss 1.3438 (0.7969) lr 2.5000e-03 eta 0:49:15
epoch [21/30] batch [120/796] time 0.342 (0.373) data 0.000 (0.007) loss 0.3088 (0.7672) lr 2.5000e-03 eta 0:48:46
epoch [21/30] batch [140/796] time 0.383 (0.372) data 0.000 (0.006) loss 0.9126 (0.7768) lr 2.5000e-03 eta 0:48:31
epoch [21/30] batch [160/796] time 0.386 (0.372) data 0.000 (0.005) loss 0.4424 (0.7814) lr 2.5000e-03 eta 0:48:23
epoch [21/30] batch [180/796] time 0.360 (0.373) data 0.000 (0.005) loss 1.1836 (0.7772) lr 2.5000e-03 eta 0:48:18
epoch [21/30] batch [200/796] time 0.390 (0.372) data 0.000 (0.004) loss 0.2886 (0.7674) lr 2.5000e-03 eta 0:48:07
epoch [21/30] batch [220/796] time 0.364 (0.372) data 0.000 (0.004) loss 0.8955 (0.7631) lr 2.5000e-03 eta 0:47:59
epoch [21/30] batch [240/796] time 0.348 (0.372) data 0.000 (0.004) loss 0.3350 (0.7528) lr 2.5000e-03 eta 0:47:49
epoch [21/30] batch [260/796] time 0.347 (0.371) data 0.000 (0.003) loss 2.9453 (0.7715) lr 2.5000e-03 eta 0:47:34
epoch [21/30] batch [280/796] time 0.352 (0.370) data 0.000 (0.003) loss 0.7344 (0.7843) lr 2.5000e-03 eta 0:47:24
epoch [21/30] batch [300/796] time 0.374 (0.370) data 0.000 (0.003) loss 0.0743 (0.7835) lr 2.5000e-03 eta 0:47:17
epoch [21/30] batch [320/796] time 0.350 (0.369) data 0.000 (0.003) loss 0.9731 (0.7862) lr 2.5000e-03 eta 0:47:02
epoch [21/30] batch [340/796] time 0.360 (0.369) data 0.000 (0.003) loss 0.6953 (0.7864) lr 2.5000e-03 eta 0:46:51
epoch [21/30] batch [360/796] time 0.347 (0.369) data 0.000 (0.003) loss 0.2927 (0.7765) lr 2.5000e-03 eta 0:46:42
epoch [21/30] batch [380/796] time 0.346 (0.369) data 0.000 (0.002) loss 0.5513 (0.7738) lr 2.5000e-03 eta 0:46:34
epoch [21/30] batch [400/796] time 0.373 (0.368) data 0.000 (0.002) loss 0.6304 (0.7696) lr 2.5000e-03 eta 0:46:25
epoch [21/30] batch [420/796] time 0.399 (0.369) data 0.000 (0.002) loss 0.0993 (0.7685) lr 2.5000e-03 eta 0:46:19
epoch [21/30] batch [440/796] time 0.406 (0.369) data 0.000 (0.002) loss 0.6528 (0.7690) lr 2.5000e-03 eta 0:46:12
epoch [21/30] batch [460/796] time 0.380 (0.369) data 0.000 (0.002) loss 1.0752 (0.7719) lr 2.5000e-03 eta 0:46:05
epoch [21/30] batch [480/796] time 0.402 (0.369) data 0.000 (0.002) loss 0.2236 (0.7703) lr 2.5000e-03 eta 0:45:59
epoch [21/30] batch [500/796] time 0.364 (0.369) data 0.000 (0.002) loss 0.0478 (0.7773) lr 2.5000e-03 eta 0:45:50
epoch [21/30] batch [520/796] time 0.384 (0.369) data 0.000 (0.002) loss 0.3733 (0.7800) lr 2.5000e-03 eta 0:45:45
epoch [21/30] batch [540/796] time 0.399 (0.369) data 0.000 (0.002) loss 0.9673 (0.7786) lr 2.5000e-03 eta 0:45:36
epoch [21/30] batch [560/796] time 0.371 (0.369) data 0.000 (0.002) loss 0.2986 (0.7797) lr 2.5000e-03 eta 0:45:31
epoch [21/30] batch [580/796] time 0.366 (0.369) data 0.000 (0.002) loss 0.5967 (0.7786) lr 2.5000e-03 eta 0:45:23
epoch [21/30] batch [600/796] time 0.405 (0.369) data 0.000 (0.002) loss 1.0771 (0.7805) lr 2.5000e-03 eta 0:45:16
epoch [21/30] batch [620/796] time 0.356 (0.369) data 0.000 (0.002) loss 0.5601 (0.7826) lr 2.5000e-03 eta 0:45:09
epoch [21/30] batch [640/796] time 0.368 (0.369) data 0.000 (0.002) loss 0.1764 (0.7790) lr 2.5000e-03 eta 0:45:03
epoch [21/30] batch [660/796] time 0.363 (0.369) data 0.000 (0.002) loss 1.6357 (0.7794) lr 2.5000e-03 eta 0:44:55
epoch [21/30] batch [680/796] time 0.403 (0.369) data 0.000 (0.002) loss 1.8086 (0.7888) lr 2.5000e-03 eta 0:44:46
epoch [21/30] batch [700/796] time 0.379 (0.369) data 0.000 (0.001) loss 1.3936 (0.7955) lr 2.5000e-03 eta 0:44:38
epoch [21/30] batch [720/796] time 0.366 (0.369) data 0.000 (0.001) loss 0.1619 (0.7933) lr 2.5000e-03 eta 0:44:30
epoch [21/30] batch [740/796] time 0.345 (0.369) data 0.000 (0.001) loss 0.2700 (0.7895) lr 2.5000e-03 eta 0:44:23
epoch [21/30] batch [760/796] time 0.352 (0.369) data 0.000 (0.001) loss 0.4626 (0.7856) lr 2.5000e-03 eta 0:44:14
epoch [21/30] batch [780/796] time 0.323 (0.368) data 0.000 (0.001) loss 1.5410 (0.7848) lr 2.5000e-03 eta 0:44:01
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<02:04,  6.57s/it] 10%|█         | 2/20 [00:07<00:59,  3.33s/it] 15%|█▌        | 3/20 [00:07<00:32,  1.93s/it] 20%|██        | 4/20 [00:08<00:20,  1.28s/it] 25%|██▌       | 5/20 [00:08<00:13,  1.09it/s] 30%|███       | 6/20 [00:08<00:09,  1.43it/s] 35%|███▌      | 7/20 [00:09<00:07,  1.78it/s] 40%|████      | 8/20 [00:09<00:05,  2.11it/s] 45%|████▌     | 9/20 [00:09<00:04,  2.42it/s] 50%|█████     | 10/20 [00:09<00:03,  2.70it/s] 55%|█████▌    | 11/20 [00:10<00:03,  2.89it/s] 60%|██████    | 12/20 [00:10<00:02,  3.06it/s] 65%|██████▌   | 13/20 [00:10<00:02,  3.35it/s] 70%|███████   | 14/20 [00:10<00:01,  3.67it/s] 75%|███████▌  | 15/20 [00:11<00:01,  3.80it/s] 80%|████████  | 16/20 [00:11<00:00,  4.00it/s] 85%|████████▌ | 17/20 [00:11<00:00,  4.23it/s] 90%|█████████ | 18/20 [00:12<00:00,  2.65it/s] 95%|█████████▌| 19/20 [00:12<00:00,  3.14it/s]100%|██████████| 20/20 [00:12<00:00,  3.65it/s]100%|██████████| 20/20 [00:12<00:00,  1.57it/s]=> result
* total: 1,990
* correct: 1,569
* accuracy: 78.8%
* error: 21.2%
* macro_f1: 78.2%

epoch [22/30] batch [20/796] time 0.399 (0.427) data 0.000 (0.050) loss 0.9785 (0.8983) lr 2.0611e-03 eta 0:50:53
epoch [22/30] batch [40/796] time 0.362 (0.402) data 0.000 (0.025) loss 0.0502 (0.7570) lr 2.0611e-03 eta 0:47:46
epoch [22/30] batch [60/796] time 0.360 (0.391) data 0.000 (0.017) loss 0.3518 (0.7595) lr 2.0611e-03 eta 0:46:16
epoch [22/30] batch [80/796] time 0.363 (0.384) data 0.000 (0.013) loss 0.3301 (0.7745) lr 2.0611e-03 eta 0:45:18
epoch [22/30] batch [100/796] time 0.355 (0.381) data 0.000 (0.010) loss 4.2148 (0.7855) lr 2.0611e-03 eta 0:44:53
epoch [22/30] batch [120/796] time 0.371 (0.379) data 0.000 (0.009) loss 0.4863 (0.7897) lr 2.0611e-03 eta 0:44:32
epoch [22/30] batch [140/796] time 0.347 (0.378) data 0.000 (0.007) loss 1.2637 (0.7566) lr 2.0611e-03 eta 0:44:17
epoch [22/30] batch [160/796] time 0.369 (0.378) data 0.000 (0.006) loss 1.9502 (0.7658) lr 2.0611e-03 eta 0:44:07
epoch [22/30] batch [180/796] time 0.414 (0.377) data 0.000 (0.006) loss 0.2681 (0.7840) lr 2.0611e-03 eta 0:43:53
epoch [22/30] batch [200/796] time 0.389 (0.378) data 0.000 (0.005) loss 3.0723 (0.7826) lr 2.0611e-03 eta 0:43:51
epoch [22/30] batch [220/796] time 0.382 (0.378) data 0.000 (0.005) loss 0.7222 (0.7983) lr 2.0611e-03 eta 0:43:41
epoch [22/30] batch [240/796] time 0.387 (0.377) data 0.000 (0.004) loss 1.7500 (0.8231) lr 2.0611e-03 eta 0:43:30
epoch [22/30] batch [260/796] time 0.419 (0.377) data 0.000 (0.004) loss 1.7363 (0.8176) lr 2.0611e-03 eta 0:43:20
epoch [22/30] batch [280/796] time 0.383 (0.377) data 0.000 (0.004) loss 0.2142 (0.8065) lr 2.0611e-03 eta 0:43:13
epoch [22/30] batch [300/796] time 0.383 (0.377) data 0.000 (0.004) loss 0.3787 (0.8061) lr 2.0611e-03 eta 0:43:05
epoch [22/30] batch [320/796] time 0.351 (0.377) data 0.000 (0.003) loss 2.4395 (0.8137) lr 2.0611e-03 eta 0:42:57
epoch [22/30] batch [340/796] time 0.384 (0.376) data 0.000 (0.003) loss 1.4473 (0.8002) lr 2.0611e-03 eta 0:42:46
epoch [22/30] batch [360/796] time 0.353 (0.375) data 0.000 (0.003) loss 0.3130 (0.7907) lr 2.0611e-03 eta 0:42:34
epoch [22/30] batch [380/796] time 0.361 (0.375) data 0.000 (0.003) loss 0.7778 (0.7813) lr 2.0611e-03 eta 0:42:23
epoch [22/30] batch [400/796] time 0.340 (0.374) data 0.000 (0.003) loss 0.6250 (0.7823) lr 2.0611e-03 eta 0:42:10
epoch [22/30] batch [420/796] time 0.419 (0.374) data 0.000 (0.003) loss 0.7676 (0.7850) lr 2.0611e-03 eta 0:42:04
epoch [22/30] batch [440/796] time 0.396 (0.374) data 0.000 (0.003) loss 1.2646 (0.7793) lr 2.0611e-03 eta 0:41:55
epoch [22/30] batch [460/796] time 0.361 (0.374) data 0.000 (0.002) loss 2.1660 (0.7782) lr 2.0611e-03 eta 0:41:44
epoch [22/30] batch [480/796] time 0.385 (0.374) data 0.000 (0.002) loss 0.7114 (0.7809) lr 2.0611e-03 eta 0:41:39
epoch [22/30] batch [500/796] time 0.382 (0.374) data 0.000 (0.002) loss 0.9707 (0.7865) lr 2.0611e-03 eta 0:41:30
epoch [22/30] batch [520/796] time 0.399 (0.373) data 0.000 (0.002) loss 0.7046 (0.7755) lr 2.0611e-03 eta 0:41:20
epoch [22/30] batch [540/796] time 0.395 (0.373) data 0.000 (0.002) loss 0.3357 (0.7731) lr 2.0611e-03 eta 0:41:10
epoch [22/30] batch [560/796] time 0.360 (0.373) data 0.000 (0.002) loss 1.5762 (0.7700) lr 2.0611e-03 eta 0:41:04
epoch [22/30] batch [580/796] time 0.382 (0.373) data 0.000 (0.002) loss 1.0244 (0.7669) lr 2.0611e-03 eta 0:40:57
epoch [22/30] batch [600/796] time 0.394 (0.373) data 0.000 (0.002) loss 2.0215 (0.7677) lr 2.0611e-03 eta 0:40:50
epoch [22/30] batch [620/796] time 0.397 (0.373) data 0.000 (0.002) loss 0.8857 (0.7672) lr 2.0611e-03 eta 0:40:43
epoch [22/30] batch [640/796] time 0.380 (0.373) data 0.000 (0.002) loss 0.3352 (0.7679) lr 2.0611e-03 eta 0:40:36
epoch [22/30] batch [660/796] time 0.368 (0.374) data 0.000 (0.002) loss 1.4805 (0.7745) lr 2.0611e-03 eta 0:40:30
epoch [22/30] batch [680/796] time 0.410 (0.373) data 0.000 (0.002) loss 0.3538 (0.7724) lr 2.0611e-03 eta 0:40:21
epoch [22/30] batch [700/796] time 0.373 (0.373) data 0.000 (0.002) loss 0.3245 (0.7712) lr 2.0611e-03 eta 0:40:12
epoch [22/30] batch [720/796] time 0.399 (0.373) data 0.000 (0.002) loss 0.3730 (0.7705) lr 2.0611e-03 eta 0:40:04
epoch [22/30] batch [740/796] time 0.344 (0.373) data 0.000 (0.002) loss 0.3674 (0.7738) lr 2.0611e-03 eta 0:39:58
epoch [22/30] batch [760/796] time 0.337 (0.373) data 0.000 (0.002) loss 1.0508 (0.7734) lr 2.0611e-03 eta 0:39:51
epoch [22/30] batch [780/796] time 0.326 (0.372) data 0.000 (0.002) loss 1.3613 (0.7764) lr 2.0611e-03 eta 0:39:37
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<01:55,  6.10s/it] 10%|█         | 2/20 [00:07<00:54,  3.05s/it] 15%|█▌        | 3/20 [00:07<00:30,  1.79s/it] 20%|██        | 4/20 [00:07<00:19,  1.19s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.16it/s] 30%|███       | 6/20 [00:08<00:09,  1.51it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.85it/s] 40%|████      | 8/20 [00:08<00:05,  2.21it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.52it/s] 50%|█████     | 10/20 [00:09<00:03,  2.78it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.98it/s] 60%|██████    | 12/20 [00:09<00:02,  3.15it/s] 65%|██████▌   | 13/20 [00:10<00:02,  3.26it/s] 70%|███████   | 14/20 [00:10<00:01,  3.45it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.60it/s] 80%|████████  | 16/20 [00:10<00:01,  3.72it/s] 85%|████████▌ | 17/20 [00:11<00:00,  3.86it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.39it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.84it/s]100%|██████████| 20/20 [00:11<00:00,  4.28it/s]100%|██████████| 20/20 [00:11<00:00,  1.68it/s]=> result
* total: 1,990
* correct: 1,578
* accuracy: 79.3%
* error: 20.7%
* macro_f1: 78.5%

epoch [23/30] batch [20/796] time 0.357 (0.431) data 0.000 (0.042) loss 1.6865 (0.9019) lr 1.6543e-03 eta 0:45:34
epoch [23/30] batch [40/796] time 0.372 (0.405) data 0.000 (0.021) loss 0.6484 (0.7995) lr 1.6543e-03 eta 0:42:39
epoch [23/30] batch [60/796] time 0.367 (0.394) data 0.000 (0.014) loss 0.1608 (0.7550) lr 1.6543e-03 eta 0:41:23
epoch [23/30] batch [80/796] time 0.372 (0.386) data 0.000 (0.011) loss 0.8086 (0.7450) lr 1.6543e-03 eta 0:40:26
epoch [23/30] batch [100/796] time 0.345 (0.384) data 0.000 (0.009) loss 1.5186 (0.7490) lr 1.6543e-03 eta 0:40:07
epoch [23/30] batch [120/796] time 0.385 (0.383) data 0.000 (0.007) loss 0.3589 (0.7464) lr 1.6543e-03 eta 0:39:50
epoch [23/30] batch [140/796] time 0.368 (0.381) data 0.000 (0.006) loss 0.4861 (0.7383) lr 1.6543e-03 eta 0:39:31
epoch [23/30] batch [160/796] time 0.381 (0.381) data 0.000 (0.005) loss 0.2839 (0.7438) lr 1.6543e-03 eta 0:39:23
epoch [23/30] batch [180/796] time 0.357 (0.379) data 0.000 (0.005) loss 0.3521 (0.7428) lr 1.6543e-03 eta 0:39:04
epoch [23/30] batch [200/796] time 0.385 (0.378) data 0.000 (0.004) loss 0.2661 (0.7411) lr 1.6543e-03 eta 0:38:53
epoch [23/30] batch [220/796] time 0.349 (0.378) data 0.000 (0.004) loss 0.2615 (0.7430) lr 1.6543e-03 eta 0:38:40
epoch [23/30] batch [240/796] time 0.360 (0.377) data 0.000 (0.004) loss 0.8354 (0.7452) lr 1.6543e-03 eta 0:38:32
epoch [23/30] batch [260/796] time 0.371 (0.377) data 0.000 (0.003) loss 1.4863 (0.7364) lr 1.6543e-03 eta 0:38:24
epoch [23/30] batch [280/796] time 0.337 (0.377) data 0.000 (0.003) loss 1.1855 (0.7266) lr 1.6543e-03 eta 0:38:12
epoch [23/30] batch [300/796] time 0.349 (0.376) data 0.000 (0.003) loss 0.5146 (0.7097) lr 1.6543e-03 eta 0:38:01
epoch [23/30] batch [320/796] time 0.375 (0.376) data 0.000 (0.003) loss 0.1162 (0.7220) lr 1.6543e-03 eta 0:37:51
epoch [23/30] batch [340/796] time 0.381 (0.376) data 0.000 (0.003) loss 0.1799 (0.7152) lr 1.6543e-03 eta 0:37:43
epoch [23/30] batch [360/796] time 0.359 (0.375) data 0.000 (0.003) loss 0.4102 (0.7286) lr 1.6543e-03 eta 0:37:35
epoch [23/30] batch [380/796] time 0.347 (0.375) data 0.000 (0.002) loss 0.8662 (0.7494) lr 1.6543e-03 eta 0:37:26
epoch [23/30] batch [400/796] time 0.382 (0.375) data 0.000 (0.002) loss 0.8408 (0.7450) lr 1.6543e-03 eta 0:37:20
epoch [23/30] batch [420/796] time 0.366 (0.375) data 0.000 (0.002) loss 0.6279 (0.7485) lr 1.6543e-03 eta 0:37:11
epoch [23/30] batch [440/796] time 0.386 (0.375) data 0.000 (0.002) loss 0.6343 (0.7555) lr 1.6543e-03 eta 0:37:03
epoch [23/30] batch [460/796] time 0.373 (0.375) data 0.000 (0.002) loss 0.2715 (0.7508) lr 1.6543e-03 eta 0:36:54
epoch [23/30] batch [480/796] time 0.406 (0.375) data 0.000 (0.002) loss 2.3730 (0.7496) lr 1.6543e-03 eta 0:36:48
epoch [23/30] batch [500/796] time 0.365 (0.375) data 0.000 (0.002) loss 0.4683 (0.7520) lr 1.6543e-03 eta 0:36:41
epoch [23/30] batch [520/796] time 0.389 (0.375) data 0.000 (0.002) loss 0.3396 (0.7478) lr 1.6543e-03 eta 0:36:31
epoch [23/30] batch [540/796] time 0.381 (0.375) data 0.000 (0.002) loss 0.5581 (0.7497) lr 1.6543e-03 eta 0:36:22
epoch [23/30] batch [560/796] time 0.349 (0.374) data 0.000 (0.002) loss 0.4739 (0.7497) lr 1.6543e-03 eta 0:36:13
epoch [23/30] batch [580/796] time 0.342 (0.374) data 0.000 (0.002) loss 0.8843 (0.7473) lr 1.6543e-03 eta 0:36:05
epoch [23/30] batch [600/796] time 0.375 (0.374) data 0.000 (0.002) loss 0.5933 (0.7564) lr 1.6543e-03 eta 0:35:57
epoch [23/30] batch [620/796] time 0.384 (0.374) data 0.000 (0.002) loss 1.6133 (0.7626) lr 1.6543e-03 eta 0:35:49
epoch [23/30] batch [640/796] time 0.392 (0.374) data 0.000 (0.002) loss 0.5293 (0.7664) lr 1.6543e-03 eta 0:35:43
epoch [23/30] batch [660/796] time 0.377 (0.374) data 0.000 (0.002) loss 0.7070 (0.7645) lr 1.6543e-03 eta 0:35:35
epoch [23/30] batch [680/796] time 0.343 (0.374) data 0.000 (0.002) loss 0.4111 (0.7724) lr 1.6543e-03 eta 0:35:27
epoch [23/30] batch [700/796] time 0.353 (0.374) data 0.000 (0.001) loss 0.6538 (0.7721) lr 1.6543e-03 eta 0:35:21
epoch [23/30] batch [720/796] time 0.390 (0.374) data 0.000 (0.001) loss 0.3494 (0.7711) lr 1.6543e-03 eta 0:35:13
epoch [23/30] batch [740/796] time 0.347 (0.374) data 0.000 (0.001) loss 0.4712 (0.7671) lr 1.6543e-03 eta 0:35:05
epoch [23/30] batch [760/796] time 0.390 (0.374) data 0.000 (0.001) loss 1.0195 (0.7703) lr 1.6543e-03 eta 0:34:57
epoch [23/30] batch [780/796] time 0.342 (0.373) data 0.000 (0.001) loss 0.1660 (0.7661) lr 1.6543e-03 eta 0:34:46
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:53,  5.96s/it] 10%|█         | 2/20 [00:07<00:57,  3.19s/it] 15%|█▌        | 3/20 [00:07<00:31,  1.86s/it] 20%|██        | 4/20 [00:07<00:19,  1.24s/it] 25%|██▌       | 5/20 [00:08<00:13,  1.12it/s] 30%|███       | 6/20 [00:08<00:09,  1.47it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.81it/s] 40%|████      | 8/20 [00:08<00:05,  2.14it/s] 45%|████▌     | 9/20 [00:09<00:04,  2.45it/s] 50%|█████     | 10/20 [00:09<00:03,  2.73it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.09it/s] 60%|██████    | 12/20 [00:09<00:02,  3.35it/s] 65%|██████▌   | 13/20 [00:10<00:01,  3.60it/s] 70%|███████   | 14/20 [00:10<00:01,  3.97it/s] 75%|███████▌  | 15/20 [00:10<00:01,  4.25it/s] 80%|████████  | 16/20 [00:10<00:00,  4.32it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.53it/s] 90%|█████████ | 18/20 [00:11<00:00,  2.50it/s] 95%|█████████▌| 19/20 [00:11<00:00,  2.99it/s]100%|██████████| 20/20 [00:12<00:00,  3.51it/s]100%|██████████| 20/20 [00:12<00:00,  1.63it/s]=> result
* total: 1,990
* correct: 1,582
* accuracy: 79.5%
* error: 20.5%
* macro_f1: 78.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar

epoch [24/30] batch [20/796] time 0.358 (0.416) data 0.000 (0.039) loss 0.2452 (0.6102) lr 1.2843e-03 eta 0:38:27
epoch [24/30] batch [40/796] time 0.356 (0.391) data 0.000 (0.020) loss 0.4226 (0.7915) lr 1.2843e-03 eta 0:36:00
epoch [24/30] batch [60/796] time 0.402 (0.383) data 0.000 (0.013) loss 0.9731 (0.7935) lr 1.2843e-03 eta 0:35:12
epoch [24/30] batch [80/796] time 0.421 (0.382) data 0.000 (0.010) loss 0.0816 (0.8199) lr 1.2843e-03 eta 0:34:58
epoch [24/30] batch [100/796] time 0.380 (0.389) data 0.000 (0.008) loss 0.2795 (0.7951) lr 1.2843e-03 eta 0:35:26
epoch [24/30] batch [120/796] time 0.396 (0.387) data 0.000 (0.007) loss 0.8589 (0.8110) lr 1.2843e-03 eta 0:35:11
epoch [24/30] batch [140/796] time 0.402 (0.388) data 0.000 (0.006) loss 0.9780 (0.7997) lr 1.2843e-03 eta 0:35:08
epoch [24/30] batch [160/796] time 0.388 (0.389) data 0.000 (0.005) loss 0.4673 (0.7716) lr 1.2843e-03 eta 0:35:04
epoch [24/30] batch [180/796] time 0.381 (0.390) data 0.000 (0.005) loss 0.6567 (0.7620) lr 1.2843e-03 eta 0:35:01
epoch [24/30] batch [200/796] time 0.370 (0.390) data 0.000 (0.004) loss 0.3003 (0.7600) lr 1.2843e-03 eta 0:34:52
epoch [24/30] batch [220/796] time 0.394 (0.389) data 0.000 (0.004) loss 0.0306 (0.7696) lr 1.2843e-03 eta 0:34:41
epoch [24/30] batch [240/796] time 0.365 (0.389) data 0.000 (0.004) loss 0.7983 (0.7788) lr 1.2843e-03 eta 0:34:36
epoch [24/30] batch [260/796] time 0.377 (0.389) data 0.000 (0.003) loss 3.1875 (0.7937) lr 1.2843e-03 eta 0:34:24
epoch [24/30] batch [280/796] time 0.367 (0.387) data 0.000 (0.003) loss 0.5527 (0.7907) lr 1.2843e-03 eta 0:34:09
epoch [24/30] batch [300/796] time 0.381 (0.387) data 0.000 (0.003) loss 0.8960 (0.7900) lr 1.2843e-03 eta 0:33:57
epoch [24/30] batch [320/796] time 0.399 (0.387) data 0.001 (0.003) loss 0.4727 (0.7825) lr 1.2843e-03 eta 0:33:50
epoch [24/30] batch [340/796] time 0.395 (0.387) data 0.000 (0.003) loss 0.7910 (0.7707) lr 1.2843e-03 eta 0:33:45
epoch [24/30] batch [360/796] time 0.363 (0.387) data 0.000 (0.002) loss 0.4521 (0.7737) lr 1.2843e-03 eta 0:33:38
epoch [24/30] batch [380/796] time 0.383 (0.388) data 0.000 (0.002) loss 0.6147 (0.7645) lr 1.2843e-03 eta 0:33:34
epoch [24/30] batch [400/796] time 0.371 (0.387) data 0.000 (0.002) loss 0.1467 (0.7679) lr 1.2843e-03 eta 0:33:22
epoch [24/30] batch [420/796] time 0.378 (0.387) data 0.000 (0.002) loss 1.4492 (0.7582) lr 1.2843e-03 eta 0:33:12
epoch [24/30] batch [440/796] time 0.380 (0.387) data 0.000 (0.002) loss 0.4202 (0.7525) lr 1.2843e-03 eta 0:33:04
epoch [24/30] batch [460/796] time 0.402 (0.387) data 0.000 (0.002) loss 0.0970 (0.7568) lr 1.2843e-03 eta 0:32:58
epoch [24/30] batch [480/796] time 0.395 (0.386) data 0.000 (0.002) loss 0.1431 (0.7514) lr 1.2843e-03 eta 0:32:48
epoch [24/30] batch [500/796] time 0.353 (0.386) data 0.000 (0.002) loss 0.7617 (0.7529) lr 1.2843e-03 eta 0:32:37
epoch [24/30] batch [520/796] time 0.369 (0.386) data 0.001 (0.002) loss 0.9941 (0.7533) lr 1.2843e-03 eta 0:32:28
epoch [24/30] batch [540/796] time 0.353 (0.386) data 0.000 (0.002) loss 0.4670 (0.7563) lr 1.2843e-03 eta 0:32:20
epoch [24/30] batch [560/796] time 0.346 (0.385) data 0.000 (0.002) loss 1.0508 (0.7660) lr 1.2843e-03 eta 0:32:11
epoch [24/30] batch [580/796] time 0.335 (0.385) data 0.000 (0.002) loss 0.9995 (0.7631) lr 1.2843e-03 eta 0:32:01
epoch [24/30] batch [600/796] time 0.345 (0.385) data 0.000 (0.002) loss 0.6055 (0.7727) lr 1.2843e-03 eta 0:31:51
epoch [24/30] batch [620/796] time 0.364 (0.384) data 0.000 (0.002) loss 0.0285 (0.7743) lr 1.2843e-03 eta 0:31:40
epoch [24/30] batch [640/796] time 0.365 (0.383) data 0.000 (0.001) loss 0.9756 (0.7761) lr 1.2843e-03 eta 0:31:29
epoch [24/30] batch [660/796] time 0.364 (0.383) data 0.000 (0.001) loss 0.7153 (0.7726) lr 1.2843e-03 eta 0:31:21
epoch [24/30] batch [680/796] time 0.346 (0.383) data 0.000 (0.001) loss 1.5537 (0.7746) lr 1.2843e-03 eta 0:31:11
epoch [24/30] batch [700/796] time 0.380 (0.382) data 0.000 (0.001) loss 0.9341 (0.7746) lr 1.2843e-03 eta 0:31:02
epoch [24/30] batch [720/796] time 0.372 (0.382) data 0.000 (0.001) loss 0.3870 (0.7732) lr 1.2843e-03 eta 0:30:53
epoch [24/30] batch [740/796] time 0.332 (0.381) data 0.000 (0.001) loss 0.5488 (0.7743) lr 1.2843e-03 eta 0:30:42
epoch [24/30] batch [760/796] time 0.356 (0.381) data 0.000 (0.001) loss 0.7314 (0.7735) lr 1.2843e-03 eta 0:30:33
epoch [24/30] batch [780/796] time 0.323 (0.380) data 0.000 (0.001) loss 0.0482 (0.7728) lr 1.2843e-03 eta 0:30:19
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:08<02:38,  8.36s/it] 10%|█         | 2/20 [00:09<01:10,  3.89s/it] 15%|█▌        | 3/20 [00:09<00:38,  2.25s/it] 20%|██        | 4/20 [00:09<00:23,  1.47s/it] 25%|██▌       | 5/20 [00:09<00:15,  1.04s/it] 30%|███       | 6/20 [00:10<00:10,  1.28it/s] 35%|███▌      | 7/20 [00:10<00:08,  1.61it/s] 40%|████      | 8/20 [00:10<00:06,  1.96it/s] 45%|████▌     | 9/20 [00:11<00:04,  2.24it/s] 50%|█████     | 10/20 [00:11<00:03,  2.53it/s] 55%|█████▌    | 11/20 [00:11<00:03,  2.79it/s] 60%|██████    | 12/20 [00:11<00:02,  3.01it/s] 65%|██████▌   | 13/20 [00:12<00:02,  3.18it/s] 70%|███████   | 14/20 [00:12<00:01,  3.33it/s] 75%|███████▌  | 15/20 [00:12<00:01,  3.42it/s] 80%|████████  | 16/20 [00:12<00:01,  3.76it/s] 85%|████████▌ | 17/20 [00:13<00:00,  4.07it/s] 90%|█████████ | 18/20 [00:13<00:00,  2.98it/s] 95%|█████████▌| 19/20 [00:13<00:00,  3.46it/s]100%|██████████| 20/20 [00:14<00:00,  3.95it/s]100%|██████████| 20/20 [00:14<00:00,  1.41it/s]=> result
* total: 1,990
* correct: 1,586
* accuracy: 79.7%
* error: 20.3%
* macro_f1: 79.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar

epoch [25/30] batch [20/796] time 0.389 (0.427) data 0.000 (0.045) loss 1.3701 (0.9439) lr 9.5492e-04 eta 0:33:49
epoch [25/30] batch [40/796] time 0.369 (0.394) data 0.000 (0.023) loss 0.6475 (0.8852) lr 9.5492e-04 eta 0:31:08
epoch [25/30] batch [60/796] time 0.402 (0.387) data 0.000 (0.015) loss 2.1523 (0.8109) lr 9.5492e-04 eta 0:30:23
epoch [25/30] batch [80/796] time 0.379 (0.384) data 0.000 (0.012) loss 0.0738 (0.7522) lr 9.5492e-04 eta 0:30:04
epoch [25/30] batch [100/796] time 0.372 (0.382) data 0.000 (0.009) loss 0.0945 (0.7551) lr 9.5492e-04 eta 0:29:47
epoch [25/30] batch [120/796] time 0.357 (0.381) data 0.000 (0.008) loss 0.3420 (0.7990) lr 9.5492e-04 eta 0:29:33
epoch [25/30] batch [140/796] time 0.358 (0.379) data 0.000 (0.007) loss 0.7773 (0.8134) lr 9.5492e-04 eta 0:29:16
epoch [25/30] batch [160/796] time 0.378 (0.378) data 0.000 (0.006) loss 0.6260 (0.8247) lr 9.5492e-04 eta 0:29:06
epoch [25/30] batch [180/796] time 0.386 (0.377) data 0.000 (0.005) loss 1.1621 (0.8072) lr 9.5492e-04 eta 0:28:50
epoch [25/30] batch [200/796] time 0.336 (0.376) data 0.000 (0.005) loss 0.4302 (0.8044) lr 9.5492e-04 eta 0:28:40
epoch [25/30] batch [220/796] time 0.348 (0.375) data 0.000 (0.004) loss 2.2285 (0.7905) lr 9.5492e-04 eta 0:28:29
epoch [25/30] batch [240/796] time 0.357 (0.375) data 0.001 (0.004) loss 0.5488 (0.7759) lr 9.5492e-04 eta 0:28:22
epoch [25/30] batch [260/796] time 0.386 (0.375) data 0.000 (0.004) loss 1.1582 (0.7727) lr 9.5492e-04 eta 0:28:14
epoch [25/30] batch [280/796] time 0.376 (0.375) data 0.000 (0.004) loss 1.3643 (0.7905) lr 9.5492e-04 eta 0:28:04
epoch [25/30] batch [300/796] time 0.361 (0.375) data 0.000 (0.003) loss 0.4131 (0.7935) lr 9.5492e-04 eta 0:27:56
epoch [25/30] batch [320/796] time 0.377 (0.375) data 0.000 (0.003) loss 0.5063 (0.7937) lr 9.5492e-04 eta 0:27:48
epoch [25/30] batch [340/796] time 0.392 (0.374) data 0.000 (0.003) loss 0.3530 (0.7993) lr 9.5492e-04 eta 0:27:39
epoch [25/30] batch [360/796] time 0.394 (0.374) data 0.000 (0.003) loss 0.3330 (0.7827) lr 9.5492e-04 eta 0:27:32
epoch [25/30] batch [380/796] time 0.353 (0.374) data 0.001 (0.003) loss 0.6704 (0.7876) lr 9.5492e-04 eta 0:27:24
epoch [25/30] batch [400/796] time 0.361 (0.374) data 0.000 (0.003) loss 0.4053 (0.7853) lr 9.5492e-04 eta 0:27:15
epoch [25/30] batch [420/796] time 0.404 (0.374) data 0.000 (0.002) loss 0.3818 (0.7787) lr 9.5492e-04 eta 0:27:06
epoch [25/30] batch [440/796] time 0.378 (0.373) data 0.000 (0.002) loss 0.9756 (0.7833) lr 9.5492e-04 eta 0:26:58
epoch [25/30] batch [460/796] time 0.367 (0.373) data 0.000 (0.002) loss 0.3525 (0.7834) lr 9.5492e-04 eta 0:26:51
epoch [25/30] batch [480/796] time 0.370 (0.373) data 0.000 (0.002) loss 0.8457 (0.7788) lr 9.5492e-04 eta 0:26:42
epoch [25/30] batch [500/796] time 0.342 (0.373) data 0.000 (0.002) loss 0.7568 (0.7782) lr 9.5492e-04 eta 0:26:34
epoch [25/30] batch [520/796] time 0.388 (0.373) data 0.000 (0.002) loss 0.6133 (0.7783) lr 9.5492e-04 eta 0:26:28
epoch [25/30] batch [540/796] time 0.360 (0.373) data 0.000 (0.002) loss 0.5405 (0.7741) lr 9.5492e-04 eta 0:26:19
epoch [25/30] batch [560/796] time 0.384 (0.373) data 0.000 (0.002) loss 1.1885 (0.7692) lr 9.5492e-04 eta 0:26:12
epoch [25/30] batch [580/796] time 0.350 (0.373) data 0.000 (0.002) loss 0.4871 (0.7696) lr 9.5492e-04 eta 0:26:03
epoch [25/30] batch [600/796] time 0.383 (0.373) data 0.000 (0.002) loss 0.1709 (0.7640) lr 9.5492e-04 eta 0:25:56
epoch [25/30] batch [620/796] time 0.415 (0.373) data 0.000 (0.002) loss 0.4880 (0.7667) lr 9.5492e-04 eta 0:25:49
epoch [25/30] batch [640/796] time 0.392 (0.373) data 0.000 (0.002) loss 0.8291 (0.7678) lr 9.5492e-04 eta 0:25:43
epoch [25/30] batch [660/796] time 0.371 (0.373) data 0.000 (0.002) loss 0.1859 (0.7635) lr 9.5492e-04 eta 0:25:36
epoch [25/30] batch [680/796] time 0.394 (0.373) data 0.000 (0.002) loss 1.4717 (0.7678) lr 9.5492e-04 eta 0:25:28
epoch [25/30] batch [700/796] time 0.371 (0.373) data 0.000 (0.002) loss 1.5010 (0.7693) lr 9.5492e-04 eta 0:25:20
epoch [25/30] batch [720/796] time 0.375 (0.373) data 0.000 (0.002) loss 0.3682 (0.7707) lr 9.5492e-04 eta 0:25:12
epoch [25/30] batch [740/796] time 0.372 (0.373) data 0.000 (0.002) loss 1.1162 (0.7735) lr 9.5492e-04 eta 0:25:04
epoch [25/30] batch [760/796] time 0.372 (0.373) data 0.000 (0.001) loss 0.6914 (0.7759) lr 9.5492e-04 eta 0:24:56
epoch [25/30] batch [780/796] time 0.332 (0.372) data 0.000 (0.001) loss 0.9595 (0.7720) lr 9.5492e-04 eta 0:24:45
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:52,  5.92s/it] 10%|█         | 2/20 [00:07<00:59,  3.33s/it] 15%|█▌        | 3/20 [00:07<00:32,  1.93s/it] 20%|██        | 4/20 [00:07<00:20,  1.28s/it] 25%|██▌       | 5/20 [00:08<00:13,  1.08it/s] 30%|███       | 6/20 [00:08<00:09,  1.42it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.85it/s] 40%|████      | 8/20 [00:09<00:05,  2.09it/s] 45%|████▌     | 9/20 [00:09<00:04,  2.41it/s] 50%|█████     | 10/20 [00:09<00:03,  2.70it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.98it/s] 60%|██████    | 12/20 [00:10<00:02,  3.19it/s] 65%|██████▌   | 13/20 [00:10<00:02,  3.46it/s] 70%|███████   | 14/20 [00:10<00:01,  3.64it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.98it/s] 80%|████████  | 16/20 [00:11<00:00,  4.08it/s] 85%|████████▌ | 17/20 [00:11<00:00,  4.30it/s] 90%|█████████ | 18/20 [00:12<00:00,  2.31it/s] 95%|█████████▌| 19/20 [00:12<00:00,  2.80it/s]100%|██████████| 20/20 [00:12<00:00,  3.32it/s]100%|██████████| 20/20 [00:12<00:00,  1.57it/s]=> result
* total: 1,990
* correct: 1,583
* accuracy: 79.5%
* error: 20.5%
* macro_f1: 78.9%

epoch [26/30] batch [20/796] time 0.383 (0.420) data 0.000 (0.049) loss 0.3120 (0.6770) lr 6.6987e-04 eta 0:27:44
epoch [26/30] batch [40/796] time 0.359 (0.393) data 0.000 (0.025) loss 0.5532 (0.7483) lr 6.6987e-04 eta 0:25:49
epoch [26/30] batch [60/796] time 0.348 (0.385) data 0.000 (0.017) loss 0.1176 (0.8237) lr 6.6987e-04 eta 0:25:08
epoch [26/30] batch [80/796] time 0.395 (0.382) data 0.000 (0.013) loss 0.6992 (0.8670) lr 6.6987e-04 eta 0:24:51
epoch [26/30] batch [100/796] time 0.414 (0.381) data 0.000 (0.010) loss 0.7104 (0.8589) lr 6.6987e-04 eta 0:24:37
epoch [26/30] batch [120/796] time 0.399 (0.381) data 0.000 (0.008) loss 1.5166 (0.8371) lr 6.6987e-04 eta 0:24:32
epoch [26/30] batch [140/796] time 0.378 (0.378) data 0.000 (0.007) loss 0.7969 (0.8346) lr 6.6987e-04 eta 0:24:13
epoch [26/30] batch [160/796] time 0.387 (0.378) data 0.000 (0.006) loss 0.7798 (0.8411) lr 6.6987e-04 eta 0:24:02
epoch [26/30] batch [180/796] time 0.342 (0.377) data 0.000 (0.006) loss 1.1074 (0.8311) lr 6.6987e-04 eta 0:23:52
epoch [26/30] batch [200/796] time 0.363 (0.377) data 0.000 (0.005) loss 0.6338 (0.8148) lr 6.6987e-04 eta 0:23:43
epoch [26/30] batch [220/796] time 0.376 (0.376) data 0.000 (0.005) loss 0.3906 (0.8250) lr 6.6987e-04 eta 0:23:32
epoch [26/30] batch [240/796] time 0.389 (0.375) data 0.000 (0.004) loss 0.2576 (0.8171) lr 6.6987e-04 eta 0:23:24
epoch [26/30] batch [260/796] time 0.370 (0.375) data 0.000 (0.004) loss 1.3818 (0.8190) lr 6.6987e-04 eta 0:23:14
epoch [26/30] batch [280/796] time 0.349 (0.375) data 0.000 (0.004) loss 0.4839 (0.8298) lr 6.6987e-04 eta 0:23:05
epoch [26/30] batch [300/796] time 0.398 (0.375) data 0.000 (0.004) loss 1.0049 (0.8266) lr 6.6987e-04 eta 0:22:59
epoch [26/30] batch [320/796] time 0.377 (0.375) data 0.000 (0.003) loss 0.5781 (0.8179) lr 6.6987e-04 eta 0:22:52
epoch [26/30] batch [340/796] time 0.378 (0.375) data 0.000 (0.003) loss 1.0518 (0.8228) lr 6.6987e-04 eta 0:22:43
epoch [26/30] batch [360/796] time 0.378 (0.375) data 0.000 (0.003) loss 0.9893 (0.8202) lr 6.6987e-04 eta 0:22:35
epoch [26/30] batch [380/796] time 0.373 (0.374) data 0.000 (0.003) loss 0.8052 (0.8158) lr 6.6987e-04 eta 0:22:28
epoch [26/30] batch [400/796] time 0.399 (0.374) data 0.000 (0.003) loss 1.4844 (0.8107) lr 6.6987e-04 eta 0:22:20
epoch [26/30] batch [420/796] time 0.348 (0.374) data 0.000 (0.003) loss 0.6484 (0.8058) lr 6.6987e-04 eta 0:22:12
epoch [26/30] batch [440/796] time 0.406 (0.374) data 0.000 (0.003) loss 1.7393 (0.8061) lr 6.6987e-04 eta 0:22:03
epoch [26/30] batch [460/796] time 0.370 (0.374) data 0.000 (0.002) loss 0.4788 (0.8013) lr 6.6987e-04 eta 0:21:56
epoch [26/30] batch [480/796] time 0.365 (0.374) data 0.000 (0.002) loss 0.1591 (0.7926) lr 6.6987e-04 eta 0:21:47
epoch [26/30] batch [500/796] time 0.401 (0.373) data 0.000 (0.002) loss 0.5088 (0.7965) lr 6.6987e-04 eta 0:21:39
epoch [26/30] batch [520/796] time 0.388 (0.373) data 0.000 (0.002) loss 0.7998 (0.7939) lr 6.6987e-04 eta 0:21:31
epoch [26/30] batch [540/796] time 0.386 (0.373) data 0.000 (0.002) loss 0.1559 (0.7878) lr 6.6987e-04 eta 0:21:23
epoch [26/30] batch [560/796] time 0.379 (0.373) data 0.000 (0.002) loss 0.4509 (0.7904) lr 6.6987e-04 eta 0:21:16
epoch [26/30] batch [580/796] time 0.389 (0.373) data 0.000 (0.002) loss 0.2153 (0.7890) lr 6.6987e-04 eta 0:21:08
epoch [26/30] batch [600/796] time 0.369 (0.373) data 0.000 (0.002) loss 0.3291 (0.7844) lr 6.6987e-04 eta 0:21:00
epoch [26/30] batch [620/796] time 0.384 (0.373) data 0.000 (0.002) loss 0.6768 (0.7865) lr 6.6987e-04 eta 0:20:53
epoch [26/30] batch [640/796] time 0.379 (0.373) data 0.000 (0.002) loss 0.5103 (0.7828) lr 6.6987e-04 eta 0:20:45
epoch [26/30] batch [660/796] time 0.349 (0.373) data 0.000 (0.002) loss 0.2729 (0.7792) lr 6.6987e-04 eta 0:20:38
epoch [26/30] batch [680/796] time 0.371 (0.373) data 0.000 (0.002) loss 0.3655 (0.7770) lr 6.6987e-04 eta 0:20:31
epoch [26/30] batch [700/796] time 0.383 (0.373) data 0.000 (0.002) loss 0.7739 (0.7741) lr 6.6987e-04 eta 0:20:23
epoch [26/30] batch [720/796] time 0.352 (0.373) data 0.000 (0.002) loss 0.3120 (0.7741) lr 6.6987e-04 eta 0:20:15
epoch [26/30] batch [740/796] time 0.356 (0.373) data 0.000 (0.002) loss 0.7397 (0.7687) lr 6.6987e-04 eta 0:20:07
epoch [26/30] batch [760/796] time 0.343 (0.373) data 0.000 (0.002) loss 0.9492 (0.7739) lr 6.6987e-04 eta 0:20:00
epoch [26/30] batch [780/796] time 0.327 (0.372) data 0.000 (0.002) loss 0.7026 (0.7725) lr 6.6987e-04 eta 0:19:50
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<01:56,  6.14s/it] 10%|█         | 2/20 [00:06<00:52,  2.93s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.72s/it] 20%|██        | 4/20 [00:07<00:18,  1.15s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.20it/s] 30%|███       | 6/20 [00:07<00:09,  1.52it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.89it/s] 40%|████      | 8/20 [00:08<00:05,  2.21it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.51it/s] 50%|█████     | 10/20 [00:09<00:03,  2.80it/s] 55%|█████▌    | 11/20 [00:09<00:02,  3.01it/s] 60%|██████    | 12/20 [00:09<00:02,  3.17it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.45it/s] 70%|███████   | 14/20 [00:10<00:01,  3.70it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.78it/s] 80%|████████  | 16/20 [00:10<00:01,  3.95it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.10it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.19it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.66it/s]100%|██████████| 20/20 [00:11<00:00,  4.13it/s]100%|██████████| 20/20 [00:11<00:00,  1.70it/s]=> result
* total: 1,990
* correct: 1,586
* accuracy: 79.7%
* error: 20.3%
* macro_f1: 79.0%

epoch [27/30] batch [20/796] time 0.349 (0.412) data 0.000 (0.040) loss 0.3896 (0.6184) lr 4.3227e-04 eta 0:21:45
epoch [27/30] batch [40/796] time 0.386 (0.397) data 0.000 (0.020) loss 0.6289 (0.6806) lr 4.3227e-04 eta 0:20:47
epoch [27/30] batch [60/796] time 0.367 (0.389) data 0.000 (0.014) loss 0.9814 (0.7290) lr 4.3227e-04 eta 0:20:14
epoch [27/30] batch [80/796] time 0.347 (0.384) data 0.000 (0.010) loss 0.8057 (0.7531) lr 4.3227e-04 eta 0:19:52
epoch [27/30] batch [100/796] time 0.453 (0.382) data 0.000 (0.008) loss 0.2186 (0.7124) lr 4.3227e-04 eta 0:19:36
epoch [27/30] batch [120/796] time 0.351 (0.380) data 0.000 (0.007) loss 1.4883 (0.7576) lr 4.3227e-04 eta 0:19:25
epoch [27/30] batch [140/796] time 0.344 (0.379) data 0.000 (0.006) loss 0.8325 (0.7651) lr 4.3227e-04 eta 0:19:12
epoch [27/30] batch [160/796] time 0.393 (0.377) data 0.000 (0.005) loss 0.3467 (0.7572) lr 4.3227e-04 eta 0:19:01
epoch [27/30] batch [180/796] time 0.373 (0.377) data 0.000 (0.005) loss 0.7393 (0.7601) lr 4.3227e-04 eta 0:18:51
epoch [27/30] batch [200/796] time 0.376 (0.376) data 0.000 (0.004) loss 1.1309 (0.7599) lr 4.3227e-04 eta 0:18:42
epoch [27/30] batch [220/796] time 0.341 (0.376) data 0.000 (0.004) loss 0.8667 (0.7558) lr 4.3227e-04 eta 0:18:34
epoch [27/30] batch [240/796] time 0.355 (0.377) data 0.000 (0.004) loss 0.5576 (0.7582) lr 4.3227e-04 eta 0:18:28
epoch [27/30] batch [260/796] time 0.350 (0.376) data 0.000 (0.003) loss 1.2627 (0.7578) lr 4.3227e-04 eta 0:18:19
epoch [27/30] batch [280/796] time 0.403 (0.376) data 0.000 (0.003) loss 0.3335 (0.7629) lr 4.3227e-04 eta 0:18:11
epoch [27/30] batch [300/796] time 0.353 (0.375) data 0.000 (0.003) loss 0.3650 (0.7579) lr 4.3227e-04 eta 0:18:01
epoch [27/30] batch [320/796] time 0.347 (0.375) data 0.000 (0.003) loss 0.1968 (0.7663) lr 4.3227e-04 eta 0:17:52
epoch [27/30] batch [340/796] time 0.460 (0.374) data 0.000 (0.003) loss 0.0671 (0.7728) lr 4.3227e-04 eta 0:17:44
epoch [27/30] batch [360/796] time 0.367 (0.374) data 0.000 (0.002) loss 1.3496 (0.7778) lr 4.3227e-04 eta 0:17:35
epoch [27/30] batch [380/796] time 0.380 (0.373) data 0.000 (0.002) loss 0.6982 (0.7806) lr 4.3227e-04 eta 0:17:26
epoch [27/30] batch [400/796] time 0.377 (0.373) data 0.000 (0.002) loss 2.7773 (0.7853) lr 4.3227e-04 eta 0:17:17
epoch [27/30] batch [420/796] time 0.344 (0.373) data 0.000 (0.002) loss 0.2468 (0.7749) lr 4.3227e-04 eta 0:17:10
epoch [27/30] batch [440/796] time 0.369 (0.373) data 0.000 (0.002) loss 0.1809 (0.7724) lr 4.3227e-04 eta 0:17:02
epoch [27/30] batch [460/796] time 0.402 (0.372) data 0.000 (0.002) loss 0.5571 (0.7641) lr 4.3227e-04 eta 0:16:54
epoch [27/30] batch [480/796] time 0.366 (0.373) data 0.000 (0.002) loss 0.3064 (0.7614) lr 4.3227e-04 eta 0:16:47
epoch [27/30] batch [500/796] time 0.360 (0.373) data 0.000 (0.002) loss 0.3472 (0.7639) lr 4.3227e-04 eta 0:16:41
epoch [27/30] batch [520/796] time 0.350 (0.373) data 0.000 (0.002) loss 1.1465 (0.7600) lr 4.3227e-04 eta 0:16:34
epoch [27/30] batch [540/796] time 0.396 (0.374) data 0.000 (0.002) loss 0.2415 (0.7700) lr 4.3227e-04 eta 0:16:27
epoch [27/30] batch [560/796] time 0.347 (0.373) data 0.000 (0.002) loss 1.0098 (0.7695) lr 4.3227e-04 eta 0:16:19
epoch [27/30] batch [580/796] time 0.364 (0.373) data 0.001 (0.002) loss 0.5454 (0.7685) lr 4.3227e-04 eta 0:16:11
epoch [27/30] batch [600/796] time 0.336 (0.373) data 0.000 (0.002) loss 3.0488 (0.7772) lr 4.3227e-04 eta 0:16:03
epoch [27/30] batch [620/796] time 0.371 (0.373) data 0.000 (0.002) loss 0.8823 (0.7774) lr 4.3227e-04 eta 0:15:55
epoch [27/30] batch [640/796] time 0.378 (0.373) data 0.000 (0.002) loss 0.3315 (0.7781) lr 4.3227e-04 eta 0:15:48
epoch [27/30] batch [660/796] time 0.360 (0.372) data 0.000 (0.001) loss 0.8613 (0.7781) lr 4.3227e-04 eta 0:15:39
epoch [27/30] batch [680/796] time 0.344 (0.373) data 0.000 (0.001) loss 0.2639 (0.7767) lr 4.3227e-04 eta 0:15:32
epoch [27/30] batch [700/796] time 0.386 (0.373) data 0.000 (0.001) loss 1.9307 (0.7758) lr 4.3227e-04 eta 0:15:25
epoch [27/30] batch [720/796] time 0.345 (0.372) data 0.000 (0.001) loss 0.2502 (0.7778) lr 4.3227e-04 eta 0:15:17
epoch [27/30] batch [740/796] time 0.391 (0.372) data 0.000 (0.001) loss 0.1750 (0.7755) lr 4.3227e-04 eta 0:15:09
epoch [27/30] batch [760/796] time 0.333 (0.372) data 0.000 (0.001) loss 0.1689 (0.7724) lr 4.3227e-04 eta 0:15:02
epoch [27/30] batch [780/796] time 0.331 (0.371) data 0.000 (0.001) loss 0.3411 (0.7734) lr 4.3227e-04 eta 0:14:52
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:06<02:00,  6.33s/it] 10%|█         | 2/20 [00:07<00:55,  3.09s/it] 15%|█▌        | 3/20 [00:07<00:30,  1.81s/it] 20%|██        | 4/20 [00:07<00:19,  1.21s/it] 25%|██▌       | 5/20 [00:07<00:13,  1.15it/s] 30%|███       | 6/20 [00:08<00:09,  1.49it/s] 35%|███▌      | 7/20 [00:08<00:07,  1.85it/s] 40%|████      | 8/20 [00:08<00:05,  2.19it/s] 45%|████▌     | 9/20 [00:09<00:04,  2.54it/s] 50%|█████     | 10/20 [00:09<00:03,  2.81it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.99it/s] 60%|██████    | 12/20 [00:09<00:02,  3.09it/s] 65%|██████▌   | 13/20 [00:10<00:02,  3.29it/s] 70%|███████   | 14/20 [00:10<00:01,  3.60it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.69it/s] 80%|████████  | 16/20 [00:10<00:01,  3.96it/s] 85%|████████▌ | 17/20 [00:11<00:00,  4.09it/s] 90%|█████████ | 18/20 [00:11<00:00,  2.65it/s] 95%|█████████▌| 19/20 [00:11<00:00,  3.15it/s]100%|██████████| 20/20 [00:12<00:00,  3.65it/s]100%|██████████| 20/20 [00:12<00:00,  1.63it/s]=> result
* total: 1,990
* correct: 1,583
* accuracy: 79.5%
* error: 20.5%
* macro_f1: 78.9%

epoch [28/30] batch [20/796] time 0.337 (0.432) data 0.000 (0.043) loss 1.6641 (0.5914) lr 2.4472e-04 eta 0:17:02
epoch [28/30] batch [40/796] time 0.345 (0.404) data 0.000 (0.022) loss 0.5815 (0.7363) lr 2.4472e-04 eta 0:15:47
epoch [28/30] batch [60/796] time 0.383 (0.395) data 0.000 (0.014) loss 0.8169 (0.7540) lr 2.4472e-04 eta 0:15:18
epoch [28/30] batch [80/796] time 0.383 (0.389) data 0.000 (0.011) loss 0.6436 (0.7317) lr 2.4472e-04 eta 0:14:57
epoch [28/30] batch [100/796] time 0.350 (0.385) data 0.000 (0.009) loss 0.4573 (0.7328) lr 2.4472e-04 eta 0:14:40
epoch [28/30] batch [120/796] time 0.369 (0.383) data 0.000 (0.007) loss 1.3955 (0.7705) lr 2.4472e-04 eta 0:14:27
epoch [28/30] batch [140/796] time 0.381 (0.381) data 0.000 (0.006) loss 1.7021 (0.7437) lr 2.4472e-04 eta 0:14:16
epoch [28/30] batch [160/796] time 0.381 (0.381) data 0.000 (0.006) loss 0.4077 (0.7275) lr 2.4472e-04 eta 0:14:08
epoch [28/30] batch [180/796] time 0.363 (0.380) data 0.000 (0.005) loss 1.3350 (0.7586) lr 2.4472e-04 eta 0:13:58
epoch [28/30] batch [200/796] time 0.375 (0.378) data 0.000 (0.005) loss 0.4326 (0.7405) lr 2.4472e-04 eta 0:13:47
epoch [28/30] batch [220/796] time 0.388 (0.378) data 0.000 (0.004) loss 2.5957 (0.7436) lr 2.4472e-04 eta 0:13:38
epoch [28/30] batch [240/796] time 0.356 (0.377) data 0.000 (0.004) loss 1.2188 (0.7518) lr 2.4472e-04 eta 0:13:30
epoch [28/30] batch [260/796] time 0.384 (0.377) data 0.000 (0.004) loss 0.1730 (0.7603) lr 2.4472e-04 eta 0:13:23
epoch [28/30] batch [280/796] time 0.382 (0.377) data 0.000 (0.003) loss 0.7373 (0.7596) lr 2.4472e-04 eta 0:13:14
epoch [28/30] batch [300/796] time 0.371 (0.376) data 0.000 (0.003) loss 1.3223 (0.7595) lr 2.4472e-04 eta 0:13:05
epoch [28/30] batch [320/796] time 0.390 (0.377) data 0.000 (0.003) loss 0.8311 (0.7710) lr 2.4472e-04 eta 0:12:59
epoch [28/30] batch [340/796] time 0.388 (0.377) data 0.000 (0.003) loss 0.5654 (0.7688) lr 2.4472e-04 eta 0:12:51
epoch [28/30] batch [360/796] time 0.371 (0.376) data 0.000 (0.003) loss 1.8252 (0.7641) lr 2.4472e-04 eta 0:12:43
epoch [28/30] batch [380/796] time 0.368 (0.376) data 0.000 (0.003) loss 0.3386 (0.7662) lr 2.4472e-04 eta 0:12:35
epoch [28/30] batch [400/796] time 0.389 (0.376) data 0.000 (0.002) loss 0.8687 (0.7625) lr 2.4472e-04 eta 0:12:27
epoch [28/30] batch [420/796] time 0.378 (0.376) data 0.000 (0.002) loss 0.5601 (0.7570) lr 2.4472e-04 eta 0:12:19
epoch [28/30] batch [440/796] time 0.356 (0.375) data 0.000 (0.002) loss 0.8247 (0.7576) lr 2.4472e-04 eta 0:12:11
epoch [28/30] batch [460/796] time 0.424 (0.375) data 0.000 (0.002) loss 0.1307 (0.7577) lr 2.4472e-04 eta 0:12:03
epoch [28/30] batch [480/796] time 0.379 (0.376) data 0.000 (0.002) loss 0.5977 (0.7603) lr 2.4472e-04 eta 0:11:56
epoch [28/30] batch [500/796] time 0.371 (0.375) data 0.000 (0.002) loss 0.4558 (0.7573) lr 2.4472e-04 eta 0:11:48
epoch [28/30] batch [520/796] time 0.379 (0.375) data 0.000 (0.002) loss 0.8940 (0.7611) lr 2.4472e-04 eta 0:11:40
epoch [28/30] batch [540/796] time 0.416 (0.375) data 0.000 (0.002) loss 0.9595 (0.7628) lr 2.4472e-04 eta 0:11:33
epoch [28/30] batch [560/796] time 0.369 (0.375) data 0.000 (0.002) loss 0.2656 (0.7630) lr 2.4472e-04 eta 0:11:26
epoch [28/30] batch [580/796] time 0.353 (0.375) data 0.000 (0.002) loss 0.8193 (0.7577) lr 2.4472e-04 eta 0:11:18
epoch [28/30] batch [600/796] time 0.370 (0.375) data 0.000 (0.002) loss 1.1875 (0.7620) lr 2.4472e-04 eta 0:11:10
epoch [28/30] batch [620/796] time 0.387 (0.375) data 0.000 (0.002) loss 1.2656 (0.7651) lr 2.4472e-04 eta 0:11:03
epoch [28/30] batch [640/796] time 0.375 (0.375) data 0.000 (0.002) loss 0.4771 (0.7699) lr 2.4472e-04 eta 0:10:55
epoch [28/30] batch [660/796] time 0.392 (0.375) data 0.000 (0.002) loss 0.7334 (0.7660) lr 2.4472e-04 eta 0:10:47
epoch [28/30] batch [680/796] time 0.390 (0.375) data 0.000 (0.002) loss 0.8340 (0.7654) lr 2.4472e-04 eta 0:10:40
epoch [28/30] batch [700/796] time 0.359 (0.375) data 0.000 (0.001) loss 0.4192 (0.7699) lr 2.4472e-04 eta 0:10:33
epoch [28/30] batch [720/796] time 0.384 (0.375) data 0.000 (0.001) loss 0.7725 (0.7655) lr 2.4472e-04 eta 0:10:25
epoch [28/30] batch [740/796] time 0.351 (0.375) data 0.001 (0.001) loss 1.3223 (0.7640) lr 2.4472e-04 eta 0:10:17
epoch [28/30] batch [760/796] time 0.373 (0.375) data 0.000 (0.001) loss 0.0455 (0.7677) lr 2.4472e-04 eta 0:10:10
epoch [28/30] batch [780/796] time 0.325 (0.374) data 0.000 (0.001) loss 0.3242 (0.7657) lr 2.4472e-04 eta 0:10:01
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:07<02:14,  7.09s/it] 10%|█         | 2/20 [00:08<01:04,  3.60s/it] 15%|█▌        | 3/20 [00:08<00:35,  2.08s/it] 20%|██        | 4/20 [00:08<00:21,  1.37s/it] 25%|██▌       | 5/20 [00:09<00:14,  1.03it/s] 30%|███       | 6/20 [00:09<00:10,  1.36it/s] 35%|███▌      | 7/20 [00:09<00:07,  1.73it/s] 40%|████      | 8/20 [00:09<00:05,  2.10it/s] 45%|████▌     | 9/20 [00:10<00:04,  2.45it/s] 50%|█████     | 10/20 [00:10<00:03,  2.71it/s] 55%|█████▌    | 11/20 [00:10<00:02,  3.06it/s] 60%|██████    | 12/20 [00:10<00:02,  3.23it/s] 65%|██████▌   | 13/20 [00:11<00:01,  3.50it/s] 70%|███████   | 14/20 [00:11<00:01,  3.81it/s] 75%|███████▌  | 15/20 [00:11<00:01,  3.75it/s] 80%|████████  | 16/20 [00:11<00:01,  3.67it/s] 85%|████████▌ | 17/20 [00:12<00:00,  3.63it/s] 90%|█████████ | 18/20 [00:13<00:01,  1.50it/s] 95%|█████████▌| 19/20 [00:13<00:00,  1.92it/s]100%|██████████| 20/20 [00:14<00:00,  2.40it/s]100%|██████████| 20/20 [00:14<00:00,  1.40it/s]=> result
* total: 1,990
* correct: 1,583
* accuracy: 79.5%
* error: 20.5%
* macro_f1: 78.9%

epoch [29/30] batch [20/796] time 0.382 (0.422) data 0.000 (0.047) loss 0.8276 (0.6945) lr 1.0926e-04 eta 0:11:03
epoch [29/30] batch [40/796] time 0.350 (0.399) data 0.000 (0.024) loss 0.4949 (0.7507) lr 1.0926e-04 eta 0:10:18
epoch [29/30] batch [60/796] time 0.342 (0.389) data 0.000 (0.016) loss 0.6250 (0.7854) lr 1.0926e-04 eta 0:09:55
epoch [29/30] batch [80/796] time 0.356 (0.382) data 0.000 (0.012) loss 0.1049 (0.7683) lr 1.0926e-04 eta 0:09:38
epoch [29/30] batch [100/796] time 0.377 (0.380) data 0.000 (0.010) loss 0.6045 (0.7621) lr 1.0926e-04 eta 0:09:26
epoch [29/30] batch [120/796] time 0.341 (0.375) data 0.000 (0.008) loss 1.1152 (0.7786) lr 1.0926e-04 eta 0:09:12
epoch [29/30] batch [140/796] time 0.377 (0.373) data 0.000 (0.007) loss 0.4119 (0.7625) lr 1.0926e-04 eta 0:09:01
epoch [29/30] batch [160/796] time 0.340 (0.371) data 0.000 (0.006) loss 0.1857 (0.7449) lr 1.0926e-04 eta 0:08:51
epoch [29/30] batch [180/796] time 0.341 (0.371) data 0.000 (0.006) loss 1.3877 (0.7546) lr 1.0926e-04 eta 0:08:43
epoch [29/30] batch [200/796] time 0.386 (0.373) data 0.000 (0.005) loss 0.3257 (0.7534) lr 1.0926e-04 eta 0:08:39
epoch [29/30] batch [220/796] time 0.383 (0.372) data 0.000 (0.005) loss 0.4714 (0.7511) lr 1.0926e-04 eta 0:08:30
epoch [29/30] batch [240/796] time 0.331 (0.371) data 0.000 (0.004) loss 0.3494 (0.7598) lr 1.0926e-04 eta 0:08:21
epoch [29/30] batch [260/796] time 0.356 (0.371) data 0.000 (0.004) loss 0.6602 (0.7624) lr 1.0926e-04 eta 0:08:13
epoch [29/30] batch [280/796] time 0.391 (0.371) data 0.000 (0.004) loss 0.9370 (0.7722) lr 1.0926e-04 eta 0:08:06
epoch [29/30] batch [300/796] time 0.394 (0.370) data 0.000 (0.003) loss 0.9565 (0.7649) lr 1.0926e-04 eta 0:07:57
epoch [29/30] batch [320/796] time 0.379 (0.370) data 0.000 (0.003) loss 1.5146 (0.7672) lr 1.0926e-04 eta 0:07:50
epoch [29/30] batch [340/796] time 0.373 (0.370) data 0.000 (0.003) loss 0.8481 (0.7708) lr 1.0926e-04 eta 0:07:42
epoch [29/30] batch [360/796] time 0.340 (0.370) data 0.000 (0.003) loss 1.1416 (0.7692) lr 1.0926e-04 eta 0:07:35
epoch [29/30] batch [380/796] time 0.352 (0.370) data 0.000 (0.003) loss 0.4653 (0.7706) lr 1.0926e-04 eta 0:07:28
epoch [29/30] batch [400/796] time 0.384 (0.370) data 0.000 (0.003) loss 1.1855 (0.7734) lr 1.0926e-04 eta 0:07:20
epoch [29/30] batch [420/796] time 0.363 (0.369) data 0.000 (0.003) loss 0.3599 (0.7663) lr 1.0926e-04 eta 0:07:12
epoch [29/30] batch [440/796] time 0.358 (0.369) data 0.000 (0.002) loss 0.6338 (0.7583) lr 1.0926e-04 eta 0:07:05
epoch [29/30] batch [460/796] time 0.338 (0.369) data 0.000 (0.002) loss 0.7729 (0.7587) lr 1.0926e-04 eta 0:06:57
epoch [29/30] batch [480/796] time 0.397 (0.369) data 0.000 (0.002) loss 1.5107 (0.7590) lr 1.0926e-04 eta 0:06:50
epoch [29/30] batch [500/796] time 0.374 (0.369) data 0.000 (0.002) loss 0.2283 (0.7622) lr 1.0926e-04 eta 0:06:43
epoch [29/30] batch [520/796] time 0.343 (0.369) data 0.000 (0.002) loss 0.6113 (0.7650) lr 1.0926e-04 eta 0:06:35
epoch [29/30] batch [540/796] time 0.378 (0.369) data 0.000 (0.002) loss 0.3801 (0.7718) lr 1.0926e-04 eta 0:06:28
epoch [29/30] batch [560/796] time 0.373 (0.369) data 0.000 (0.002) loss 1.5684 (0.7737) lr 1.0926e-04 eta 0:06:20
epoch [29/30] batch [580/796] time 0.351 (0.369) data 0.000 (0.002) loss 1.1992 (0.7757) lr 1.0926e-04 eta 0:06:13
epoch [29/30] batch [600/796] time 0.379 (0.369) data 0.000 (0.002) loss 0.2808 (0.7799) lr 1.0926e-04 eta 0:06:05
epoch [29/30] batch [620/796] time 0.332 (0.369) data 0.000 (0.002) loss 0.6812 (0.7863) lr 1.0926e-04 eta 0:05:58
epoch [29/30] batch [640/796] time 0.390 (0.368) data 0.000 (0.002) loss 0.3247 (0.7878) lr 1.0926e-04 eta 0:05:50
epoch [29/30] batch [660/796] time 0.335 (0.368) data 0.000 (0.002) loss 0.2900 (0.7878) lr 1.0926e-04 eta 0:05:42
epoch [29/30] batch [680/796] time 0.342 (0.368) data 0.000 (0.002) loss 0.7188 (0.7830) lr 1.0926e-04 eta 0:05:35
epoch [29/30] batch [700/796] time 0.366 (0.368) data 0.000 (0.002) loss 1.0439 (0.7797) lr 1.0926e-04 eta 0:05:28
epoch [29/30] batch [720/796] time 0.356 (0.368) data 0.000 (0.002) loss 0.2561 (0.7778) lr 1.0926e-04 eta 0:05:20
epoch [29/30] batch [740/796] time 0.364 (0.368) data 0.000 (0.002) loss 0.6260 (0.7767) lr 1.0926e-04 eta 0:05:13
epoch [29/30] batch [760/796] time 0.377 (0.368) data 0.000 (0.002) loss 0.5430 (0.7808) lr 1.0926e-04 eta 0:05:06
epoch [29/30] batch [780/796] time 0.337 (0.368) data 0.000 (0.002) loss 1.0479 (0.7831) lr 1.0926e-04 eta 0:04:58
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:11<03:35, 11.34s/it] 10%|█         | 2/20 [00:11<01:30,  5.05s/it] 15%|█▌        | 3/20 [00:12<00:48,  2.87s/it] 20%|██        | 4/20 [00:12<00:29,  1.85s/it] 25%|██▌       | 5/20 [00:12<00:19,  1.29s/it] 30%|███       | 6/20 [00:13<00:13,  1.06it/s] 35%|███▌      | 7/20 [00:13<00:09,  1.37it/s] 40%|████      | 8/20 [00:13<00:07,  1.70it/s] 45%|████▌     | 9/20 [00:13<00:05,  2.05it/s] 50%|█████     | 10/20 [00:14<00:04,  2.34it/s] 55%|█████▌    | 11/20 [00:14<00:03,  2.61it/s] 60%|██████    | 12/20 [00:14<00:02,  2.86it/s] 65%|██████▌   | 13/20 [00:15<00:02,  3.06it/s] 70%|███████   | 14/20 [00:15<00:01,  3.30it/s] 75%|███████▌  | 15/20 [00:15<00:01,  3.54it/s] 80%|████████  | 16/20 [00:15<00:01,  3.88it/s] 85%|████████▌ | 17/20 [00:15<00:00,  4.03it/s] 90%|█████████ | 18/20 [00:16<00:00,  2.64it/s] 95%|█████████▌| 19/20 [00:16<00:00,  3.13it/s]100%|██████████| 20/20 [00:17<00:00,  3.63it/s]100%|██████████| 20/20 [00:17<00:00,  1.17it/s]=> result
* total: 1,990
* correct: 1,585
* accuracy: 79.6%
* error: 20.4%
* macro_f1: 79.0%

epoch [30/30] batch [20/796] time 0.353 (0.429) data 0.000 (0.050) loss 0.3455 (0.7077) lr 2.7391e-05 eta 0:05:32
epoch [30/30] batch [40/796] time 0.383 (0.398) data 0.000 (0.025) loss 0.3413 (0.8198) lr 2.7391e-05 eta 0:05:01
epoch [30/30] batch [60/796] time 0.351 (0.386) data 0.000 (0.017) loss 0.3484 (0.8591) lr 2.7391e-05 eta 0:04:44
epoch [30/30] batch [80/796] time 0.368 (0.379) data 0.000 (0.013) loss 0.9961 (0.7884) lr 2.7391e-05 eta 0:04:31
epoch [30/30] batch [100/796] time 0.369 (0.377) data 0.000 (0.010) loss 1.7627 (0.7740) lr 2.7391e-05 eta 0:04:22
epoch [30/30] batch [120/796] time 0.407 (0.377) data 0.000 (0.009) loss 0.1653 (0.7544) lr 2.7391e-05 eta 0:04:15
epoch [30/30] batch [140/796] time 0.367 (0.377) data 0.000 (0.007) loss 0.7212 (0.7425) lr 2.7391e-05 eta 0:04:07
epoch [30/30] batch [160/796] time 0.339 (0.375) data 0.001 (0.006) loss 1.4307 (0.7393) lr 2.7391e-05 eta 0:03:58
epoch [30/30] batch [180/796] time 0.330 (0.373) data 0.000 (0.006) loss 1.4141 (0.7487) lr 2.7391e-05 eta 0:03:50
epoch [30/30] batch [200/796] time 0.382 (0.373) data 0.000 (0.005) loss 0.1736 (0.7346) lr 2.7391e-05 eta 0:03:42
epoch [30/30] batch [220/796] time 0.331 (0.372) data 0.000 (0.005) loss 0.2891 (0.7266) lr 2.7391e-05 eta 0:03:34
epoch [30/30] batch [240/796] time 0.372 (0.372) data 0.000 (0.004) loss 0.5010 (0.7284) lr 2.7391e-05 eta 0:03:26
epoch [30/30] batch [260/796] time 0.384 (0.371) data 0.000 (0.004) loss 0.6548 (0.7445) lr 2.7391e-05 eta 0:03:18
epoch [30/30] batch [280/796] time 0.369 (0.371) data 0.000 (0.004) loss 0.3120 (0.7446) lr 2.7391e-05 eta 0:03:11
epoch [30/30] batch [300/796] time 0.372 (0.371) data 0.000 (0.004) loss 0.8687 (0.7433) lr 2.7391e-05 eta 0:03:04
epoch [30/30] batch [320/796] time 0.385 (0.371) data 0.000 (0.003) loss 0.5615 (0.7427) lr 2.7391e-05 eta 0:02:56
epoch [30/30] batch [340/796] time 0.343 (0.371) data 0.000 (0.003) loss 0.8428 (0.7442) lr 2.7391e-05 eta 0:02:49
epoch [30/30] batch [360/796] time 0.384 (0.372) data 0.000 (0.003) loss 0.7505 (0.7457) lr 2.7391e-05 eta 0:02:41
epoch [30/30] batch [380/796] time 0.384 (0.372) data 0.000 (0.003) loss 1.0908 (0.7370) lr 2.7391e-05 eta 0:02:34
epoch [30/30] batch [400/796] time 0.387 (0.372) data 0.000 (0.003) loss 0.8662 (0.7402) lr 2.7391e-05 eta 0:02:27
epoch [30/30] batch [420/796] time 0.365 (0.371) data 0.000 (0.003) loss 1.4004 (0.7380) lr 2.7391e-05 eta 0:02:19
epoch [30/30] batch [440/796] time 0.371 (0.371) data 0.000 (0.003) loss 0.6128 (0.7441) lr 2.7391e-05 eta 0:02:12
epoch [30/30] batch [460/796] time 0.351 (0.371) data 0.000 (0.002) loss 0.2646 (0.7423) lr 2.7391e-05 eta 0:02:04
epoch [30/30] batch [480/796] time 0.365 (0.371) data 0.000 (0.002) loss 0.9282 (0.7380) lr 2.7391e-05 eta 0:01:57
epoch [30/30] batch [500/796] time 0.388 (0.371) data 0.000 (0.002) loss 1.0713 (0.7377) lr 2.7391e-05 eta 0:01:49
epoch [30/30] batch [520/796] time 0.331 (0.371) data 0.000 (0.002) loss 0.9390 (0.7376) lr 2.7391e-05 eta 0:01:42
epoch [30/30] batch [540/796] time 0.346 (0.370) data 0.000 (0.002) loss 0.4495 (0.7325) lr 2.7391e-05 eta 0:01:34
epoch [30/30] batch [560/796] time 0.405 (0.370) data 0.000 (0.002) loss 0.9785 (0.7331) lr 2.7391e-05 eta 0:01:27
epoch [30/30] batch [580/796] time 0.383 (0.370) data 0.000 (0.002) loss 0.4778 (0.7326) lr 2.7391e-05 eta 0:01:19
epoch [30/30] batch [600/796] time 0.355 (0.370) data 0.001 (0.002) loss 0.5571 (0.7353) lr 2.7391e-05 eta 0:01:12
epoch [30/30] batch [620/796] time 0.358 (0.370) data 0.000 (0.002) loss 0.2886 (0.7371) lr 2.7391e-05 eta 0:01:05
epoch [30/30] batch [640/796] time 0.392 (0.370) data 0.000 (0.002) loss 0.6235 (0.7343) lr 2.7391e-05 eta 0:00:57
epoch [30/30] batch [660/796] time 0.371 (0.370) data 0.000 (0.002) loss 0.2620 (0.7338) lr 2.7391e-05 eta 0:00:50
epoch [30/30] batch [680/796] time 0.404 (0.370) data 0.000 (0.002) loss 0.6021 (0.7338) lr 2.7391e-05 eta 0:00:42
epoch [30/30] batch [700/796] time 0.379 (0.370) data 0.000 (0.002) loss 0.7192 (0.7346) lr 2.7391e-05 eta 0:00:35
epoch [30/30] batch [720/796] time 0.366 (0.370) data 0.001 (0.002) loss 0.8247 (0.7350) lr 2.7391e-05 eta 0:00:28
epoch [30/30] batch [740/796] time 0.365 (0.369) data 0.000 (0.002) loss 0.2316 (0.7339) lr 2.7391e-05 eta 0:00:20
epoch [30/30] batch [760/796] time 0.374 (0.369) data 0.000 (0.002) loss 0.9326 (0.7398) lr 2.7391e-05 eta 0:00:13
epoch [30/30] batch [780/796] time 0.339 (0.369) data 0.000 (0.002) loss 0.7261 (0.7380) lr 2.7391e-05 eta 0:00:05
Evaluate on the *val* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:53,  5.97s/it] 10%|█         | 2/20 [00:06<00:53,  2.98s/it] 15%|█▌        | 3/20 [00:07<00:29,  1.75s/it] 20%|██        | 4/20 [00:07<00:18,  1.17s/it] 25%|██▌       | 5/20 [00:07<00:12,  1.18it/s] 30%|███       | 6/20 [00:07<00:09,  1.53it/s] 35%|███▌      | 7/20 [00:08<00:06,  1.88it/s] 40%|████      | 8/20 [00:08<00:05,  2.22it/s] 45%|████▌     | 9/20 [00:08<00:04,  2.52it/s] 50%|█████     | 10/20 [00:09<00:03,  2.78it/s] 55%|█████▌    | 11/20 [00:09<00:03,  2.94it/s] 60%|██████    | 12/20 [00:09<00:02,  3.26it/s] 65%|██████▌   | 13/20 [00:09<00:02,  3.42it/s] 70%|███████   | 14/20 [00:10<00:01,  3.58it/s] 75%|███████▌  | 15/20 [00:10<00:01,  3.91it/s] 80%|████████  | 16/20 [00:10<00:00,  4.13it/s] 85%|████████▌ | 17/20 [00:10<00:00,  4.23it/s] 90%|█████████ | 18/20 [00:11<00:00,  3.64it/s] 95%|█████████▌| 19/20 [00:11<00:00,  4.06it/s]100%|██████████| 20/20 [00:11<00:00,  4.47it/s]100%|██████████| 20/20 [00:11<00:00,  1.72it/s]
=> result
* total: 1,990
* correct: 1,584
* accuracy: 79.6%
* error: 20.4%
* macro_f1: 78.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar" (epoch = 24)
Evaluate on the *test* set
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:05<09:28,  5.74s/it]  2%|▏         | 2/100 [00:06<04:18,  2.64s/it]  3%|▎         | 3/100 [00:07<03:33,  2.20s/it]  4%|▍         | 4/100 [00:08<02:25,  1.52s/it]  5%|▌         | 5/100 [00:08<01:48,  1.14s/it]  6%|▌         | 6/100 [00:09<01:24,  1.11it/s]  7%|▋         | 7/100 [00:09<01:09,  1.33it/s]  8%|▊         | 8/100 [00:10<01:01,  1.50it/s]  9%|▉         | 9/100 [00:10<00:56,  1.60it/s] 10%|█         | 10/100 [00:11<00:51,  1.73it/s] 11%|█         | 11/100 [00:11<00:46,  1.90it/s] 12%|█▏        | 12/100 [00:12<00:43,  2.01it/s] 13%|█▎        | 13/100 [00:12<00:41,  2.12it/s] 14%|█▍        | 14/100 [00:12<00:40,  2.15it/s] 15%|█▌        | 15/100 [00:13<00:40,  2.12it/s] 16%|█▌        | 16/100 [00:13<00:39,  2.14it/s] 17%|█▋        | 17/100 [00:14<00:38,  2.17it/s] 18%|█▊        | 18/100 [00:14<00:37,  2.21it/s] 19%|█▉        | 19/100 [00:15<00:36,  2.20it/s] 20%|██        | 20/100 [00:15<00:37,  2.14it/s] 21%|██        | 21/100 [00:16<00:35,  2.22it/s] 22%|██▏       | 22/100 [00:16<00:33,  2.31it/s] 23%|██▎       | 23/100 [00:16<00:32,  2.34it/s] 24%|██▍       | 24/100 [00:17<00:32,  2.33it/s] 25%|██▌       | 25/100 [00:17<00:32,  2.28it/s] 26%|██▌       | 26/100 [00:18<00:33,  2.21it/s] 27%|██▋       | 27/100 [00:18<00:32,  2.27it/s] 28%|██▊       | 28/100 [00:19<00:30,  2.38it/s] 29%|██▉       | 29/100 [00:19<00:29,  2.40it/s] 30%|███       | 30/100 [00:19<00:29,  2.41it/s] 31%|███       | 31/100 [00:20<00:28,  2.45it/s] 32%|███▏      | 32/100 [00:20<00:27,  2.46it/s] 33%|███▎      | 33/100 [00:21<00:27,  2.43it/s] 34%|███▍      | 34/100 [00:21<00:27,  2.39it/s] 35%|███▌      | 35/100 [00:21<00:27,  2.33it/s] 36%|███▌      | 36/100 [00:22<00:27,  2.34it/s] 37%|███▋      | 37/100 [00:22<00:27,  2.27it/s] 38%|███▊      | 38/100 [00:23<00:26,  2.30it/s] 39%|███▉      | 39/100 [00:23<00:25,  2.36it/s] 40%|████      | 40/100 [00:24<00:25,  2.37it/s] 41%|████      | 41/100 [00:24<00:25,  2.36it/s] 42%|████▏     | 42/100 [00:24<00:24,  2.40it/s] 43%|████▎     | 43/100 [00:25<00:24,  2.37it/s] 44%|████▍     | 44/100 [00:25<00:23,  2.42it/s] 45%|████▌     | 45/100 [00:26<00:22,  2.50it/s] 46%|████▌     | 46/100 [00:26<00:21,  2.49it/s] 47%|████▋     | 47/100 [00:26<00:21,  2.49it/s] 48%|████▊     | 48/100 [00:27<00:21,  2.46it/s] 49%|████▉     | 49/100 [00:27<00:20,  2.47it/s] 50%|█████     | 50/100 [00:28<00:20,  2.40it/s] 51%|█████     | 51/100 [00:28<00:21,  2.31it/s] 52%|█████▏    | 52/100 [00:29<00:20,  2.38it/s] 53%|█████▎    | 53/100 [00:29<00:18,  2.51it/s] 54%|█████▍    | 54/100 [00:29<00:18,  2.53it/s] 55%|█████▌    | 55/100 [00:30<00:17,  2.57it/s] 56%|█████▌    | 56/100 [00:30<00:17,  2.52it/s] 57%|█████▋    | 57/100 [00:31<00:17,  2.46it/s] 58%|█████▊    | 58/100 [00:31<00:16,  2.47it/s] 59%|█████▉    | 59/100 [00:31<00:15,  2.57it/s] 60%|██████    | 60/100 [00:32<00:15,  2.58it/s] 61%|██████    | 61/100 [00:32<00:14,  2.63it/s] 62%|██████▏   | 62/100 [00:32<00:14,  2.56it/s] 63%|██████▎   | 63/100 [00:33<00:15,  2.45it/s] 64%|██████▍   | 64/100 [00:33<00:14,  2.45it/s] 65%|██████▌   | 65/100 [00:34<00:14,  2.40it/s] 66%|██████▌   | 66/100 [00:34<00:14,  2.32it/s] 67%|██████▋   | 67/100 [00:35<00:14,  2.28it/s] 68%|██████▊   | 68/100 [00:35<00:14,  2.23it/s] 69%|██████▉   | 69/100 [00:36<00:13,  2.30it/s] 70%|███████   | 70/100 [00:36<00:13,  2.29it/s] 71%|███████   | 71/100 [00:36<00:12,  2.35it/s] 72%|███████▏  | 72/100 [00:37<00:11,  2.45it/s] 73%|███████▎  | 73/100 [00:37<00:10,  2.58it/s] 74%|███████▍  | 74/100 [00:37<00:09,  2.65it/s] 75%|███████▌  | 75/100 [00:38<00:09,  2.76it/s] 76%|███████▌  | 76/100 [00:38<00:08,  2.84it/s] 77%|███████▋  | 77/100 [00:38<00:07,  2.89it/s] 78%|███████▊  | 78/100 [00:39<00:07,  3.03it/s] 79%|███████▉  | 79/100 [00:39<00:06,  3.14it/s] 80%|████████  | 80/100 [00:39<00:06,  3.24it/s] 81%|████████  | 81/100 [00:40<00:05,  3.38it/s] 82%|████████▏ | 82/100 [00:40<00:05,  3.55it/s] 83%|████████▎ | 83/100 [00:40<00:04,  3.73it/s] 84%|████████▍ | 84/100 [00:40<00:03,  4.13it/s] 85%|████████▌ | 85/100 [00:40<00:03,  4.47it/s] 86%|████████▌ | 86/100 [00:41<00:02,  4.74it/s] 87%|████████▋ | 87/100 [00:41<00:02,  4.94it/s] 88%|████████▊ | 88/100 [00:41<00:02,  5.10it/s] 89%|████████▉ | 89/100 [00:41<00:02,  5.22it/s] 90%|█████████ | 90/100 [00:41<00:01,  5.30it/s] 91%|█████████ | 91/100 [00:42<00:01,  5.36it/s] 92%|█████████▏| 92/100 [00:42<00:01,  5.41it/s] 93%|█████████▎| 93/100 [00:42<00:01,  5.43it/s] 94%|█████████▍| 94/100 [00:42<00:01,  5.46it/s] 95%|█████████▌| 95/100 [00:42<00:00,  5.48it/s] 96%|█████████▌| 96/100 [00:42<00:00,  5.48it/s] 97%|█████████▋| 97/100 [00:43<00:00,  5.49it/s] 98%|█████████▊| 98/100 [00:43<00:00,  5.51it/s] 99%|█████████▉| 99/100 [00:43<00:00,  5.50it/s]100%|██████████| 100/100 [00:43<00:00,  6.04it/s]100%|██████████| 100/100 [00:43<00:00,  2.29it/s]
=> result
* total: 9,950
* correct: 7,934
* accuracy: 79.7%
* error: 20.3%
* macro_f1: 79.4%
Elapsed: 2:34:13
+ sh scripts/rpo_prime/base2new_test.sh sun397 3 0 main_tmp 16 new
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp.yaml
dataset_config_file: configs/datasets/sun397.yaml
eval_only: True
head: 
load_epoch: None
model_dir: output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output/rpo_prime/base2new/test_new/sun397/shots_16/RPO_prime/main_tmp/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: SUN397
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/test_new/sun397/shots_16/RPO_prime/main_tmp/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:SUN397
Loading dataset: SUN397
Reading split from /shared/s2/lab01/dataset/clip/sun397/split_zhou_SUN397.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/sun397/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
3168 1980 9900
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    SUN397
# classes  198
# train_x  3,168
# val      1,980
# test     9,900
---------  ------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/sun397/shots_16/RPO_prime/main_tmp/seed3/prompt_learner/model-best.pth.tar" (epoch = 24)
Evaluate on the *test* set
  0%|          | 0/99 [00:00<?, ?it/s]  1%|          | 1/99 [00:09<16:08,  9.88s/it]  2%|▏         | 2/99 [00:10<06:57,  4.31s/it]  3%|▎         | 3/99 [00:10<04:01,  2.51s/it]  4%|▍         | 4/99 [00:11<02:38,  1.67s/it]  5%|▌         | 5/99 [00:11<01:52,  1.19s/it]  6%|▌         | 6/99 [00:11<01:23,  1.11it/s]  7%|▋         | 7/99 [00:12<01:05,  1.41it/s]  8%|▊         | 8/99 [00:12<00:52,  1.72it/s]  9%|▉         | 9/99 [00:12<00:44,  2.01it/s] 10%|█         | 10/99 [00:12<00:39,  2.24it/s] 11%|█         | 11/99 [00:13<00:36,  2.41it/s] 12%|█▏        | 12/99 [00:13<00:34,  2.49it/s] 13%|█▎        | 13/99 [00:14<00:33,  2.55it/s] 14%|█▍        | 14/99 [00:14<00:35,  2.41it/s] 15%|█▌        | 15/99 [00:14<00:35,  2.35it/s] 16%|█▌        | 16/99 [00:15<00:35,  2.31it/s] 17%|█▋        | 17/99 [00:15<00:33,  2.42it/s] 18%|█▊        | 18/99 [00:16<00:33,  2.41it/s] 19%|█▉        | 19/99 [00:16<00:33,  2.38it/s] 20%|██        | 20/99 [00:17<00:32,  2.40it/s] 21%|██        | 21/99 [00:17<00:33,  2.30it/s] 22%|██▏       | 22/99 [00:17<00:33,  2.29it/s] 23%|██▎       | 23/99 [00:18<00:31,  2.40it/s] 24%|██▍       | 24/99 [00:18<00:30,  2.47it/s] 25%|██▌       | 25/99 [00:19<00:30,  2.41it/s] 26%|██▋       | 26/99 [00:19<00:30,  2.39it/s] 27%|██▋       | 27/99 [00:19<00:29,  2.48it/s] 28%|██▊       | 28/99 [00:20<00:28,  2.53it/s] 29%|██▉       | 29/99 [00:20<00:27,  2.59it/s] 30%|███       | 30/99 [00:21<00:25,  2.75it/s] 31%|███▏      | 31/99 [00:21<00:22,  3.01it/s] 32%|███▏      | 32/99 [00:21<00:20,  3.34it/s] 33%|███▎      | 33/99 [00:21<00:20,  3.15it/s] 34%|███▍      | 34/99 [00:22<00:21,  2.97it/s] 35%|███▌      | 35/99 [00:22<00:22,  2.82it/s] 36%|███▋      | 36/99 [00:23<00:24,  2.60it/s] 37%|███▋      | 37/99 [00:23<00:24,  2.50it/s] 38%|███▊      | 38/99 [00:24<00:36,  1.68it/s] 39%|███▉      | 39/99 [00:25<00:32,  1.84it/s] 40%|████      | 40/99 [00:25<00:29,  1.98it/s] 41%|████▏     | 41/99 [00:25<00:27,  2.11it/s] 42%|████▏     | 42/99 [00:26<00:25,  2.27it/s] 43%|████▎     | 43/99 [00:26<00:24,  2.30it/s] 44%|████▍     | 44/99 [00:26<00:23,  2.38it/s] 45%|████▌     | 45/99 [00:27<00:22,  2.42it/s] 46%|████▋     | 46/99 [00:27<00:22,  2.37it/s] 47%|████▋     | 47/99 [00:28<00:21,  2.41it/s] 48%|████▊     | 48/99 [00:28<00:21,  2.40it/s] 49%|████▉     | 49/99 [00:29<00:20,  2.45it/s] 51%|█████     | 50/99 [00:29<00:19,  2.47it/s] 52%|█████▏    | 51/99 [00:29<00:19,  2.43it/s] 53%|█████▎    | 52/99 [00:30<00:19,  2.37it/s] 54%|█████▎    | 53/99 [00:30<00:19,  2.33it/s] 55%|█████▍    | 54/99 [00:31<00:24,  1.86it/s] 56%|█████▌    | 55/99 [00:31<00:21,  2.00it/s] 57%|█████▋    | 56/99 [00:32<00:19,  2.16it/s] 58%|█████▊    | 57/99 [00:32<00:17,  2.34it/s] 59%|█████▊    | 58/99 [00:33<00:16,  2.47it/s] 60%|█████▉    | 59/99 [00:33<00:15,  2.63it/s] 61%|██████    | 60/99 [00:33<00:15,  2.56it/s] 62%|██████▏   | 61/99 [00:34<00:14,  2.60it/s] 63%|██████▎   | 62/99 [00:34<00:14,  2.64it/s] 64%|██████▎   | 63/99 [00:34<00:13,  2.61it/s] 65%|██████▍   | 64/99 [00:35<00:13,  2.60it/s] 66%|██████▌   | 65/99 [00:35<00:13,  2.58it/s] 67%|██████▋   | 66/99 [00:36<00:12,  2.57it/s] 68%|██████▊   | 67/99 [00:36<00:12,  2.52it/s] 69%|██████▊   | 68/99 [00:36<00:12,  2.52it/s] 70%|██████▉   | 69/99 [00:37<00:12,  2.50it/s] 71%|███████   | 70/99 [00:37<00:11,  2.61it/s] 72%|███████▏  | 71/99 [00:37<00:10,  2.70it/s] 73%|███████▎  | 72/99 [00:38<00:09,  2.73it/s] 74%|███████▎  | 73/99 [00:38<00:09,  2.82it/s] 75%|███████▍  | 74/99 [00:38<00:08,  2.90it/s] 76%|███████▌  | 75/99 [00:39<00:08,  2.97it/s] 77%|███████▋  | 76/99 [00:39<00:07,  3.06it/s] 78%|███████▊  | 77/99 [00:39<00:06,  3.20it/s] 79%|███████▉  | 78/99 [00:40<00:06,  3.34it/s] 80%|███████▉  | 79/99 [00:40<00:05,  3.43it/s] 81%|████████  | 80/99 [00:40<00:05,  3.62it/s] 82%|████████▏ | 81/99 [00:40<00:04,  3.95it/s] 83%|████████▎ | 82/99 [00:41<00:04,  4.16it/s] 84%|████████▍ | 83/99 [00:41<00:03,  4.28it/s] 85%|████████▍ | 84/99 [00:41<00:03,  4.60it/s] 86%|████████▌ | 85/99 [00:41<00:02,  4.85it/s] 87%|████████▋ | 86/99 [00:41<00:02,  5.03it/s] 88%|████████▊ | 87/99 [00:42<00:02,  5.18it/s] 89%|████████▉ | 88/99 [00:42<00:02,  5.29it/s] 90%|████████▉ | 89/99 [00:42<00:01,  5.36it/s] 91%|█████████ | 90/99 [00:42<00:01,  5.42it/s] 92%|█████████▏| 91/99 [00:42<00:01,  5.47it/s] 93%|█████████▎| 92/99 [00:42<00:01,  5.49it/s] 94%|█████████▍| 93/99 [00:43<00:01,  5.51it/s] 95%|█████████▍| 94/99 [00:43<00:00,  5.50it/s] 96%|█████████▌| 95/99 [00:43<00:00,  5.51it/s] 97%|█████████▋| 96/99 [00:43<00:00,  5.53it/s] 98%|█████████▊| 97/99 [00:43<00:00,  5.54it/s] 99%|█████████▉| 98/99 [00:44<00:00,  5.53it/s]100%|██████████| 99/99 [00:44<00:00,  5.54it/s]100%|██████████| 99/99 [00:44<00:00,  2.23it/s]
=> result
* total: 9,900
* correct: 7,792
* accuracy: 78.7%
* error: 21.3%
* macro_f1: 77.8%
+ for dataset in caltech101 sun397 imagenet
+ for seed in 1 2 3
+ sh scripts/rpo_prime/base2new_train.sh imagenet 1 0 main_tmp 16
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp.yaml
dataset_config_file: configs/datasets/imagenet.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/imagenet/shots_16/RPO_prime/main_tmp/seed1
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 1
source_domains: None
target_domains: None
trainer: RPO_prime
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: ImageNet
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/imagenet/shots_16/RPO_prime/main_tmp/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime
Loading trainer: RPO_prime
requested:ImageNet
Loading dataset: ImageNet
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/ImageNet/split_fewshot_taesup/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  --------
Dataset    ImageNet
# classes  500
# train_x  8,000
# val      25,000
# test     25,000
---------  --------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/imagenet/shots_16/RPO_prime/main_tmp/seed1/tensorboard)
/shared/s2/lab01/myungjoo/RPO_v2/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
/home/s2/mjoolee/anaconda/envs/dassl/lib/python3.7/site-packages/torch/autograd/__init__.py:199: UserWarning: Error detected in SoftmaxBackward0. Traceback of forward call that caused the error:
  File "train.py", line 233, in <module>
    main(args)
  File "train.py", line 175, in main
    trainer.train()
  File "/home/s2/mjoolee/CLIP/KgCoop/Dassl.pytorch/dassl/engine/trainer.py", line 386, in train
    super().train(self.start_epoch, self.max_epoch)
  File "/home/s2/mjoolee/CLIP/KgCoop/Dassl.pytorch/dassl/engine/trainer.py", line 250, in train
    self.run_epoch()
  File "/home/s2/mjoolee/CLIP/KgCoop/Dassl.pytorch/dassl/engine/trainer.py", line 597, in run_epoch
    loss_summary = self.forward_backward(batch)
  File "/shared/s2/lab01/myungjoo/RPO_v2/trainers/rpo_prime.py", line 406, in forward_backward
    loss = model(image, label)
  File "/home/s2/mjoolee/anaconda/envs/dassl/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1199, in _call_impl
    return forward_call(*input, **kwargs)
  File "/shared/s2/lab01/myungjoo/RPO_v2/trainers/rpo_prime.py", line 250, in forward
    mix_logit = F.softmax(mix_logit, dim=2).mean(0) #batch, class
  File "/home/s2/mjoolee/anaconda/envs/dassl/lib/python3.7/site-packages/torch/nn/functional.py", line 1841, in softmax
    ret = input.softmax(dim)
  File "/home/s2/mjoolee/anaconda/envs/dassl/lib/python3.7/site-packages/torch/fx/traceback.py", line 57, in format_stack
    return traceback.format_stack()
 (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541035/work/torch/csrc/autograd/python_anomaly_mode.cpp:114.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
epoch [1/30] batch [20/2000] time 0.582 (0.740) data 0.000 (0.070) loss 3.0293 (2.2023) lr 1.0000e-02 eta 12:20:04
epoch [1/30] batch [40/2000] time 0.662 (0.669) data 0.000 (0.035) loss 1.6885 (1.8570) lr 1.0000e-02 eta 11:08:42
epoch [1/30] batch [60/2000] time 0.584 (0.646) data 0.000 (0.024) loss 0.6021 (1.7095) lr 1.0000e-02 eta 10:45:31
epoch [1/30] batch [80/2000] time 0.593 (0.642) data 0.000 (0.018) loss 1.3271 (1.6812) lr 1.0000e-02 eta 10:40:40
Traceback (most recent call last):
  File "train.py", line 233, in <module>
    main(args)
  File "train.py", line 175, in main
    trainer.train()
  File "/home/s2/mjoolee/CLIP/KgCoop/Dassl.pytorch/dassl/engine/trainer.py", line 386, in train
    super().train(self.start_epoch, self.max_epoch)
  File "/home/s2/mjoolee/CLIP/KgCoop/Dassl.pytorch/dassl/engine/trainer.py", line 250, in train
    self.run_epoch()
  File "/home/s2/mjoolee/CLIP/KgCoop/Dassl.pytorch/dassl/engine/trainer.py", line 597, in run_epoch
    loss_summary = self.forward_backward(batch)
  File "/shared/s2/lab01/myungjoo/RPO_v2/trainers/rpo_prime.py", line 408, in forward_backward
    loss.backward()
  File "/home/s2/mjoolee/anaconda/envs/dassl/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/home/s2/mjoolee/anaconda/envs/dassl/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: Function 'SoftmaxBackward0' returned nan values in its 0th output.
