***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 3
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:StanfordCars
Loading dataset: StanfordCars
Reading split from /shared/s2/lab01/dataset/clip/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/stanford_cars/split_fewshot_taesup/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
1568 812 4002
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  98
# train_x  1,568
# val      812
# test     4,002
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/tensorboard)
epoch [1/30] batch [20/392] time 0.288 (0.413) data 0.000 (0.053) loss 2.8203 (3.4122) lr 1.0000e-02 eta 1:20:48
epoch [1/30] batch [40/392] time 0.278 (0.354) data 0.000 (0.026) loss 2.2422 (3.4284) lr 1.0000e-02 eta 1:09:05
epoch [1/30] batch [60/392] time 0.301 (0.334) data 0.000 (0.018) loss 4.8203 (3.4206) lr 1.0000e-02 eta 1:05:06
epoch [1/30] batch [80/392] time 0.280 (0.323) data 0.000 (0.013) loss 3.3496 (3.3197) lr 1.0000e-02 eta 1:02:48
epoch [1/30] batch [100/392] time 0.299 (0.316) data 0.000 (0.011) loss 2.9746 (3.2671) lr 1.0000e-02 eta 1:01:29
epoch [1/30] batch [120/392] time 0.328 (0.312) data 0.000 (0.009) loss 2.1348 (3.1889) lr 1.0000e-02 eta 1:00:36
epoch [1/30] batch [140/392] time 0.288 (0.311) data 0.000 (0.008) loss 1.6836 (3.1106) lr 1.0000e-02 eta 1:00:13
epoch [1/30] batch [160/392] time 0.290 (0.308) data 0.000 (0.007) loss 2.6113 (3.0713) lr 1.0000e-02 eta 0:59:31
epoch [1/30] batch [180/392] time 0.287 (0.306) data 0.000 (0.006) loss 2.8008 (3.0747) lr 1.0000e-02 eta 0:59:03
epoch [1/30] batch [200/392] time 0.296 (0.305) data 0.000 (0.006) loss 4.8203 (3.0643) lr 1.0000e-02 eta 0:58:45
epoch [1/30] batch [220/392] time 0.289 (0.304) data 0.000 (0.005) loss 3.0352 (3.0557) lr 1.0000e-02 eta 0:58:25
epoch [1/30] batch [240/392] time 0.290 (0.303) data 0.000 (0.005) loss 2.1738 (3.0461) lr 1.0000e-02 eta 0:58:09
epoch [1/30] batch [260/392] time 0.281 (0.302) data 0.000 (0.004) loss 1.9385 (3.0161) lr 1.0000e-02 eta 0:57:51
epoch [1/30] batch [280/392] time 0.290 (0.301) data 0.000 (0.004) loss 5.3516 (2.9951) lr 1.0000e-02 eta 0:57:40
epoch [1/30] batch [300/392] time 0.302 (0.301) data 0.000 (0.004) loss 2.2031 (2.9915) lr 1.0000e-02 eta 0:57:25
epoch [1/30] batch [320/392] time 0.282 (0.300) data 0.000 (0.004) loss 1.1865 (2.9903) lr 1.0000e-02 eta 0:57:13
epoch [1/30] batch [340/392] time 0.287 (0.300) data 0.000 (0.003) loss 1.2188 (3.0059) lr 1.0000e-02 eta 0:57:00
epoch [1/30] batch [360/392] time 0.299 (0.299) data 0.000 (0.003) loss 4.4922 (2.9974) lr 1.0000e-02 eta 0:56:52
epoch [1/30] batch [380/392] time 0.275 (0.298) data 0.000 (0.003) loss 1.2568 (2.9802) lr 1.0000e-02 eta 0:56:31
Evaluate on the *val* set
=> result
* total: 812
* correct: 543
* accuracy: 66.9%
* error: 33.1%
* macro_f1: 65.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
epoch [2/30] batch [20/392] time 0.288 (0.325) data 0.000 (0.034) loss 2.4883 (2.9643) lr 9.9726e-03 eta 1:01:31
epoch [2/30] batch [40/392] time 0.284 (0.307) data 0.000 (0.017) loss 6.9648 (3.1353) lr 9.9726e-03 eta 0:58:02
epoch [2/30] batch [60/392] time 0.287 (0.302) data 0.000 (0.012) loss 3.4980 (3.0324) lr 9.9726e-03 eta 0:56:54
epoch [2/30] batch [80/392] time 0.283 (0.299) data 0.000 (0.009) loss 4.1094 (3.0204) lr 9.9726e-03 eta 0:56:09
epoch [2/30] batch [100/392] time 0.282 (0.298) data 0.000 (0.007) loss 3.2051 (3.0303) lr 9.9726e-03 eta 0:56:03
epoch [2/30] batch [120/392] time 0.298 (0.297) data 0.000 (0.006) loss 6.8477 (3.0951) lr 9.9726e-03 eta 0:55:39
epoch [2/30] batch [140/392] time 0.295 (0.296) data 0.000 (0.005) loss 1.9160 (3.0831) lr 9.9726e-03 eta 0:55:24
epoch [2/30] batch [160/392] time 0.287 (0.295) data 0.000 (0.005) loss 1.0889 (3.0439) lr 9.9726e-03 eta 0:55:11
epoch [2/30] batch [180/392] time 0.280 (0.294) data 0.000 (0.004) loss 1.6699 (2.9948) lr 9.9726e-03 eta 0:54:54
epoch [2/30] batch [200/392] time 0.287 (0.295) data 0.001 (0.004) loss 2.9297 (3.0193) lr 9.9726e-03 eta 0:54:49
epoch [2/30] batch [220/392] time 0.284 (0.294) data 0.000 (0.003) loss 4.4453 (2.9877) lr 9.9726e-03 eta 0:54:37
epoch [2/30] batch [240/392] time 0.282 (0.294) data 0.000 (0.003) loss 4.3438 (3.0077) lr 9.9726e-03 eta 0:54:33
epoch [2/30] batch [260/392] time 0.284 (0.294) data 0.000 (0.003) loss 1.3174 (2.9904) lr 9.9726e-03 eta 0:54:22
epoch [2/30] batch [280/392] time 0.281 (0.294) data 0.000 (0.003) loss 1.0732 (2.9719) lr 9.9726e-03 eta 0:54:17
epoch [2/30] batch [300/392] time 0.283 (0.294) data 0.000 (0.003) loss 1.9678 (2.9653) lr 9.9726e-03 eta 0:54:13
epoch [2/30] batch [320/392] time 0.289 (0.294) data 0.000 (0.002) loss 1.7812 (2.9419) lr 9.9726e-03 eta 0:54:06
epoch [2/30] batch [340/392] time 0.294 (0.294) data 0.000 (0.002) loss 2.6914 (2.9390) lr 9.9726e-03 eta 0:53:59
epoch [2/30] batch [360/392] time 0.288 (0.294) data 0.000 (0.002) loss 2.9668 (2.9243) lr 9.9726e-03 eta 0:53:52
epoch [2/30] batch [380/392] time 0.271 (0.293) data 0.000 (0.002) loss 3.5254 (2.9542) lr 9.9726e-03 eta 0:53:35
Evaluate on the *val* set
=> result
* total: 812
* correct: 547
* accuracy: 67.4%
* error: 32.6%
* macro_f1: 65.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
epoch [3/30] batch [20/392] time 0.288 (0.333) data 0.000 (0.033) loss 5.5234 (2.7200) lr 9.8907e-03 eta 1:00:43
epoch [3/30] batch [40/392] time 0.303 (0.312) data 0.000 (0.017) loss 3.0527 (2.7859) lr 9.8907e-03 eta 0:56:49
epoch [3/30] batch [60/392] time 0.314 (0.305) data 0.000 (0.011) loss 2.1250 (2.9131) lr 9.8907e-03 eta 0:55:28
epoch [3/30] batch [80/392] time 0.285 (0.302) data 0.000 (0.008) loss 4.0156 (2.8644) lr 9.8907e-03 eta 0:54:50
epoch [3/30] batch [100/392] time 0.299 (0.300) data 0.000 (0.007) loss 2.9648 (2.7504) lr 9.8907e-03 eta 0:54:27
epoch [3/30] batch [120/392] time 0.289 (0.299) data 0.000 (0.006) loss 2.7090 (2.7745) lr 9.8907e-03 eta 0:54:03
epoch [3/30] batch [140/392] time 0.282 (0.298) data 0.000 (0.005) loss 3.1719 (2.7739) lr 9.8907e-03 eta 0:53:46
epoch [3/30] batch [160/392] time 0.289 (0.298) data 0.000 (0.004) loss 3.5215 (2.7218) lr 9.8907e-03 eta 0:53:39
epoch [3/30] batch [180/392] time 0.283 (0.297) data 0.000 (0.004) loss 2.1484 (2.7498) lr 9.8907e-03 eta 0:53:26
epoch [3/30] batch [200/392] time 0.290 (0.297) data 0.000 (0.004) loss 3.1113 (2.7756) lr 9.8907e-03 eta 0:53:15
epoch [3/30] batch [220/392] time 0.291 (0.296) data 0.000 (0.003) loss 2.6914 (2.7532) lr 9.8907e-03 eta 0:52:58
epoch [3/30] batch [240/392] time 0.284 (0.296) data 0.000 (0.003) loss 3.6426 (2.7846) lr 9.8907e-03 eta 0:52:58
epoch [3/30] batch [260/392] time 0.280 (0.296) data 0.000 (0.003) loss 6.0547 (2.7990) lr 9.8907e-03 eta 0:52:51
epoch [3/30] batch [280/392] time 0.283 (0.296) data 0.000 (0.003) loss 1.9443 (2.7939) lr 9.8907e-03 eta 0:52:41
epoch [3/30] batch [300/392] time 0.287 (0.296) data 0.000 (0.002) loss 2.3242 (2.7945) lr 9.8907e-03 eta 0:52:35
epoch [3/30] batch [320/392] time 0.284 (0.295) data 0.000 (0.002) loss 3.8672 (2.7903) lr 9.8907e-03 eta 0:52:28
epoch [3/30] batch [340/392] time 0.290 (0.295) data 0.000 (0.002) loss 1.1191 (2.7789) lr 9.8907e-03 eta 0:52:19
epoch [3/30] batch [360/392] time 0.285 (0.295) data 0.000 (0.002) loss 0.8662 (2.7532) lr 9.8907e-03 eta 0:52:10
epoch [3/30] batch [380/392] time 0.275 (0.294) data 0.000 (0.002) loss 1.1152 (2.7549) lr 9.8907e-03 eta 0:51:54
Evaluate on the *val* set
=> result
* total: 812
* correct: 549
* accuracy: 67.6%
* error: 32.4%
* macro_f1: 66.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
epoch [4/30] batch [20/392] time 0.285 (0.340) data 0.000 (0.036) loss 0.8418 (3.2378) lr 9.7553e-03 eta 0:59:47
epoch [4/30] batch [40/392] time 0.293 (0.314) data 0.000 (0.018) loss 1.9492 (3.1611) lr 9.7553e-03 eta 0:55:15
epoch [4/30] batch [60/392] time 0.292 (0.307) data 0.000 (0.012) loss 2.5605 (2.9290) lr 9.7553e-03 eta 0:53:53
epoch [4/30] batch [80/392] time 0.307 (0.303) data 0.000 (0.009) loss 4.5625 (2.9856) lr 9.7553e-03 eta 0:53:04
epoch [4/30] batch [100/392] time 0.284 (0.300) data 0.000 (0.007) loss 1.9023 (2.9261) lr 9.7553e-03 eta 0:52:26
epoch [4/30] batch [120/392] time 0.283 (0.299) data 0.000 (0.006) loss 3.9121 (2.9387) lr 9.7553e-03 eta 0:52:12
epoch [4/30] batch [140/392] time 0.285 (0.298) data 0.000 (0.005) loss 1.9805 (2.9000) lr 9.7553e-03 eta 0:51:55
epoch [4/30] batch [160/392] time 0.285 (0.297) data 0.000 (0.005) loss 4.3984 (2.8812) lr 9.7553e-03 eta 0:51:39
epoch [4/30] batch [180/392] time 0.297 (0.296) data 0.000 (0.004) loss 2.6230 (2.8701) lr 9.7553e-03 eta 0:51:22
epoch [4/30] batch [200/392] time 0.322 (0.296) data 0.000 (0.004) loss 3.4277 (2.8710) lr 9.7553e-03 eta 0:51:15
epoch [4/30] batch [220/392] time 0.286 (0.295) data 0.000 (0.003) loss 2.8398 (2.8749) lr 9.7553e-03 eta 0:51:00
epoch [4/30] batch [240/392] time 0.295 (0.295) data 0.000 (0.003) loss 2.9375 (2.8586) lr 9.7553e-03 eta 0:50:52
epoch [4/30] batch [260/392] time 0.282 (0.295) data 0.000 (0.003) loss 1.6289 (2.8655) lr 9.7553e-03 eta 0:50:42
epoch [4/30] batch [280/392] time 0.285 (0.294) data 0.000 (0.003) loss 2.4121 (2.8861) lr 9.7553e-03 eta 0:50:32
epoch [4/30] batch [300/392] time 0.283 (0.294) data 0.000 (0.003) loss 2.6191 (2.8930) lr 9.7553e-03 eta 0:50:24
epoch [4/30] batch [320/392] time 0.283 (0.294) data 0.000 (0.002) loss 0.7715 (2.8966) lr 9.7553e-03 eta 0:50:17
epoch [4/30] batch [340/392] time 0.336 (0.294) data 0.000 (0.002) loss 2.5449 (2.9107) lr 9.7553e-03 eta 0:50:13
epoch [4/30] batch [360/392] time 0.299 (0.294) data 0.000 (0.002) loss 3.0742 (2.9042) lr 9.7553e-03 eta 0:50:08
epoch [4/30] batch [380/392] time 0.276 (0.293) data 0.000 (0.002) loss 2.1289 (2.9122) lr 9.7553e-03 eta 0:49:51
Evaluate on the *val* set
=> result
* total: 812
* correct: 543
* accuracy: 66.9%
* error: 33.1%
* macro_f1: 65.1%
epoch [5/30] batch [20/392] time 0.283 (0.334) data 0.000 (0.034) loss 4.1094 (2.7658) lr 9.5677e-03 eta 0:56:33
epoch [5/30] batch [40/392] time 0.340 (0.310) data 0.000 (0.017) loss 2.0742 (2.5996) lr 9.5677e-03 eta 0:52:31
epoch [5/30] batch [60/392] time 0.286 (0.306) data 0.000 (0.012) loss 5.1836 (2.6733) lr 9.5677e-03 eta 0:51:40
epoch [5/30] batch [80/392] time 0.282 (0.302) data 0.000 (0.009) loss 2.6113 (2.7532) lr 9.5677e-03 eta 0:50:55
epoch [5/30] batch [100/392] time 0.291 (0.300) data 0.000 (0.007) loss 1.1426 (2.6801) lr 9.5677e-03 eta 0:50:31
epoch [5/30] batch [120/392] time 0.287 (0.299) data 0.000 (0.006) loss 4.7148 (2.6869) lr 9.5677e-03 eta 0:50:07
epoch [5/30] batch [140/392] time 0.314 (0.297) data 0.000 (0.005) loss 3.1797 (2.6431) lr 9.5677e-03 eta 0:49:49
epoch [5/30] batch [160/392] time 0.320 (0.297) data 0.000 (0.005) loss 1.6240 (2.6845) lr 9.5677e-03 eta 0:49:39
epoch [5/30] batch [180/392] time 0.284 (0.296) data 0.000 (0.004) loss 3.0645 (2.7614) lr 9.5677e-03 eta 0:49:22
epoch [5/30] batch [200/392] time 0.278 (0.295) data 0.000 (0.004) loss 4.4883 (2.7848) lr 9.5677e-03 eta 0:49:07
epoch [5/30] batch [220/392] time 0.283 (0.294) data 0.000 (0.003) loss 2.7051 (2.7573) lr 9.5677e-03 eta 0:48:50
epoch [5/30] batch [240/392] time 0.293 (0.294) data 0.000 (0.003) loss 0.8184 (2.7719) lr 9.5677e-03 eta 0:48:43
epoch [5/30] batch [260/392] time 0.283 (0.293) data 0.000 (0.003) loss 1.0762 (2.7876) lr 9.5677e-03 eta 0:48:34
epoch [5/30] batch [280/392] time 0.280 (0.293) data 0.000 (0.003) loss 3.9746 (2.7988) lr 9.5677e-03 eta 0:48:26
epoch [5/30] batch [300/392] time 0.287 (0.293) data 0.000 (0.003) loss 1.5586 (2.7810) lr 9.5677e-03 eta 0:48:20
epoch [5/30] batch [320/392] time 0.282 (0.293) data 0.000 (0.002) loss 4.2812 (2.7940) lr 9.5677e-03 eta 0:48:12
epoch [5/30] batch [340/392] time 0.280 (0.293) data 0.000 (0.002) loss 1.4512 (2.8116) lr 9.5677e-03 eta 0:48:05
epoch [5/30] batch [360/392] time 0.314 (0.293) data 0.000 (0.002) loss 2.4766 (2.7940) lr 9.5677e-03 eta 0:47:58
epoch [5/30] batch [380/392] time 0.273 (0.292) data 0.000 (0.002) loss 2.0684 (2.7846) lr 9.5677e-03 eta 0:47:42
Evaluate on the *val* set
=> result
* total: 812
* correct: 558
* accuracy: 68.7%
* error: 31.3%
* macro_f1: 67.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
epoch [6/30] batch [20/392] time 0.293 (0.336) data 0.000 (0.034) loss 1.1299 (2.4116) lr 9.3301e-03 eta 0:54:46
epoch [6/30] batch [40/392] time 0.314 (0.314) data 0.000 (0.017) loss 1.0312 (2.4013) lr 9.3301e-03 eta 0:51:09
epoch [6/30] batch [60/392] time 0.284 (0.307) data 0.000 (0.011) loss 1.5498 (2.5453) lr 9.3301e-03 eta 0:49:53
epoch [6/30] batch [80/392] time 0.288 (0.303) data 0.000 (0.009) loss 4.6523 (2.5295) lr 9.3301e-03 eta 0:49:06
epoch [6/30] batch [100/392] time 0.293 (0.300) data 0.000 (0.007) loss 2.3594 (2.5092) lr 9.3301e-03 eta 0:48:32
epoch [6/30] batch [120/392] time 0.287 (0.299) data 0.000 (0.006) loss 3.9941 (2.5292) lr 9.3301e-03 eta 0:48:11
epoch [6/30] batch [140/392] time 0.275 (0.297) data 0.000 (0.005) loss 2.7168 (2.5215) lr 9.3301e-03 eta 0:47:53
epoch [6/30] batch [160/392] time 0.299 (0.296) data 0.000 (0.004) loss 2.9277 (2.5931) lr 9.3301e-03 eta 0:47:36
epoch [6/30] batch [180/392] time 0.276 (0.295) data 0.000 (0.004) loss 3.1484 (2.5887) lr 9.3301e-03 eta 0:47:18
epoch [6/30] batch [200/392] time 0.281 (0.294) data 0.000 (0.004) loss 5.3555 (2.5844) lr 9.3301e-03 eta 0:47:06
epoch [6/30] batch [220/392] time 0.279 (0.294) data 0.000 (0.003) loss 2.1211 (2.5485) lr 9.3301e-03 eta 0:46:59
epoch [6/30] batch [240/392] time 0.284 (0.294) data 0.000 (0.003) loss 3.2695 (2.5517) lr 9.3301e-03 eta 0:46:50
epoch [6/30] batch [260/392] time 0.286 (0.294) data 0.000 (0.003) loss 1.7090 (2.5438) lr 9.3301e-03 eta 0:46:41
epoch [6/30] batch [280/392] time 0.291 (0.294) data 0.000 (0.003) loss 4.1719 (2.5610) lr 9.3301e-03 eta 0:46:35
epoch [6/30] batch [300/392] time 0.289 (0.293) data 0.000 (0.002) loss 3.9570 (2.6040) lr 9.3301e-03 eta 0:46:26
epoch [6/30] batch [320/392] time 0.300 (0.293) data 0.000 (0.002) loss 1.6064 (2.6396) lr 9.3301e-03 eta 0:46:21
epoch [6/30] batch [340/392] time 0.292 (0.293) data 0.000 (0.002) loss 3.9922 (2.6680) lr 9.3301e-03 eta 0:46:15
epoch [6/30] batch [360/392] time 0.299 (0.293) data 0.000 (0.002) loss 1.9688 (2.7082) lr 9.3301e-03 eta 0:46:07
epoch [6/30] batch [380/392] time 0.273 (0.292) data 0.000 (0.002) loss 1.3018 (2.7422) lr 9.3301e-03 eta 0:45:53
Evaluate on the *val* set
=> result
* total: 812
* correct: 565
* accuracy: 69.6%
* error: 30.4%
* macro_f1: 68.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
epoch [7/30] batch [20/392] time 0.279 (0.333) data 0.000 (0.033) loss 5.7617 (3.0780) lr 9.0451e-03 eta 0:52:10
epoch [7/30] batch [40/392] time 0.281 (0.311) data 0.000 (0.017) loss 1.4893 (2.7355) lr 9.0451e-03 eta 0:48:30
epoch [7/30] batch [60/392] time 0.292 (0.303) data 0.000 (0.011) loss 3.1914 (2.6176) lr 9.0451e-03 eta 0:47:08
epoch [7/30] batch [80/392] time 0.281 (0.299) data 0.000 (0.009) loss 7.9141 (2.7088) lr 9.0451e-03 eta 0:46:30
epoch [7/30] batch [100/392] time 0.287 (0.297) data 0.000 (0.007) loss 3.8730 (2.7175) lr 9.0451e-03 eta 0:46:02
epoch [7/30] batch [120/392] time 0.285 (0.295) data 0.000 (0.006) loss 2.8359 (2.7078) lr 9.0451e-03 eta 0:45:43
epoch [7/30] batch [140/392] time 0.299 (0.295) data 0.000 (0.005) loss 2.4727 (2.6847) lr 9.0451e-03 eta 0:45:38
epoch [7/30] batch [160/392] time 0.283 (0.295) data 0.000 (0.004) loss 1.4414 (2.6828) lr 9.0451e-03 eta 0:45:26
epoch [7/30] batch [180/392] time 0.282 (0.295) data 0.000 (0.004) loss 3.0215 (2.6991) lr 9.0451e-03 eta 0:45:18
epoch [7/30] batch [200/392] time 0.290 (0.294) data 0.000 (0.004) loss 2.9609 (2.6879) lr 9.0451e-03 eta 0:45:06
epoch [7/30] batch [220/392] time 0.289 (0.294) data 0.000 (0.003) loss 3.9785 (2.6782) lr 9.0451e-03 eta 0:44:57
epoch [7/30] batch [240/392] time 0.281 (0.293) data 0.000 (0.003) loss 3.2598 (2.6605) lr 9.0451e-03 eta 0:44:49
epoch [7/30] batch [260/392] time 0.295 (0.293) data 0.000 (0.003) loss 1.4717 (2.6408) lr 9.0451e-03 eta 0:44:39
epoch [7/30] batch [280/392] time 0.287 (0.293) data 0.000 (0.003) loss 6.3398 (2.6397) lr 9.0451e-03 eta 0:44:37
epoch [7/30] batch [300/392] time 0.282 (0.293) data 0.000 (0.002) loss 2.1523 (2.6300) lr 9.0451e-03 eta 0:44:28
epoch [7/30] batch [320/392] time 0.296 (0.293) data 0.000 (0.002) loss 1.0703 (2.6192) lr 9.0451e-03 eta 0:44:22
epoch [7/30] batch [340/392] time 0.300 (0.293) data 0.000 (0.002) loss 3.0645 (2.6257) lr 9.0451e-03 eta 0:44:16
epoch [7/30] batch [360/392] time 0.297 (0.293) data 0.001 (0.002) loss 2.3008 (2.6364) lr 9.0451e-03 eta 0:44:07
epoch [7/30] batch [380/392] time 0.270 (0.292) data 0.000 (0.002) loss 2.4180 (2.6457) lr 9.0451e-03 eta 0:43:52
Evaluate on the *val* set
=> result
* total: 812
* correct: 570
* accuracy: 70.2%
* error: 29.8%
* macro_f1: 68.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
epoch [8/30] batch [20/392] time 0.282 (0.326) data 0.000 (0.034) loss 2.0859 (2.7930) lr 8.7157e-03 eta 0:48:50
epoch [8/30] batch [40/392] time 0.305 (0.308) data 0.000 (0.017) loss 3.6016 (2.5931) lr 8.7157e-03 eta 0:46:08
epoch [8/30] batch [60/392] time 0.306 (0.303) data 0.000 (0.012) loss 1.6006 (2.6769) lr 8.7157e-03 eta 0:45:09
epoch [8/30] batch [80/392] time 0.281 (0.299) data 0.000 (0.009) loss 1.5908 (2.6616) lr 8.7157e-03 eta 0:44:28
epoch [8/30] batch [100/392] time 0.312 (0.297) data 0.000 (0.007) loss 3.2715 (2.6729) lr 8.7157e-03 eta 0:44:10
epoch [8/30] batch [120/392] time 0.276 (0.296) data 0.000 (0.006) loss 4.9297 (2.6694) lr 8.7157e-03 eta 0:43:56
epoch [8/30] batch [140/392] time 0.288 (0.295) data 0.000 (0.005) loss 4.9609 (2.6992) lr 8.7157e-03 eta 0:43:42
epoch [8/30] batch [160/392] time 0.282 (0.295) data 0.000 (0.004) loss 1.1758 (2.6534) lr 8.7157e-03 eta 0:43:28
epoch [8/30] batch [180/392] time 0.328 (0.294) data 0.000 (0.004) loss 2.5430 (2.5909) lr 8.7157e-03 eta 0:43:17
epoch [8/30] batch [200/392] time 0.320 (0.294) data 0.000 (0.004) loss 0.6377 (2.6079) lr 8.7157e-03 eta 0:43:09
epoch [8/30] batch [220/392] time 0.289 (0.294) data 0.000 (0.003) loss 1.5684 (2.6275) lr 8.7157e-03 eta 0:43:04
epoch [8/30] batch [240/392] time 0.309 (0.294) data 0.000 (0.003) loss 2.7734 (2.5909) lr 8.7157e-03 eta 0:42:58
epoch [8/30] batch [260/392] time 0.299 (0.294) data 0.000 (0.003) loss 3.8711 (2.5950) lr 8.7157e-03 eta 0:42:52
epoch [8/30] batch [280/392] time 0.277 (0.294) data 0.000 (0.003) loss 1.2070 (2.5685) lr 8.7157e-03 eta 0:42:45
epoch [8/30] batch [300/392] time 0.292 (0.293) data 0.000 (0.003) loss 4.8203 (2.5790) lr 8.7157e-03 eta 0:42:37
epoch [8/30] batch [320/392] time 0.295 (0.293) data 0.000 (0.002) loss 2.3926 (2.5838) lr 8.7157e-03 eta 0:42:31
epoch [8/30] batch [340/392] time 0.288 (0.293) data 0.000 (0.002) loss 1.8281 (2.5912) lr 8.7157e-03 eta 0:42:24
epoch [8/30] batch [360/392] time 0.281 (0.293) data 0.000 (0.002) loss 2.7773 (2.5913) lr 8.7157e-03 eta 0:42:15
epoch [8/30] batch [380/392] time 0.270 (0.292) data 0.000 (0.002) loss 2.7324 (2.6156) lr 8.7157e-03 eta 0:42:01
Evaluate on the *val* set
=> result
* total: 812
* correct: 561
* accuracy: 69.1%
* error: 30.9%
* macro_f1: 68.1%
epoch [9/30] batch [20/392] time 0.282 (0.326) data 0.000 (0.034) loss 1.1299 (1.9565) lr 8.3457e-03 eta 0:46:41
epoch [9/30] batch [40/392] time 0.288 (0.308) data 0.000 (0.017) loss 2.7324 (2.2528) lr 8.3457e-03 eta 0:44:03
epoch [9/30] batch [60/392] time 0.382 (0.303) data 0.000 (0.011) loss 5.6484 (2.4626) lr 8.3457e-03 eta 0:43:12
epoch [9/30] batch [80/392] time 0.287 (0.299) data 0.000 (0.009) loss 1.5391 (2.5537) lr 8.3457e-03 eta 0:42:38
epoch [9/30] batch [100/392] time 0.279 (0.296) data 0.000 (0.007) loss 1.6982 (2.5680) lr 8.3457e-03 eta 0:42:05
epoch [9/30] batch [120/392] time 0.331 (0.295) data 0.000 (0.006) loss 1.8008 (2.5218) lr 8.3457e-03 eta 0:41:51
epoch [9/30] batch [140/392] time 0.278 (0.295) data 0.000 (0.005) loss 1.8203 (2.4873) lr 8.3457e-03 eta 0:41:44
epoch [9/30] batch [160/392] time 0.300 (0.294) data 0.000 (0.004) loss 1.2285 (2.4704) lr 8.3457e-03 eta 0:41:31
epoch [9/30] batch [180/392] time 0.305 (0.294) data 0.000 (0.004) loss 3.2910 (2.4927) lr 8.3457e-03 eta 0:41:23
epoch [9/30] batch [200/392] time 0.282 (0.294) data 0.000 (0.004) loss 3.0352 (2.5220) lr 8.3457e-03 eta 0:41:13
epoch [9/30] batch [220/392] time 0.295 (0.293) data 0.000 (0.003) loss 1.7754 (2.5123) lr 8.3457e-03 eta 0:41:05
epoch [9/30] batch [240/392] time 0.292 (0.293) data 0.000 (0.003) loss 3.7988 (2.5056) lr 8.3457e-03 eta 0:40:58
epoch [9/30] batch [260/392] time 0.287 (0.293) data 0.000 (0.003) loss 4.6406 (2.5148) lr 8.3457e-03 eta 0:40:49
epoch [9/30] batch [280/392] time 0.288 (0.293) data 0.000 (0.003) loss 1.9893 (2.5284) lr 8.3457e-03 eta 0:40:41
epoch [9/30] batch [300/392] time 0.286 (0.292) data 0.000 (0.003) loss 2.5957 (2.5171) lr 8.3457e-03 eta 0:40:31
epoch [9/30] batch [320/392] time 0.334 (0.292) data 0.000 (0.002) loss 0.4756 (2.5329) lr 8.3457e-03 eta 0:40:26
epoch [9/30] batch [340/392] time 0.285 (0.292) data 0.000 (0.002) loss 2.8359 (2.5518) lr 8.3457e-03 eta 0:40:20
epoch [9/30] batch [360/392] time 0.302 (0.292) data 0.000 (0.002) loss 6.4062 (2.5359) lr 8.3457e-03 eta 0:40:13
epoch [9/30] batch [380/392] time 0.270 (0.291) data 0.000 (0.002) loss 2.9023 (2.5625) lr 8.3457e-03 eta 0:39:58
Evaluate on the *val* set
=> result
* total: 812
* correct: 579
* accuracy: 71.3%
* error: 28.7%
* macro_f1: 70.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
epoch [10/30] batch [20/392] time 0.394 (0.331) data 0.000 (0.037) loss 3.4062 (2.5324) lr 7.9389e-03 eta 0:45:15
epoch [10/30] batch [40/392] time 0.278 (0.312) data 0.000 (0.019) loss 2.5840 (2.5197) lr 7.9389e-03 eta 0:42:35
epoch [10/30] batch [60/392] time 0.287 (0.305) data 0.000 (0.013) loss 2.1914 (2.5318) lr 7.9389e-03 eta 0:41:32
epoch [10/30] batch [80/392] time 0.280 (0.302) data 0.000 (0.010) loss 2.6523 (2.4346) lr 7.9389e-03 eta 0:41:00
epoch [10/30] batch [100/392] time 0.295 (0.300) data 0.000 (0.008) loss 1.7979 (2.4027) lr 7.9389e-03 eta 0:40:36
epoch [10/30] batch [120/392] time 0.289 (0.297) data 0.000 (0.006) loss 7.9453 (2.4874) lr 7.9389e-03 eta 0:40:12
epoch [10/30] batch [140/392] time 0.282 (0.296) data 0.000 (0.006) loss 1.7871 (2.5137) lr 7.9389e-03 eta 0:39:53
epoch [10/30] batch [160/392] time 0.317 (0.295) data 0.000 (0.005) loss 2.9863 (2.5540) lr 7.9389e-03 eta 0:39:39
epoch [10/30] batch [180/392] time 0.276 (0.294) data 0.000 (0.004) loss 2.1230 (2.5358) lr 7.9389e-03 eta 0:39:28
epoch [10/30] batch [200/392] time 0.284 (0.294) data 0.000 (0.004) loss 3.0566 (2.5789) lr 7.9389e-03 eta 0:39:17
epoch [10/30] batch [220/392] time 0.311 (0.293) data 0.001 (0.004) loss 0.5117 (2.5940) lr 7.9389e-03 eta 0:39:10
epoch [10/30] batch [240/392] time 0.285 (0.293) data 0.000 (0.003) loss 1.0273 (2.5749) lr 7.9389e-03 eta 0:39:00
epoch [10/30] batch [260/392] time 0.301 (0.292) data 0.000 (0.003) loss 1.5488 (2.5930) lr 7.9389e-03 eta 0:38:50
epoch [10/30] batch [280/392] time 0.320 (0.293) data 0.000 (0.003) loss 1.8799 (2.5956) lr 7.9389e-03 eta 0:38:48
epoch [10/30] batch [300/392] time 0.290 (0.293) data 0.000 (0.003) loss 1.7275 (2.5732) lr 7.9389e-03 eta 0:38:41
epoch [10/30] batch [320/392] time 0.285 (0.293) data 0.000 (0.003) loss 2.7090 (2.6031) lr 7.9389e-03 eta 0:38:34
epoch [10/30] batch [340/392] time 0.307 (0.292) data 0.000 (0.002) loss 1.2305 (2.5794) lr 7.9389e-03 eta 0:38:26
epoch [10/30] batch [360/392] time 0.281 (0.292) data 0.000 (0.002) loss 4.6953 (2.5861) lr 7.9389e-03 eta 0:38:21
epoch [10/30] batch [380/392] time 0.270 (0.291) data 0.000 (0.002) loss 2.3945 (2.5741) lr 7.9389e-03 eta 0:38:06
Evaluate on the *val* set
=> result
* total: 812
* correct: 581
* accuracy: 71.6%
* error: 28.4%
* macro_f1: 70.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-10
epoch [11/30] batch [20/392] time 0.274 (0.337) data 0.000 (0.035) loss 1.3672 (2.2605) lr 7.5000e-03 eta 0:43:52
epoch [11/30] batch [40/392] time 0.302 (0.317) data 0.000 (0.018) loss 3.7031 (2.3085) lr 7.5000e-03 eta 0:41:15
epoch [11/30] batch [60/392] time 0.292 (0.311) data 0.000 (0.012) loss 1.6348 (2.4723) lr 7.5000e-03 eta 0:40:19
epoch [11/30] batch [80/392] time 0.314 (0.307) data 0.000 (0.009) loss 0.8823 (2.4231) lr 7.5000e-03 eta 0:39:38
epoch [11/30] batch [100/392] time 0.300 (0.305) data 0.000 (0.007) loss 2.8359 (2.4285) lr 7.5000e-03 eta 0:39:18
epoch [11/30] batch [120/392] time 0.293 (0.303) data 0.000 (0.006) loss 1.0605 (2.4747) lr 7.5000e-03 eta 0:38:58
epoch [11/30] batch [140/392] time 0.290 (0.302) data 0.000 (0.005) loss 5.2734 (2.4777) lr 7.5000e-03 eta 0:38:42
epoch [11/30] batch [160/392] time 0.323 (0.300) data 0.000 (0.005) loss 2.0449 (2.4594) lr 7.5000e-03 eta 0:38:27
epoch [11/30] batch [180/392] time 0.284 (0.299) data 0.000 (0.004) loss 1.5342 (2.4364) lr 7.5000e-03 eta 0:38:10
epoch [11/30] batch [200/392] time 0.289 (0.299) data 0.000 (0.004) loss 2.7734 (2.4504) lr 7.5000e-03 eta 0:38:01
epoch [11/30] batch [220/392] time 0.303 (0.299) data 0.000 (0.003) loss 3.6895 (2.4847) lr 7.5000e-03 eta 0:37:55
epoch [11/30] batch [240/392] time 0.289 (0.299) data 0.000 (0.003) loss 3.7090 (2.5185) lr 7.5000e-03 eta 0:37:48
epoch [11/30] batch [260/392] time 0.299 (0.298) data 0.000 (0.003) loss 1.4131 (2.4949) lr 7.5000e-03 eta 0:37:38
epoch [11/30] batch [280/392] time 0.308 (0.298) data 0.000 (0.003) loss 2.7383 (2.4842) lr 7.5000e-03 eta 0:37:30
epoch [11/30] batch [300/392] time 0.307 (0.297) data 0.000 (0.003) loss 3.7188 (2.4938) lr 7.5000e-03 eta 0:37:22
epoch [11/30] batch [320/392] time 0.304 (0.297) data 0.000 (0.002) loss 2.3418 (2.5030) lr 7.5000e-03 eta 0:37:14
epoch [11/30] batch [340/392] time 0.290 (0.297) data 0.000 (0.002) loss 1.6250 (2.4951) lr 7.5000e-03 eta 0:37:06
epoch [11/30] batch [360/392] time 0.283 (0.297) data 0.000 (0.002) loss 3.1035 (2.4710) lr 7.5000e-03 eta 0:37:03
epoch [11/30] batch [380/392] time 0.282 (0.296) data 0.000 (0.002) loss 1.7920 (2.4705) lr 7.5000e-03 eta 0:36:49
Evaluate on the *val* set
=> result
* total: 812
* correct: 582
* accuracy: 71.7%
* error: 28.3%
* macro_f1: 70.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
epoch [12/30] batch [20/392] time 0.285 (0.334) data 0.000 (0.034) loss 3.2754 (2.4424) lr 7.0337e-03 eta 0:41:23
epoch [12/30] batch [40/392] time 0.289 (0.313) data 0.000 (0.017) loss 5.5547 (2.6959) lr 7.0337e-03 eta 0:38:40
epoch [12/30] batch [60/392] time 0.339 (0.308) data 0.000 (0.012) loss 2.6250 (2.5986) lr 7.0337e-03 eta 0:37:51
epoch [12/30] batch [80/392] time 0.287 (0.304) data 0.000 (0.009) loss 3.0684 (2.5821) lr 7.0337e-03 eta 0:37:18
epoch [12/30] batch [100/392] time 0.286 (0.302) data 0.000 (0.007) loss 4.2617 (2.6028) lr 7.0337e-03 eta 0:36:56
epoch [12/30] batch [120/392] time 0.292 (0.299) data 0.000 (0.006) loss 1.8164 (2.5915) lr 7.0337e-03 eta 0:36:31
epoch [12/30] batch [140/392] time 0.291 (0.298) data 0.000 (0.005) loss 3.2129 (2.5631) lr 7.0337e-03 eta 0:36:16
epoch [12/30] batch [160/392] time 0.297 (0.297) data 0.000 (0.004) loss 3.3477 (2.5748) lr 7.0337e-03 eta 0:36:02
epoch [12/30] batch [180/392] time 0.295 (0.296) data 0.000 (0.004) loss 0.6816 (2.5500) lr 7.0337e-03 eta 0:35:53
epoch [12/30] batch [200/392] time 0.293 (0.296) data 0.000 (0.004) loss 2.2617 (2.5319) lr 7.0337e-03 eta 0:35:45
epoch [12/30] batch [220/392] time 0.290 (0.296) data 0.000 (0.003) loss 0.4980 (2.5268) lr 7.0337e-03 eta 0:35:41
epoch [12/30] batch [240/392] time 0.300 (0.296) data 0.000 (0.003) loss 2.3340 (2.5655) lr 7.0337e-03 eta 0:35:34
epoch [12/30] batch [260/392] time 0.309 (0.296) data 0.000 (0.003) loss 3.9238 (2.5190) lr 7.0337e-03 eta 0:35:28
epoch [12/30] batch [280/392] time 0.295 (0.296) data 0.000 (0.003) loss 3.6699 (2.5268) lr 7.0337e-03 eta 0:35:21
epoch [12/30] batch [300/392] time 0.289 (0.296) data 0.000 (0.003) loss 2.1934 (2.5596) lr 7.0337e-03 eta 0:35:13
epoch [12/30] batch [320/392] time 0.287 (0.295) data 0.000 (0.002) loss 1.2012 (2.5420) lr 7.0337e-03 eta 0:35:05
epoch [12/30] batch [340/392] time 0.287 (0.295) data 0.001 (0.002) loss 1.4463 (2.5609) lr 7.0337e-03 eta 0:34:58
epoch [12/30] batch [360/392] time 0.290 (0.295) data 0.000 (0.002) loss 3.3555 (2.5487) lr 7.0337e-03 eta 0:34:51
epoch [12/30] batch [380/392] time 0.274 (0.294) data 0.000 (0.002) loss 1.3672 (2.5247) lr 7.0337e-03 eta 0:34:38
Evaluate on the *val* set
=> result
* total: 812
* correct: 582
* accuracy: 71.7%
* error: 28.3%
* macro_f1: 70.4%
epoch [13/30] batch [20/392] time 0.287 (0.337) data 0.000 (0.037) loss 3.2637 (2.7512) lr 6.5451e-03 eta 0:39:31
epoch [13/30] batch [40/392] time 0.286 (0.315) data 0.000 (0.018) loss 0.7583 (2.6188) lr 6.5451e-03 eta 0:36:49
epoch [13/30] batch [60/392] time 0.291 (0.311) data 0.000 (0.012) loss 2.3906 (2.7155) lr 6.5451e-03 eta 0:36:12
epoch [13/30] batch [80/392] time 0.286 (0.305) data 0.000 (0.009) loss 4.1758 (2.7191) lr 6.5451e-03 eta 0:35:30
epoch [13/30] batch [100/392] time 0.298 (0.303) data 0.000 (0.008) loss 1.6152 (2.6151) lr 6.5451e-03 eta 0:35:06
epoch [13/30] batch [120/392] time 0.298 (0.302) data 0.000 (0.006) loss 1.2188 (2.5272) lr 6.5451e-03 eta 0:34:51
epoch [13/30] batch [140/392] time 0.305 (0.300) data 0.000 (0.005) loss 4.0117 (2.5436) lr 6.5451e-03 eta 0:34:36
epoch [13/30] batch [160/392] time 0.328 (0.300) data 0.000 (0.005) loss 3.9219 (2.5883) lr 6.5451e-03 eta 0:34:28
epoch [13/30] batch [180/392] time 0.283 (0.299) data 0.000 (0.004) loss 1.9121 (2.5399) lr 6.5451e-03 eta 0:34:15
epoch [13/30] batch [200/392] time 0.281 (0.298) data 0.000 (0.004) loss 0.8418 (2.5233) lr 6.5451e-03 eta 0:34:05
epoch [13/30] batch [220/392] time 0.286 (0.297) data 0.000 (0.004) loss 1.2793 (2.5269) lr 6.5451e-03 eta 0:33:52
epoch [13/30] batch [240/392] time 0.295 (0.297) data 0.000 (0.003) loss 3.9121 (2.5148) lr 6.5451e-03 eta 0:33:44
epoch [13/30] batch [260/392] time 0.329 (0.297) data 0.000 (0.003) loss 1.9150 (2.4828) lr 6.5451e-03 eta 0:33:38
epoch [13/30] batch [280/392] time 0.292 (0.297) data 0.000 (0.003) loss 2.6875 (2.4861) lr 6.5451e-03 eta 0:33:31
epoch [13/30] batch [300/392] time 0.306 (0.297) data 0.000 (0.003) loss 0.7480 (2.4902) lr 6.5451e-03 eta 0:33:24
epoch [13/30] batch [320/392] time 0.285 (0.297) data 0.000 (0.003) loss 6.2305 (2.4965) lr 6.5451e-03 eta 0:33:17
epoch [13/30] batch [340/392] time 0.330 (0.296) data 0.000 (0.002) loss 2.1484 (2.5102) lr 6.5451e-03 eta 0:33:10
epoch [13/30] batch [360/392] time 0.289 (0.296) data 0.000 (0.002) loss 2.2363 (2.4884) lr 6.5451e-03 eta 0:33:04
epoch [13/30] batch [380/392] time 0.277 (0.295) data 0.000 (0.002) loss 1.0830 (2.4782) lr 6.5451e-03 eta 0:32:50
Evaluate on the *val* set
=> result
* total: 812
* correct: 590
* accuracy: 72.7%
* error: 27.3%
* macro_f1: 71.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
epoch [14/30] batch [20/392] time 0.294 (0.333) data 0.000 (0.036) loss 5.3789 (2.4272) lr 6.0396e-03 eta 0:36:51
epoch [14/30] batch [40/392] time 0.289 (0.311) data 0.000 (0.018) loss 4.8125 (2.3028) lr 6.0396e-03 eta 0:34:16
epoch [14/30] batch [60/392] time 0.293 (0.306) data 0.000 (0.012) loss 1.3496 (2.4710) lr 6.0396e-03 eta 0:33:38
epoch [14/30] batch [80/392] time 0.286 (0.301) data 0.000 (0.009) loss 0.8457 (2.3254) lr 6.0396e-03 eta 0:33:03
epoch [14/30] batch [100/392] time 0.283 (0.298) data 0.000 (0.007) loss 2.1016 (2.3911) lr 6.0396e-03 eta 0:32:37
epoch [14/30] batch [120/392] time 0.277 (0.297) data 0.000 (0.006) loss 3.0020 (2.4593) lr 6.0396e-03 eta 0:32:23
epoch [14/30] batch [140/392] time 0.308 (0.296) data 0.000 (0.005) loss 1.5547 (2.5428) lr 6.0396e-03 eta 0:32:13
epoch [14/30] batch [160/392] time 0.286 (0.296) data 0.000 (0.005) loss 1.7227 (2.4850) lr 6.0396e-03 eta 0:32:05
epoch [14/30] batch [180/392] time 0.288 (0.295) data 0.000 (0.004) loss 4.5430 (2.5374) lr 6.0396e-03 eta 0:31:55
epoch [14/30] batch [200/392] time 0.289 (0.296) data 0.000 (0.004) loss 1.6855 (2.5165) lr 6.0396e-03 eta 0:31:51
epoch [14/30] batch [220/392] time 0.299 (0.296) data 0.000 (0.003) loss 1.4492 (2.5106) lr 6.0396e-03 eta 0:31:47
epoch [14/30] batch [240/392] time 0.285 (0.296) data 0.000 (0.003) loss 1.7178 (2.4721) lr 6.0396e-03 eta 0:31:41
epoch [14/30] batch [260/392] time 0.315 (0.296) data 0.000 (0.003) loss 1.8965 (2.4403) lr 6.0396e-03 eta 0:31:37
epoch [14/30] batch [280/392] time 0.284 (0.296) data 0.000 (0.003) loss 1.6631 (2.4193) lr 6.0396e-03 eta 0:31:29
epoch [14/30] batch [300/392] time 0.291 (0.296) data 0.000 (0.003) loss 3.8379 (2.4185) lr 6.0396e-03 eta 0:31:24
epoch [14/30] batch [320/392] time 0.289 (0.296) data 0.000 (0.002) loss 1.9316 (2.4530) lr 6.0396e-03 eta 0:31:15
epoch [14/30] batch [340/392] time 0.282 (0.295) data 0.000 (0.002) loss 1.4980 (2.4538) lr 6.0396e-03 eta 0:31:08
epoch [14/30] batch [360/392] time 0.291 (0.295) data 0.000 (0.002) loss 5.2891 (2.4432) lr 6.0396e-03 eta 0:31:01
epoch [14/30] batch [380/392] time 0.275 (0.295) data 0.000 (0.002) loss 1.1377 (2.4235) lr 6.0396e-03 eta 0:30:51
Evaluate on the *val* set
=> result
* total: 812
* correct: 584
* accuracy: 71.9%
* error: 28.1%
* macro_f1: 70.9%
epoch [15/30] batch [20/392] time 0.288 (0.327) data 0.000 (0.033) loss 3.5430 (2.1810) lr 5.5226e-03 eta 0:34:03
epoch [15/30] batch [40/392] time 0.291 (0.308) data 0.000 (0.017) loss 1.1133 (2.4374) lr 5.5226e-03 eta 0:32:01
epoch [15/30] batch [60/392] time 0.381 (0.305) data 0.000 (0.011) loss 5.5391 (2.4528) lr 5.5226e-03 eta 0:31:35
epoch [15/30] batch [80/392] time 0.306 (0.302) data 0.000 (0.009) loss 2.1895 (2.4868) lr 5.5226e-03 eta 0:31:07
epoch [15/30] batch [100/392] time 0.298 (0.300) data 0.000 (0.007) loss 2.2324 (2.5057) lr 5.5226e-03 eta 0:30:49
epoch [15/30] batch [120/392] time 0.281 (0.299) data 0.000 (0.006) loss 2.0117 (2.5115) lr 5.5226e-03 eta 0:30:37
epoch [15/30] batch [140/392] time 0.286 (0.298) data 0.000 (0.005) loss 1.3955 (2.4906) lr 5.5226e-03 eta 0:30:25
epoch [15/30] batch [160/392] time 0.285 (0.297) data 0.000 (0.004) loss 1.6172 (2.4699) lr 5.5226e-03 eta 0:30:14
epoch [15/30] batch [180/392] time 0.288 (0.297) data 0.000 (0.004) loss 0.6543 (2.4844) lr 5.5226e-03 eta 0:30:08
epoch [15/30] batch [200/392] time 0.330 (0.297) data 0.000 (0.004) loss 2.0664 (2.4712) lr 5.5226e-03 eta 0:30:03
epoch [15/30] batch [220/392] time 0.304 (0.296) data 0.000 (0.003) loss 2.0293 (2.4362) lr 5.5226e-03 eta 0:29:53
epoch [15/30] batch [240/392] time 0.304 (0.296) data 0.000 (0.003) loss 4.0039 (2.4381) lr 5.5226e-03 eta 0:29:45
epoch [15/30] batch [260/392] time 0.284 (0.296) data 0.000 (0.003) loss 0.5532 (2.4183) lr 5.5226e-03 eta 0:29:39
epoch [15/30] batch [280/392] time 0.312 (0.296) data 0.000 (0.003) loss 0.8491 (2.4011) lr 5.5226e-03 eta 0:29:31
epoch [15/30] batch [300/392] time 0.280 (0.295) data 0.000 (0.002) loss 0.4924 (2.3757) lr 5.5226e-03 eta 0:29:22
epoch [15/30] batch [320/392] time 0.289 (0.295) data 0.000 (0.002) loss 1.0713 (2.3778) lr 5.5226e-03 eta 0:29:15
epoch [15/30] batch [340/392] time 0.288 (0.295) data 0.000 (0.002) loss 1.5732 (2.3964) lr 5.5226e-03 eta 0:29:09
epoch [15/30] batch [360/392] time 0.294 (0.295) data 0.000 (0.002) loss 2.9941 (2.3842) lr 5.5226e-03 eta 0:29:02
epoch [15/30] batch [380/392] time 0.272 (0.294) data 0.000 (0.002) loss 0.8848 (2.3803) lr 5.5226e-03 eta 0:28:51
Evaluate on the *val* set
=> result
* total: 812
* correct: 583
* accuracy: 71.8%
* error: 28.2%
* macro_f1: 70.7%
epoch [16/30] batch [20/392] time 0.281 (0.324) data 0.000 (0.033) loss 2.1445 (2.2354) lr 5.0000e-03 eta 0:31:41
epoch [16/30] batch [40/392] time 0.281 (0.307) data 0.000 (0.017) loss 1.9941 (2.5052) lr 5.0000e-03 eta 0:29:53
epoch [16/30] batch [60/392] time 0.282 (0.304) data 0.000 (0.011) loss 3.2207 (2.4587) lr 5.0000e-03 eta 0:29:27
epoch [16/30] batch [80/392] time 0.294 (0.301) data 0.000 (0.008) loss 1.3936 (2.3920) lr 5.0000e-03 eta 0:29:05
epoch [16/30] batch [100/392] time 0.290 (0.300) data 0.000 (0.007) loss 3.6309 (2.3725) lr 5.0000e-03 eta 0:28:52
epoch [16/30] batch [120/392] time 0.279 (0.298) data 0.000 (0.006) loss 4.4922 (2.3938) lr 5.0000e-03 eta 0:28:35
epoch [16/30] batch [140/392] time 0.287 (0.297) data 0.000 (0.005) loss 2.4512 (2.4172) lr 5.0000e-03 eta 0:28:23
epoch [16/30] batch [160/392] time 0.289 (0.296) data 0.000 (0.004) loss 0.8184 (2.4203) lr 5.0000e-03 eta 0:28:10
epoch [16/30] batch [180/392] time 0.301 (0.295) data 0.000 (0.004) loss 3.4727 (2.4082) lr 5.0000e-03 eta 0:28:02
epoch [16/30] batch [200/392] time 0.284 (0.295) data 0.000 (0.004) loss 2.5117 (2.4599) lr 5.0000e-03 eta 0:27:54
epoch [16/30] batch [220/392] time 0.283 (0.295) data 0.000 (0.003) loss 2.7148 (2.4694) lr 5.0000e-03 eta 0:27:48
epoch [16/30] batch [240/392] time 0.377 (0.295) data 0.000 (0.003) loss 1.4131 (2.4559) lr 5.0000e-03 eta 0:27:42
epoch [16/30] batch [260/392] time 0.289 (0.294) data 0.000 (0.003) loss 1.2490 (2.4380) lr 5.0000e-03 eta 0:27:33
epoch [16/30] batch [280/392] time 0.303 (0.294) data 0.000 (0.003) loss 1.2773 (2.4461) lr 5.0000e-03 eta 0:27:26
epoch [16/30] batch [300/392] time 0.286 (0.294) data 0.000 (0.002) loss 2.1934 (2.4363) lr 5.0000e-03 eta 0:27:20
epoch [16/30] batch [320/392] time 0.283 (0.294) data 0.000 (0.002) loss 1.6377 (2.4661) lr 5.0000e-03 eta 0:27:14
epoch [16/30] batch [340/392] time 0.313 (0.294) data 0.000 (0.002) loss 2.8574 (2.4751) lr 5.0000e-03 eta 0:27:07
epoch [16/30] batch [360/392] time 0.306 (0.294) data 0.000 (0.002) loss 1.9580 (2.4534) lr 5.0000e-03 eta 0:27:01
epoch [16/30] batch [380/392] time 0.272 (0.293) data 0.000 (0.002) loss 1.4180 (2.4547) lr 5.0000e-03 eta 0:26:50
Evaluate on the *val* set
=> result
* total: 812
* correct: 580
* accuracy: 71.4%
* error: 28.6%
* macro_f1: 70.3%
epoch [17/30] batch [20/392] time 0.285 (0.335) data 0.000 (0.040) loss 3.9102 (2.5258) lr 4.4774e-03 eta 0:30:32
epoch [17/30] batch [40/392] time 0.296 (0.313) data 0.000 (0.020) loss 4.2578 (2.5124) lr 4.4774e-03 eta 0:28:25
epoch [17/30] batch [60/392] time 0.295 (0.309) data 0.000 (0.013) loss 1.1738 (2.5442) lr 4.4774e-03 eta 0:27:59
epoch [17/30] batch [80/392] time 0.352 (0.306) data 0.000 (0.010) loss 3.3789 (2.4916) lr 4.4774e-03 eta 0:27:37
epoch [17/30] batch [100/392] time 0.287 (0.303) data 0.000 (0.008) loss 1.6055 (2.5156) lr 4.4774e-03 eta 0:27:11
epoch [17/30] batch [120/392] time 0.298 (0.301) data 0.000 (0.007) loss 5.7656 (2.5187) lr 4.4774e-03 eta 0:26:53
epoch [17/30] batch [140/392] time 0.288 (0.300) data 0.000 (0.006) loss 3.1699 (2.4409) lr 4.4774e-03 eta 0:26:41
epoch [17/30] batch [160/392] time 0.279 (0.298) data 0.000 (0.005) loss 5.5977 (2.4737) lr 4.4774e-03 eta 0:26:29
epoch [17/30] batch [180/392] time 0.302 (0.297) data 0.000 (0.005) loss 2.4316 (2.4530) lr 4.4774e-03 eta 0:26:17
epoch [17/30] batch [200/392] time 0.288 (0.297) data 0.000 (0.004) loss 1.4658 (2.4656) lr 4.4774e-03 eta 0:26:08
epoch [17/30] batch [220/392] time 0.279 (0.296) data 0.000 (0.004) loss 0.3318 (2.4581) lr 4.4774e-03 eta 0:26:00
epoch [17/30] batch [240/392] time 0.303 (0.295) data 0.000 (0.004) loss 3.5000 (2.4489) lr 4.4774e-03 eta 0:25:50
epoch [17/30] batch [260/392] time 0.288 (0.295) data 0.000 (0.003) loss 1.9268 (2.4639) lr 4.4774e-03 eta 0:25:40
epoch [17/30] batch [280/392] time 0.312 (0.294) data 0.000 (0.003) loss 4.1914 (2.4745) lr 4.4774e-03 eta 0:25:32
epoch [17/30] batch [300/392] time 0.288 (0.294) data 0.000 (0.003) loss 0.7690 (2.4466) lr 4.4774e-03 eta 0:25:23
epoch [17/30] batch [320/392] time 0.284 (0.293) data 0.000 (0.003) loss 1.6299 (2.4203) lr 4.4774e-03 eta 0:25:16
epoch [17/30] batch [340/392] time 0.299 (0.294) data 0.000 (0.003) loss 1.5977 (2.4106) lr 4.4774e-03 eta 0:25:11
epoch [17/30] batch [360/392] time 0.287 (0.294) data 0.000 (0.002) loss 6.1328 (2.4593) lr 4.4774e-03 eta 0:25:05
epoch [17/30] batch [380/392] time 0.271 (0.292) data 0.000 (0.002) loss 1.5811 (2.4513) lr 4.4774e-03 eta 0:24:53
Evaluate on the *val* set
=> result
* total: 812
* correct: 596
* accuracy: 73.4%
* error: 26.6%
* macro_f1: 72.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
epoch [18/30] batch [20/392] time 0.299 (0.327) data 0.000 (0.033) loss 2.7578 (2.1215) lr 3.9604e-03 eta 0:27:39
epoch [18/30] batch [40/392] time 0.337 (0.312) data 0.000 (0.017) loss 2.0234 (1.8838) lr 3.9604e-03 eta 0:26:18
epoch [18/30] batch [60/392] time 0.285 (0.304) data 0.000 (0.011) loss 2.7812 (2.0134) lr 3.9604e-03 eta 0:25:28
epoch [18/30] batch [80/392] time 0.285 (0.301) data 0.000 (0.009) loss 1.7695 (2.0158) lr 3.9604e-03 eta 0:25:10
epoch [18/30] batch [100/392] time 0.300 (0.299) data 0.000 (0.007) loss 1.4131 (2.1563) lr 3.9604e-03 eta 0:24:55
epoch [18/30] batch [120/392] time 0.287 (0.298) data 0.000 (0.006) loss 3.1680 (2.1755) lr 3.9604e-03 eta 0:24:43
epoch [18/30] batch [140/392] time 0.278 (0.297) data 0.000 (0.005) loss 3.9941 (2.1989) lr 3.9604e-03 eta 0:24:34
epoch [18/30] batch [160/392] time 0.288 (0.297) data 0.000 (0.004) loss 2.9492 (2.1965) lr 3.9604e-03 eta 0:24:25
epoch [18/30] batch [180/392] time 0.284 (0.296) data 0.000 (0.004) loss 0.8926 (2.2211) lr 3.9604e-03 eta 0:24:14
epoch [18/30] batch [200/392] time 0.295 (0.295) data 0.000 (0.004) loss 4.2891 (2.2413) lr 3.9604e-03 eta 0:24:06
epoch [18/30] batch [220/392] time 0.281 (0.295) data 0.000 (0.003) loss 0.6245 (2.2566) lr 3.9604e-03 eta 0:23:59
epoch [18/30] batch [240/392] time 0.336 (0.295) data 0.000 (0.003) loss 1.8066 (2.2632) lr 3.9604e-03 eta 0:23:50
epoch [18/30] batch [260/392] time 0.306 (0.295) data 0.000 (0.003) loss 2.9863 (2.3295) lr 3.9604e-03 eta 0:23:44
epoch [18/30] batch [280/392] time 0.310 (0.294) data 0.000 (0.003) loss 1.4688 (2.3471) lr 3.9604e-03 eta 0:23:37
epoch [18/30] batch [300/392] time 0.282 (0.294) data 0.000 (0.002) loss 2.9160 (2.3590) lr 3.9604e-03 eta 0:23:29
epoch [18/30] batch [320/392] time 0.287 (0.294) data 0.000 (0.002) loss 1.3223 (2.3684) lr 3.9604e-03 eta 0:23:23
epoch [18/30] batch [340/392] time 0.286 (0.294) data 0.000 (0.002) loss 0.8315 (2.3755) lr 3.9604e-03 eta 0:23:18
epoch [18/30] batch [360/392] time 0.314 (0.294) data 0.000 (0.002) loss 4.3008 (2.4010) lr 3.9604e-03 eta 0:23:11
epoch [18/30] batch [380/392] time 0.272 (0.293) data 0.000 (0.002) loss 0.6260 (2.3934) lr 3.9604e-03 eta 0:23:00
Evaluate on the *val* set
=> result
* total: 812
* correct: 590
* accuracy: 72.7%
* error: 27.3%
* macro_f1: 71.7%
epoch [19/30] batch [20/392] time 0.288 (0.336) data 0.000 (0.036) loss 1.2734 (2.1756) lr 3.4549e-03 eta 0:26:11
epoch [19/30] batch [40/392] time 0.279 (0.312) data 0.000 (0.018) loss 2.9727 (2.3364) lr 3.4549e-03 eta 0:24:17
epoch [19/30] batch [60/392] time 0.286 (0.304) data 0.000 (0.012) loss 3.0820 (2.4524) lr 3.4549e-03 eta 0:23:32
epoch [19/30] batch [80/392] time 0.323 (0.302) data 0.000 (0.009) loss 3.5215 (2.5918) lr 3.4549e-03 eta 0:23:16
epoch [19/30] batch [100/392] time 0.283 (0.301) data 0.000 (0.007) loss 4.5000 (2.5238) lr 3.4549e-03 eta 0:23:03
epoch [19/30] batch [120/392] time 0.292 (0.300) data 0.000 (0.006) loss 4.0703 (2.4939) lr 3.4549e-03 eta 0:22:54
epoch [19/30] batch [140/392] time 0.290 (0.299) data 0.000 (0.005) loss 1.1787 (2.5236) lr 3.4549e-03 eta 0:22:42
epoch [19/30] batch [160/392] time 0.286 (0.298) data 0.000 (0.005) loss 1.6338 (2.4899) lr 3.4549e-03 eta 0:22:32
epoch [19/30] batch [180/392] time 0.302 (0.297) data 0.000 (0.004) loss 2.7031 (2.5029) lr 3.4549e-03 eta 0:22:24
epoch [19/30] batch [200/392] time 0.283 (0.297) data 0.000 (0.004) loss 2.3066 (2.5140) lr 3.4549e-03 eta 0:22:19
epoch [19/30] batch [220/392] time 0.286 (0.297) data 0.000 (0.003) loss 1.4336 (2.4504) lr 3.4549e-03 eta 0:22:11
epoch [19/30] batch [240/392] time 0.280 (0.296) data 0.000 (0.003) loss 1.4053 (2.4667) lr 3.4549e-03 eta 0:22:03
epoch [19/30] batch [260/392] time 0.282 (0.296) data 0.000 (0.003) loss 1.8008 (2.4317) lr 3.4549e-03 eta 0:21:56
epoch [19/30] batch [280/392] time 0.286 (0.296) data 0.000 (0.003) loss 3.4277 (2.4037) lr 3.4549e-03 eta 0:21:51
epoch [19/30] batch [300/392] time 0.295 (0.296) data 0.000 (0.003) loss 1.5850 (2.4214) lr 3.4549e-03 eta 0:21:45
epoch [19/30] batch [320/392] time 0.281 (0.296) data 0.000 (0.002) loss 2.2754 (2.4273) lr 3.4549e-03 eta 0:21:37
epoch [19/30] batch [340/392] time 0.293 (0.295) data 0.000 (0.002) loss 7.6172 (2.4465) lr 3.4549e-03 eta 0:21:29
epoch [19/30] batch [360/392] time 0.298 (0.295) data 0.000 (0.002) loss 1.5371 (2.4162) lr 3.4549e-03 eta 0:21:22
epoch [19/30] batch [380/392] time 0.273 (0.294) data 0.000 (0.002) loss 2.9688 (2.4255) lr 3.4549e-03 eta 0:21:11
Evaluate on the *val* set
=> result
* total: 812
* correct: 583
* accuracy: 71.8%
* error: 28.2%
* macro_f1: 70.4%
epoch [20/30] batch [20/392] time 0.286 (0.346) data 0.000 (0.035) loss 5.4375 (2.3971) lr 2.9663e-03 eta 0:24:46
epoch [20/30] batch [40/392] time 0.293 (0.319) data 0.000 (0.018) loss 1.7080 (2.2412) lr 2.9663e-03 eta 0:22:43
epoch [20/30] batch [60/392] time 0.282 (0.309) data 0.000 (0.012) loss 1.0137 (2.1938) lr 2.9663e-03 eta 0:21:52
epoch [20/30] batch [80/392] time 0.306 (0.307) data 0.000 (0.009) loss 0.6250 (2.2673) lr 2.9663e-03 eta 0:21:40
epoch [20/30] batch [100/392] time 0.278 (0.304) data 0.000 (0.007) loss 0.8125 (2.2553) lr 2.9663e-03 eta 0:21:18
epoch [20/30] batch [120/392] time 0.284 (0.301) data 0.000 (0.006) loss 2.3809 (2.2263) lr 2.9663e-03 eta 0:21:03
epoch [20/30] batch [140/392] time 0.311 (0.300) data 0.000 (0.005) loss 0.7158 (2.2706) lr 2.9663e-03 eta 0:20:51
epoch [20/30] batch [160/392] time 0.325 (0.299) data 0.000 (0.005) loss 2.0430 (2.2924) lr 2.9663e-03 eta 0:20:41
epoch [20/30] batch [180/392] time 0.295 (0.298) data 0.000 (0.004) loss 2.3027 (2.2530) lr 2.9663e-03 eta 0:20:31
epoch [20/30] batch [200/392] time 0.293 (0.298) data 0.000 (0.004) loss 2.5391 (2.2333) lr 2.9663e-03 eta 0:20:26
epoch [20/30] batch [220/392] time 0.291 (0.298) data 0.000 (0.003) loss 1.4209 (2.2671) lr 2.9663e-03 eta 0:20:20
epoch [20/30] batch [240/392] time 0.317 (0.298) data 0.000 (0.003) loss 2.7070 (2.2768) lr 2.9663e-03 eta 0:20:13
epoch [20/30] batch [260/392] time 0.289 (0.298) data 0.000 (0.003) loss 2.0664 (2.2862) lr 2.9663e-03 eta 0:20:05
epoch [20/30] batch [280/392] time 0.284 (0.297) data 0.000 (0.003) loss 3.6816 (2.3135) lr 2.9663e-03 eta 0:19:58
epoch [20/30] batch [300/392] time 0.288 (0.297) data 0.000 (0.003) loss 1.8242 (2.3123) lr 2.9663e-03 eta 0:19:50
epoch [20/30] batch [320/392] time 0.329 (0.297) data 0.000 (0.002) loss 4.3945 (2.3377) lr 2.9663e-03 eta 0:19:44
epoch [20/30] batch [340/392] time 0.288 (0.296) data 0.000 (0.002) loss 2.3867 (2.3380) lr 2.9663e-03 eta 0:19:36
epoch [20/30] batch [360/392] time 0.288 (0.296) data 0.000 (0.002) loss 1.4502 (2.3557) lr 2.9663e-03 eta 0:19:31
epoch [20/30] batch [380/392] time 0.276 (0.295) data 0.000 (0.002) loss 3.1465 (2.3771) lr 2.9663e-03 eta 0:19:20
Evaluate on the *val* set
=> result
* total: 812
* correct: 599
* accuracy: 73.8%
* error: 26.2%
* macro_f1: 72.6%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-20
epoch [21/30] batch [20/392] time 0.304 (0.335) data 0.000 (0.035) loss 4.6406 (2.1720) lr 2.5000e-03 eta 0:21:45
epoch [21/30] batch [40/392] time 0.284 (0.312) data 0.000 (0.018) loss 4.0781 (2.1723) lr 2.5000e-03 eta 0:20:11
epoch [21/30] batch [60/392] time 0.290 (0.306) data 0.000 (0.012) loss 1.4551 (2.1687) lr 2.5000e-03 eta 0:19:40
epoch [21/30] batch [80/392] time 0.319 (0.302) data 0.000 (0.009) loss 3.4102 (2.1011) lr 2.5000e-03 eta 0:19:19
epoch [21/30] batch [100/392] time 0.283 (0.301) data 0.000 (0.007) loss 3.7168 (2.1525) lr 2.5000e-03 eta 0:19:08
epoch [21/30] batch [120/392] time 0.279 (0.299) data 0.000 (0.006) loss 0.4587 (2.1769) lr 2.5000e-03 eta 0:18:57
epoch [21/30] batch [140/392] time 0.288 (0.299) data 0.000 (0.005) loss 5.6133 (2.2460) lr 2.5000e-03 eta 0:18:50
epoch [21/30] batch [160/392] time 0.288 (0.298) data 0.001 (0.005) loss 2.6250 (2.2680) lr 2.5000e-03 eta 0:18:39
epoch [21/30] batch [180/392] time 0.295 (0.297) data 0.000 (0.004) loss 7.3906 (2.3155) lr 2.5000e-03 eta 0:18:32
epoch [21/30] batch [200/392] time 0.289 (0.297) data 0.000 (0.004) loss 1.5498 (2.3086) lr 2.5000e-03 eta 0:18:24
epoch [21/30] batch [220/392] time 0.319 (0.297) data 0.000 (0.003) loss 1.2393 (2.3167) lr 2.5000e-03 eta 0:18:20
epoch [21/30] batch [240/392] time 0.284 (0.296) data 0.000 (0.003) loss 1.3262 (2.2990) lr 2.5000e-03 eta 0:18:10
epoch [21/30] batch [260/392] time 0.296 (0.296) data 0.000 (0.003) loss 0.8057 (2.2821) lr 2.5000e-03 eta 0:18:02
epoch [21/30] batch [280/392] time 0.293 (0.295) data 0.000 (0.003) loss 0.4700 (2.2871) lr 2.5000e-03 eta 0:17:55
epoch [21/30] batch [300/392] time 0.306 (0.295) data 0.000 (0.003) loss 7.0703 (2.3073) lr 2.5000e-03 eta 0:17:48
epoch [21/30] batch [320/392] time 0.280 (0.295) data 0.000 (0.002) loss 2.5273 (2.3055) lr 2.5000e-03 eta 0:17:42
epoch [21/30] batch [340/392] time 0.288 (0.295) data 0.000 (0.002) loss 2.2734 (2.3311) lr 2.5000e-03 eta 0:17:35
epoch [21/30] batch [360/392] time 0.283 (0.295) data 0.000 (0.002) loss 3.5957 (2.3356) lr 2.5000e-03 eta 0:17:29
epoch [21/30] batch [380/392] time 0.272 (0.294) data 0.000 (0.002) loss 1.6738 (2.3135) lr 2.5000e-03 eta 0:17:19
Evaluate on the *val* set
=> result
* total: 812
* correct: 595
* accuracy: 73.3%
* error: 26.7%
* macro_f1: 72.2%
epoch [22/30] batch [20/392] time 0.311 (0.333) data 0.000 (0.036) loss 6.2930 (2.2405) lr 2.0611e-03 eta 0:19:28
epoch [22/30] batch [40/392] time 0.286 (0.312) data 0.000 (0.018) loss 0.9604 (2.1986) lr 2.0611e-03 eta 0:18:09
epoch [22/30] batch [60/392] time 0.277 (0.307) data 0.000 (0.012) loss 1.5947 (2.2373) lr 2.0611e-03 eta 0:17:43
epoch [22/30] batch [80/392] time 0.287 (0.302) data 0.000 (0.009) loss 2.3027 (2.2274) lr 2.0611e-03 eta 0:17:22
epoch [22/30] batch [100/392] time 0.294 (0.300) data 0.000 (0.007) loss 4.1289 (2.1406) lr 2.0611e-03 eta 0:17:08
epoch [22/30] batch [120/392] time 0.288 (0.297) data 0.000 (0.006) loss 1.1865 (2.1824) lr 2.0611e-03 eta 0:16:53
epoch [22/30] batch [140/392] time 0.278 (0.297) data 0.000 (0.005) loss 3.3691 (2.2164) lr 2.0611e-03 eta 0:16:44
epoch [22/30] batch [160/392] time 0.337 (0.296) data 0.000 (0.005) loss 1.7529 (2.2296) lr 2.0611e-03 eta 0:16:35
epoch [22/30] batch [180/392] time 0.293 (0.295) data 0.000 (0.004) loss 1.3535 (2.2299) lr 2.0611e-03 eta 0:16:27
epoch [22/30] batch [200/392] time 0.281 (0.295) data 0.000 (0.004) loss 2.2402 (2.2247) lr 2.0611e-03 eta 0:16:20
epoch [22/30] batch [220/392] time 0.301 (0.294) data 0.000 (0.003) loss 1.7119 (2.2482) lr 2.0611e-03 eta 0:16:13
epoch [22/30] batch [240/392] time 0.299 (0.294) data 0.000 (0.003) loss 3.4512 (2.2719) lr 2.0611e-03 eta 0:16:08
epoch [22/30] batch [260/392] time 0.299 (0.294) data 0.000 (0.003) loss 3.1152 (2.2470) lr 2.0611e-03 eta 0:16:00
epoch [22/30] batch [280/392] time 0.280 (0.294) data 0.000 (0.003) loss 1.9658 (2.2398) lr 2.0611e-03 eta 0:15:53
epoch [22/30] batch [300/392] time 0.290 (0.294) data 0.000 (0.003) loss 1.1387 (2.2331) lr 2.0611e-03 eta 0:15:47
epoch [22/30] batch [320/392] time 0.284 (0.293) data 0.000 (0.002) loss 1.6270 (2.2356) lr 2.0611e-03 eta 0:15:41
epoch [22/30] batch [340/392] time 0.286 (0.294) data 0.000 (0.002) loss 4.1797 (2.2361) lr 2.0611e-03 eta 0:15:35
epoch [22/30] batch [360/392] time 0.286 (0.293) data 0.000 (0.002) loss 1.0000 (2.2518) lr 2.0611e-03 eta 0:15:29
epoch [22/30] batch [380/392] time 0.272 (0.292) data 0.000 (0.002) loss 1.1875 (2.2401) lr 2.0611e-03 eta 0:15:20
Evaluate on the *val* set
=> result
* total: 812
* correct: 603
* accuracy: 74.3%
* error: 25.7%
* macro_f1: 72.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
epoch [23/30] batch [20/392] time 0.289 (0.331) data 0.000 (0.034) loss 5.0938 (2.2997) lr 1.6543e-03 eta 0:17:12
epoch [23/30] batch [40/392] time 0.286 (0.314) data 0.000 (0.017) loss 2.7832 (2.3432) lr 1.6543e-03 eta 0:16:12
epoch [23/30] batch [60/392] time 0.288 (0.306) data 0.000 (0.011) loss 2.7148 (2.3867) lr 1.6543e-03 eta 0:15:40
epoch [23/30] batch [80/392] time 0.284 (0.303) data 0.000 (0.009) loss 1.7383 (2.2642) lr 1.6543e-03 eta 0:15:25
epoch [23/30] batch [100/392] time 0.296 (0.301) data 0.000 (0.007) loss 1.2129 (2.1849) lr 1.6543e-03 eta 0:15:14
epoch [23/30] batch [120/392] time 0.288 (0.299) data 0.000 (0.006) loss 2.2207 (2.1896) lr 1.6543e-03 eta 0:15:03
epoch [23/30] batch [140/392] time 0.278 (0.299) data 0.000 (0.005) loss 2.1523 (2.2125) lr 1.6543e-03 eta 0:14:55
epoch [23/30] batch [160/392] time 0.295 (0.299) data 0.000 (0.004) loss 2.5293 (2.2676) lr 1.6543e-03 eta 0:14:50
epoch [23/30] batch [180/392] time 0.299 (0.300) data 0.000 (0.004) loss 1.0068 (2.2645) lr 1.6543e-03 eta 0:14:45
epoch [23/30] batch [200/392] time 0.301 (0.299) data 0.000 (0.004) loss 1.4180 (2.2266) lr 1.6543e-03 eta 0:14:37
epoch [23/30] batch [220/392] time 0.297 (0.299) data 0.000 (0.003) loss 1.2393 (2.2142) lr 1.6543e-03 eta 0:14:30
epoch [23/30] batch [240/392] time 0.301 (0.298) data 0.000 (0.003) loss 0.9351 (2.2148) lr 1.6543e-03 eta 0:14:23
epoch [23/30] batch [260/392] time 0.327 (0.298) data 0.000 (0.003) loss 2.4043 (2.2239) lr 1.6543e-03 eta 0:14:16
epoch [23/30] batch [280/392] time 0.292 (0.297) data 0.000 (0.003) loss 0.3535 (2.1845) lr 1.6543e-03 eta 0:14:08
epoch [23/30] batch [300/392] time 0.278 (0.297) data 0.000 (0.003) loss 4.1211 (2.2132) lr 1.6543e-03 eta 0:14:01
epoch [23/30] batch [320/392] time 0.286 (0.297) data 0.000 (0.002) loss 1.7119 (2.2273) lr 1.6543e-03 eta 0:13:55
epoch [23/30] batch [340/392] time 0.360 (0.297) data 0.000 (0.002) loss 5.7930 (2.2479) lr 1.6543e-03 eta 0:13:49
epoch [23/30] batch [360/392] time 0.279 (0.296) data 0.000 (0.002) loss 2.9414 (2.2584) lr 1.6543e-03 eta 0:13:42
epoch [23/30] batch [380/392] time 0.279 (0.295) data 0.000 (0.002) loss 2.0234 (2.2480) lr 1.6543e-03 eta 0:13:33
Evaluate on the *val* set
=> result
* total: 812
* correct: 602
* accuracy: 74.1%
* error: 25.9%
* macro_f1: 73.0%
epoch [24/30] batch [20/392] time 0.302 (0.331) data 0.000 (0.035) loss 0.5068 (2.2573) lr 1.2843e-03 eta 0:15:02
epoch [24/30] batch [40/392] time 0.291 (0.312) data 0.000 (0.018) loss 1.7520 (2.2963) lr 1.2843e-03 eta 0:14:03
epoch [24/30] batch [60/392] time 0.294 (0.306) data 0.000 (0.012) loss 0.8765 (2.2395) lr 1.2843e-03 eta 0:13:40
epoch [24/30] batch [80/392] time 0.286 (0.301) data 0.000 (0.009) loss 0.7720 (2.2297) lr 1.2843e-03 eta 0:13:22
epoch [24/30] batch [100/392] time 0.280 (0.299) data 0.000 (0.007) loss 2.7207 (2.2408) lr 1.2843e-03 eta 0:13:10
epoch [24/30] batch [120/392] time 0.288 (0.298) data 0.000 (0.006) loss 4.2930 (2.2558) lr 1.2843e-03 eta 0:13:01
epoch [24/30] batch [140/392] time 0.290 (0.297) data 0.000 (0.005) loss 2.0195 (2.2853) lr 1.2843e-03 eta 0:12:52
epoch [24/30] batch [160/392] time 0.301 (0.296) data 0.000 (0.005) loss 2.1953 (2.2159) lr 1.2843e-03 eta 0:12:46
epoch [24/30] batch [180/392] time 0.302 (0.296) data 0.000 (0.004) loss 2.7266 (2.2229) lr 1.2843e-03 eta 0:12:40
epoch [24/30] batch [200/392] time 0.308 (0.296) data 0.000 (0.004) loss 1.3848 (2.2193) lr 1.2843e-03 eta 0:12:33
epoch [24/30] batch [220/392] time 0.282 (0.296) data 0.000 (0.003) loss 1.1797 (2.1923) lr 1.2843e-03 eta 0:12:26
epoch [24/30] batch [240/392] time 0.284 (0.295) data 0.000 (0.003) loss 1.9229 (2.1790) lr 1.2843e-03 eta 0:12:18
epoch [24/30] batch [260/392] time 0.276 (0.294) data 0.000 (0.003) loss 1.5791 (2.2059) lr 1.2843e-03 eta 0:12:11
epoch [24/30] batch [280/392] time 0.285 (0.294) data 0.000 (0.003) loss 1.6885 (2.2116) lr 1.2843e-03 eta 0:12:04
epoch [24/30] batch [300/392] time 0.303 (0.294) data 0.000 (0.003) loss 2.8203 (2.2056) lr 1.2843e-03 eta 0:11:58
epoch [24/30] batch [320/392] time 0.287 (0.294) data 0.000 (0.002) loss 4.5820 (2.2358) lr 1.2843e-03 eta 0:11:53
epoch [24/30] batch [340/392] time 0.304 (0.294) data 0.000 (0.002) loss 2.6816 (2.2480) lr 1.2843e-03 eta 0:11:47
epoch [24/30] batch [360/392] time 0.311 (0.294) data 0.000 (0.002) loss 0.5430 (2.2283) lr 1.2843e-03 eta 0:11:41
epoch [24/30] batch [380/392] time 0.272 (0.293) data 0.000 (0.002) loss 2.2871 (2.2302) lr 1.2843e-03 eta 0:11:33
Evaluate on the *val* set
=> result
* total: 812
* correct: 606
* accuracy: 74.6%
* error: 25.4%
* macro_f1: 73.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
epoch [25/30] batch [20/392] time 0.311 (0.331) data 0.000 (0.038) loss 2.6270 (2.2555) lr 9.5492e-04 eta 0:12:52
epoch [25/30] batch [40/392] time 0.287 (0.309) data 0.000 (0.019) loss 1.9492 (2.2858) lr 9.5492e-04 eta 0:11:54
epoch [25/30] batch [60/392] time 0.282 (0.303) data 0.000 (0.013) loss 0.0704 (2.2897) lr 9.5492e-04 eta 0:11:33
epoch [25/30] batch [80/392] time 0.281 (0.300) data 0.000 (0.010) loss 1.3066 (2.3615) lr 9.5492e-04 eta 0:11:22
epoch [25/30] batch [100/392] time 0.286 (0.298) data 0.000 (0.008) loss 1.6670 (2.3404) lr 9.5492e-04 eta 0:11:11
epoch [25/30] batch [120/392] time 0.304 (0.297) data 0.000 (0.007) loss 2.5898 (2.3185) lr 9.5492e-04 eta 0:11:03
epoch [25/30] batch [140/392] time 0.289 (0.296) data 0.000 (0.006) loss 2.2988 (2.3051) lr 9.5492e-04 eta 0:10:55
epoch [25/30] batch [160/392] time 0.277 (0.295) data 0.000 (0.005) loss 0.9092 (2.2947) lr 9.5492e-04 eta 0:10:47
epoch [25/30] batch [180/392] time 0.284 (0.295) data 0.000 (0.004) loss 5.2031 (2.3359) lr 9.5492e-04 eta 0:10:41
epoch [25/30] batch [200/392] time 0.281 (0.295) data 0.000 (0.004) loss 2.3125 (2.2972) lr 9.5492e-04 eta 0:10:35
epoch [25/30] batch [220/392] time 0.292 (0.295) data 0.000 (0.004) loss 0.8296 (2.3014) lr 9.5492e-04 eta 0:10:28
epoch [25/30] batch [240/392] time 0.283 (0.295) data 0.000 (0.003) loss 1.4375 (2.2691) lr 9.5492e-04 eta 0:10:22
epoch [25/30] batch [260/392] time 0.288 (0.295) data 0.000 (0.003) loss 3.4531 (2.3025) lr 9.5492e-04 eta 0:10:16
epoch [25/30] batch [280/392] time 0.302 (0.294) data 0.000 (0.003) loss 2.4824 (2.3357) lr 9.5492e-04 eta 0:10:09
epoch [25/30] batch [300/392] time 0.284 (0.294) data 0.000 (0.003) loss 7.2227 (2.3698) lr 9.5492e-04 eta 0:10:04
epoch [25/30] batch [320/392] time 0.311 (0.294) data 0.000 (0.003) loss 2.4004 (2.3958) lr 9.5492e-04 eta 0:09:58
epoch [25/30] batch [340/392] time 0.291 (0.294) data 0.000 (0.002) loss 3.9961 (2.3826) lr 9.5492e-04 eta 0:09:52
epoch [25/30] batch [360/392] time 0.290 (0.294) data 0.000 (0.002) loss 1.7666 (2.3668) lr 9.5492e-04 eta 0:09:46
epoch [25/30] batch [380/392] time 0.275 (0.293) data 0.000 (0.002) loss 0.4568 (2.3646) lr 9.5492e-04 eta 0:09:38
Evaluate on the *val* set
=> result
* total: 812
* correct: 616
* accuracy: 75.9%
* error: 24.1%
* macro_f1: 74.7%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar
epoch [26/30] batch [20/392] time 0.281 (0.345) data 0.000 (0.039) loss 2.8105 (2.0843) lr 6.6987e-04 eta 0:11:08
epoch [26/30] batch [40/392] time 0.281 (0.320) data 0.000 (0.020) loss 1.9424 (2.1176) lr 6.6987e-04 eta 0:10:14
epoch [26/30] batch [60/392] time 0.278 (0.310) data 0.000 (0.013) loss 2.7734 (2.2254) lr 6.6987e-04 eta 0:09:49
epoch [26/30] batch [80/392] time 0.279 (0.305) data 0.000 (0.010) loss 2.0273 (2.2282) lr 6.6987e-04 eta 0:09:33
epoch [26/30] batch [100/392] time 0.293 (0.303) data 0.000 (0.008) loss 3.0156 (2.3814) lr 6.6987e-04 eta 0:09:22
epoch [26/30] batch [120/392] time 0.292 (0.301) data 0.000 (0.007) loss 1.7314 (2.3311) lr 6.6987e-04 eta 0:09:13
epoch [26/30] batch [140/392] time 0.294 (0.299) data 0.000 (0.006) loss 2.3145 (2.2927) lr 6.6987e-04 eta 0:09:04
epoch [26/30] batch [160/392] time 0.306 (0.298) data 0.000 (0.005) loss 2.9375 (2.2223) lr 6.6987e-04 eta 0:08:56
epoch [26/30] batch [180/392] time 0.284 (0.298) data 0.000 (0.005) loss 1.3018 (2.2144) lr 6.6987e-04 eta 0:08:49
epoch [26/30] batch [200/392] time 0.287 (0.297) data 0.000 (0.004) loss 2.0762 (2.2140) lr 6.6987e-04 eta 0:08:43
epoch [26/30] batch [220/392] time 0.293 (0.297) data 0.000 (0.004) loss 4.0664 (2.2104) lr 6.6987e-04 eta 0:08:35
epoch [26/30] batch [240/392] time 0.294 (0.297) data 0.000 (0.003) loss 1.3252 (2.2020) lr 6.6987e-04 eta 0:08:30
epoch [26/30] batch [260/392] time 0.283 (0.297) data 0.000 (0.003) loss 2.0508 (2.1890) lr 6.6987e-04 eta 0:08:24
epoch [26/30] batch [280/392] time 0.285 (0.296) data 0.000 (0.003) loss 0.3550 (2.1744) lr 6.6987e-04 eta 0:08:17
epoch [26/30] batch [300/392] time 0.292 (0.296) data 0.000 (0.003) loss 1.5381 (2.1998) lr 6.6987e-04 eta 0:08:11
epoch [26/30] batch [320/392] time 0.292 (0.296) data 0.000 (0.003) loss 8.3594 (2.2109) lr 6.6987e-04 eta 0:08:05
epoch [26/30] batch [340/392] time 0.316 (0.296) data 0.000 (0.003) loss 0.0524 (2.1924) lr 6.6987e-04 eta 0:07:59
epoch [26/30] batch [360/392] time 0.286 (0.296) data 0.000 (0.002) loss 5.4102 (2.2320) lr 6.6987e-04 eta 0:07:53
epoch [26/30] batch [380/392] time 0.274 (0.295) data 0.000 (0.002) loss 3.6035 (2.2589) lr 6.6987e-04 eta 0:07:45
Evaluate on the *val* set
=> result
* total: 812
* correct: 609
* accuracy: 75.0%
* error: 25.0%
* macro_f1: 74.0%
epoch [27/30] batch [20/392] time 0.317 (0.341) data 0.000 (0.037) loss 3.1621 (2.0202) lr 4.3227e-04 eta 0:08:47
epoch [27/30] batch [40/392] time 0.280 (0.317) data 0.000 (0.018) loss 2.1582 (2.0696) lr 4.3227e-04 eta 0:08:03
epoch [27/30] batch [60/392] time 0.294 (0.309) data 0.000 (0.012) loss 1.3145 (2.0221) lr 4.3227e-04 eta 0:07:45
epoch [27/30] batch [80/392] time 0.278 (0.305) data 0.000 (0.009) loss 1.4609 (2.0352) lr 4.3227e-04 eta 0:07:33
epoch [27/30] batch [100/392] time 0.286 (0.302) data 0.000 (0.008) loss 2.2383 (2.0548) lr 4.3227e-04 eta 0:07:23
epoch [27/30] batch [120/392] time 0.287 (0.301) data 0.000 (0.006) loss 0.0636 (2.0456) lr 4.3227e-04 eta 0:07:15
epoch [27/30] batch [140/392] time 0.283 (0.300) data 0.000 (0.005) loss 2.6797 (2.0945) lr 4.3227e-04 eta 0:07:08
epoch [27/30] batch [160/392] time 0.282 (0.298) data 0.000 (0.005) loss 2.1289 (2.0774) lr 4.3227e-04 eta 0:07:00
epoch [27/30] batch [180/392] time 0.277 (0.297) data 0.000 (0.004) loss 1.7217 (2.1044) lr 4.3227e-04 eta 0:06:52
epoch [27/30] batch [200/392] time 0.291 (0.297) data 0.000 (0.004) loss 4.3555 (2.1078) lr 4.3227e-04 eta 0:06:45
epoch [27/30] batch [220/392] time 0.296 (0.297) data 0.000 (0.004) loss 2.4316 (2.0951) lr 4.3227e-04 eta 0:06:39
epoch [27/30] batch [240/392] time 0.290 (0.296) data 0.000 (0.003) loss 2.8320 (2.0849) lr 4.3227e-04 eta 0:06:32
epoch [27/30] batch [260/392] time 0.285 (0.295) data 0.000 (0.003) loss 1.9570 (2.1187) lr 4.3227e-04 eta 0:06:26
epoch [27/30] batch [280/392] time 0.286 (0.295) data 0.000 (0.003) loss 0.4951 (2.1651) lr 4.3227e-04 eta 0:06:20
epoch [27/30] batch [300/392] time 0.314 (0.295) data 0.000 (0.003) loss 3.1855 (2.1601) lr 4.3227e-04 eta 0:06:14
epoch [27/30] batch [320/392] time 0.301 (0.295) data 0.000 (0.003) loss 1.4219 (2.1220) lr 4.3227e-04 eta 0:06:08
epoch [27/30] batch [340/392] time 0.296 (0.295) data 0.000 (0.002) loss 3.3145 (2.1547) lr 4.3227e-04 eta 0:06:02
epoch [27/30] batch [360/392] time 0.299 (0.295) data 0.000 (0.002) loss 0.5972 (2.1576) lr 4.3227e-04 eta 0:05:56
epoch [27/30] batch [380/392] time 0.277 (0.294) data 0.000 (0.002) loss 2.5391 (2.1353) lr 4.3227e-04 eta 0:05:49
Evaluate on the *val* set
=> result
* total: 812
* correct: 607
* accuracy: 74.8%
* error: 25.2%
* macro_f1: 73.8%
epoch [28/30] batch [20/392] time 0.283 (0.335) data 0.000 (0.036) loss 2.2891 (2.1055) lr 2.4472e-04 eta 0:06:27
epoch [28/30] batch [40/392] time 0.295 (0.317) data 0.000 (0.018) loss 4.4727 (2.1794) lr 2.4472e-04 eta 0:05:59
epoch [28/30] batch [60/392] time 0.278 (0.308) data 0.001 (0.012) loss 1.7168 (2.2100) lr 2.4472e-04 eta 0:05:43
epoch [28/30] batch [80/392] time 0.284 (0.303) data 0.000 (0.009) loss 3.6738 (2.1510) lr 2.4472e-04 eta 0:05:32
epoch [28/30] batch [100/392] time 0.293 (0.301) data 0.000 (0.007) loss 3.2930 (2.1602) lr 2.4472e-04 eta 0:05:23
epoch [28/30] batch [120/392] time 0.293 (0.300) data 0.000 (0.006) loss 3.9570 (2.2021) lr 2.4472e-04 eta 0:05:16
epoch [28/30] batch [140/392] time 0.285 (0.298) data 0.000 (0.005) loss 0.3547 (2.1379) lr 2.4472e-04 eta 0:05:09
epoch [28/30] batch [160/392] time 0.294 (0.298) data 0.000 (0.005) loss 1.6504 (2.1009) lr 2.4472e-04 eta 0:05:03
epoch [28/30] batch [180/392] time 0.311 (0.298) data 0.000 (0.004) loss 1.5156 (2.1361) lr 2.4472e-04 eta 0:04:56
epoch [28/30] batch [200/392] time 0.334 (0.298) data 0.000 (0.004) loss 1.5527 (2.1374) lr 2.4472e-04 eta 0:04:50
epoch [28/30] batch [220/392] time 0.290 (0.297) data 0.000 (0.004) loss 1.5596 (2.1279) lr 2.4472e-04 eta 0:04:44
epoch [28/30] batch [240/392] time 0.301 (0.297) data 0.000 (0.003) loss 2.1523 (2.1552) lr 2.4472e-04 eta 0:04:37
epoch [28/30] batch [260/392] time 0.287 (0.297) data 0.000 (0.003) loss 4.5312 (2.1961) lr 2.4472e-04 eta 0:04:31
epoch [28/30] batch [280/392] time 0.284 (0.296) data 0.000 (0.003) loss 1.0186 (2.1980) lr 2.4472e-04 eta 0:04:25
epoch [28/30] batch [300/392] time 0.290 (0.296) data 0.000 (0.003) loss 1.3906 (2.1991) lr 2.4472e-04 eta 0:04:19
epoch [28/30] batch [320/392] time 0.294 (0.296) data 0.000 (0.003) loss 2.0352 (2.2047) lr 2.4472e-04 eta 0:04:13
epoch [28/30] batch [340/392] time 0.335 (0.296) data 0.000 (0.002) loss 0.7124 (2.1913) lr 2.4472e-04 eta 0:04:07
epoch [28/30] batch [360/392] time 0.293 (0.296) data 0.000 (0.002) loss 1.4736 (2.1923) lr 2.4472e-04 eta 0:04:01
epoch [28/30] batch [380/392] time 0.274 (0.295) data 0.000 (0.002) loss 0.5879 (2.1886) lr 2.4472e-04 eta 0:03:54
Evaluate on the *val* set
=> result
* total: 812
* correct: 611
* accuracy: 75.2%
* error: 24.8%
* macro_f1: 74.2%
epoch [29/30] batch [20/392] time 0.290 (0.328) data 0.000 (0.036) loss 0.6538 (2.6695) lr 1.0926e-04 eta 0:04:10
epoch [29/30] batch [40/392] time 0.293 (0.311) data 0.000 (0.018) loss 0.6011 (2.5280) lr 1.0926e-04 eta 0:03:51
epoch [29/30] batch [60/392] time 0.276 (0.303) data 0.000 (0.012) loss 2.2578 (2.4408) lr 1.0926e-04 eta 0:03:39
epoch [29/30] batch [80/392] time 0.285 (0.299) data 0.000 (0.009) loss 1.5703 (2.4141) lr 1.0926e-04 eta 0:03:30
epoch [29/30] batch [100/392] time 0.308 (0.299) data 0.000 (0.007) loss 0.8594 (2.3064) lr 1.0926e-04 eta 0:03:24
epoch [29/30] batch [120/392] time 0.305 (0.298) data 0.000 (0.006) loss 1.7246 (2.2881) lr 1.0926e-04 eta 0:03:17
epoch [29/30] batch [140/392] time 0.279 (0.297) data 0.000 (0.005) loss 0.9014 (2.3000) lr 1.0926e-04 eta 0:03:11
epoch [29/30] batch [160/392] time 0.285 (0.298) data 0.000 (0.005) loss 0.2827 (2.2466) lr 1.0926e-04 eta 0:03:05
epoch [29/30] batch [180/392] time 0.305 (0.297) data 0.000 (0.004) loss 3.6367 (2.2466) lr 1.0926e-04 eta 0:02:59
epoch [29/30] batch [200/392] time 0.291 (0.297) data 0.000 (0.004) loss 2.2930 (2.2129) lr 1.0926e-04 eta 0:02:53
epoch [29/30] batch [220/392] time 0.285 (0.296) data 0.000 (0.004) loss 3.0039 (2.2314) lr 1.0926e-04 eta 0:02:47
epoch [29/30] batch [240/392] time 0.288 (0.296) data 0.000 (0.003) loss 4.2656 (2.2479) lr 1.0926e-04 eta 0:02:41
epoch [29/30] batch [260/392] time 0.288 (0.295) data 0.000 (0.003) loss 1.2920 (2.2345) lr 1.0926e-04 eta 0:02:34
epoch [29/30] batch [280/392] time 0.282 (0.295) data 0.000 (0.003) loss 4.7227 (2.2380) lr 1.0926e-04 eta 0:02:28
epoch [29/30] batch [300/392] time 0.292 (0.295) data 0.001 (0.003) loss 2.1777 (2.2579) lr 1.0926e-04 eta 0:02:22
epoch [29/30] batch [320/392] time 0.290 (0.295) data 0.000 (0.003) loss 1.5566 (2.2628) lr 1.0926e-04 eta 0:02:17
epoch [29/30] batch [340/392] time 0.285 (0.295) data 0.000 (0.002) loss 1.3887 (2.2770) lr 1.0926e-04 eta 0:02:11
epoch [29/30] batch [360/392] time 0.284 (0.295) data 0.000 (0.002) loss 2.4512 (2.2577) lr 1.0926e-04 eta 0:02:05
epoch [29/30] batch [380/392] time 0.273 (0.294) data 0.000 (0.002) loss 1.4170 (2.2531) lr 1.0926e-04 eta 0:01:58
Evaluate on the *val* set
=> result
* total: 812
* correct: 610
* accuracy: 75.1%
* error: 24.9%
* macro_f1: 74.1%
epoch [30/30] batch [20/392] time 0.288 (0.329) data 0.000 (0.034) loss 3.0547 (1.9740) lr 2.7391e-05 eta 0:02:02
epoch [30/30] batch [40/392] time 0.309 (0.311) data 0.000 (0.017) loss 1.4512 (2.1518) lr 2.7391e-05 eta 0:01:49
epoch [30/30] batch [60/392] time 0.287 (0.305) data 0.000 (0.011) loss 0.5835 (2.1241) lr 2.7391e-05 eta 0:01:41
epoch [30/30] batch [80/392] time 0.300 (0.304) data 0.000 (0.009) loss 2.7422 (2.1491) lr 2.7391e-05 eta 0:01:34
epoch [30/30] batch [100/392] time 0.286 (0.301) data 0.000 (0.007) loss 1.7656 (2.0991) lr 2.7391e-05 eta 0:01:27
epoch [30/30] batch [120/392] time 0.310 (0.299) data 0.000 (0.006) loss 3.1973 (2.1508) lr 2.7391e-05 eta 0:01:21
epoch [30/30] batch [140/392] time 0.281 (0.298) data 0.000 (0.005) loss 2.0684 (2.1256) lr 2.7391e-05 eta 0:01:15
epoch [30/30] batch [160/392] time 0.286 (0.298) data 0.000 (0.004) loss 1.1807 (2.1459) lr 2.7391e-05 eta 0:01:09
epoch [30/30] batch [180/392] time 0.287 (0.298) data 0.000 (0.004) loss 1.6084 (2.1355) lr 2.7391e-05 eta 0:01:03
epoch [30/30] batch [200/392] time 0.292 (0.297) data 0.000 (0.004) loss 3.7812 (2.1259) lr 2.7391e-05 eta 0:00:57
epoch [30/30] batch [220/392] time 0.289 (0.297) data 0.000 (0.003) loss 0.6621 (2.0821) lr 2.7391e-05 eta 0:00:51
epoch [30/30] batch [240/392] time 0.314 (0.296) data 0.000 (0.003) loss 1.4590 (2.0637) lr 2.7391e-05 eta 0:00:45
epoch [30/30] batch [260/392] time 0.297 (0.296) data 0.000 (0.003) loss 0.6362 (2.0669) lr 2.7391e-05 eta 0:00:39
epoch [30/30] batch [280/392] time 0.282 (0.296) data 0.000 (0.003) loss 1.1230 (2.0909) lr 2.7391e-05 eta 0:00:33
epoch [30/30] batch [300/392] time 0.278 (0.296) data 0.000 (0.003) loss 1.2900 (2.0901) lr 2.7391e-05 eta 0:00:27
epoch [30/30] batch [320/392] time 0.292 (0.296) data 0.000 (0.002) loss 0.7070 (2.0610) lr 2.7391e-05 eta 0:00:21
epoch [30/30] batch [340/392] time 0.302 (0.295) data 0.000 (0.002) loss 1.4619 (2.0709) lr 2.7391e-05 eta 0:00:15
epoch [30/30] batch [360/392] time 0.287 (0.295) data 0.000 (0.002) loss 1.7969 (2.0603) lr 2.7391e-05 eta 0:00:09
epoch [30/30] batch [380/392] time 0.270 (0.294) data 0.000 (0.002) loss 2.1621 (2.0655) lr 2.7391e-05 eta 0:00:03
Evaluate on the *val* set
=> result
* total: 812
* correct: 611
* accuracy: 75.2%
* error: 24.8%
* macro_f1: 74.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed3/prompt_learner/model-best.pth.tar" (epoch = 25)
Evaluate on the *test* set
=> result
* total: 4,002
* correct: 3,064
* accuracy: 76.6%
* error: 23.4%
* macro_f1: 75.9%
Elapsed: 0:59:33
