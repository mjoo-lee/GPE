***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO_prime/main_tmp1_0.1sdl.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
resume: 
root: /shared/s2/lab01/dataset/clip
seed: 2
source_domains: None
target_domains: None
trainer: RPO_prime_sdl
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /shared/s2/lab01/dataset/clip
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 30
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: best_val
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO_prime_sdl
  RPO:
    CTX_INIT: a photo of a
    K1: 18
    K2: 6
    PREC: fp16
    cov_loss: 500
    sdl_loss: 1
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.10

Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-100-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090
Nvidia driver version: 520.61.05
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] imagenetv2-pytorch==0.1
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchvision==0.14.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] imagenetv2-pytorch        0.1                      pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.6                   pypi_0    pypi
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] pytorch                   1.13.1          py3.7_cuda11.7_cudnn8.5.0_0    pytorch
[conda] pytorch-cuda              11.7                 h778d358_5    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.14.1               py37_cu117    pytorch
        Pillow (9.4.0)

requested:RPO_prime_sdl
Loading trainer: RPO_prime_sdl
requested:StanfordCars
Loading dataset: StanfordCars
Reading split from /shared/s2/lab01/dataset/clip/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /shared/s2/lab01/dataset/clip/stanford_cars/split_fewshot_taesup/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
1568 812 4002
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  98
# train_x  1,568
# val      812
# test     4,002
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
requested:Classification
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/tensorboard)
epoch [1/30] batch [20/392] time 0.294 (0.458) data 0.000 (0.066) loss 2.8281 (3.7125) lr 1.0000e-02 eta 1:29:36
epoch [1/30] batch [40/392] time 0.294 (0.381) data 0.000 (0.033) loss 4.2695 (3.4705) lr 1.0000e-02 eta 1:14:30
epoch [1/30] batch [60/392] time 0.292 (0.355) data 0.000 (0.022) loss 1.9775 (3.3346) lr 1.0000e-02 eta 1:09:13
epoch [1/30] batch [80/392] time 0.286 (0.341) data 0.000 (0.017) loss 3.6660 (3.2477) lr 1.0000e-02 eta 1:06:26
epoch [1/30] batch [100/392] time 0.303 (0.334) data 0.000 (0.013) loss 3.1074 (3.2104) lr 1.0000e-02 eta 1:04:56
epoch [1/30] batch [120/392] time 0.296 (0.329) data 0.000 (0.011) loss 2.9023 (3.1852) lr 1.0000e-02 eta 1:03:48
epoch [1/30] batch [140/392] time 0.290 (0.326) data 0.000 (0.010) loss 3.2422 (3.0786) lr 1.0000e-02 eta 1:03:12
epoch [1/30] batch [160/392] time 0.326 (0.323) data 0.000 (0.008) loss 6.0977 (3.1065) lr 1.0000e-02 eta 1:02:30
epoch [1/30] batch [180/392] time 0.292 (0.320) data 0.000 (0.008) loss 2.3047 (3.0628) lr 1.0000e-02 eta 1:01:51
epoch [1/30] batch [200/392] time 0.283 (0.319) data 0.000 (0.007) loss 3.1797 (3.0079) lr 1.0000e-02 eta 1:01:30
epoch [1/30] batch [220/392] time 0.301 (0.317) data 0.000 (0.006) loss 5.2695 (3.0435) lr 1.0000e-02 eta 1:00:59
epoch [1/30] batch [240/392] time 0.324 (0.316) data 0.000 (0.006) loss 2.9082 (3.0637) lr 1.0000e-02 eta 1:00:35
epoch [1/30] batch [260/392] time 0.306 (0.315) data 0.000 (0.005) loss 3.1504 (3.0788) lr 1.0000e-02 eta 1:00:19
epoch [1/30] batch [280/392] time 0.299 (0.314) data 0.000 (0.005) loss 2.8418 (3.0992) lr 1.0000e-02 eta 1:00:01
epoch [1/30] batch [300/392] time 0.301 (0.313) data 0.000 (0.005) loss 2.1758 (3.0953) lr 1.0000e-02 eta 0:59:42
epoch [1/30] batch [320/392] time 0.294 (0.312) data 0.000 (0.004) loss 1.7490 (3.0791) lr 1.0000e-02 eta 0:59:29
epoch [1/30] batch [340/392] time 0.293 (0.311) data 0.000 (0.004) loss 2.8203 (3.0709) lr 1.0000e-02 eta 0:59:16
epoch [1/30] batch [360/392] time 0.313 (0.311) data 0.000 (0.004) loss 0.5815 (3.0522) lr 1.0000e-02 eta 0:59:05
epoch [1/30] batch [380/392] time 0.279 (0.309) data 0.000 (0.004) loss 2.1953 (3.0343) lr 1.0000e-02 eta 0:58:40
Evaluate on the *val* set
=> result
* total: 812
* correct: 536
* accuracy: 66.0%
* error: 34.0%
* macro_f1: 64.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar
epoch [2/30] batch [20/392] time 0.289 (0.354) data 0.000 (0.056) loss 1.3311 (3.0601) lr 9.9726e-03 eta 1:06:56
epoch [2/30] batch [40/392] time 0.287 (0.327) data 0.000 (0.028) loss 1.5820 (2.9881) lr 9.9726e-03 eta 1:01:42
epoch [2/30] batch [60/392] time 0.285 (0.318) data 0.000 (0.019) loss 2.5547 (2.8600) lr 9.9726e-03 eta 1:00:00
epoch [2/30] batch [80/392] time 0.301 (0.314) data 0.000 (0.014) loss 4.2617 (2.9548) lr 9.9726e-03 eta 0:59:07
epoch [2/30] batch [100/392] time 0.291 (0.311) data 0.000 (0.011) loss 4.0391 (2.9942) lr 9.9726e-03 eta 0:58:25
epoch [2/30] batch [120/392] time 0.306 (0.309) data 0.000 (0.010) loss 1.6826 (3.0022) lr 9.9726e-03 eta 0:57:56
epoch [2/30] batch [140/392] time 0.341 (0.309) data 0.000 (0.008) loss 2.1484 (2.9822) lr 9.9726e-03 eta 0:57:53
epoch [2/30] batch [160/392] time 0.320 (0.308) data 0.000 (0.007) loss 1.8682 (2.9206) lr 9.9726e-03 eta 0:57:30
epoch [2/30] batch [180/392] time 0.301 (0.307) data 0.000 (0.006) loss 4.0273 (2.9113) lr 9.9726e-03 eta 0:57:16
epoch [2/30] batch [200/392] time 0.293 (0.307) data 0.000 (0.006) loss 1.1182 (2.8393) lr 9.9726e-03 eta 0:57:06
epoch [2/30] batch [220/392] time 0.348 (0.307) data 0.000 (0.005) loss 3.3262 (2.8516) lr 9.9726e-03 eta 0:56:57
epoch [2/30] batch [240/392] time 0.293 (0.306) data 0.000 (0.005) loss 3.6758 (2.8680) lr 9.9726e-03 eta 0:56:42
epoch [2/30] batch [260/392] time 0.309 (0.306) data 0.000 (0.005) loss 3.4375 (2.8997) lr 9.9726e-03 eta 0:56:44
epoch [2/30] batch [280/392] time 0.298 (0.306) data 0.000 (0.004) loss 2.8359 (2.8937) lr 9.9726e-03 eta 0:56:37
epoch [2/30] batch [300/392] time 0.298 (0.306) data 0.000 (0.004) loss 3.2441 (2.8923) lr 9.9726e-03 eta 0:56:29
epoch [2/30] batch [320/392] time 0.301 (0.306) data 0.000 (0.004) loss 4.3086 (2.9222) lr 9.9726e-03 eta 0:56:23
epoch [2/30] batch [340/392] time 0.297 (0.306) data 0.000 (0.004) loss 1.7529 (2.9123) lr 9.9726e-03 eta 0:56:17
epoch [2/30] batch [360/392] time 0.303 (0.306) data 0.000 (0.003) loss 3.2246 (2.9196) lr 9.9726e-03 eta 0:56:05
epoch [2/30] batch [380/392] time 0.383 (0.305) data 0.000 (0.003) loss 1.7080 (2.9189) lr 9.9726e-03 eta 0:55:49
Evaluate on the *val* set
=> result
* total: 812
* correct: 548
* accuracy: 67.5%
* error: 32.5%
* macro_f1: 65.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar
epoch [3/30] batch [20/392] time 0.293 (0.366) data 0.000 (0.053) loss 2.8691 (2.6475) lr 9.8907e-03 eta 1:06:48
epoch [3/30] batch [40/392] time 0.309 (0.335) data 0.000 (0.027) loss 3.8223 (2.5118) lr 9.8907e-03 eta 1:01:07
epoch [3/30] batch [60/392] time 0.420 (0.326) data 0.000 (0.018) loss 3.5918 (2.5777) lr 9.8907e-03 eta 0:59:15
epoch [3/30] batch [80/392] time 0.303 (0.321) data 0.000 (0.014) loss 2.9453 (2.5001) lr 9.8907e-03 eta 0:58:20
epoch [3/30] batch [100/392] time 0.295 (0.317) data 0.000 (0.011) loss 2.3691 (2.5205) lr 9.8907e-03 eta 0:57:28
epoch [3/30] batch [120/392] time 0.294 (0.314) data 0.001 (0.009) loss 3.7949 (2.5660) lr 9.8907e-03 eta 0:56:46
epoch [3/30] batch [140/392] time 0.291 (0.312) data 0.000 (0.008) loss 0.9312 (2.5794) lr 9.8907e-03 eta 0:56:21
epoch [3/30] batch [160/392] time 0.300 (0.311) data 0.000 (0.007) loss 3.3535 (2.6372) lr 9.8907e-03 eta 0:56:01
epoch [3/30] batch [180/392] time 0.289 (0.309) data 0.000 (0.006) loss 4.7812 (2.7030) lr 9.8907e-03 eta 0:55:38
epoch [3/30] batch [200/392] time 0.295 (0.309) data 0.000 (0.006) loss 3.4805 (2.7000) lr 9.8907e-03 eta 0:55:26
epoch [3/30] batch [220/392] time 0.301 (0.308) data 0.000 (0.005) loss 2.7168 (2.7472) lr 9.8907e-03 eta 0:55:13
epoch [3/30] batch [240/392] time 0.337 (0.308) data 0.000 (0.005) loss 5.1289 (2.7427) lr 9.8907e-03 eta 0:55:06
epoch [3/30] batch [260/392] time 0.292 (0.307) data 0.000 (0.004) loss 2.1875 (2.7555) lr 9.8907e-03 eta 0:54:51
epoch [3/30] batch [280/392] time 0.295 (0.306) data 0.000 (0.004) loss 4.0469 (2.7413) lr 9.8907e-03 eta 0:54:37
epoch [3/30] batch [300/392] time 0.293 (0.306) data 0.001 (0.004) loss 2.8906 (2.7656) lr 9.8907e-03 eta 0:54:28
epoch [3/30] batch [320/392] time 0.307 (0.306) data 0.000 (0.004) loss 2.9160 (2.7726) lr 9.8907e-03 eta 0:54:18
epoch [3/30] batch [340/392] time 0.321 (0.306) data 0.000 (0.003) loss 3.6836 (2.7647) lr 9.8907e-03 eta 0:54:12
epoch [3/30] batch [360/392] time 0.297 (0.306) data 0.000 (0.003) loss 2.6211 (2.7777) lr 9.8907e-03 eta 0:54:07
epoch [3/30] batch [380/392] time 0.281 (0.305) data 0.000 (0.003) loss 3.8691 (2.7896) lr 9.8907e-03 eta 0:53:47
Evaluate on the *val* set
=> result
* total: 812
* correct: 551
* accuracy: 67.9%
* error: 32.1%
* macro_f1: 66.2%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar
epoch [4/30] batch [20/392] time 0.298 (0.367) data 0.000 (0.052) loss 2.7969 (2.4167) lr 9.7553e-03 eta 1:04:35
epoch [4/30] batch [40/392] time 0.295 (0.334) data 0.000 (0.026) loss 3.5547 (2.8059) lr 9.7553e-03 eta 0:58:39
epoch [4/30] batch [60/392] time 0.288 (0.323) data 0.000 (0.018) loss 3.5352 (2.7667) lr 9.7553e-03 eta 0:56:39
epoch [4/30] batch [80/392] time 0.290 (0.317) data 0.000 (0.013) loss 2.5488 (2.7937) lr 9.7553e-03 eta 0:55:29
epoch [4/30] batch [100/392] time 0.293 (0.313) data 0.000 (0.011) loss 2.8418 (2.8404) lr 9.7553e-03 eta 0:54:40
epoch [4/30] batch [120/392] time 0.284 (0.311) data 0.000 (0.009) loss 1.9297 (2.8293) lr 9.7553e-03 eta 0:54:18
epoch [4/30] batch [140/392] time 0.290 (0.310) data 0.000 (0.008) loss 3.7305 (2.8231) lr 9.7553e-03 eta 0:53:55
epoch [4/30] batch [160/392] time 0.311 (0.310) data 0.000 (0.007) loss 1.0957 (2.7460) lr 9.7553e-03 eta 0:53:46
epoch [4/30] batch [180/392] time 0.307 (0.309) data 0.000 (0.006) loss 1.4287 (2.7062) lr 9.7553e-03 eta 0:53:37
epoch [4/30] batch [200/392] time 0.295 (0.308) data 0.000 (0.005) loss 1.2139 (2.7396) lr 9.7553e-03 eta 0:53:15
epoch [4/30] batch [220/392] time 0.291 (0.307) data 0.000 (0.005) loss 2.0762 (2.7833) lr 9.7553e-03 eta 0:53:03
epoch [4/30] batch [240/392] time 0.296 (0.306) data 0.000 (0.005) loss 1.5479 (2.7409) lr 9.7553e-03 eta 0:52:47
epoch [4/30] batch [260/392] time 0.285 (0.306) data 0.000 (0.004) loss 5.1680 (2.7515) lr 9.7553e-03 eta 0:52:35
epoch [4/30] batch [280/392] time 0.289 (0.305) data 0.000 (0.004) loss 2.1484 (2.7524) lr 9.7553e-03 eta 0:52:21
epoch [4/30] batch [300/392] time 0.291 (0.305) data 0.000 (0.004) loss 4.9375 (2.7469) lr 9.7553e-03 eta 0:52:12
epoch [4/30] batch [320/392] time 0.313 (0.304) data 0.000 (0.004) loss 0.4514 (2.7079) lr 9.7553e-03 eta 0:52:02
epoch [4/30] batch [340/392] time 0.292 (0.304) data 0.000 (0.003) loss 1.9473 (2.7094) lr 9.7553e-03 eta 0:51:50
epoch [4/30] batch [360/392] time 0.286 (0.303) data 0.000 (0.003) loss 2.9980 (2.6992) lr 9.7553e-03 eta 0:51:42
epoch [4/30] batch [380/392] time 0.276 (0.302) data 0.000 (0.003) loss 2.3730 (2.6995) lr 9.7553e-03 eta 0:51:21
Evaluate on the *val* set
=> result
* total: 812
* correct: 557
* accuracy: 68.6%
* error: 31.4%
* macro_f1: 66.8%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar
epoch [5/30] batch [20/392] time 0.273 (0.351) data 0.000 (0.051) loss 2.9004 (2.2614) lr 9.5677e-03 eta 0:59:33
epoch [5/30] batch [40/392] time 0.278 (0.320) data 0.000 (0.026) loss 1.9717 (2.7560) lr 9.5677e-03 eta 0:54:05
epoch [5/30] batch [60/392] time 0.287 (0.310) data 0.000 (0.017) loss 2.0430 (2.8987) lr 9.5677e-03 eta 0:52:19
epoch [5/30] batch [80/392] time 0.306 (0.308) data 0.000 (0.013) loss 5.1523 (2.8835) lr 9.5677e-03 eta 0:51:50
epoch [5/30] batch [100/392] time 0.295 (0.304) data 0.000 (0.010) loss 1.7256 (2.9612) lr 9.5677e-03 eta 0:51:09
epoch [5/30] batch [120/392] time 0.285 (0.302) data 0.000 (0.009) loss 3.9043 (2.8918) lr 9.5677e-03 eta 0:50:44
epoch [5/30] batch [140/392] time 0.281 (0.301) data 0.000 (0.008) loss 4.4922 (2.8176) lr 9.5677e-03 eta 0:50:23
epoch [5/30] batch [160/392] time 0.283 (0.299) data 0.000 (0.007) loss 2.4746 (2.8473) lr 9.5677e-03 eta 0:50:04
epoch [5/30] batch [180/392] time 0.283 (0.299) data 0.000 (0.006) loss 3.2695 (2.8335) lr 9.5677e-03 eta 0:49:53
epoch [5/30] batch [200/392] time 0.278 (0.298) data 0.000 (0.005) loss 5.3828 (2.8610) lr 9.5677e-03 eta 0:49:38
epoch [5/30] batch [220/392] time 0.291 (0.297) data 0.000 (0.005) loss 3.5391 (2.8135) lr 9.5677e-03 eta 0:49:24
epoch [5/30] batch [240/392] time 0.284 (0.296) data 0.000 (0.004) loss 2.5449 (2.8179) lr 9.5677e-03 eta 0:49:09
epoch [5/30] batch [260/392] time 0.281 (0.296) data 0.000 (0.004) loss 1.8379 (2.7817) lr 9.5677e-03 eta 0:49:02
epoch [5/30] batch [280/392] time 0.292 (0.296) data 0.000 (0.004) loss 1.5850 (2.7859) lr 9.5677e-03 eta 0:48:54
epoch [5/30] batch [300/392] time 0.299 (0.296) data 0.000 (0.004) loss 3.3516 (2.7912) lr 9.5677e-03 eta 0:48:45
epoch [5/30] batch [320/392] time 0.289 (0.296) data 0.000 (0.003) loss 5.4297 (2.7899) lr 9.5677e-03 eta 0:48:41
epoch [5/30] batch [340/392] time 0.284 (0.296) data 0.000 (0.003) loss 0.8745 (2.7909) lr 9.5677e-03 eta 0:48:34
epoch [5/30] batch [360/392] time 0.283 (0.295) data 0.000 (0.003) loss 3.0840 (2.7845) lr 9.5677e-03 eta 0:48:24
epoch [5/30] batch [380/392] time 0.272 (0.294) data 0.000 (0.003) loss 2.9023 (2.7974) lr 9.5677e-03 eta 0:48:06
Evaluate on the *val* set
=> result
* total: 812
* correct: 558
* accuracy: 68.7%
* error: 31.3%
* macro_f1: 66.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar
epoch [6/30] batch [20/392] time 0.323 (0.346) data 0.000 (0.050) loss 2.3672 (3.2399) lr 9.3301e-03 eta 0:56:27
epoch [6/30] batch [40/392] time 0.285 (0.322) data 0.000 (0.025) loss 3.0332 (2.8658) lr 9.3301e-03 eta 0:52:24
epoch [6/30] batch [60/392] time 0.297 (0.313) data 0.000 (0.017) loss 1.0088 (2.7162) lr 9.3301e-03 eta 0:50:48
epoch [6/30] batch [80/392] time 0.284 (0.307) data 0.000 (0.013) loss 2.0176 (2.7277) lr 9.3301e-03 eta 0:49:39
epoch [6/30] batch [100/392] time 0.290 (0.303) data 0.000 (0.010) loss 2.6523 (2.7183) lr 9.3301e-03 eta 0:49:00
epoch [6/30] batch [120/392] time 0.289 (0.301) data 0.000 (0.009) loss 5.4922 (2.7457) lr 9.3301e-03 eta 0:48:37
epoch [6/30] batch [140/392] time 0.284 (0.301) data 0.000 (0.007) loss 1.2676 (2.7150) lr 9.3301e-03 eta 0:48:24
epoch [6/30] batch [160/392] time 0.301 (0.299) data 0.000 (0.006) loss 3.0508 (2.7096) lr 9.3301e-03 eta 0:48:05
epoch [6/30] batch [180/392] time 0.301 (0.298) data 0.000 (0.006) loss 2.5371 (2.6900) lr 9.3301e-03 eta 0:47:49
epoch [6/30] batch [200/392] time 0.282 (0.299) data 0.000 (0.005) loss 2.2129 (2.7024) lr 9.3301e-03 eta 0:47:46
epoch [6/30] batch [220/392] time 0.287 (0.298) data 0.000 (0.005) loss 1.9395 (2.6879) lr 9.3301e-03 eta 0:47:32
epoch [6/30] batch [240/392] time 0.277 (0.297) data 0.000 (0.004) loss 0.7007 (2.6671) lr 9.3301e-03 eta 0:47:19
epoch [6/30] batch [260/392] time 0.278 (0.296) data 0.000 (0.004) loss 4.2578 (2.6690) lr 9.3301e-03 eta 0:47:08
epoch [6/30] batch [280/392] time 0.312 (0.296) data 0.000 (0.004) loss 3.4570 (2.6363) lr 9.3301e-03 eta 0:46:57
epoch [6/30] batch [300/392] time 0.293 (0.296) data 0.000 (0.004) loss 5.2930 (2.6516) lr 9.3301e-03 eta 0:46:48
epoch [6/30] batch [320/392] time 0.278 (0.296) data 0.000 (0.003) loss 3.1445 (2.6742) lr 9.3301e-03 eta 0:46:42
epoch [6/30] batch [340/392] time 0.293 (0.295) data 0.000 (0.003) loss 0.8838 (2.6355) lr 9.3301e-03 eta 0:46:35
epoch [6/30] batch [360/392] time 0.289 (0.295) data 0.000 (0.003) loss 1.4912 (2.6571) lr 9.3301e-03 eta 0:46:25
epoch [6/30] batch [380/392] time 0.269 (0.294) data 0.000 (0.003) loss 3.6035 (2.6575) lr 9.3301e-03 eta 0:46:09
Evaluate on the *val* set
=> result
* total: 812
* correct: 564
* accuracy: 69.5%
* error: 30.5%
* macro_f1: 67.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar
epoch [7/30] batch [20/392] time 0.283 (0.346) data 0.000 (0.049) loss 4.0391 (2.7023) lr 9.0451e-03 eta 0:54:03
epoch [7/30] batch [40/392] time 0.321 (0.320) data 0.000 (0.025) loss 1.1162 (2.9123) lr 9.0451e-03 eta 0:49:57
epoch [7/30] batch [60/392] time 0.385 (0.313) data 0.000 (0.017) loss 1.2041 (2.7908) lr 9.0451e-03 eta 0:48:41
epoch [7/30] batch [80/392] time 0.301 (0.307) data 0.000 (0.013) loss 1.3389 (2.8377) lr 9.0451e-03 eta 0:47:47
epoch [7/30] batch [100/392] time 0.283 (0.303) data 0.000 (0.010) loss 2.5117 (2.7948) lr 9.0451e-03 eta 0:47:03
epoch [7/30] batch [120/392] time 0.295 (0.302) data 0.000 (0.008) loss 1.6465 (2.8256) lr 9.0451e-03 eta 0:46:42
epoch [7/30] batch [140/392] time 0.290 (0.300) data 0.000 (0.007) loss 3.1758 (2.7577) lr 9.0451e-03 eta 0:46:20
epoch [7/30] batch [160/392] time 0.281 (0.299) data 0.000 (0.006) loss 0.8379 (2.7266) lr 9.0451e-03 eta 0:46:02
epoch [7/30] batch [180/392] time 0.279 (0.298) data 0.000 (0.006) loss 4.3594 (2.6826) lr 9.0451e-03 eta 0:45:47
epoch [7/30] batch [200/392] time 0.300 (0.297) data 0.000 (0.005) loss 2.5508 (2.6805) lr 9.0451e-03 eta 0:45:37
epoch [7/30] batch [220/392] time 0.296 (0.297) data 0.000 (0.005) loss 3.6289 (2.6573) lr 9.0451e-03 eta 0:45:24
epoch [7/30] batch [240/392] time 0.295 (0.297) data 0.000 (0.004) loss 3.5684 (2.6892) lr 9.0451e-03 eta 0:45:18
epoch [7/30] batch [260/392] time 0.297 (0.297) data 0.000 (0.004) loss 2.5918 (2.6951) lr 9.0451e-03 eta 0:45:15
epoch [7/30] batch [280/392] time 0.280 (0.296) data 0.000 (0.004) loss 1.8516 (2.6873) lr 9.0451e-03 eta 0:45:05
epoch [7/30] batch [300/392] time 0.304 (0.296) data 0.000 (0.004) loss 1.4111 (2.7081) lr 9.0451e-03 eta 0:44:57
epoch [7/30] batch [320/392] time 0.315 (0.296) data 0.000 (0.003) loss 4.5195 (2.6950) lr 9.0451e-03 eta 0:44:50
epoch [7/30] batch [340/392] time 0.284 (0.296) data 0.000 (0.003) loss 1.3438 (2.6695) lr 9.0451e-03 eta 0:44:42
epoch [7/30] batch [360/392] time 0.316 (0.295) data 0.000 (0.003) loss 1.3555 (2.6743) lr 9.0451e-03 eta 0:44:33
epoch [7/30] batch [380/392] time 0.273 (0.294) data 0.000 (0.003) loss 2.7500 (2.6763) lr 9.0451e-03 eta 0:44:17
Evaluate on the *val* set
=> result
* total: 812
* correct: 577
* accuracy: 71.1%
* error: 28.9%
* macro_f1: 70.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar
epoch [8/30] batch [20/392] time 0.333 (0.350) data 0.000 (0.051) loss 3.1328 (2.6956) lr 8.7157e-03 eta 0:52:27
epoch [8/30] batch [40/392] time 0.299 (0.323) data 0.000 (0.025) loss 0.4609 (2.5445) lr 8.7157e-03 eta 0:48:17
epoch [8/30] batch [60/392] time 0.300 (0.313) data 0.000 (0.017) loss 1.0303 (2.6249) lr 8.7157e-03 eta 0:46:45
epoch [8/30] batch [80/392] time 0.325 (0.309) data 0.000 (0.013) loss 2.6406 (2.6502) lr 8.7157e-03 eta 0:46:04
epoch [8/30] batch [100/392] time 0.296 (0.305) data 0.000 (0.010) loss 0.8862 (2.5898) lr 8.7157e-03 eta 0:45:20
epoch [8/30] batch [120/392] time 0.288 (0.302) data 0.000 (0.009) loss 2.4023 (2.6116) lr 8.7157e-03 eta 0:44:50
epoch [8/30] batch [140/392] time 0.280 (0.302) data 0.000 (0.007) loss 1.0098 (2.6014) lr 8.7157e-03 eta 0:44:39
epoch [8/30] batch [160/392] time 0.297 (0.300) data 0.000 (0.007) loss 4.4102 (2.6417) lr 8.7157e-03 eta 0:44:19
epoch [8/30] batch [180/392] time 0.287 (0.299) data 0.000 (0.006) loss 1.0049 (2.6952) lr 8.7157e-03 eta 0:44:04
epoch [8/30] batch [200/392] time 0.286 (0.298) data 0.000 (0.005) loss 0.9033 (2.6965) lr 8.7157e-03 eta 0:43:47
epoch [8/30] batch [220/392] time 0.283 (0.298) data 0.000 (0.005) loss 4.2148 (2.6635) lr 8.7157e-03 eta 0:43:39
epoch [8/30] batch [240/392] time 0.290 (0.297) data 0.000 (0.004) loss 2.8730 (2.6323) lr 8.7157e-03 eta 0:43:27
epoch [8/30] batch [260/392] time 0.278 (0.297) data 0.000 (0.004) loss 3.0352 (2.6446) lr 8.7157e-03 eta 0:43:20
epoch [8/30] batch [280/392] time 0.281 (0.296) data 0.000 (0.004) loss 4.3789 (2.6603) lr 8.7157e-03 eta 0:43:09
epoch [8/30] batch [300/392] time 0.285 (0.296) data 0.000 (0.004) loss 1.0723 (2.6739) lr 8.7157e-03 eta 0:43:00
epoch [8/30] batch [320/392] time 0.285 (0.296) data 0.000 (0.003) loss 5.0312 (2.6757) lr 8.7157e-03 eta 0:42:53
epoch [8/30] batch [340/392] time 0.285 (0.296) data 0.000 (0.003) loss 2.3691 (2.6812) lr 8.7157e-03 eta 0:42:44
epoch [8/30] batch [360/392] time 0.286 (0.295) data 0.000 (0.003) loss 2.6562 (2.6953) lr 8.7157e-03 eta 0:42:33
epoch [8/30] batch [380/392] time 0.271 (0.294) data 0.000 (0.003) loss 1.6025 (2.6863) lr 8.7157e-03 eta 0:42:17
Evaluate on the *val* set
=> result
* total: 812
* correct: 565
* accuracy: 69.6%
* error: 30.4%
* macro_f1: 68.3%
epoch [9/30] batch [20/392] time 0.291 (0.346) data 0.000 (0.052) loss 3.6055 (2.4327) lr 8.3457e-03 eta 0:49:38
epoch [9/30] batch [40/392] time 0.284 (0.320) data 0.000 (0.026) loss 2.7363 (2.4478) lr 8.3457e-03 eta 0:45:49
epoch [9/30] batch [60/392] time 0.288 (0.310) data 0.000 (0.017) loss 3.1133 (2.4976) lr 8.3457e-03 eta 0:44:13
epoch [9/30] batch [80/392] time 0.309 (0.304) data 0.000 (0.013) loss 4.7500 (2.6126) lr 8.3457e-03 eta 0:43:21
epoch [9/30] batch [100/392] time 0.281 (0.301) data 0.000 (0.011) loss 2.7520 (2.6057) lr 8.3457e-03 eta 0:42:47
epoch [9/30] batch [120/392] time 0.317 (0.300) data 0.000 (0.009) loss 2.7793 (2.5809) lr 8.3457e-03 eta 0:42:28
epoch [9/30] batch [140/392] time 0.288 (0.299) data 0.000 (0.008) loss 2.4316 (2.5477) lr 8.3457e-03 eta 0:42:12
epoch [9/30] batch [160/392] time 0.323 (0.298) data 0.000 (0.007) loss 1.1094 (2.4956) lr 8.3457e-03 eta 0:42:05
epoch [9/30] batch [180/392] time 0.332 (0.298) data 0.000 (0.006) loss 1.5322 (2.4702) lr 8.3457e-03 eta 0:41:52
epoch [9/30] batch [200/392] time 0.282 (0.297) data 0.000 (0.005) loss 1.6484 (2.4989) lr 8.3457e-03 eta 0:41:40
epoch [9/30] batch [220/392] time 0.295 (0.296) data 0.000 (0.005) loss 1.9365 (2.4879) lr 8.3457e-03 eta 0:41:30
epoch [9/30] batch [240/392] time 0.296 (0.296) data 0.000 (0.005) loss 3.0840 (2.4742) lr 8.3457e-03 eta 0:41:23
epoch [9/30] batch [260/392] time 0.285 (0.296) data 0.000 (0.004) loss 1.8564 (2.5032) lr 8.3457e-03 eta 0:41:13
epoch [9/30] batch [280/392] time 0.284 (0.295) data 0.000 (0.004) loss 6.9375 (2.4795) lr 8.3457e-03 eta 0:41:05
epoch [9/30] batch [300/392] time 0.283 (0.295) data 0.000 (0.004) loss 2.6348 (2.4933) lr 8.3457e-03 eta 0:40:55
epoch [9/30] batch [320/392] time 0.365 (0.295) data 0.000 (0.003) loss 2.2012 (2.5047) lr 8.3457e-03 eta 0:40:51
epoch [9/30] batch [340/392] time 0.284 (0.295) data 0.000 (0.003) loss 1.8271 (2.5045) lr 8.3457e-03 eta 0:40:41
epoch [9/30] batch [360/392] time 0.286 (0.294) data 0.000 (0.003) loss 4.4492 (2.5195) lr 8.3457e-03 eta 0:40:32
epoch [9/30] batch [380/392] time 0.268 (0.293) data 0.000 (0.003) loss 4.3320 (2.5294) lr 8.3457e-03 eta 0:40:17
Evaluate on the *val* set
=> result
* total: 812
* correct: 570
* accuracy: 70.2%
* error: 29.8%
* macro_f1: 68.3%
epoch [10/30] batch [20/392] time 0.288 (0.346) data 0.000 (0.051) loss 1.8682 (2.2136) lr 7.9389e-03 eta 0:47:23
epoch [10/30] batch [40/392] time 0.308 (0.319) data 0.000 (0.026) loss 2.5391 (2.4332) lr 7.9389e-03 eta 0:43:30
epoch [10/30] batch [60/392] time 0.288 (0.309) data 0.000 (0.017) loss 3.2852 (2.6901) lr 7.9389e-03 eta 0:42:04
epoch [10/30] batch [80/392] time 0.284 (0.304) data 0.000 (0.013) loss 2.8672 (2.6216) lr 7.9389e-03 eta 0:41:19
epoch [10/30] batch [100/392] time 0.284 (0.301) data 0.000 (0.011) loss 1.8008 (2.6392) lr 7.9389e-03 eta 0:40:46
epoch [10/30] batch [120/392] time 0.281 (0.299) data 0.000 (0.009) loss 2.3438 (2.6512) lr 7.9389e-03 eta 0:40:28
epoch [10/30] batch [140/392] time 0.310 (0.298) data 0.000 (0.008) loss 1.4277 (2.6250) lr 7.9389e-03 eta 0:40:11
epoch [10/30] batch [160/392] time 0.286 (0.298) data 0.000 (0.007) loss 1.2725 (2.5561) lr 7.9389e-03 eta 0:40:01
epoch [10/30] batch [180/392] time 0.325 (0.297) data 0.000 (0.006) loss 2.0801 (2.5404) lr 7.9389e-03 eta 0:39:49
epoch [10/30] batch [200/392] time 0.301 (0.296) data 0.000 (0.005) loss 3.8105 (2.5817) lr 7.9389e-03 eta 0:39:39
epoch [10/30] batch [220/392] time 0.279 (0.296) data 0.000 (0.005) loss 2.2969 (2.5530) lr 7.9389e-03 eta 0:39:32
epoch [10/30] batch [240/392] time 0.276 (0.296) data 0.000 (0.005) loss 2.7051 (2.6059) lr 7.9389e-03 eta 0:39:22
epoch [10/30] batch [260/392] time 0.290 (0.295) data 0.000 (0.004) loss 0.9541 (2.6006) lr 7.9389e-03 eta 0:39:13
epoch [10/30] batch [280/392] time 0.296 (0.295) data 0.000 (0.004) loss 2.5137 (2.6063) lr 7.9389e-03 eta 0:39:05
epoch [10/30] batch [300/392] time 0.279 (0.294) data 0.000 (0.004) loss 3.3164 (2.6141) lr 7.9389e-03 eta 0:38:55
epoch [10/30] batch [320/392] time 0.287 (0.295) data 0.000 (0.003) loss 1.2705 (2.6145) lr 7.9389e-03 eta 0:38:51
epoch [10/30] batch [340/392] time 0.292 (0.294) data 0.000 (0.003) loss 3.3418 (2.6027) lr 7.9389e-03 eta 0:38:42
epoch [10/30] batch [360/392] time 0.302 (0.294) data 0.000 (0.003) loss 3.8828 (2.5848) lr 7.9389e-03 eta 0:38:36
epoch [10/30] batch [380/392] time 0.270 (0.293) data 0.000 (0.003) loss 3.4355 (2.5805) lr 7.9389e-03 eta 0:38:20
Evaluate on the *val* set
=> result
* total: 812
* correct: 572
* accuracy: 70.4%
* error: 29.6%
* macro_f1: 69.4%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-10
epoch [11/30] batch [20/392] time 0.289 (0.354) data 0.000 (0.050) loss 4.4766 (2.5071) lr 7.5000e-03 eta 0:46:07
epoch [11/30] batch [40/392] time 0.283 (0.324) data 0.000 (0.025) loss 2.5176 (2.3660) lr 7.5000e-03 eta 0:42:07
epoch [11/30] batch [60/392] time 0.293 (0.313) data 0.001 (0.017) loss 2.8555 (2.4156) lr 7.5000e-03 eta 0:40:36
epoch [11/30] batch [80/392] time 0.286 (0.308) data 0.000 (0.013) loss 2.8203 (2.5594) lr 7.5000e-03 eta 0:39:47
epoch [11/30] batch [100/392] time 0.282 (0.304) data 0.000 (0.010) loss 1.0146 (2.5171) lr 7.5000e-03 eta 0:39:15
epoch [11/30] batch [120/392] time 0.298 (0.302) data 0.000 (0.009) loss 1.5312 (2.5457) lr 7.5000e-03 eta 0:38:49
epoch [11/30] batch [140/392] time 0.300 (0.302) data 0.000 (0.007) loss 4.3281 (2.4818) lr 7.5000e-03 eta 0:38:43
epoch [11/30] batch [160/392] time 0.286 (0.300) data 0.000 (0.006) loss 1.7441 (2.4817) lr 7.5000e-03 eta 0:38:26
epoch [11/30] batch [180/392] time 0.324 (0.299) data 0.000 (0.006) loss 1.2510 (2.5138) lr 7.5000e-03 eta 0:38:12
epoch [11/30] batch [200/392] time 0.298 (0.299) data 0.000 (0.005) loss 1.6797 (2.4963) lr 7.5000e-03 eta 0:38:03
epoch [11/30] batch [220/392] time 0.294 (0.299) data 0.000 (0.005) loss 4.9297 (2.5282) lr 7.5000e-03 eta 0:37:55
epoch [11/30] batch [240/392] time 0.287 (0.299) data 0.000 (0.004) loss 5.2344 (2.5255) lr 7.5000e-03 eta 0:37:49
epoch [11/30] batch [260/392] time 0.283 (0.298) data 0.000 (0.004) loss 4.5898 (2.5302) lr 7.5000e-03 eta 0:37:39
epoch [11/30] batch [280/392] time 0.318 (0.298) data 0.000 (0.004) loss 7.3203 (2.5388) lr 7.5000e-03 eta 0:37:30
epoch [11/30] batch [300/392] time 0.283 (0.297) data 0.000 (0.004) loss 1.9600 (2.5117) lr 7.5000e-03 eta 0:37:22
epoch [11/30] batch [320/392] time 0.284 (0.297) data 0.000 (0.003) loss 2.7070 (2.5127) lr 7.5000e-03 eta 0:37:15
epoch [11/30] batch [340/392] time 0.287 (0.297) data 0.000 (0.003) loss 1.3213 (2.5310) lr 7.5000e-03 eta 0:37:05
epoch [11/30] batch [360/392] time 0.310 (0.297) data 0.000 (0.003) loss 2.6289 (2.5327) lr 7.5000e-03 eta 0:36:58
epoch [11/30] batch [380/392] time 0.271 (0.295) data 0.000 (0.003) loss 2.5605 (2.5382) lr 7.5000e-03 eta 0:36:42
Evaluate on the *val* set
=> result
* total: 812
* correct: 570
* accuracy: 70.2%
* error: 29.8%
* macro_f1: 69.1%
epoch [12/30] batch [20/392] time 0.334 (0.352) data 0.000 (0.053) loss 1.8271 (3.0760) lr 7.0337e-03 eta 0:43:35
epoch [12/30] batch [40/392] time 0.300 (0.324) data 0.000 (0.026) loss 1.1201 (2.5987) lr 7.0337e-03 eta 0:40:00
epoch [12/30] batch [60/392] time 0.283 (0.312) data 0.000 (0.018) loss 1.8457 (2.4512) lr 7.0337e-03 eta 0:38:28
epoch [12/30] batch [80/392] time 0.299 (0.307) data 0.000 (0.013) loss 5.6992 (2.5266) lr 7.0337e-03 eta 0:37:42
epoch [12/30] batch [100/392] time 0.290 (0.303) data 0.000 (0.011) loss 5.1211 (2.4903) lr 7.0337e-03 eta 0:37:09
epoch [12/30] batch [120/392] time 0.278 (0.301) data 0.000 (0.009) loss 2.3594 (2.5054) lr 7.0337e-03 eta 0:36:46
epoch [12/30] batch [140/392] time 0.291 (0.299) data 0.000 (0.008) loss 3.1250 (2.5440) lr 7.0337e-03 eta 0:36:27
epoch [12/30] batch [160/392] time 0.300 (0.299) data 0.000 (0.007) loss 1.4932 (2.6199) lr 7.0337e-03 eta 0:36:19
epoch [12/30] batch [180/392] time 0.281 (0.298) data 0.000 (0.006) loss 2.4570 (2.6881) lr 7.0337e-03 eta 0:36:05
epoch [12/30] batch [200/392] time 0.281 (0.297) data 0.000 (0.005) loss 4.1719 (2.7064) lr 7.0337e-03 eta 0:35:55
epoch [12/30] batch [220/392] time 0.289 (0.297) data 0.000 (0.005) loss 1.9150 (2.7292) lr 7.0337e-03 eta 0:35:45
epoch [12/30] batch [240/392] time 0.277 (0.296) data 0.000 (0.005) loss 6.3203 (2.7728) lr 7.0337e-03 eta 0:35:36
epoch [12/30] batch [260/392] time 0.292 (0.297) data 0.000 (0.004) loss 2.8770 (2.7592) lr 7.0337e-03 eta 0:35:33
epoch [12/30] batch [280/392] time 0.275 (0.296) data 0.000 (0.004) loss 4.7656 (2.7405) lr 7.0337e-03 eta 0:35:22
epoch [12/30] batch [300/392] time 0.306 (0.296) data 0.000 (0.004) loss 3.0684 (2.7505) lr 7.0337e-03 eta 0:35:15
epoch [12/30] batch [320/392] time 0.290 (0.296) data 0.000 (0.004) loss 2.4551 (2.7097) lr 7.0337e-03 eta 0:35:10
epoch [12/30] batch [340/392] time 0.302 (0.296) data 0.000 (0.003) loss 2.2676 (2.7034) lr 7.0337e-03 eta 0:35:01
epoch [12/30] batch [360/392] time 0.283 (0.296) data 0.000 (0.003) loss 2.9844 (2.6903) lr 7.0337e-03 eta 0:34:55
epoch [12/30] batch [380/392] time 0.268 (0.294) data 0.000 (0.003) loss 0.7661 (2.6786) lr 7.0337e-03 eta 0:34:39
Evaluate on the *val* set
=> result
* total: 812
* correct: 587
* accuracy: 72.3%
* error: 27.7%
* macro_f1: 71.0%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar
epoch [13/30] batch [20/392] time 0.283 (0.346) data 0.000 (0.054) loss 1.3867 (2.3518) lr 6.5451e-03 eta 0:40:37
epoch [13/30] batch [40/392] time 0.281 (0.320) data 0.000 (0.027) loss 1.4727 (2.2471) lr 6.5451e-03 eta 0:37:28
epoch [13/30] batch [60/392] time 0.291 (0.311) data 0.000 (0.018) loss 2.9590 (2.3313) lr 6.5451e-03 eta 0:36:13
epoch [13/30] batch [80/392] time 0.280 (0.305) data 0.000 (0.014) loss 1.9775 (2.3189) lr 6.5451e-03 eta 0:35:25
epoch [13/30] batch [100/392] time 0.293 (0.302) data 0.000 (0.011) loss 1.6631 (2.3601) lr 6.5451e-03 eta 0:35:02
epoch [13/30] batch [120/392] time 0.288 (0.301) data 0.000 (0.009) loss 2.4473 (2.3312) lr 6.5451e-03 eta 0:34:44
epoch [13/30] batch [140/392] time 0.283 (0.299) data 0.000 (0.008) loss 5.7891 (2.4376) lr 6.5451e-03 eta 0:34:25
epoch [13/30] batch [160/392] time 0.285 (0.298) data 0.000 (0.007) loss 1.1855 (2.4985) lr 6.5451e-03 eta 0:34:13
epoch [13/30] batch [180/392] time 0.283 (0.297) data 0.000 (0.006) loss 2.0938 (2.4920) lr 6.5451e-03 eta 0:34:04
epoch [13/30] batch [200/392] time 0.284 (0.297) data 0.000 (0.006) loss 1.6182 (2.4872) lr 6.5451e-03 eta 0:33:57
epoch [13/30] batch [220/392] time 0.287 (0.297) data 0.000 (0.005) loss 2.1797 (2.4643) lr 6.5451e-03 eta 0:33:48
epoch [13/30] batch [240/392] time 0.281 (0.296) data 0.000 (0.005) loss 1.3320 (2.4406) lr 6.5451e-03 eta 0:33:39
epoch [13/30] batch [260/392] time 0.274 (0.297) data 0.000 (0.004) loss 1.1240 (2.4595) lr 6.5451e-03 eta 0:33:36
epoch [13/30] batch [280/392] time 0.290 (0.296) data 0.000 (0.004) loss 1.0576 (2.4443) lr 6.5451e-03 eta 0:33:27
epoch [13/30] batch [300/392] time 0.290 (0.296) data 0.000 (0.004) loss 2.3867 (2.4050) lr 6.5451e-03 eta 0:33:19
epoch [13/30] batch [320/392] time 0.280 (0.296) data 0.000 (0.004) loss 2.1621 (2.4073) lr 6.5451e-03 eta 0:33:11
epoch [13/30] batch [340/392] time 0.304 (0.295) data 0.000 (0.003) loss 1.2197 (2.4090) lr 6.5451e-03 eta 0:33:04
epoch [13/30] batch [360/392] time 0.324 (0.295) data 0.000 (0.003) loss 1.0039 (2.4160) lr 6.5451e-03 eta 0:32:57
epoch [13/30] batch [380/392] time 0.272 (0.294) data 0.000 (0.003) loss 1.7529 (2.4173) lr 6.5451e-03 eta 0:32:42
Evaluate on the *val* set
=> result
* total: 812
* correct: 585
* accuracy: 72.0%
* error: 28.0%
* macro_f1: 70.8%
epoch [14/30] batch [20/392] time 0.290 (0.349) data 0.000 (0.049) loss 2.4492 (2.9676) lr 6.0396e-03 eta 0:38:37
epoch [14/30] batch [40/392] time 0.313 (0.322) data 0.000 (0.025) loss 1.2529 (2.5644) lr 6.0396e-03 eta 0:35:30
epoch [14/30] batch [60/392] time 0.293 (0.311) data 0.000 (0.017) loss 3.3926 (2.4260) lr 6.0396e-03 eta 0:34:14
epoch [14/30] batch [80/392] time 0.274 (0.307) data 0.000 (0.013) loss 2.4082 (2.3925) lr 6.0396e-03 eta 0:33:38
epoch [14/30] batch [100/392] time 0.289 (0.303) data 0.000 (0.010) loss 2.4883 (2.4229) lr 6.0396e-03 eta 0:33:08
epoch [14/30] batch [120/392] time 0.294 (0.301) data 0.000 (0.008) loss 1.3076 (2.3772) lr 6.0396e-03 eta 0:32:48
epoch [14/30] batch [140/392] time 0.320 (0.300) data 0.000 (0.007) loss 1.5381 (2.5340) lr 6.0396e-03 eta 0:32:37
epoch [14/30] batch [160/392] time 0.305 (0.299) data 0.000 (0.006) loss 1.8643 (2.4904) lr 6.0396e-03 eta 0:32:25
epoch [14/30] batch [180/392] time 0.295 (0.298) data 0.000 (0.006) loss 4.7070 (2.5066) lr 6.0396e-03 eta 0:32:12
epoch [14/30] batch [200/392] time 0.288 (0.297) data 0.000 (0.005) loss 4.8281 (2.5180) lr 6.0396e-03 eta 0:32:01
epoch [14/30] batch [220/392] time 0.293 (0.297) data 0.000 (0.005) loss 2.3828 (2.5087) lr 6.0396e-03 eta 0:31:55
epoch [14/30] batch [240/392] time 0.280 (0.297) data 0.000 (0.004) loss 0.7197 (2.4715) lr 6.0396e-03 eta 0:31:46
epoch [14/30] batch [260/392] time 0.296 (0.297) data 0.000 (0.004) loss 1.4668 (2.4520) lr 6.0396e-03 eta 0:31:39
epoch [14/30] batch [280/392] time 0.292 (0.296) data 0.000 (0.004) loss 1.5469 (2.4599) lr 6.0396e-03 eta 0:31:31
epoch [14/30] batch [300/392] time 0.292 (0.296) data 0.000 (0.004) loss 1.2891 (2.4350) lr 6.0396e-03 eta 0:31:24
epoch [14/30] batch [320/392] time 0.290 (0.296) data 0.000 (0.003) loss 1.3008 (2.4281) lr 6.0396e-03 eta 0:31:14
epoch [14/30] batch [340/392] time 0.279 (0.296) data 0.000 (0.003) loss 6.7812 (2.4586) lr 6.0396e-03 eta 0:31:09
epoch [14/30] batch [360/392] time 0.300 (0.296) data 0.000 (0.003) loss 4.6953 (2.4631) lr 6.0396e-03 eta 0:31:02
epoch [14/30] batch [380/392] time 0.272 (0.294) data 0.000 (0.003) loss 1.2725 (2.4786) lr 6.0396e-03 eta 0:30:49
Evaluate on the *val* set
=> result
* total: 812
* correct: 590
* accuracy: 72.7%
* error: 27.3%
* macro_f1: 71.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar
epoch [15/30] batch [20/392] time 0.278 (0.352) data 0.000 (0.051) loss 3.9141 (2.5217) lr 5.5226e-03 eta 0:36:38
epoch [15/30] batch [40/392] time 0.276 (0.320) data 0.000 (0.026) loss 0.9971 (2.2803) lr 5.5226e-03 eta 0:33:16
epoch [15/30] batch [60/392] time 0.282 (0.310) data 0.000 (0.017) loss 4.2461 (2.3036) lr 5.5226e-03 eta 0:32:03
epoch [15/30] batch [80/392] time 0.277 (0.305) data 0.000 (0.013) loss 4.9844 (2.3945) lr 5.5226e-03 eta 0:31:28
epoch [15/30] batch [100/392] time 0.292 (0.302) data 0.000 (0.010) loss 3.4727 (2.4480) lr 5.5226e-03 eta 0:31:02
epoch [15/30] batch [120/392] time 0.288 (0.300) data 0.000 (0.009) loss 3.5762 (2.4283) lr 5.5226e-03 eta 0:30:47
epoch [15/30] batch [140/392] time 0.281 (0.300) data 0.000 (0.008) loss 4.1406 (2.3645) lr 5.5226e-03 eta 0:30:38
epoch [15/30] batch [160/392] time 0.297 (0.299) data 0.000 (0.007) loss 1.2021 (2.3674) lr 5.5226e-03 eta 0:30:25
epoch [15/30] batch [180/392] time 0.294 (0.297) data 0.000 (0.006) loss 2.0430 (2.4094) lr 5.5226e-03 eta 0:30:09
epoch [15/30] batch [200/392] time 0.284 (0.296) data 0.000 (0.005) loss 2.1250 (2.4472) lr 5.5226e-03 eta 0:30:00
epoch [15/30] batch [220/392] time 0.295 (0.296) data 0.000 (0.005) loss 1.1611 (2.4561) lr 5.5226e-03 eta 0:29:50
epoch [15/30] batch [240/392] time 0.336 (0.296) data 0.000 (0.005) loss 2.3359 (2.4318) lr 5.5226e-03 eta 0:29:45
epoch [15/30] batch [260/392] time 0.293 (0.296) data 0.000 (0.004) loss 2.5508 (2.4039) lr 5.5226e-03 eta 0:29:37
epoch [15/30] batch [280/392] time 0.313 (0.295) data 0.000 (0.004) loss 3.1914 (2.3951) lr 5.5226e-03 eta 0:29:29
epoch [15/30] batch [300/392] time 0.296 (0.295) data 0.000 (0.004) loss 0.7007 (2.3914) lr 5.5226e-03 eta 0:29:21
epoch [15/30] batch [320/392] time 0.280 (0.294) data 0.000 (0.003) loss 3.7266 (2.4138) lr 5.5226e-03 eta 0:29:12
epoch [15/30] batch [340/392] time 0.301 (0.294) data 0.000 (0.003) loss 2.2676 (2.4288) lr 5.5226e-03 eta 0:29:05
epoch [15/30] batch [360/392] time 0.303 (0.294) data 0.000 (0.003) loss 4.7422 (2.4319) lr 5.5226e-03 eta 0:29:01
epoch [15/30] batch [380/392] time 0.271 (0.293) data 0.000 (0.003) loss 4.5391 (2.4225) lr 5.5226e-03 eta 0:28:48
Evaluate on the *val* set
=> result
* total: 812
* correct: 575
* accuracy: 70.8%
* error: 29.2%
* macro_f1: 69.6%
epoch [16/30] batch [20/392] time 0.283 (0.346) data 0.000 (0.050) loss 2.1465 (2.4471) lr 5.0000e-03 eta 0:33:50
epoch [16/30] batch [40/392] time 0.287 (0.319) data 0.000 (0.025) loss 1.7871 (2.2417) lr 5.0000e-03 eta 0:31:04
epoch [16/30] batch [60/392] time 0.290 (0.310) data 0.000 (0.017) loss 2.7852 (2.2799) lr 5.0000e-03 eta 0:30:03
epoch [16/30] batch [80/392] time 0.285 (0.305) data 0.000 (0.013) loss 1.1309 (2.2367) lr 5.0000e-03 eta 0:29:28
epoch [16/30] batch [100/392] time 0.294 (0.303) data 0.000 (0.010) loss 2.3242 (2.2828) lr 5.0000e-03 eta 0:29:08
epoch [16/30] batch [120/392] time 0.308 (0.301) data 0.000 (0.009) loss 2.7012 (2.2841) lr 5.0000e-03 eta 0:28:52
epoch [16/30] batch [140/392] time 0.288 (0.300) data 0.000 (0.007) loss 3.0332 (2.3249) lr 5.0000e-03 eta 0:28:39
epoch [16/30] batch [160/392] time 0.302 (0.299) data 0.000 (0.007) loss 1.7803 (2.2813) lr 5.0000e-03 eta 0:28:27
epoch [16/30] batch [180/392] time 0.284 (0.297) data 0.000 (0.006) loss 2.8477 (2.3056) lr 5.0000e-03 eta 0:28:13
epoch [16/30] batch [200/392] time 0.294 (0.297) data 0.000 (0.005) loss 5.6016 (2.3563) lr 5.0000e-03 eta 0:28:05
epoch [16/30] batch [220/392] time 0.285 (0.296) data 0.000 (0.005) loss 4.6719 (2.4196) lr 5.0000e-03 eta 0:27:56
epoch [16/30] batch [240/392] time 0.326 (0.296) data 0.000 (0.004) loss 4.6680 (2.3883) lr 5.0000e-03 eta 0:27:48
epoch [16/30] batch [260/392] time 0.290 (0.296) data 0.000 (0.004) loss 4.6992 (2.4449) lr 5.0000e-03 eta 0:27:41
epoch [16/30] batch [280/392] time 0.337 (0.296) data 0.000 (0.004) loss 2.9062 (2.4462) lr 5.0000e-03 eta 0:27:37
epoch [16/30] batch [300/392] time 0.279 (0.296) data 0.000 (0.004) loss 4.5781 (2.4721) lr 5.0000e-03 eta 0:27:31
epoch [16/30] batch [320/392] time 0.280 (0.296) data 0.000 (0.003) loss 3.4453 (2.4860) lr 5.0000e-03 eta 0:27:23
epoch [16/30] batch [340/392] time 0.289 (0.295) data 0.000 (0.003) loss 2.7617 (2.4760) lr 5.0000e-03 eta 0:27:15
epoch [16/30] batch [360/392] time 0.279 (0.295) data 0.000 (0.003) loss 3.7051 (2.4769) lr 5.0000e-03 eta 0:27:08
epoch [16/30] batch [380/392] time 0.270 (0.294) data 0.000 (0.003) loss 3.3184 (2.4721) lr 5.0000e-03 eta 0:26:56
Evaluate on the *val* set
=> result
* total: 812
* correct: 583
* accuracy: 71.8%
* error: 28.2%
* macro_f1: 70.8%
epoch [17/30] batch [20/392] time 0.283 (0.349) data 0.000 (0.051) loss 2.0078 (2.8331) lr 4.4774e-03 eta 0:31:50
epoch [17/30] batch [40/392] time 0.285 (0.318) data 0.000 (0.026) loss 5.1094 (2.6123) lr 4.4774e-03 eta 0:28:49
epoch [17/30] batch [60/392] time 0.288 (0.310) data 0.000 (0.017) loss 1.8857 (2.4540) lr 4.4774e-03 eta 0:28:02
epoch [17/30] batch [80/392] time 0.279 (0.305) data 0.000 (0.013) loss 1.3506 (2.3889) lr 4.4774e-03 eta 0:27:27
epoch [17/30] batch [100/392] time 0.281 (0.302) data 0.000 (0.010) loss 3.6484 (2.4335) lr 4.4774e-03 eta 0:27:04
epoch [17/30] batch [120/392] time 0.291 (0.300) data 0.000 (0.009) loss 5.0781 (2.4623) lr 4.4774e-03 eta 0:26:50
epoch [17/30] batch [140/392] time 0.279 (0.298) data 0.000 (0.008) loss 1.0088 (2.4494) lr 4.4774e-03 eta 0:26:35
epoch [17/30] batch [160/392] time 0.301 (0.297) data 0.000 (0.007) loss 3.2461 (2.4726) lr 4.4774e-03 eta 0:26:23
epoch [17/30] batch [180/392] time 0.286 (0.297) data 0.000 (0.006) loss 0.3418 (2.4722) lr 4.4774e-03 eta 0:26:15
epoch [17/30] batch [200/392] time 0.291 (0.297) data 0.000 (0.005) loss 2.6719 (2.4797) lr 4.4774e-03 eta 0:26:08
epoch [17/30] batch [220/392] time 0.283 (0.297) data 0.000 (0.005) loss 2.7227 (2.5420) lr 4.4774e-03 eta 0:26:01
epoch [17/30] batch [240/392] time 0.304 (0.296) data 0.000 (0.004) loss 1.5166 (2.5181) lr 4.4774e-03 eta 0:25:51
epoch [17/30] batch [260/392] time 0.282 (0.295) data 0.000 (0.004) loss 2.0547 (2.4897) lr 4.4774e-03 eta 0:25:43
epoch [17/30] batch [280/392] time 0.294 (0.295) data 0.000 (0.004) loss 2.1543 (2.4949) lr 4.4774e-03 eta 0:25:37
epoch [17/30] batch [300/392] time 0.286 (0.295) data 0.000 (0.004) loss 0.7720 (2.4983) lr 4.4774e-03 eta 0:25:28
epoch [17/30] batch [320/392] time 0.325 (0.295) data 0.000 (0.003) loss 1.1816 (2.4728) lr 4.4774e-03 eta 0:25:22
epoch [17/30] batch [340/392] time 0.291 (0.295) data 0.000 (0.003) loss 0.5703 (2.4721) lr 4.4774e-03 eta 0:25:17
epoch [17/30] batch [360/392] time 0.311 (0.295) data 0.001 (0.003) loss 2.3398 (2.4974) lr 4.4774e-03 eta 0:25:11
epoch [17/30] batch [380/392] time 0.270 (0.294) data 0.000 (0.003) loss 2.2246 (2.4892) lr 4.4774e-03 eta 0:24:59
Evaluate on the *val* set
=> result
* total: 812
* correct: 595
* accuracy: 73.3%
* error: 26.7%
* macro_f1: 72.5%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar
epoch [18/30] batch [20/392] time 0.292 (0.371) data 0.000 (0.066) loss 1.5635 (1.7829) lr 3.9604e-03 eta 0:31:23
epoch [18/30] batch [40/392] time 0.284 (0.330) data 0.000 (0.033) loss 3.7812 (2.0559) lr 3.9604e-03 eta 0:27:49
epoch [18/30] batch [60/392] time 0.291 (0.318) data 0.000 (0.022) loss 2.4336 (2.1742) lr 3.9604e-03 eta 0:26:39
epoch [18/30] batch [80/392] time 0.305 (0.311) data 0.000 (0.017) loss 1.1914 (2.2179) lr 3.9604e-03 eta 0:26:00
epoch [18/30] batch [100/392] time 0.284 (0.306) data 0.000 (0.013) loss 3.1250 (2.2329) lr 3.9604e-03 eta 0:25:29
epoch [18/30] batch [120/392] time 0.283 (0.303) data 0.000 (0.011) loss 3.0391 (2.2334) lr 3.9604e-03 eta 0:25:07
epoch [18/30] batch [140/392] time 0.288 (0.301) data 0.000 (0.010) loss 4.1523 (2.2039) lr 3.9604e-03 eta 0:24:52
epoch [18/30] batch [160/392] time 0.290 (0.300) data 0.000 (0.009) loss 4.3672 (2.2677) lr 3.9604e-03 eta 0:24:41
epoch [18/30] batch [180/392] time 0.288 (0.300) data 0.000 (0.008) loss 0.3359 (2.2402) lr 3.9604e-03 eta 0:24:33
epoch [18/30] batch [200/392] time 0.286 (0.299) data 0.000 (0.007) loss 1.4580 (2.2463) lr 3.9604e-03 eta 0:24:25
epoch [18/30] batch [220/392] time 0.280 (0.298) data 0.000 (0.006) loss 1.0850 (2.2027) lr 3.9604e-03 eta 0:24:14
epoch [18/30] batch [240/392] time 0.297 (0.298) data 0.000 (0.006) loss 1.9385 (2.2265) lr 3.9604e-03 eta 0:24:05
epoch [18/30] batch [260/392] time 0.297 (0.297) data 0.000 (0.005) loss 2.8320 (2.2470) lr 3.9604e-03 eta 0:23:57
epoch [18/30] batch [280/392] time 0.299 (0.297) data 0.000 (0.005) loss 1.5010 (2.2397) lr 3.9604e-03 eta 0:23:50
epoch [18/30] batch [300/392] time 0.276 (0.297) data 0.000 (0.005) loss 2.6504 (2.2457) lr 3.9604e-03 eta 0:23:43
epoch [18/30] batch [320/392] time 0.296 (0.297) data 0.000 (0.004) loss 1.7373 (2.2634) lr 3.9604e-03 eta 0:23:36
epoch [18/30] batch [340/392] time 0.294 (0.296) data 0.000 (0.004) loss 2.0840 (2.2579) lr 3.9604e-03 eta 0:23:28
epoch [18/30] batch [360/392] time 0.280 (0.296) data 0.000 (0.004) loss 2.4375 (2.2764) lr 3.9604e-03 eta 0:23:21
epoch [18/30] batch [380/392] time 0.271 (0.295) data 0.000 (0.004) loss 3.4883 (2.2997) lr 3.9604e-03 eta 0:23:10
Evaluate on the *val* set
=> result
* total: 812
* correct: 601
* accuracy: 74.0%
* error: 26.0%
* macro_f1: 73.1%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar
epoch [19/30] batch [20/392] time 0.285 (0.350) data 0.000 (0.054) loss 3.0312 (2.4160) lr 3.4549e-03 eta 0:27:18
epoch [19/30] batch [40/392] time 0.315 (0.320) data 0.000 (0.027) loss 1.1377 (2.3389) lr 3.4549e-03 eta 0:24:50
epoch [19/30] batch [60/392] time 0.289 (0.309) data 0.000 (0.018) loss 3.2734 (2.4237) lr 3.4549e-03 eta 0:23:56
epoch [19/30] batch [80/392] time 0.296 (0.307) data 0.000 (0.014) loss 3.4570 (2.4915) lr 3.4549e-03 eta 0:23:38
epoch [19/30] batch [100/392] time 0.285 (0.303) data 0.000 (0.011) loss 5.3125 (2.4433) lr 3.4549e-03 eta 0:23:16
epoch [19/30] batch [120/392] time 0.285 (0.301) data 0.000 (0.009) loss 2.4824 (2.3299) lr 3.4549e-03 eta 0:23:01
epoch [19/30] batch [140/392] time 0.298 (0.301) data 0.000 (0.008) loss 1.3740 (2.3284) lr 3.4549e-03 eta 0:22:52
epoch [19/30] batch [160/392] time 0.281 (0.300) data 0.000 (0.007) loss 2.4629 (2.3100) lr 3.4549e-03 eta 0:22:42
epoch [19/30] batch [180/392] time 0.284 (0.298) data 0.000 (0.006) loss 2.3516 (2.2719) lr 3.4549e-03 eta 0:22:30
epoch [19/30] batch [200/392] time 0.285 (0.298) data 0.000 (0.006) loss 3.5527 (2.2920) lr 3.4549e-03 eta 0:22:22
epoch [19/30] batch [220/392] time 0.295 (0.297) data 0.000 (0.005) loss 1.7090 (2.2651) lr 3.4549e-03 eta 0:22:13
epoch [19/30] batch [240/392] time 0.291 (0.297) data 0.000 (0.005) loss 4.2812 (2.2809) lr 3.4549e-03 eta 0:22:04
epoch [19/30] batch [260/392] time 0.281 (0.296) data 0.000 (0.004) loss 2.8164 (2.3140) lr 3.4549e-03 eta 0:21:56
epoch [19/30] batch [280/392] time 0.284 (0.296) data 0.000 (0.004) loss 1.8389 (2.3397) lr 3.4549e-03 eta 0:21:48
epoch [19/30] batch [300/392] time 0.281 (0.295) data 0.000 (0.004) loss 2.1367 (2.3428) lr 3.4549e-03 eta 0:21:40
epoch [19/30] batch [320/392] time 0.280 (0.295) data 0.000 (0.004) loss 2.2969 (2.3480) lr 3.4549e-03 eta 0:21:32
epoch [19/30] batch [340/392] time 0.282 (0.295) data 0.000 (0.003) loss 1.8398 (2.3531) lr 3.4549e-03 eta 0:21:25
epoch [19/30] batch [360/392] time 0.286 (0.295) data 0.000 (0.003) loss 0.6572 (2.3515) lr 3.4549e-03 eta 0:21:20
epoch [19/30] batch [380/392] time 0.275 (0.294) data 0.000 (0.003) loss 2.8184 (2.3407) lr 3.4549e-03 eta 0:21:09
Evaluate on the *val* set
=> result
* total: 812
* correct: 594
* accuracy: 73.2%
* error: 26.8%
* macro_f1: 72.2%
epoch [20/30] batch [20/392] time 0.299 (0.354) data 0.000 (0.053) loss 3.9141 (2.5186) lr 2.9663e-03 eta 0:25:18
epoch [20/30] batch [40/392] time 0.300 (0.324) data 0.000 (0.026) loss 1.2539 (2.3726) lr 2.9663e-03 eta 0:23:05
epoch [20/30] batch [60/392] time 0.278 (0.313) data 0.000 (0.018) loss 1.1699 (2.4388) lr 2.9663e-03 eta 0:22:09
epoch [20/30] batch [80/392] time 0.317 (0.307) data 0.000 (0.013) loss 5.0703 (2.3365) lr 2.9663e-03 eta 0:21:40
epoch [20/30] batch [100/392] time 0.280 (0.304) data 0.000 (0.011) loss 1.0596 (2.2920) lr 2.9663e-03 eta 0:21:20
epoch [20/30] batch [120/392] time 0.277 (0.302) data 0.000 (0.009) loss 2.0840 (2.4048) lr 2.9663e-03 eta 0:21:04
epoch [20/30] batch [140/392] time 0.295 (0.300) data 0.000 (0.008) loss 2.0918 (2.3938) lr 2.9663e-03 eta 0:20:50
epoch [20/30] batch [160/392] time 0.285 (0.299) data 0.000 (0.007) loss 2.6895 (2.3830) lr 2.9663e-03 eta 0:20:41
epoch [20/30] batch [180/392] time 0.282 (0.298) data 0.000 (0.006) loss 3.9355 (2.3754) lr 2.9663e-03 eta 0:20:30
epoch [20/30] batch [200/392] time 0.291 (0.297) data 0.000 (0.006) loss 1.6875 (2.3954) lr 2.9663e-03 eta 0:20:20
epoch [20/30] batch [220/392] time 0.288 (0.296) data 0.000 (0.005) loss 1.5498 (2.4345) lr 2.9663e-03 eta 0:20:10
epoch [20/30] batch [240/392] time 0.283 (0.295) data 0.000 (0.005) loss 1.3145 (2.4472) lr 2.9663e-03 eta 0:20:02
epoch [20/30] batch [260/392] time 0.279 (0.295) data 0.000 (0.004) loss 3.1973 (2.4226) lr 2.9663e-03 eta 0:19:54
epoch [20/30] batch [280/392] time 0.283 (0.295) data 0.000 (0.004) loss 1.8584 (2.4269) lr 2.9663e-03 eta 0:19:48
epoch [20/30] batch [300/392] time 0.308 (0.295) data 0.000 (0.004) loss 2.1504 (2.4283) lr 2.9663e-03 eta 0:19:42
epoch [20/30] batch [320/392] time 0.294 (0.294) data 0.000 (0.004) loss 4.0469 (2.4330) lr 2.9663e-03 eta 0:19:34
epoch [20/30] batch [340/392] time 0.305 (0.294) data 0.001 (0.003) loss 1.3818 (2.4312) lr 2.9663e-03 eta 0:19:28
epoch [20/30] batch [360/392] time 0.290 (0.295) data 0.000 (0.003) loss 0.4521 (2.4346) lr 2.9663e-03 eta 0:19:24
epoch [20/30] batch [380/392] time 0.269 (0.293) data 0.000 (0.003) loss 3.1719 (2.4156) lr 2.9663e-03 eta 0:19:13
Evaluate on the *val* set
=> result
* total: 812
* correct: 592
* accuracy: 72.9%
* error: 27.1%
* macro_f1: 72.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-20
epoch [21/30] batch [20/392] time 0.293 (0.349) data 0.000 (0.053) loss 1.5996 (2.3783) lr 2.5000e-03 eta 0:22:40
epoch [21/30] batch [40/392] time 0.283 (0.321) data 0.000 (0.027) loss 3.5898 (2.3598) lr 2.5000e-03 eta 0:20:45
epoch [21/30] batch [60/392] time 0.299 (0.309) data 0.000 (0.018) loss 2.0996 (2.3865) lr 2.5000e-03 eta 0:19:53
epoch [21/30] batch [80/392] time 0.286 (0.305) data 0.000 (0.014) loss 1.7686 (2.3258) lr 2.5000e-03 eta 0:19:33
epoch [21/30] batch [100/392] time 0.281 (0.303) data 0.000 (0.011) loss 2.4297 (2.3553) lr 2.5000e-03 eta 0:19:17
epoch [21/30] batch [120/392] time 0.293 (0.302) data 0.000 (0.009) loss 0.9121 (2.3036) lr 2.5000e-03 eta 0:19:05
epoch [21/30] batch [140/392] time 0.283 (0.300) data 0.000 (0.008) loss 3.0117 (2.2940) lr 2.5000e-03 eta 0:18:52
epoch [21/30] batch [160/392] time 0.287 (0.299) data 0.000 (0.007) loss 1.3760 (2.3097) lr 2.5000e-03 eta 0:18:45
epoch [21/30] batch [180/392] time 0.293 (0.299) data 0.000 (0.006) loss 2.1211 (2.3037) lr 2.5000e-03 eta 0:18:36
epoch [21/30] batch [200/392] time 0.282 (0.298) data 0.000 (0.006) loss 4.7070 (2.3060) lr 2.5000e-03 eta 0:18:27
epoch [21/30] batch [220/392] time 0.281 (0.297) data 0.000 (0.005) loss 1.7236 (2.2958) lr 2.5000e-03 eta 0:18:18
epoch [21/30] batch [240/392] time 0.277 (0.296) data 0.000 (0.005) loss 1.0352 (2.2572) lr 2.5000e-03 eta 0:18:10
epoch [21/30] batch [260/392] time 0.278 (0.296) data 0.000 (0.004) loss 1.7979 (2.2605) lr 2.5000e-03 eta 0:18:02
epoch [21/30] batch [280/392] time 0.295 (0.295) data 0.000 (0.004) loss 1.1504 (2.2701) lr 2.5000e-03 eta 0:17:55
epoch [21/30] batch [300/392] time 0.290 (0.296) data 0.000 (0.004) loss 2.9121 (2.3029) lr 2.5000e-03 eta 0:17:50
epoch [21/30] batch [320/392] time 0.282 (0.295) data 0.000 (0.004) loss 2.9688 (2.3461) lr 2.5000e-03 eta 0:17:43
epoch [21/30] batch [340/392] time 0.285 (0.295) data 0.000 (0.003) loss 1.3945 (2.3461) lr 2.5000e-03 eta 0:17:36
epoch [21/30] batch [360/392] time 0.309 (0.295) data 0.000 (0.003) loss 1.7832 (2.3453) lr 2.5000e-03 eta 0:17:29
epoch [21/30] batch [380/392] time 0.271 (0.294) data 0.000 (0.003) loss 1.6826 (2.3472) lr 2.5000e-03 eta 0:17:19
Evaluate on the *val* set
=> result
* total: 812
* correct: 597
* accuracy: 73.5%
* error: 26.5%
* macro_f1: 72.8%
epoch [22/30] batch [20/392] time 0.291 (0.345) data 0.000 (0.054) loss 4.2070 (3.1841) lr 2.0611e-03 eta 0:20:09
epoch [22/30] batch [40/392] time 0.276 (0.320) data 0.000 (0.027) loss 2.7383 (2.8983) lr 2.0611e-03 eta 0:18:35
epoch [22/30] batch [60/392] time 0.288 (0.312) data 0.000 (0.018) loss 1.4170 (2.6291) lr 2.0611e-03 eta 0:18:02
epoch [22/30] batch [80/392] time 0.285 (0.307) data 0.000 (0.014) loss 0.9551 (2.5702) lr 2.0611e-03 eta 0:17:38
epoch [22/30] batch [100/392] time 0.291 (0.305) data 0.000 (0.011) loss 5.0391 (2.4981) lr 2.0611e-03 eta 0:17:24
epoch [22/30] batch [120/392] time 0.279 (0.302) data 0.000 (0.009) loss 3.2383 (2.4981) lr 2.0611e-03 eta 0:17:08
epoch [22/30] batch [140/392] time 0.287 (0.300) data 0.000 (0.008) loss 1.1084 (2.4311) lr 2.0611e-03 eta 0:16:56
epoch [22/30] batch [160/392] time 0.281 (0.299) data 0.000 (0.007) loss 0.2639 (2.4340) lr 2.0611e-03 eta 0:16:46
epoch [22/30] batch [180/392] time 0.295 (0.297) data 0.000 (0.006) loss 3.3184 (2.5189) lr 2.0611e-03 eta 0:16:35
epoch [22/30] batch [200/392] time 0.276 (0.297) data 0.000 (0.006) loss 1.8125 (2.4834) lr 2.0611e-03 eta 0:16:28
epoch [22/30] batch [220/392] time 0.298 (0.297) data 0.000 (0.005) loss 2.3320 (2.4941) lr 2.0611e-03 eta 0:16:22
epoch [22/30] batch [240/392] time 0.307 (0.297) data 0.000 (0.005) loss 2.1270 (2.4641) lr 2.0611e-03 eta 0:16:16
epoch [22/30] batch [260/392] time 0.289 (0.297) data 0.000 (0.004) loss 0.9321 (2.4287) lr 2.0611e-03 eta 0:16:09
epoch [22/30] batch [280/392] time 0.286 (0.296) data 0.000 (0.004) loss 2.7734 (2.4562) lr 2.0611e-03 eta 0:16:01
epoch [22/30] batch [300/392] time 0.283 (0.296) data 0.000 (0.004) loss 0.9434 (2.4379) lr 2.0611e-03 eta 0:15:54
epoch [22/30] batch [320/392] time 0.291 (0.295) data 0.000 (0.004) loss 1.4697 (2.4111) lr 2.0611e-03 eta 0:15:46
epoch [22/30] batch [340/392] time 0.293 (0.295) data 0.000 (0.003) loss 0.9736 (2.3644) lr 2.0611e-03 eta 0:15:41
epoch [22/30] batch [360/392] time 0.298 (0.295) data 0.000 (0.003) loss 1.5684 (2.3586) lr 2.0611e-03 eta 0:15:35
epoch [22/30] batch [380/392] time 0.272 (0.294) data 0.000 (0.003) loss 4.2227 (2.3700) lr 2.0611e-03 eta 0:15:25
Evaluate on the *val* set
=> result
* total: 812
* correct: 591
* accuracy: 72.8%
* error: 27.2%
* macro_f1: 71.7%
epoch [23/30] batch [20/392] time 0.286 (0.353) data 0.000 (0.050) loss 0.8838 (2.2251) lr 1.6543e-03 eta 0:18:20
epoch [23/30] batch [40/392] time 0.310 (0.321) data 0.000 (0.025) loss 4.1328 (2.2355) lr 1.6543e-03 eta 0:16:32
epoch [23/30] batch [60/392] time 0.282 (0.310) data 0.000 (0.017) loss 1.5518 (2.1115) lr 1.6543e-03 eta 0:15:52
epoch [23/30] batch [80/392] time 0.285 (0.306) data 0.000 (0.013) loss 2.4453 (2.1273) lr 1.6543e-03 eta 0:15:36
epoch [23/30] batch [100/392] time 0.295 (0.302) data 0.000 (0.010) loss 1.1104 (2.1561) lr 1.6543e-03 eta 0:15:17
epoch [23/30] batch [120/392] time 0.288 (0.300) data 0.000 (0.009) loss 1.1074 (2.2330) lr 1.6543e-03 eta 0:15:05
epoch [23/30] batch [140/392] time 0.279 (0.298) data 0.000 (0.007) loss 2.9961 (2.1988) lr 1.6543e-03 eta 0:14:54
epoch [23/30] batch [160/392] time 0.289 (0.298) data 0.000 (0.007) loss 0.7925 (2.1638) lr 1.6543e-03 eta 0:14:46
epoch [23/30] batch [180/392] time 0.300 (0.297) data 0.000 (0.006) loss 1.2500 (2.2035) lr 1.6543e-03 eta 0:14:38
epoch [23/30] batch [200/392] time 0.288 (0.297) data 0.000 (0.005) loss 2.3164 (2.2350) lr 1.6543e-03 eta 0:14:31
epoch [23/30] batch [220/392] time 0.292 (0.296) data 0.000 (0.005) loss 2.6816 (2.2166) lr 1.6543e-03 eta 0:14:24
epoch [23/30] batch [240/392] time 0.275 (0.296) data 0.000 (0.004) loss 1.3018 (2.2577) lr 1.6543e-03 eta 0:14:16
epoch [23/30] batch [260/392] time 0.292 (0.296) data 0.000 (0.004) loss 1.8037 (2.2724) lr 1.6543e-03 eta 0:14:10
epoch [23/30] batch [280/392] time 0.279 (0.296) data 0.000 (0.004) loss 4.4297 (2.2666) lr 1.6543e-03 eta 0:14:04
epoch [23/30] batch [300/392] time 0.288 (0.295) data 0.000 (0.004) loss 2.8672 (2.2679) lr 1.6543e-03 eta 0:13:56
epoch [23/30] batch [320/392] time 0.286 (0.295) data 0.000 (0.003) loss 1.9053 (2.2666) lr 1.6543e-03 eta 0:13:50
epoch [23/30] batch [340/392] time 0.282 (0.295) data 0.000 (0.003) loss 1.5791 (2.2360) lr 1.6543e-03 eta 0:13:43
epoch [23/30] batch [360/392] time 0.278 (0.294) data 0.000 (0.003) loss 2.2285 (2.2360) lr 1.6543e-03 eta 0:13:37
epoch [23/30] batch [380/392] time 0.273 (0.293) data 0.000 (0.003) loss 2.4512 (2.2464) lr 1.6543e-03 eta 0:13:28
Evaluate on the *val* set
=> result
* total: 812
* correct: 596
* accuracy: 73.4%
* error: 26.6%
* macro_f1: 72.7%
epoch [24/30] batch [20/392] time 0.272 (0.342) data 0.000 (0.050) loss 2.1113 (2.3211) lr 1.2843e-03 eta 0:15:30
epoch [24/30] batch [40/392] time 0.317 (0.318) data 0.000 (0.025) loss 2.4023 (2.1299) lr 1.2843e-03 eta 0:14:21
epoch [24/30] batch [60/392] time 0.283 (0.309) data 0.000 (0.017) loss 1.2207 (2.0799) lr 1.2843e-03 eta 0:13:49
epoch [24/30] batch [80/392] time 0.294 (0.305) data 0.000 (0.013) loss 2.4141 (2.2064) lr 1.2843e-03 eta 0:13:32
epoch [24/30] batch [100/392] time 0.291 (0.303) data 0.000 (0.010) loss 0.5444 (2.1948) lr 1.2843e-03 eta 0:13:21
epoch [24/30] batch [120/392] time 0.289 (0.301) data 0.000 (0.009) loss 3.7617 (2.3438) lr 1.2843e-03 eta 0:13:09
epoch [24/30] batch [140/392] time 0.283 (0.300) data 0.000 (0.007) loss 2.3086 (2.3084) lr 1.2843e-03 eta 0:13:01
epoch [24/30] batch [160/392] time 0.297 (0.300) data 0.000 (0.007) loss 2.4668 (2.2925) lr 1.2843e-03 eta 0:12:54
epoch [24/30] batch [180/392] time 0.287 (0.299) data 0.001 (0.006) loss 1.6992 (2.2606) lr 1.2843e-03 eta 0:12:46
epoch [24/30] batch [200/392] time 0.287 (0.298) data 0.000 (0.005) loss 2.6445 (2.2611) lr 1.2843e-03 eta 0:12:38
epoch [24/30] batch [220/392] time 0.297 (0.298) data 0.000 (0.005) loss 1.4951 (2.2761) lr 1.2843e-03 eta 0:12:30
epoch [24/30] batch [240/392] time 0.302 (0.297) data 0.000 (0.004) loss 2.8496 (2.2711) lr 1.2843e-03 eta 0:12:23
epoch [24/30] batch [260/392] time 0.281 (0.296) data 0.000 (0.004) loss 2.4590 (2.2480) lr 1.2843e-03 eta 0:12:16
epoch [24/30] batch [280/392] time 0.286 (0.296) data 0.000 (0.004) loss 2.9902 (2.2668) lr 1.2843e-03 eta 0:12:09
epoch [24/30] batch [300/392] time 0.277 (0.296) data 0.000 (0.004) loss 3.1113 (2.2506) lr 1.2843e-03 eta 0:12:02
epoch [24/30] batch [320/392] time 0.289 (0.296) data 0.000 (0.003) loss 0.3054 (2.2484) lr 1.2843e-03 eta 0:11:56
epoch [24/30] batch [340/392] time 0.292 (0.296) data 0.000 (0.003) loss 4.2109 (2.2492) lr 1.2843e-03 eta 0:11:50
epoch [24/30] batch [360/392] time 0.289 (0.295) data 0.000 (0.003) loss 0.1389 (2.2404) lr 1.2843e-03 eta 0:11:44
epoch [24/30] batch [380/392] time 0.272 (0.294) data 0.000 (0.003) loss 3.0996 (2.2632) lr 1.2843e-03 eta 0:11:35
Evaluate on the *val* set
=> result
* total: 812
* correct: 606
* accuracy: 74.6%
* error: 25.4%
* macro_f1: 73.9%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar
epoch [25/30] batch [20/392] time 0.296 (0.353) data 0.000 (0.050) loss 0.9150 (1.8657) lr 9.5492e-04 eta 0:13:44
epoch [25/30] batch [40/392] time 0.286 (0.322) data 0.000 (0.025) loss 2.9902 (2.2340) lr 9.5492e-04 eta 0:12:24
epoch [25/30] batch [60/392] time 0.295 (0.311) data 0.000 (0.017) loss 0.4380 (2.2632) lr 9.5492e-04 eta 0:11:52
epoch [25/30] batch [80/392] time 0.283 (0.306) data 0.000 (0.013) loss 1.6592 (2.3254) lr 9.5492e-04 eta 0:11:35
epoch [25/30] batch [100/392] time 0.285 (0.303) data 0.000 (0.010) loss 0.5029 (2.3105) lr 9.5492e-04 eta 0:11:22
epoch [25/30] batch [120/392] time 0.283 (0.300) data 0.000 (0.009) loss 2.2578 (2.2799) lr 9.5492e-04 eta 0:11:10
epoch [25/30] batch [140/392] time 0.294 (0.300) data 0.000 (0.007) loss 3.2266 (2.2545) lr 9.5492e-04 eta 0:11:03
epoch [25/30] batch [160/392] time 0.284 (0.299) data 0.000 (0.007) loss 1.5811 (2.2494) lr 9.5492e-04 eta 0:10:55
epoch [25/30] batch [180/392] time 0.276 (0.297) data 0.000 (0.006) loss 1.0400 (2.2499) lr 9.5492e-04 eta 0:10:45
epoch [25/30] batch [200/392] time 0.284 (0.297) data 0.000 (0.005) loss 1.7617 (2.2389) lr 9.5492e-04 eta 0:10:38
epoch [25/30] batch [220/392] time 0.282 (0.297) data 0.000 (0.005) loss 0.9507 (2.2556) lr 9.5492e-04 eta 0:10:32
epoch [25/30] batch [240/392] time 0.282 (0.296) data 0.000 (0.004) loss 2.9434 (2.2771) lr 9.5492e-04 eta 0:10:26
epoch [25/30] batch [260/392] time 0.281 (0.296) data 0.001 (0.004) loss 1.1396 (2.2633) lr 9.5492e-04 eta 0:10:19
epoch [25/30] batch [280/392] time 0.302 (0.296) data 0.000 (0.004) loss 0.7661 (2.2720) lr 9.5492e-04 eta 0:10:12
epoch [25/30] batch [300/392] time 0.283 (0.295) data 0.000 (0.004) loss 3.1113 (2.2833) lr 9.5492e-04 eta 0:10:06
epoch [25/30] batch [320/392] time 0.284 (0.295) data 0.000 (0.003) loss 2.9336 (2.2771) lr 9.5492e-04 eta 0:10:00
epoch [25/30] batch [340/392] time 0.284 (0.295) data 0.000 (0.003) loss 2.9766 (2.2539) lr 9.5492e-04 eta 0:09:53
epoch [25/30] batch [360/392] time 0.282 (0.295) data 0.000 (0.003) loss 1.1934 (2.2597) lr 9.5492e-04 eta 0:09:47
epoch [25/30] batch [380/392] time 0.276 (0.294) data 0.000 (0.003) loss 1.8896 (2.2709) lr 9.5492e-04 eta 0:09:39
Evaluate on the *val* set
=> result
* total: 812
* correct: 610
* accuracy: 75.1%
* error: 24.9%
* macro_f1: 74.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar
epoch [26/30] batch [20/392] time 0.281 (0.341) data 0.000 (0.050) loss 1.3701 (2.3679) lr 6.6987e-04 eta 0:11:01
epoch [26/30] batch [40/392] time 0.291 (0.318) data 0.000 (0.025) loss 3.8574 (2.3284) lr 6.6987e-04 eta 0:10:09
epoch [26/30] batch [60/392] time 0.291 (0.309) data 0.000 (0.017) loss 1.1797 (2.2717) lr 6.6987e-04 eta 0:09:47
epoch [26/30] batch [80/392] time 0.280 (0.304) data 0.000 (0.013) loss 1.7832 (2.2904) lr 6.6987e-04 eta 0:09:32
epoch [26/30] batch [100/392] time 0.281 (0.301) data 0.000 (0.010) loss 0.7090 (2.2356) lr 6.6987e-04 eta 0:09:20
epoch [26/30] batch [120/392] time 0.298 (0.301) data 0.000 (0.009) loss 5.0352 (2.2802) lr 6.6987e-04 eta 0:09:13
epoch [26/30] batch [140/392] time 0.282 (0.299) data 0.000 (0.007) loss 2.0938 (2.2662) lr 6.6987e-04 eta 0:09:03
epoch [26/30] batch [160/392] time 0.286 (0.298) data 0.000 (0.007) loss 1.4795 (2.2488) lr 6.6987e-04 eta 0:08:56
epoch [26/30] batch [180/392] time 0.289 (0.298) data 0.000 (0.006) loss 3.1230 (2.2776) lr 6.6987e-04 eta 0:08:49
epoch [26/30] batch [200/392] time 0.288 (0.297) data 0.000 (0.005) loss 3.4199 (2.2363) lr 6.6987e-04 eta 0:08:43
epoch [26/30] batch [220/392] time 0.285 (0.297) data 0.000 (0.005) loss 3.1094 (2.2028) lr 6.6987e-04 eta 0:08:36
epoch [26/30] batch [240/392] time 0.306 (0.296) data 0.000 (0.004) loss 2.0664 (2.1631) lr 6.6987e-04 eta 0:08:29
epoch [26/30] batch [260/392] time 0.284 (0.296) data 0.000 (0.004) loss 3.0117 (2.1565) lr 6.6987e-04 eta 0:08:23
epoch [26/30] batch [280/392] time 0.284 (0.296) data 0.000 (0.004) loss 2.7227 (2.1718) lr 6.6987e-04 eta 0:08:16
epoch [26/30] batch [300/392] time 0.285 (0.295) data 0.000 (0.004) loss 2.5000 (2.1746) lr 6.6987e-04 eta 0:08:10
epoch [26/30] batch [320/392] time 0.280 (0.295) data 0.000 (0.003) loss 1.8438 (2.1757) lr 6.6987e-04 eta 0:08:04
epoch [26/30] batch [340/392] time 0.287 (0.295) data 0.000 (0.003) loss 1.1211 (2.1607) lr 6.6987e-04 eta 0:07:58
epoch [26/30] batch [360/392] time 0.299 (0.295) data 0.000 (0.003) loss 3.9727 (2.1629) lr 6.6987e-04 eta 0:07:52
epoch [26/30] batch [380/392] time 0.270 (0.294) data 0.000 (0.003) loss 1.8135 (2.1517) lr 6.6987e-04 eta 0:07:44
Evaluate on the *val* set
=> result
* total: 812
* correct: 597
* accuracy: 73.5%
* error: 26.5%
* macro_f1: 72.8%
epoch [27/30] batch [20/392] time 0.326 (0.350) data 0.000 (0.053) loss 1.3701 (2.7070) lr 4.3227e-04 eta 0:09:01
epoch [27/30] batch [40/392] time 0.326 (0.320) data 0.000 (0.027) loss 0.6499 (2.5016) lr 4.3227e-04 eta 0:08:09
epoch [27/30] batch [60/392] time 0.279 (0.312) data 0.000 (0.018) loss 1.4961 (2.3883) lr 4.3227e-04 eta 0:07:51
epoch [27/30] batch [80/392] time 0.331 (0.307) data 0.000 (0.013) loss 1.6602 (2.2615) lr 4.3227e-04 eta 0:07:36
epoch [27/30] batch [100/392] time 0.291 (0.304) data 0.000 (0.011) loss 1.6211 (2.2022) lr 4.3227e-04 eta 0:07:26
epoch [27/30] batch [120/392] time 0.319 (0.302) data 0.000 (0.009) loss 1.8740 (2.1691) lr 4.3227e-04 eta 0:07:17
epoch [27/30] batch [140/392] time 0.297 (0.301) data 0.000 (0.008) loss 1.5400 (2.1802) lr 4.3227e-04 eta 0:07:09
epoch [27/30] batch [160/392] time 0.299 (0.299) data 0.000 (0.007) loss 2.4746 (2.1432) lr 4.3227e-04 eta 0:07:01
epoch [27/30] batch [180/392] time 0.277 (0.298) data 0.000 (0.006) loss 2.1914 (2.1505) lr 4.3227e-04 eta 0:06:53
epoch [27/30] batch [200/392] time 0.311 (0.298) data 0.000 (0.006) loss 3.2344 (2.1736) lr 4.3227e-04 eta 0:06:47
epoch [27/30] batch [220/392] time 0.291 (0.297) data 0.000 (0.005) loss 0.9048 (2.1591) lr 4.3227e-04 eta 0:06:40
epoch [27/30] batch [240/392] time 0.281 (0.296) data 0.000 (0.005) loss 5.4961 (2.1679) lr 4.3227e-04 eta 0:06:33
epoch [27/30] batch [260/392] time 0.284 (0.296) data 0.000 (0.004) loss 1.6885 (2.2036) lr 4.3227e-04 eta 0:06:27
epoch [27/30] batch [280/392] time 0.295 (0.296) data 0.000 (0.004) loss 1.3340 (2.1986) lr 4.3227e-04 eta 0:06:20
epoch [27/30] batch [300/392] time 0.284 (0.295) data 0.000 (0.004) loss 0.8506 (2.2248) lr 4.3227e-04 eta 0:06:14
epoch [27/30] batch [320/392] time 0.296 (0.295) data 0.000 (0.004) loss 2.7930 (2.2319) lr 4.3227e-04 eta 0:06:07
epoch [27/30] batch [340/392] time 0.278 (0.294) data 0.000 (0.003) loss 1.3564 (2.2344) lr 4.3227e-04 eta 0:06:01
epoch [27/30] batch [360/392] time 0.326 (0.294) data 0.000 (0.003) loss 0.6357 (2.2586) lr 4.3227e-04 eta 0:05:55
epoch [27/30] batch [380/392] time 0.271 (0.293) data 0.000 (0.003) loss 2.1465 (2.2752) lr 4.3227e-04 eta 0:05:48
Evaluate on the *val* set
=> result
* total: 812
* correct: 603
* accuracy: 74.3%
* error: 25.7%
* macro_f1: 73.6%
epoch [28/30] batch [20/392] time 0.280 (0.355) data 0.000 (0.051) loss 2.3789 (2.5174) lr 2.4472e-04 eta 0:06:50
epoch [28/30] batch [40/392] time 0.278 (0.323) data 0.000 (0.026) loss 1.4434 (2.4228) lr 2.4472e-04 eta 0:06:07
epoch [28/30] batch [60/392] time 0.331 (0.313) data 0.000 (0.017) loss 1.2061 (2.4455) lr 2.4472e-04 eta 0:05:49
epoch [28/30] batch [80/392] time 0.287 (0.306) data 0.000 (0.013) loss 0.3701 (2.3267) lr 2.4472e-04 eta 0:05:35
epoch [28/30] batch [100/392] time 0.283 (0.304) data 0.000 (0.010) loss 3.7754 (2.2402) lr 2.4472e-04 eta 0:05:26
epoch [28/30] batch [120/392] time 0.277 (0.302) data 0.000 (0.009) loss 2.4199 (2.1679) lr 2.4472e-04 eta 0:05:19
epoch [28/30] batch [140/392] time 0.285 (0.300) data 0.000 (0.008) loss 4.1836 (2.2454) lr 2.4472e-04 eta 0:05:10
epoch [28/30] batch [160/392] time 0.279 (0.299) data 0.000 (0.007) loss 4.8711 (2.2246) lr 2.4472e-04 eta 0:05:03
epoch [28/30] batch [180/392] time 0.305 (0.299) data 0.000 (0.006) loss 0.8813 (2.1691) lr 2.4472e-04 eta 0:04:57
epoch [28/30] batch [200/392] time 0.282 (0.298) data 0.000 (0.005) loss 2.5742 (2.1695) lr 2.4472e-04 eta 0:04:51
epoch [28/30] batch [220/392] time 0.314 (0.297) data 0.000 (0.005) loss 1.3818 (2.1745) lr 2.4472e-04 eta 0:04:44
epoch [28/30] batch [240/392] time 0.296 (0.297) data 0.000 (0.005) loss 1.0322 (2.2118) lr 2.4472e-04 eta 0:04:37
epoch [28/30] batch [260/392] time 0.290 (0.296) data 0.000 (0.004) loss 0.5903 (2.2179) lr 2.4472e-04 eta 0:04:31
epoch [28/30] batch [280/392] time 0.290 (0.296) data 0.000 (0.004) loss 1.8223 (2.2810) lr 2.4472e-04 eta 0:04:25
epoch [28/30] batch [300/392] time 0.280 (0.296) data 0.000 (0.004) loss 2.7793 (2.3076) lr 2.4472e-04 eta 0:04:19
epoch [28/30] batch [320/392] time 0.287 (0.295) data 0.000 (0.003) loss 0.9268 (2.3172) lr 2.4472e-04 eta 0:04:12
epoch [28/30] batch [340/392] time 0.296 (0.295) data 0.000 (0.003) loss 2.3633 (2.2978) lr 2.4472e-04 eta 0:04:06
epoch [28/30] batch [360/392] time 0.277 (0.295) data 0.000 (0.003) loss 3.5352 (2.2857) lr 2.4472e-04 eta 0:04:00
epoch [28/30] batch [380/392] time 0.270 (0.294) data 0.000 (0.003) loss 2.1230 (2.2984) lr 2.4472e-04 eta 0:03:53
Evaluate on the *val* set
=> result
* total: 812
* correct: 602
* accuracy: 74.1%
* error: 25.9%
* macro_f1: 73.3%
epoch [29/30] batch [20/392] time 0.280 (0.347) data 0.000 (0.051) loss 3.2441 (2.4802) lr 1.0926e-04 eta 0:04:25
epoch [29/30] batch [40/392] time 0.344 (0.318) data 0.000 (0.026) loss 3.1953 (2.3589) lr 1.0926e-04 eta 0:03:56
epoch [29/30] batch [60/392] time 0.280 (0.306) data 0.000 (0.017) loss 1.9326 (2.5003) lr 1.0926e-04 eta 0:03:41
epoch [29/30] batch [80/392] time 0.298 (0.303) data 0.000 (0.013) loss 3.0273 (2.4273) lr 1.0926e-04 eta 0:03:33
epoch [29/30] batch [100/392] time 0.297 (0.300) data 0.000 (0.010) loss 2.1523 (2.4598) lr 1.0926e-04 eta 0:03:24
epoch [29/30] batch [120/392] time 0.288 (0.298) data 0.000 (0.009) loss 2.2656 (2.4107) lr 1.0926e-04 eta 0:03:17
epoch [29/30] batch [140/392] time 0.289 (0.297) data 0.000 (0.008) loss 4.0195 (2.4041) lr 1.0926e-04 eta 0:03:11
epoch [29/30] batch [160/392] time 0.297 (0.296) data 0.000 (0.007) loss 2.1328 (2.3367) lr 1.0926e-04 eta 0:03:04
epoch [29/30] batch [180/392] time 0.281 (0.296) data 0.000 (0.006) loss 1.5059 (2.3509) lr 1.0926e-04 eta 0:02:58
epoch [29/30] batch [200/392] time 0.285 (0.296) data 0.000 (0.005) loss 2.2305 (2.3296) lr 1.0926e-04 eta 0:02:52
epoch [29/30] batch [220/392] time 0.295 (0.295) data 0.000 (0.005) loss 2.2773 (2.3042) lr 1.0926e-04 eta 0:02:46
epoch [29/30] batch [240/392] time 0.290 (0.295) data 0.000 (0.004) loss 2.2734 (2.2592) lr 1.0926e-04 eta 0:02:40
epoch [29/30] batch [260/392] time 0.283 (0.294) data 0.000 (0.004) loss 3.6445 (2.2685) lr 1.0926e-04 eta 0:02:34
epoch [29/30] batch [280/392] time 0.286 (0.294) data 0.000 (0.004) loss 2.2480 (2.2479) lr 1.0926e-04 eta 0:02:28
epoch [29/30] batch [300/392] time 0.279 (0.294) data 0.000 (0.004) loss 0.7358 (2.2205) lr 1.0926e-04 eta 0:02:22
epoch [29/30] batch [320/392] time 0.321 (0.294) data 0.000 (0.003) loss 2.4453 (2.2281) lr 1.0926e-04 eta 0:02:16
epoch [29/30] batch [340/392] time 0.328 (0.295) data 0.000 (0.003) loss 1.2910 (2.2405) lr 1.0926e-04 eta 0:02:10
epoch [29/30] batch [360/392] time 0.301 (0.294) data 0.000 (0.003) loss 1.5830 (2.2569) lr 1.0926e-04 eta 0:02:04
epoch [29/30] batch [380/392] time 0.270 (0.293) data 0.000 (0.003) loss 3.1504 (2.2792) lr 1.0926e-04 eta 0:01:58
Evaluate on the *val* set
=> result
* total: 812
* correct: 602
* accuracy: 74.1%
* error: 25.9%
* macro_f1: 73.3%
epoch [30/30] batch [20/392] time 0.286 (0.352) data 0.000 (0.050) loss 1.6689 (2.0952) lr 2.7391e-05 eta 0:02:10
epoch [30/30] batch [40/392] time 0.282 (0.322) data 0.000 (0.025) loss 3.5312 (2.1810) lr 2.7391e-05 eta 0:01:53
epoch [30/30] batch [60/392] time 0.282 (0.311) data 0.000 (0.017) loss 0.9600 (2.2057) lr 2.7391e-05 eta 0:01:43
epoch [30/30] batch [80/392] time 0.299 (0.306) data 0.000 (0.013) loss 1.3516 (2.2838) lr 2.7391e-05 eta 0:01:35
epoch [30/30] batch [100/392] time 0.281 (0.304) data 0.000 (0.010) loss 1.2158 (2.3847) lr 2.7391e-05 eta 0:01:28
epoch [30/30] batch [120/392] time 0.283 (0.301) data 0.000 (0.008) loss 0.6465 (2.3200) lr 2.7391e-05 eta 0:01:21
epoch [30/30] batch [140/392] time 0.293 (0.299) data 0.000 (0.007) loss 3.1816 (2.2993) lr 2.7391e-05 eta 0:01:15
epoch [30/30] batch [160/392] time 0.286 (0.298) data 0.000 (0.006) loss 3.8203 (2.2745) lr 2.7391e-05 eta 0:01:09
epoch [30/30] batch [180/392] time 0.286 (0.297) data 0.000 (0.006) loss 3.0801 (2.2951) lr 2.7391e-05 eta 0:01:03
epoch [30/30] batch [200/392] time 0.320 (0.297) data 0.000 (0.005) loss 5.0938 (2.2934) lr 2.7391e-05 eta 0:00:56
epoch [30/30] batch [220/392] time 0.300 (0.297) data 0.000 (0.005) loss 0.2742 (2.2565) lr 2.7391e-05 eta 0:00:51
epoch [30/30] batch [240/392] time 0.283 (0.296) data 0.000 (0.004) loss 0.8628 (2.2767) lr 2.7391e-05 eta 0:00:45
epoch [30/30] batch [260/392] time 0.346 (0.296) data 0.000 (0.004) loss 2.0508 (2.2873) lr 2.7391e-05 eta 0:00:39
epoch [30/30] batch [280/392] time 0.286 (0.296) data 0.000 (0.004) loss 1.9893 (2.2716) lr 2.7391e-05 eta 0:00:33
epoch [30/30] batch [300/392] time 0.284 (0.296) data 0.000 (0.004) loss 1.5889 (2.2737) lr 2.7391e-05 eta 0:00:27
epoch [30/30] batch [320/392] time 0.288 (0.295) data 0.000 (0.003) loss 1.6924 (2.2923) lr 2.7391e-05 eta 0:00:21
epoch [30/30] batch [340/392] time 0.283 (0.295) data 0.000 (0.003) loss 2.1465 (2.3039) lr 2.7391e-05 eta 0:00:15
epoch [30/30] batch [360/392] time 0.299 (0.294) data 0.000 (0.003) loss 2.8730 (2.3007) lr 2.7391e-05 eta 0:00:09
epoch [30/30] batch [380/392] time 0.270 (0.293) data 0.000 (0.003) loss 1.7139 (2.2799) lr 2.7391e-05 eta 0:00:03
Evaluate on the *val* set
=> result
* total: 812
* correct: 602
* accuracy: 74.1%
* error: 25.9%
* macro_f1: 73.3%
Checkpoint saved to output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model.pth.tar-30
Finish training
Deploy the model with the best val performance
Loading weights to prompt_learner from "output/rpo_prime/base2new/train_base/stanford_cars/shots_16/RPO_prime_sdl/main_tmp1_0.1sdl/seed2/prompt_learner/model-best.pth.tar" (epoch = 25)
Evaluate on the *test* set
=> result
* total: 4,002
* correct: 3,040
* accuracy: 76.0%
* error: 24.0%
* macro_f1: 75.3%
Elapsed: 1:00:06
